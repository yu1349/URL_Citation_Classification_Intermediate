[
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2108\nCitation Context: At a higher level of granularity, Tan et al. (2015) [URL_CITE] analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia).\nFootnote or Reference text (if exists): Luchen Tan, Haotian Zhang, Charles LA Clarke, and Mark D Smucker. 2015. Lexical compari-son between Wikipedia and Twitter corpora by us-ing word embeddings. In Assocation for Computa-tional Linguistics (ACL). pages 657\u2013661. http: //www.aclweb.org/anthology/P15-2108.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\nCitation Context: We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fast- Text embeddings (Grave et al., 2018) [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/\nCitation Context: An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [URL_CITE] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time.\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ictclas.org/\nCitation Context: (2012b). Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) [URL_CITE] with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees.\nFootnote or Reference text (if exists): 1 http://ictclas.org/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: In this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [URL_CITE] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to han-dle large-scale resources such as Wikipedia (Chen et al., 2017).\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 3: Stability of both word2vec and GloVe as properties of the starting word position in the training data of the PTB. Stability is measured across ten ran-domized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Boxes with more than 0.02% of the total vocabulary mass are outlined.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['5 Lessons Learned: What Contributes to the Stability of an Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ibi-square.jp/index.htm\nCitation Context: We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical time-series data, which were collected from IBI-Square Stocks [URL_CITE] , and 7,351 descriptions as market com-ments, which are written in Japanese and provided by Nikkei QUICK News. We divided the dataset into three parts: 5,880 for training, 730 for val-idation, and 741 for testing. For a human eval-uation, we randomly selected 100 comments and their time-series data included in the test set.\nFootnote or Reference text (if exists): 1 http://www.ibi-square.jp/index.htm\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [URL_CITE] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.\nFootnote or Reference text (if exists): 2 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ihtsdo.org/snomed-ct/\nCitation Context: After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) and SNOMED CT (Url5, 2013) [URL_CITE] , MeSH and SNOMED CT descriptors.\nFootnote or Reference text (if exists): Url5. 2013. Snomed ct: Snomed clinical terms (http://www.ihtsdo.org/snomed-ct/) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn/tree/master/data\nCitation Context: (2019). [URL_CITE] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets.\nFootnote or Reference text (if exists): 10 https://github.com/sameenmaruf/selective-attn/tree/master/data\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: In this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [URL_CITE] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to han-dle large-scale resources such as Wikipedia (Chen et al., 2017).\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politifact.com/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, [URL_CITE] FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 3 http://www.politifact.com/ 4 http://www.factcheck.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [URL_CITE] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 7 https://www.reddit.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/prdwb/bert_hae\nCitation Context: To evaluate the baseline performance (semi-automatic), we train BERT-HAE and BERT-PHAE on QuAC using the protocol described by the au-thors (Qu et al., 2019a) and the same hyperparame-ters: history markers from up to 6 turns, and spe-cific optimization parameters (12 as batch size, 3e- 5 as learning rate with a linear decrease to 0 over 24k training steps). We implement our own train-ing script on the basis of codes pieces from the transformers library (Wolf et al., 2019) and BERT-HAE\u2019s authors [URL_CITE] . Experiments are run with a Nvidia Tesla V100 GPU.\nFootnote or Reference text (if exists): 1 https://github.com/prdwb/bert_hae\nSection Titles (if exists):  ['4 A more Robust Protocol', '4.1 Reproducing the Regular Evaluation in the Semi-automatic Scenario']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cosyne.h-its.org/vex\nCitation Context: VEX is available as free, open-source soft-ware for download at http://github.com/noutenki/vex and as a web service at [URL_CITE] http://cosyne.h-its.org/vex.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website and Tool)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit.ai/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai [URL_CITE] , snips.ai , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 5 https://wit.ai/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Friends\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends [URL_CITE] and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Friends\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://syntagnet.org/\nCitation Context: SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at [URL_CITE] http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/glampouras/JLOLS_NLG\nCitation Context: \u2022 LOLS : [URL_CITE] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.\nFootnote or Reference text (if exists): 4 https://github.com/glampouras/JLOLS_ NLG\nSection Titles (if exists):  ['2 End-to-End NLG Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sfu.ca/~mtaboada\nCitation Context: This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [URL_CITE] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7\nFootnote or Reference text (if exists): 6 https://www.sfu.ca/\u02dcmtaboada\nSection Titles (if exists):  ['5 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://visagetechnologies.com/products-and-services/visagesdk/facetrack\nCitation Context: To extract face and head movement information from the video, a face-tracker (Visage, 2016) [URL_CITE] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation.\nFootnote or Reference text (if exists): Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.\nSection Titles (if exists):  ['2 Method', '2.1 Dataset and Feature Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/higher\nCitation Context: In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [URL_CITE] (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ higher\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pltrdy/files2rouge\nCitation Context: Actors say satirical Western\u2019s script is insulting to Native Americans and women . used files2rouge [URL_CITE] . Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT-GEN+COV (See et al., 2017), B ERT S UM E XT A BS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al.\nFootnote or Reference text (if exists): 3 files2rouge usage : https://github.com/pltrdy/files2rouge\nSection Titles (if exists):  ['4 Experiments', '[Gold Summary]']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Tom556/OrthogonalTransformerProbing\nCitation Context: This approach assumes that the representations are isomorphic across lan-guages; we examine this claim in our experiments. Our implementation is available at GitHub: [URL_CITE] https://github.com/Tom556/OrthogonalTransformerProbing.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [URL_CITE] This cor-pus has 75k training reviews and 25k test reviews.\nFootnote or Reference text (if exists): 1 http://ai.stanford.edu/\u02dcamaas/data/sentiment/\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geniaproject.org/genia-corpus/treebank\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [URL_CITE] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 3 http://www.geniaproject.org/genia-corpus/treebank\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/wav2vec\nCitation Context: We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from [URL_CITE] https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://jsoup.org/\nCitation Context: Text Collection: Using jsoup (Hedley, 2017) [URL_CITE] , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath.\nFootnote or Reference text (if exists): Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.\nSection Titles (if exists):  ['3 Approach', '3.1 Domain-Independent Approach (DI)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nCitation Context: NeuronBlocks provides convenient user interface [URL_CITE] for users to build, train, and test DNN models. The details are described in the following.\nFootnote or Reference text (if exists): 4 https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nSection Titles (if exists):  ['3 Design', '3.3 User Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/CRF-NER.shtml\nCitation Context: To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [URL_CITE] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types. We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to .\nFootnote or Reference text (if exists): 5 We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it.\nSection Titles (if exists):  ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikivs.com/wiki/MainPage\nCitation Context: Both of these output purely structured attribute-value information and are unable to compare along more qualitative and de-scriptive dimensions such as ease of living or qual-ity of nightlife when comparing cities, for exam-ple. Other websites such as WikiVS [URL_CITE] contain user-contributed comparisons that have been categorized based on the nature of the entities being compared. These are manually curated and therefore do not scale to the quadratic number of entity pairs.\nFootnote or Reference text (if exists): 1 http://www.wikivs.com/wiki/Main Page\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://visagetechnologies.com/products-and-services/visagesdk/facetrack\nCitation Context: To extract face and head movement information from the video, a face-tracker (Visage, 2016) [URL_CITE] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation.\nFootnote or Reference text (if exists): Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.\nSection Titles (if exists):  ['2 Method', '2.1 Dataset and Feature Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.rd.nacsis.ac.jp/\"ntcadm/,Sep\nCitation Context: for testing. The result of \"short query\" is shown in A number of filters have been considered (ta-page 25 of(Kando et al., 1999) [URL_CITE] , which shows that ble 9). Results vary somewhat depending on these \"short query\" is hard for statistical methods.\nFootnote or Reference text (if exists): Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hi-daka. 1999. Overview of ir tasks at the first nt-cir workshop. In NTCIR Workshop 1, pages 11-44, http://www.rd.nacsis.ac.jp/\"ntcadm/,Sep.\nSection Titles (if exists): [\\'3 Burstiness\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1073083.1073126\nCitation Context: Association for Compu-tational Linguistics, Philadelphia, Pennsylva-nia, USA, pages 255\u2013262. [URL_CITE] https://doi.org/10.3115/1073083.1073126.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asiddhant/Active-NLP\nCitation Context: In this paper, we present a large-scale study [URL_CITE] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions.\nFootnote or Reference text (if exists): 1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/danieliu/play-scraper\nCitation Context: We crawled those websites\u2019 privacy policies in November 2019 since the released privacy policies are outdated. For mobile application privacy policies, we scrape application information from Google Play Store us-ing play-scraper public API [URL_CITE] and crawl their privacy policy. We ended up with 7,500 mobile applications\u2019 privacy policies.\nFootnote or Reference text (if exists): 2 https://github.com/danieliu/ play-scraper\nSection Titles (if exists):  ['2 Construction of PolicyIE Corpus', '2.1 Privacy Policies Selection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.lighttag.io/\nCitation Context: Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [URL_CITE] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.\nFootnote or Reference text (if exists): 4 https://www.lighttag.io/ Demonstrations, pages 99\u2013105\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/C10-1094\nCitation Context: (2009) and Nivre et al. (2010) [URL_CITE] . The Total and Avg columns indicate the percentage of correctly recovered dependencies out of all dependencies and the average of accuracy on the 7 constructions.\nFootnote or Reference text (if exists): Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los Go\u0301mez Rodr\u0131\u0301guez. 2010. Evaluation of depen-dency parsers on unbounded dependencies. In COL-ING. Coling 2010 Organizing Committee, Beijing, China, pages 833\u2013841. http://www.aclweb.org/anthology/C10-1094.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: Our experiments are based on the publicly avail-able word2vec and GloVe [URL_CITE] packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: The algorithm first creates a smaller pool U \u2032 containing unlabeled data from U. It uses L i (i = 1, 2) to train two distinct classifiers: the acoustic classifier h [URL_CITE] , and the lexical classifier h 2 . We use function V i (i = 1, 2) to represent that only a single view is used for training h 1 or h 2 .\nFootnote or Reference text (if exists): 1 LIBSVM \u2013 A Library for Support Vector Machines, loca-tion: http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nSection Titles (if exists):  ['3 Prosodic Model', '3.3 Prosodic Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Experiment Details We adopt the T5 base model from huggingface Transformer library [URL_CITE] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE/tree/repro\nCitation Context: ERNIE baselines. E RNIE [URL_CITE] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER. E RNIE +F UR +E NT is a stronger baseline, which uses the same entity dictionary as ours for entity-level masking and further pre-trains E RNIE on the same raw text as ours.\nFootnote or Reference text (if exists): 5 https://github.com/PaddlePaddle/ERNIE/tree/repro\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/H/H05/H05-1066\nCitation Context: Graph-based parsers (e.g. MST (McDonald et al., 2005) [URL_CITE] ) are trained to directly assign scores to dependency graphs.\nFootnote or Reference text (if exists): Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective depen-dency parsing using spanning tree algorithms. In EMNLP. Association for Computational Lin-guistics, Vancouver, British Columbia, Canada, pages 523\u2013530. http://www.aclweb.org/anthology/H/H05/H05-1066.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE\nCitation Context: \u2022 ERNIE (115M) [URL_CITE] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc.\nFootnote or Reference text (if exists): 3 https://github.com/PaddlePaddle/ERNIE\nSection Titles (if exists):  ['5 Experiments', '5.1 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~ytsvetko/metaphor/datasets.zip\nCitation Context: Tsvetkov et al. (2013) pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs [URL_CITE] and classify them. Gersh-man et al.\nFootnote or Reference text (if exists): 10 http://www.cs.cmu.edu/\u02dcytsvetko/metaphor/datasets.zip\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.4 Metaphor Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/NAACL2019-literary-entities\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at: [URL_CITE] https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2016/task13/index.php\nCitation Context: Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task [URL_CITE] , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016). TAXI harvests hypernyms with substring inclusion and lexical-syntactic patterns by obtaining domain-specific texts via focused web crawling.\nFootnote or Reference text (if exists): 4 http://alt.qcri.org/semeval2016/task13/index.php\nSection Titles (if exists):  ['4 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers [URL_CITE] , Quora and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EdoFrank/emnlp2019_morio_egawa\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/decomp-sem/neural-sprl\nCitation Context: achieves state-of-the-art performance for SPRL. [URL_CITE] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art.\nFootnote or Reference text (if exists): 2 Implementation available at https://github.com/decomp-sem/neural-sprl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2XjkE2T\nCitation Context: To demonstrate the relative effectiveness of our proposed approach, we compared its quality to state-of-the-art alternative baselines. On TREC-COVID, we directly compared against the three best automatic performing systems [URL_CITE] (out of 141 system runs submitted to the Round-1 challenge by 56 different teams), namely: sabir, IRIT markers and unipd.it.\nFootnote or Reference text (if exists): 8 The details of these systems as well as other competing systems are available in https://bit.ly/2XjkE2T\nSection Titles (if exists):  ['4 Evaluation', '4.3 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flan.cs.pitt.edu/~hwa/align/align.html\nCitation Context: In summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags for Chinese. The system is in place and the annotation process is underway. [URL_CITE]\nFootnote or Reference text (if exists): 4 The annotation interface is open to public. Please visit http://flan.cs.pitt.edu/~hwa/align/align.html\nSection Titles (if exists):  ['4 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.urbandictionary.com/\nCitation Context: Considering situations where many slang terms have literal meanings, it may be unfair to re-trieve target terms from such machine translators by solely inputing slang terms without specific contexts. Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban Dictionary [URL_CITE] ). The following example shows how we obtain the target translation terms for the slang word \u201cfruitcake\u201d (an insane person):\nFootnote or Reference text (if exists): 11 http://www.urbandictionary.com/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://babelnovel.com/\nCitation Context: Data collection. We construct our corpus from a professional Chinese novel reading site named Babel Novel [URL_CITE] . Unlike news, the novel dataset cov-ers a mixture of literary style including historical novels, and martial arts novels in the genre of fan-tasy, mystery, romance, military, etc.\nFootnote or Reference text (if exists): 9 https://babelnovel.com/\nSection Titles (if exists):  ['B Details of the Datasets', 'B.2 Novel Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sinantie/NeuralAmr\nCitation Context: We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [URL_CITE] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017).\nFootnote or Reference text (if exists): 1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/NAACL2019-literary-entities\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: https://github.com/dbamman/ litbank; code to support this work can be found at: [URL_CITE] https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doug919/multi_relational_script_learning\nCitation Context: We leave the details about hyperparameter tuning in the appendix. The source code and pre-trained models are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/doug919/multi_ relational_script_learning\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Other (The URL citation is used to provide additional resources but not explicitly used or produced in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:Administrators\nCitation Context: Administra-tors (admins) are editors who have been granted certain rights, including the ability to block other editors and to protect or delete articles. [URL_CITE] Ad-mins have a higher status than common editors (non-admins), and this distinction seems to be widely acknowledged by the community (Burke and Kraut, 2008b; Leskovec et al., 2010; Danescu-Niculescu-Mizil et al., 2012). Aspiring editors become admins through public elections, so we know when the status change from non-admin to admins occurred and can study users\u2019 language use in relation to that time.\nFootnote or Reference text (if exists): 7 http://en.wikipedia.org/wiki/ Wikipedia:Administrators\nSection Titles (if exists):  ['5 Relation to social factors', '5.1 Relation to social outcome']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://help.twitter.com/en/rules-and-policies/twitter-limits\nCitation Context: User tweets are simply all the tweets from that user\u2019s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [URL_CITE] https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day (https://business.twitter.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NVIDIA/nv-wavenet\nCitation Context: Our full synthesis model consists of a bidirectional LSTM of 512 dimen-sions, a linear projection down to 128 dimensions, and finally the WaveNet decoder which generates samples at 16 kHz. We use a WaveNet implementa-tion from NVIDIA [URL_CITE] which provides efficient GPU inference. WaveNet hyperparameters can be found in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/NVIDIA/nv-wavenet\nSection Titles (if exists):  ['\u03b4 CCA [i, j] = P S E 0S [i] \u2212 P V E V0 [j]', '3.3 WaveNet Synthesis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Experiment Details We adopt the T5 base model from huggingface Transformer library [URL_CITE] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Victor0118/cross_domain_embedding/\nCitation Context: Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code and left unchanged in RegFreq [URL_CITE] (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly.\nFootnote or Reference text (if exists): 8 https://github.com/Victor0118/cross_domain_embedding/\nSection Titles (if exists):  ['Topics and tasks', '4.2 Epochs vs. regularization results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/mteval.shtml\nCitation Context: M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [URL_CITE]\nFootnote or Reference text (if exists): 3 Software for R TE R and M T +R TE R is available from http://nlp.stanford.edu/software/mteval.shtml.\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.4 Combination Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/Word2Vec/\nCitation Context: Word2Vec: use pre-trained Google word vectors. These were trained using Word2Vec tool [URL_CITE] on the Google News corpus. The vocabulary size for Word2Vec is 3,000,000.\nFootnote or Reference text (if exists): 8 https://code.google.com/archive/p/Word2Vec/\nSection Titles (if exists):  ['5 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cond.org/schbase.html\nCitation Context: Finally, we illustrate how authors build upon each others\u2019 terminology over time to create new keyphrases. [URL_CITE]\nFootnote or Reference text (if exists): 1 Full database available at: http://cond.org/schbase.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/E99-1026\nCitation Context: H EAD F INAL Among SOV languages, Japanese is known as a strictly head final language, mean-ing that the head of every word always follows it. Japanese dependency parsing (Uchimoto et al., 1999 [URL_CITE] ; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-to-right dependency arcs. Inspired by this tradition, we try a simple H EAD F INAL rule in Japanese CCG parsing, in which we always select the right argument as the head.\nFootnote or Reference text (if exists): Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Ninth Conference of the European Chapter of the Association for Computational Linguistics. http://aclweb.org/anthology/E99-1026.\nSection Titles (if exists):  ['4 CCG to Dependency Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://saifmohammad.com/WebPages/BestWorst.html\nCitation Context: For creation of the best\u2013worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by Kiritchenko and Mohammad (2016). [URL_CITE] We use a best-worst scaling factor of 1.5 and four items per tuple. Thus, each metaphor appears in six differ-ent best-worst scaling comparisons.\nFootnote or Reference text (if exists): 3 http://saifmohammad.com/WebPages/BestWorst.html\nSection Titles (if exists):  ['3 Corpus', '3.2 Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/\nCitation Context: For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector [URL_CITE] , and 100-dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese.\nFootnote or Reference text (if exists): 10 http://www.cl.ecei.tohoku.ac.jp/\u02dcm-suzuki/jawiki_vector/\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clearnlp.com,v2.0.2\nCitation Context: The syntactic and semantic features were extracted using the ClearNLP parser. [URL_CITE] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features.\nFootnote or Reference text (if exists): 4 http://www.clearnlp.com,v2.0.2\nSection Titles (if exists):  ['3 Models for Short Answer Scoring', '3.1 Response-based']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [URL_CITE] , forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org\nCitation Context: To alleviate the data sparsity problem, EB is first trained on a very large corpus [URL_CITE] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R.\nFootnote or Reference text (if exists): 3 Wikipedia(http://www.wikipedia.org) is used in practice.\nSection Titles (if exists):  ['3 The Proposed Method', '3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph', '3.2.1 Learning Word Embedding for']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxjiao/UncSamp\nCitation Context: Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code is available at https://github.com/wxjiao/UncSamp\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html\nCitation Context: http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. ( [URL_CITE] http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL. (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Other (The URL is used to provide additional information or context, but it's not clear how it's used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.open.weibo.com/wiki/2/search/topics\nCitation Context: Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API [URL_CITE] to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.\nFootnote or Reference text (if exists): 7 http://www.open.weibo.com/wiki/2/search/topics\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asiddhant/Active-NLP\nCitation Context: In this paper, we present a large-scale study [URL_CITE] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions.\nFootnote or Reference text (if exists): 1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017 [URL_CITE] ) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ckip.iis.sinica.edu.tw/DOMCAT/\nCitation Context: In this paper, we propose a web-based bilingual concordancer, DOMCAT [URL_CITE] , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents.\nFootnote or Reference text (if exists): 1 http://ckip.iis.sinica.edu.tw/DOMCAT/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering\nCitation Context: Longest Common Subsequence (LCS): It cal-culates the percentage of longest correct sub-sequence between the predicted order and the gold order (Gong et al., 2016). The consecutiveness is not necessary for it [URL_CITE] .\nFootnote or Reference text (if exists): 10 Codes for metric: https://github.com/shrimai/Topological-Sort-for-Sentence-Ordering\nSection Titles (if exists):  ['A Appendix', 'A.2 Two other metrics used in the Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.washington.edu/triviaqa/\nCitation Context: TriviaQA [URL_CITE] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions.\nFootnote or Reference text (if exists): 3 http://nlp.cs.washington.edu/triviaqa/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca-demic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/\nCitation Context: Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia [URL_CITE] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/solr/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR [URL_CITE] for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/solr/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mjpost/sacrebleu\nCitation Context: Evaluation We evaluate our system outputs us-ing two automatic evaluation metrics: detokenized BLEU (Papineni et al., 2002; Post, 2018) and chrF (Popovic\u0301, 2015). Although BLEU is a standard metric for machine translation, being cognizant of the morphological richness of the languages in the A FRO MT benchmark, we use chrF to measure per-formance at a character level. Both metrics are measured using the SacreBLEU library [URL_CITE] (Post, 2018).\nFootnote or Reference text (if exists): 13 https://github.com/mjpost/sacrebleu\nSection Titles (if exists):  ['4 Experimental Setup', '4.2 Finetuning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com\nCitation Context: Once post-edits had been created, DA was em-ployed in two separate runs on Amazon Mechani-cal Turk, [URL_CITE] once for GEN - REF and once for POST - EDIT . Besides employing distinct reference trans-lations in the assessment, all other set-up crite-ria were identical for both evaluation settings, in-cluding the conventional segment-level DA set-ting, where a minimum of 15 human assessments are combined into a mean DA score for a given translation, after strict quality control measures and score standardization have been applied.\nFootnote or Reference text (if exists): 11 https://www.mturk.com\nSection Titles (if exists):  ['3 Alternate Reference Bias Investigation', '3.1 Reference Bias Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/comlex/refman.ps\nCitation Context: Information about noun countability was obtained from two sources: COMLEX 3.0 (Grishman et al., 1998) [URL_CITE] and the common noun part of ALT-J/E \u2019s Japanese-to-English semantic transfer dictio-nary (Ikehara et al., 1991). Of the approximately 22,000 noun entries in COMLEX , 13,622 are marked as countable , 710 as uncountable and the remainder are unmarked for countability. ALT-J/E has 56,245 English noun types with distinct countability.\nFootnote or Reference text (if exists): Ralph Grishman, Catherine Macleod, and Adam Myers, 1998. COMLEX Syntax Reference Manual. Proteus Project, NYU. (http://nlp.cs.nyu.edu/comlex/refman.ps).\nSection Titles (if exists):  ['2 Preliminaries', '2.2 Gold standard data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/uhh-lt/sensegram\nCitation Context: We experiment with three recent techniques that release 300-d pre-trained multi-prototype embeddings . JBT [URL_CITE] (Pelevina et al., 2016) induces different senses by clustering graphs constructed using word embed-dings and computes embedding for each cluster (sense). DeConf 12 (Pilehvar and Collier, 2016) exploits the knowledge encoded in WordNet.\nFootnote or Reference text (if exists): 11 https://github.com/uhh-lt/sensegram\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IsakZhang/Generative-ABSA\nCitation Context: This also validates the strong generality of the pro-posed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design. [URL_CITE]\nFootnote or Reference text (if exists): 1 The data and code can be found at https://github.com/IsakZhang/Generative-ABSA\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhcross/span-parser\nCitation Context: The Penn Treebank dataset and splits are available at [URL_CITE] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.3 Syntactic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/9rs6pa\nCitation Context: (rumor) \u201cRT @johnnyA99 Ann Coulter Tells Larry King Why People Think Obama Is A Muslim [URL_CITE] http://bit.ly/9rs6pa #Hussein via @NewsBusters #tcot ..\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/raylin1000/dropbert\nCitation Context: Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koc\u030cisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 2.0 (Rajpurkar et al., 2018). We use the NABERT [URL_CITE] (Numerically Augmented BERT) model with an additional reasoning type to allow \u201cNo Answer\u201d as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 \u201cNo Answer\u201d questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch.\nFootnote or Reference text (if exists): 3 https://github.com/raylin1000/drop bert\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flask.pocoo.org\nCitation Context: This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [URL_CITE] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters.\nFootnote or Reference text (if exists): 4 http://flask.pocoo.org\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.byronwallace.com/EBM_abstracts_data\nCitation Context: There are 18,849 (831), 44,329 (1,808), 41,454 (1,711) variable length sequences for P , I , and O in the training (testing) data. [URL_CITE]\nFootnote or Reference text (if exists): 1 The complete details of the corpus, along with inter-annotator analysis and links for download of the full cor-pus will be described in a forthcoming paper and eventually made available here: http://www.byronwallace.com/EBM_abstracts_data.\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pan.webis.de/clef16/pan16-web/author-profiling.html\nCitation Context: Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset [URL_CITE] ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset ) is to predict the number of stars the user assigned to by analysis their reviews.\nFootnote or Reference text (if exists): 1 https://pan.webis.de/clef16/pan16-web/author-profiling.html\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/tree/master/research/object_detection\nCitation Context: Out-of-vocabulary words are initialized with zero vectors. We adopt the \u201cssd resnet 50 fpn coco\u201d model from Tensorflow detection model zoo [URL_CITE] as the object detection mod-ule. The size of hidden states in the LSTM module is set to 200, and the size of the projected embed-ding for computing similarity between text spans and object labels is 512.\nFootnote or Reference text (if exists): 5 https://github.com/tensorflow/models/tree/master/research/object_detection\nSection Titles (if exists):  ['5 The Experiment', '5.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jwyang/faster-rcnn.pytorch\nCitation Context: Using the novel labeled dataset described in Sec-tion 4, a baseline model was trained using a stan-dard Faster R-CNN implementation (Yang et al., 2017) [URL_CITE] . The model was trained using a single NVIDIA P100 GPU for 30 epochs on 600 images, and tested on the remaining 222 in 5 randomized sessions, using a ResNet-101 base network pre-trained on ImageNet (Russakovsky et al., 2015), with a batch size of 8, Adam optimizer (Kingma and Ba, 2014), and a starting learning rate of 0.0001, with decay of 0.1 every 5 epochs.\nFootnote or Reference text (if exists): Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh. 2017. A faster pytorch implementation of faster r-cnn. https://github.com/jwyang/faster-rcnn.pytorch.\nSection Titles (if exists):  ['5 Implementation and Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sjmielke.com/papers/tokenize/\nCitation Context: Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [URL_CITE] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively.\nFootnote or Reference text (if exists): 31 http://sjmielke.com/papers/tokenize/\nSection Titles (if exists):  ['D Data selection: Europarl']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mounicam/lexical_simplification\nCitation Context: We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. [URL_CITE] Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus was effective for simplifica-tion purposes (see experiments in \u00a74) as our neural ranking model (\u00a73) can estimate the complexity of any word or phrase even out-of-vocabulary.\nFootnote or Reference text (if exists): 3 Download at https://github.com/mounicam/ lexical_simplification\nSection Titles (if exists):  ['2 Constructing A Word-Complexity Lexicon with Human Judgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mecab/files/\nCitation Context: Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [URL_CITE] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task.\nFootnote or Reference text (if exists): 4 http://sourceforge.net/projects/mecab/files/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.twitter.com/en/docs.html\nCitation Context: \u2022 Negative examples: We have col-lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API ( [URL_CITE] https://developer.twitter.com/en/docs.html) to use as negative examples.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: More closely related is our prior work (Vyas and Carpuat, 2016) [URL_CITE] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (\u00a76.1), and show that the improvements hold even when exposed to data scarce settings (\u00a76.3).\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3\nCitation Context: To make the disambiguation task more challenging, we also ensure that at least one other distinct personal name is present in the text in a position before the masked occurrence. We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [URL_CITE] . We show its value by using it to fine-tune the B ERT language model (Devlin et al., 2018) for pronoun resolution.\nFootnote or Reference text (if exists): 1 The code can be found at https://github.com/vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: c83e94bb-7584-41a1-aef9-85b0e764d9e3\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gordonscruton.blogspot.in/2011/08/\nCitation Context: Table 1: Examples of coherence and cohesion [URL_CITE] .\nFootnote or Reference text (if exists): 2 We took the examples from this site for explaining coherence and cohesion: http: //gordonscruton.blogspot.in/2011/08/ what-is-cohesion-coherence-cambridge. html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://res.qyzhou.me/redistribute.zip\nCitation Context: The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) [URL_CITE] and (Du et al., 2017) . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%).\nFootnote or Reference text (if exists): 2 https://res.qyzhou.me/redistribute.zip\nSection Titles (if exists):  ['3 Experimental Setting', '3.1 Dataset & Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [URL_CITE] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sfu.ca/~mtaboada\nCitation Context: This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [URL_CITE] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7\nFootnote or Reference text (if exists): 6 https://www.sfu.ca/\u02dcmtaboada\nSection Titles (if exists):  ['5 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/nlp/dpo3/\nCitation Context: 3. A free distribution of our implementation. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://groups.csail.mit.edu/nlp/dpo3/\nSection Titles (if exists):  ['3. A free distribution of our implementation. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/10K\nCitation Context: In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. [URL_CITE] We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report (v (\u221212) ) and the twelve months after the report (v (+12) ).\nFootnote or Reference text (if exists): 4 The text and volatility data are publicly available at http: //www.ark.cs.cmu.edu/10K.\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/cabocha/\nCitation Context: For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [URL_CITE] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai).\nFootnote or Reference text (if exists): 11 http://taku910.github.io/cabocha/\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.opendata.aws/amazon-reviews-ml/readme.html\nCitation Context: The Multilingual Amazon Reviews Corpus (MARC) can be found at https://registry.opendata.aws/amazon-reviews-ml/. The dataset description, code snippets, and li-cense agreement can be retrieved at [URL_CITE] https: //docs.opendata.aws/amazon-reviews-ml/ readme.html .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/dynet\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [URL_CITE] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 5 https://github.com/clab/dynet\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [URL_CITE] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/zrRIBxMUYJ\nCitation Context: Tweet Officer Wilson will be returned to active duty if no indictment, says #Ferguson Police Chief [URL_CITE] http://t.co/zrRIBxMUYJ\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/tkbc\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [URL_CITE] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 6 https://github.com/facebookresearch/tkbc\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://math-qa.github.io/math-QA/\nCitation Context: In this paper, we introduce a new operation-based representation language for solving math word problems. We use this representation lan-guage to construct MathQA [URL_CITE] , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math do-main categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neu-ral model for mapping problems to operation pro-grams with domain categorization.\nFootnote or Reference text (if exists): 1 The dataset is available at: https://math-qa.github.io/math-QA/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2011/03/numbers.html\nCitation Context: Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! [URL_CITE] Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in.\nFootnote or Reference text (if exists): 2 In fact, the peak record is 6,939 tweets per second, reported by http://blog.twitter.com/2011/03/numbers.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/\nCitation Context: To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [URL_CITE] (Kipper et al., 2004; Kipper et al., 2008).\nFootnote or Reference text (if exists): 4 http://verbs.colorado.edu/verb-index/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csc.fi/kielipankki/\nCitation Context: Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [URL_CITE] . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words.\nFootnote or Reference text (if exists): CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.2 Finnish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/tpu\nCitation Context: Using the proposed T IME D IAL challenge set, we next conduct extensive experiments and analyses on the different model variants and context settings. We use either 4x4 or 8x8 Cloud TPUs V3 pod slices [URL_CITE] for fine-tuning and one V100 GPU for infer-ence. We provide more details of the experiment configurations in the appendix.\nFootnote or Reference text (if exists): 5 https://cloud.google.com/tpu\nSection Titles (if exists):  ['5 Experiments and Analyses']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/K542wvSNVz\u2019\nCitation Context: Tweet \u2018#RiggsReport: #CA as the #Election-Night exception. Voters rewarded #GOP nationally, but not in the #GoldenState. [URL_CITE] http://t.co/K542wvSNVz\u2019\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.sourceforge.net/\nCitation Context: We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepo-sitional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and ar-gument head words. We use the OpenNLP [URL_CITE] coref-erence engine to resolve entity mentions.\nFootnote or Reference text (if exists): 1 http://opennlp.sourceforge.net/\nSection Titles (if exists):  ['6 Evaluation: Cloze', '6.2 Training and Test Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017\nCitation Context: Also, we benchmark the intent detection perfor-mance of the proposed model with existing natu-ral language understanding services [URL_CITE] in Figure 3.\nFootnote or Reference text (if exists): 9 https://www.slideshare.net/KonstantinSavenkov/nlu- intent-detection-benchmark-by-intento-august-2017\nSection Titles (if exists):  ['4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/mKqvro\nCitation Context: We show that our proposed regularizers are faster than the state-of-the-art ones and still improve text classification accuracy. Code and data are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://goo.gl/mKqvro\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We take the same character un-igram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword [URL_CITE] . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018).\nFootnote or Reference text (if exists): 3 https://catalog.ldc.upenn.edu/LDC2011T13.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/birch\nCitation Context: The final score used by Birch for re-reranking is: \u03b3\u00b7S BM25 +(1\u2212\u03b3)\u00b7P i \u03ba i \u00b7S(x q , x i ) where S BM25 is the baseline BM25 score and \u03b3, \u03ba i are weight coefficients. We use the published mod-els [URL_CITE] and coefficient values for evaluation.\nFootnote or Reference text (if exists): 3 https://github.com/castorini/birch\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://commoncrawl.org/2016/10/newsdatasetavailable\nCitation Context: 5 Similar to RoBERTa, BART uses the combination of BookCorpus (Zhu et al., 2015), CC-News (Nagel, 2016) [URL_CITE] , OpenWebText (Gokaslan and Cohen, 2019), and Stories (Trinh and Le, 2018) as pre-training corpus.\nFootnote or Reference text (if exists): Sebastian Nagel. 2016. Cc-news. URL: http://web.archive. org/save/http://commoncrawl.org/2016/10/newsdatasetavailable.\nSection Titles (if exists):  ['mixture of corpus 5 used to pre-train BART.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [URL_CITE] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).\nFootnote or Reference text (if exists): 2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).\nSection Titles (if exists):  ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincare\u0301 Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rkadlec/ubuntu-ranking-dataset-creator\nCitation Context: We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus [URL_CITE] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012.\nFootnote or Reference text (if exists): 1 https://github.com/rkadlec/ubuntu-ranking-dataset- creator\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org\nCitation Context: At QCRI, he leads the Tanbih project, [URL_CITE] devel-oped in collaboration with MIT, which aims to limit the effect of \u201cfake news\u201d, propaganda and media bias by making users aware of what they are reading. The project was featured by over 100 news outlets, including Forbes, Boston Globe, Al-jazeera, MIT Technology Review, Science Daily, Popular Science, Fast Company, The Register, WIRED, and Engadget, among others.\nFootnote or Reference text (if exists): 2 Tanbih project: http://tanbih.qcri.org\nSection Titles (if exists):  ['6 Lecturers', '6.1 Preslav Nakov']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~ozaidan/AOC\nCitation Context: We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [URL_CITE] This corpus is highly dialectal\nFootnote or Reference text (if exists): 1 Available from http://www.cs.jhu.edu/~ozaidan/AOC\nSection Titles (if exists):  ['3.2. Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seatgeek/fuzzywuzzy\nCitation Context: Figure 7: Average number of citations per paper that are 15 years or older, by venue of publication. tance ratio [URL_CITE] is \u2264 95%.\nFootnote or Reference text (if exists): 12 As implemented by https://github.com/seatgeek/fuzzywuzzy.\nSection Titles (if exists):  ['A Fuzzy paper matching']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: In prior work (Huenerfauth and Kacorri, 2015b), we investigated key methodological con-siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc-tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work (Kacorri et al., 2015) [URL_CITE] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation. Specifically, we screened for par-ticipants that identified themselves as \u201cdeaf/Deaf\u201d or \u201chard-of-hearing,\u201d who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jkkummerfeld/berkeley-parser-analyser\nCitation Context: 3. The Berkeley Parser Analyser [URL_CITE] (Kummerfeld et al., 2012) to analyze parsing error types.\nFootnote or Reference text (if exists): 5 https://github.com/jkkummerfeld/berkeley-parser-analyser\nSection Titles (if exists):  ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://opennlp.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [URL_CITE] or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 2 https://opennlp.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/resources/\nCitation Context: Table 1 shows the distribution over the stance labels, [URL_CITE] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.\nFootnote or Reference text (if exists): 7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: Figure 1 shows our biaffine parsing ar-chitecture. Following Dozat and Manning (2017) [URL_CITE] and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence. We add highway connections in the same fashion as our supertagging model.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: In this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [URL_CITE] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to han-dle large-scale resources such as Wikipedia (Chen et al., 2017).\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983xxx\nCitation Context: We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [URL_CITE] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).\nFootnote or Reference text (if exists): Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: Our experiments are based on the publicly avail-able word2vec and GloVe [URL_CITE] packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/ace/index.htm\nCitation Context: Templates were used in the main IE competitions, the Message Understanding Con-ferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE, [URL_CITE] uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/ace/index.htm\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship\nCitation Context: Ad-mins have a higher status than common editors (non-admins), and this distinction seems to be widely acknowledged by the community (Burke and Kraut, 2008b; Leskovec et al., 2010; Danescu-Niculescu-Mizil et al., 2012). Aspiring editors become admins through public elections, [URL_CITE] so we know when the status change from non-admin to admins occurred and can study users\u2019 language use in relation to that time.\nFootnote or Reference text (if exists): 8 http://en.wikipedia.org/wiki/ Wikipedia:Requests_for_adminship\nSection Titles (if exists):  ['5 Relation to social factors', '5.1 Relation to social outcome']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [URL_CITE] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 5 http://pytorch.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/natural-language/docs/analyzing-entities\nCitation Context: We analyze four QA tasks: NQ , [URL_CITE] SQuAD (Rajpurkar et al., 2016), QB (Boyd-Graber et al., 2012) and Trivia QA (Joshi et al., 2017). Google CLOUD - NL 2 finds and links entity mentions in QA examples.\nFootnote or Reference text (if exists): 1 For NQ , we only consider questions with short answers. 2https://cloud.google.com/natural-language/docs/ analyzing-entities\nSection Titles (if exists):  ['2 Mapping Questions to Entities']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/noa/naacl2021\nCitation Context: Our code, data splits, and scripts to repro-duce our experiments are available at [URL_CITE] http://github.com/noa/naacl2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/teapot123/JASen\nCitation Context: We propose to first learn hsentiment, aspecti joint topic embeddings in the word embedding space by imposing regularizations to encour-age topic distinctiveness, and then use neural models to generalize the word-level discrim-inative information by pre-training the clas-sifiers with embedding-based predictions and self-training them on unlabeled data. Our com-prehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our code and data are available at https://github.com/teapot123/JASen.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/gender-guesser/\nCitation Context: We analyze our dataset for gender bias. We use the Gender guesser li-brary [URL_CITE] to determine the gender of the candidates. To mimic the analysis of pronoun genders per-formed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we ob-serve the gender of the correct candidates only.\nFootnote or Reference text (if exists): 4 https://pypi.org/project/gender-guesser/\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/1OJGL9Q\nCitation Context: We compare UrduPhone with Soundex and its variants. [URL_CITE] These algorithms are used to group words based on their encoding and then evalu-ated against the gold standard. Table 2 shows the results on Web dataset.\nFootnote or Reference text (if exists): 10 We use NLTK-Trainer\u2019s phonetic library http://bit.ly/1OJGL9Q\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.2 UrduPhone Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/martiansideofthemoon/hurdles-longform-qa\nCitation Context: We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. [URL_CITE]\nFootnote or Reference text (if exists): 1 Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Supplement"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/statnlp-framework\nCitation Context: In this tutorial, we will be discussing how such a wide spectrum of existing structured predic-tion models can all be implemented under a uni-fied framework [URL_CITE] that involves some basic building blocks. Based on such a framework, we show how some seemingly complicated structured prediction models such as a semantic parsing model (Lu et al., 2008; Lu, 2014) can be implemented conve-niently and quickly.\nFootnote or Reference text (if exists): 1 http://statnlp.org/statnlp-framework\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: The statistics of the three corpora are shown in Table 1. The first two corpora were preprocessed by using the python jieba segmenter [URL_CITE] for word seg-mentation and filtering. The third corpus SemEval is in English and can be tokenized by white spaces.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.biendata.xyz/competition/ccks_2019_6/data/\nCitation Context: Dataset We utilize the dataset published in CCKS Chinese KBQA Contest [URL_CITE] for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.\nFootnote or Reference text (if exists): 4 https://www.biendata.xyz/competition/ccks_2019_6/data/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "TYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://time2002.org/\nCitation Context: For the evaluation a corpus of questions containing as many simple as complex questions is required. Due to the fact that question corpora used in TREC (TREC, ) and CLEF (CLEF, ) do not contain com-plex questions, the TERQAS question corpus has been chosen (Radev and Sundheim, 2002; Puste-jovsky, 2002 [URL_CITE] ). It consists of 123 temporal questions.\nFootnote or Reference text (if exists): J. Pustejovsky. 2002. Terqas:time and event recognition for question answering systems. http://time2002.org/.\nSection Titles (if exists):  ['4 Question Decomposition Unit', '4.3.3 Example']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa/\nCitation Context: For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [URL_CITE] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases.\nFootnote or Reference text (if exists): 6 http://www.cs.pitt.edu/mpqa/\nSection Titles (if exists):  ['3 Experimental Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gutenberg.org\nCitation Context: We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [URL_CITE] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align.\nFootnote or Reference text (if exists): 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/\nSection Titles (if exists):  ['2 A Parallel Corpus of Literary Texts', '2.1 Data Selection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pstadler1990/nmt_paper21_appendix\nCitation Context: We created 10 sentences per phe-nomenon, resulting in a total of 585 sentences, ex-amining 49 phenomena organised in 13 categories. The raw test items, as well as the translations eval-uated can be found in our repository [URL_CITE] . The phe-nomena selected for this experiment are a subset of the ones of German\u2192English MT, as described in Macketanz et al.\nFootnote or Reference text (if exists): 1 https://github.com/pstadler1990/nmt_ paper21_appendix\nSection Titles (if exists):  ['4 Experiment setup', '4.1 Test suite setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Dataset)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://twitter.com\nCitation Context: \u2022 Tweets - Twitter [URL_CITE] is a microblogging plat-form which allows users to post statuses of less than 140 characters. We use two collections for sarcasm detection on tweets.\nFootnote or Reference text (if exists): 2 https://twitter.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf\nCitation Context: \u2022 T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003) [URL_CITE] , which implements a memory-based learning algorithm ( IB 1) which pre-dicts the class of a test data point by looking at its distance to all examples from the training data, us-ing some distance metric. In our experiments, we have used the weighted-overlap method, which as-signs weights to all features. timation.\nFootnote or Reference text (if exists): Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILC Technical Report 03-10, Induction of Linguis-tic Knowledge; Tilburg University. Available from http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/nasari/\nCitation Context: Thanks to its use of explicit semantics combined with a novel cluster-based dimensionality reduction and an effective weighting scheme, our repre-sentation attains state-of-the-art performance on multiple datasets in two standard bench-marks: word similarity and sense clustering. We are releasing our vector representations at [URL_CITE] http://lcl.uniroma1.it/nasari/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://amr.isi.edu\nCitation Context: We com-pare rule set sizes of the various systems in Ta-ble 5; initially we improve the rule set by remov-ing numerous overly brittle rules but then succes-sive changes progressively add useful rules. The parser is available for public download and use at [URL_CITE] http://amr.isi.edu.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://luululu.com/tweet/\nCitation Context: We use a \u201csoft\u201d normalization strategy to all of the experimental datasets by translating word to-kens of user mentions and web/url links into spe-cial tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corre-sponding strings. We also apply a \u201chard\u201d strategy by further applying lexical normalization dictio-naries (Aramaki, 2010 [URL_CITE] ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets.\nFootnote or Reference text (if exists): Eiji Aramaki. 2010. TYPO CORPUS. http://luululu.com/tweet/.\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: Rouge-S: This metric (Chen et al., 2016; Gong et al., 2016) measures the fraction of pairs of sen-tences whose predicted relative order is the same as the ground truth order [URL_CITE] . It allows for any arbi-trary gaps between two sentences as long as their relative order is correctly identified.\nFootnote or Reference text (if exists): 11 Code for metric: https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['A Appendix', 'A.2 Two other metrics used in the Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.uni-weimar.de/medien/webis/corpora\nCitation Context: The Webis-Debate-16 corpus will be made freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://www.uni-weimar.de/medien/webis/corpora\nSection Titles (if exists):  ['3 Mining Argumentative Text through Distant Supervision', '3.2 The Webis-Debate-16 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tesseract-ocr/\nCitation Context: These data were originally available in PDF version. We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [URL_CITE] to ex-tract the Cherokee and English text. Then our co-author, a proficient second-language speaker of Cherokee, manually aligned the sentences and fixed the errors introduced by OCR.\nFootnote or Reference text (if exists): 6 https://github.com/tesseract-ocr/\nSection Titles (if exists):  ['3 Data Description', '3.1 Parallel Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fromhuggingface/transformers\nCitation Context: We compute the average cosine similarity contri-bution, CC( f `i ), for each dimension in all layers of BERT, RoBERTa, GPT-2, and XLNet. [URL_CITE] We then normalize by the total expected cosine similarity A\u0302(f ` ) to get the proportion of the total expected cosine similarity contributed by each dimension. All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer.\nFootnote or Reference text (if exists): 2 All models https://github.com/fromhuggingface/transformers\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cxsoto/article-regions\nCitation Context: Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set. The col-lection will be available at [URL_CITE] https://github.com/cxsoto/article-regions, and in-cludes scripts to download and render the original article PDFs to images, as well as to convert the annotations to various formats. The default format is PASCAL VOC.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Novel Labeled Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcg-www.uia.ac.be/conll2000/chunking\nCitation Context: In this paper, we compare regularized Winnow and Winnow algorithms on text chunking (Ab-ney, 1991). In order for us to rigorously com-pare our system with others, we use the CoNLL-2000 shared task dataset (Sang and Buchholz, 2000), which is publicly available from [URL_CITE] http://lcg-www.uia.ac.be/conll2000/chunking. An advan-tage of using this dataset is that a large number of state of the art statistical natural language pro-cessing methods have already been applied to the data.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aka.ms/WikiQA\nCitation Context: In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen-tences s 1 and s 2 , and the SIN interaction proce-dure is then applied to these phrase representations as before to model phrase interactions. The aver-age of all hidden states are treated as sentence vec-tors v scnn 1 and v scnn [URL_CITE] . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors.\nFootnote or Reference text (if exists): 2 http://aka.ms/WikiQA\nSection Titles (if exists):  ['3 Method', '3.3 SIN with Convolution (SIN-CONV)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [URL_CITE] http: //sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [URL_CITE] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL citation is used to provide additional information about the tool used, but it's not clear how it was used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://writing.wisc.edu/Handbook/Transitions.html\nCitation Context: The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases [URL_CITE] etc. (Persing and Ng, 2015; Zesch et al., 2015).\nFootnote or Reference text (if exists): 6 https://writing.wisc.edu/Handbook/Transitions.html\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/\nCitation Context: (2016), existing methods often use language and task-specific rules and strate-gies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code re-trieval (Wei et al., 2015 [URL_CITE] ; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).\nFootnote or Reference text (if exists): Yi Wei, Nirupama Chandrasekaran, Sumit Gul-wani, and Youssef Hamadi. 2015. Build-ing bing developer assistant. Techni-cal report. https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/shashiongithub/Split-and-Rephrase\nCitation Context: This is to be expected given that WebSplit\u2019s small vocabulary of 7k words must account for the 344k tokens that make up the distinct complex sen-tences themselves. [URL_CITE]\nFootnote or Reference text (if exists): 3 We use WebSplit v1.0 throughout, which is the scaled-up re-release by Narayan et al. (2017) at http://github.com/shashiongithub/Split-and-Rephrase, commit a9a288c. Preliminary experiments showed the same trends on the smaller v0.1 corpus, as resplit by Aharoni and Goldberg (2018).\nSection Titles (if exists):  ['2 The WikiSplit Corpus', '2.3 Comparison to WebSplit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt15/\nCitation Context: (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [URL_CITE] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English\u2013Turkish (En\u2013Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt15/\nSection Titles (if exists):  ['4 Experimental Study', '4.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MT-DNN\nCitation Context: We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [URL_CITE] . Training details are presented in Appendix B.2.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/MT-DNN\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/emorynlp/reading-comprehension\nCitation Context: For the future work, we will expand the annotation for more entity types and automatically link men-tions with respect to their entities using an entity linker. All our resources including the annotated corpus and source codes of the models are avail-able at: [URL_CITE] https://github.com/emorynlp/reading-comprehension.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: Hypernymy is then determined using sparse, dependency based embeddings in English. B I S PARSE -L EX (Window context) Predeces-sor of the B I S PARSE -D EP model from our previ-ous work (Vyas and Carpuat, 2016) [URL_CITE] . This model induces sparse, cross-lingual embeddings using window based context.\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['5 Experimental Setup', '5.2 Contrastive Approaches']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Languageisolate\nCitation Context: This method also has drawbacks. First, the tax-onomy built on language family does not cover all the languages in the world since some languages are isolate [URL_CITE] . Second, there are many language families (152 according to Paul et al.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/wiki/Language isolate\nSection Titles (if exists):  ['3 Multilingual NMT with Language Clustering', '3.3 Discussions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://reporterslab.org/fact-checking/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. [URL_CITE] While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 2 http://reporterslab.org/fact-checking/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JulySinceAndrew/HieRec\nCitation Context: All hyper-parameters of HieRec and baseline methods are manually tuned on the valida-tion set. [URL_CITE] Following Wu et al. (2019e), we use four ranking metrics, i.e., AUC, MRR, nDCG@5, and nDCG@10, for performance evaluation.\nFootnote or Reference text (if exists): 2 https://github.com/JulySinceAndrew/HieRec\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental Datasets and Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/WeiboTreebank.zip\nCitation Context: \u2022 We present a freely available Chinese Weibo dependency treebank [URL_CITE] , manually annotated with more than 18,000 tokens;\nFootnote or Reference text (if exists): 3 http://www.cs.cmu.edu/\u02dcyww/data/WeiboTreebank.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/bioprocess\nCitation Context: The data and code for this paper are avail-able at [URL_CITE] http://www-nlp.stanford.edu/software/bioprocess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['b Transfer of ions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [URL_CITE] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['6 Experiments', '6.1 System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiesutd/SubwordEncoding-CWS\nCitation Context: In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [URL_CITE] is based on NCRF++ (Yang and Zhang, 2018).\nFootnote or Reference text (if exists): 1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [URL_CITE] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/HKUST-KnowComp/RINANTE\nCitation Context: Our code is available at [URL_CITE] https://github.com/HKUST-KnowComp/RINANTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org/\nCitation Context: This research is part of the Tanbih project, [URL_CITE] which aims to limit the effect of \u2018fake news,\u201d disinforma-tion, propaganda, and media bias by making users aware of what they are reading.\nFootnote or Reference text (if exists): 14 http://tanbih.qcri.org/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nCitation Context: This intuition is reflected in the bigram transition probabilities obtained from our corpus. [URL_CITE]\nFootnote or Reference text (if exists): 1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nSection Titles (if exists):  ['4 Training on Speech Acts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J11-2003\nCitation Context: Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [URL_CITE] ; Higashinaka and Isozaki, 2008). We primar-ily chose this architecture because it allows the user to directly relate the results of the system to the an-swer selection model.\nFootnote or Reference text (if exists): Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank an-swers to non-factoid questions from web collections. Computational Linguistics 37(2). http://aclweb.org/anthology/J11-2003.\nSection Titles (if exists):  ['2 System Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/parag1604/A2L\nCitation Context: : where, y max = arg max y s(y). The strategy se-lects examples for which M margin \u2264 \u03c4 [URL_CITE] , where \u03c4 1 is a hyper-parameter. We use Viterbi\u2019s algorithm (Ryan and Nudd, 1993) to compute the scores s(y).\nFootnote or Reference text (if exists): 1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.\nSection Titles (if exists):  ['4 Experiments', '4.1 Active Learning Strategies for Sequence Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/redpony/cdec\nCitation Context: \u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/redpony/cdec\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anserini.io/\nCitation Context: At inference time, we retrieve k text segments (one of the above conditions) using the question as a \u201cbag of words\u201d query. We use a post-v0.3.0 branch of Anserini, [URL_CITE] with BM25 as the ranking function (Anserini\u2019s default parameters).\nFootnote or Reference text (if exists): 1 http://anserini.io/\nSection Titles (if exists):  ['3 System Architecture', '3.1 Anserini Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nCitation Context: When performing self-training, we constructed the synthetic data by translating the monolingual sen-tences via beam search with beam width 5, and followed Edunov et al. (2018) [URL_CITE] to remove sen-tences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5. The \u201cteacher\u201d NMT model for self-training is the T RANSFORMER -B IG model to ensure the quality of synthetic data.\nFootnote or Reference text (if exists): 7 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nSection Titles (if exists):  ['A Appendix', 'A.1 Synthetic Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences). As creating manual alignment data is costly, we also created two training sets that con-sisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with smaller amounts of manually annotated data. The details of data size and alignment accuracy are shown in Table 3.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ASRU.2015.7404872\nCitation Context: The service implementation is based on Scala and the Play Framework. Our implementation con-tains data readers that allow to index InsuranceQA (Feng et al., 2015) [URL_CITE] and all publicly available dumps of the StackExchange platform. Researchers can easily add new datasets by implementing a single data reader class.\nFootnote or Reference text (if exists): Minwei Feng, Bing Xiang, Michael R. Glass, Li-dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. pages 813\u2013 820. https://doi.org/10.1109/ASRU.2015.7404872.\nSection Titles (if exists):  ['3 Candidate Retrieval']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) [URL_CITE] and semantic role la-beling (Chen and Rambow, 2003). In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/language/tree/master/language/realm\nCitation Context: All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [URL_CITE] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search).\nFootnote or Reference text (if exists): 19 https://github.com/google-research/language/tree/master/language/realm\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.python.org/\nCitation Context: \u2022 Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [URL_CITE] with a few basic packages and a Unix-type operating system.\nFootnote or Reference text (if exists): 1 https://www.python.org/\nSection Titles (if exists):  ['1 Introduction', '1.2 Design Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.863data.org.cn/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [URL_CITE] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 3 http://www.863data.org.cn/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/The_Big_Bang_Theory\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [URL_CITE] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [URL_CITE] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification.\nFootnote or Reference text (if exists): 1 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['1 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/\nCitation Context: Open source We release an implementation of CheckList at [URL_CITE] https://github.com/marcotcr/checklist. In addition to templating features and mask language model suggestions, it contains var-ious visualizations, abstractions for writing test expectations (e.g. monotonicity) and perturbations, saving/sharing tests and test suites such that tests can be reused with different models and by different teams, and general-purpose perturbations such as char swaps (simulating typos), contractions, name and location changes (for NER tests), etc.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 CheckList', '2.3 Generating Test Cases at Scale']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser/tree/easy-to-use\nCitation Context: DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [URL_CITE] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02\u201321; development: 22).\nFootnote or Reference text (if exists): 2 https://github.com/clab/lstm-parser/tree/easy-to-use.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/coastalcph/acl-citations\nCitation Context: Figure 2 shows the distribution of the age of cited articles with respect to the year in which the source article was published; Table 1 gives some comple-mentary statistics. The mean age of a cited paper has steadily decreased since 2013, from 7.69 years to 5.53 years in 2019; the median has dropped from [URL_CITE] to 3 years in the same period.\nFootnote or Reference text (if exists): 6 Datasets and code are available at: https://github.com/coastalcph/acl-citations\nSection Titles (if exists):  ['3 Analysis', '3.1 Are more recently published papers citing more recently published papers?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php\nCitation Context: The French Treebank [URL_CITE] [FTB] (Abeille\u0301 et al., 2003) is a syntactically annotated corpus made up of jour-nalistic articles from Le Monde newspaper. We used the latest edition of the corpus (June 2010) that we preprocessed with the Stanford Parser pre-processing tools (Green et al., 2011).\nFootnote or Reference text (if exists): 2 http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php\nSection Titles (if exists):  ['4 Resources', '4.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cocoxu/simplification\nCitation Context: SARI (Xu et al., 2016) [URL_CITE] is designed to evalu-ate text simplification systems by comparing sys-tem output against references and against the input sentence. It explicitly measures the goodness of words that are added, deleted and kept by the sys-tems.\nFootnote or Reference text (if exists): 3 https://github.com/cocoxu/ simplification\nSection Titles (if exists):  ['5 Experiment Settings', '5.1 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asiddhant/Active-NLP\nCitation Context: In this paper, we present a large-scale study [URL_CITE] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions.\nFootnote or Reference text (if exists): 1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bitterlemons.org\nCitation Context: Our corpus consists of articles published on the bitterlemons website [URL_CITE] . The website is set up to \u201ccontribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas\u201d.\nFootnote or Reference text (if exists): 1 http://www.bitterlemons.org\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.casmacat.eu/uploads/Deliverables/d6.1.pdf\nCitation Context: Such data has been made available by the CAS - MACAT project . In the project\u2019s first field trial [URL_CITE] , professional translators corrected machine transla-tions of news stories from a competitive English\u2013 Spanish machine translation system (Koehn and Haddow, 2012). This test set consists of 24,444 word predictions and 141,662 letter predictions.\nFootnote or Reference text (if exists): 2 http://www.casmacat.eu/uploads/Deliverables/d6.1.pdf\nSection Titles (if exists):  ['3 Properties of Core Algorithm', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.org\nCitation Context: The UD (universal dependencies) project [URL_CITE] releases a more detailed language-generic guideline to facilitate cross-linguistically consistent annotation, containing 37 relation labels. However, after in-depth study, we find that the UD guideline is very useful and comprehensive, but may not be completely compact for realistic annotation of Chinese-specific syntax.\nFootnote or Reference text (if exists): 3 http://universaldependencies.org\nSection Titles (if exists):  ['2 Annotation of Bi-tree Aligned Data', '2.1 Data Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability\nCitation Context: \u2022 Notgood,quantifiablebetter, best etcgradable. usually [URL_CITE] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability\nSection Titles (if exists):  ['3 Annotation Scheme']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/K17-3014\nCitation Context: Nguyen et al. (2017) [URL_CITE] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.\nFootnote or Reference text (if exists): Dat Quoc Nguyen, Mark Dras, and Mark John-son. 2017. A novel neural network model for joint pos tagging and graph-based dependency pars-ing. In CoNLL 2017 Shared Task: Multilin-gual Parsing from Raw Text to Universal Depen-dencies. Association for Computational Linguis-tics, pages 134\u2013142. https://doi.org/10.18653/v1/K17-3014.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/streaming/\nCitation Context: The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [URL_CITE] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/compare-mt\nCitation Context: For the En\u21d2Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [URL_CITE] (Neubig et al., 2019).\nFootnote or Reference text (if exists): 3 https://github.com/neulab/compare-mt\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jzhou316/Unsupervised-Sentence-Summarization\nCitation Context: We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [URL_CITE] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.imsdb.com\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [URL_CITE] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 4 http://www.imsdb.com\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/CunchaoZ/CyGNet\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx , and CyGNet [URL_CITE] . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 7 https://github.com/CunchaoZ/CyGNet\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tsafavi/NegatER\nCitation Context: Most notably, training KB completion models with highly-ranked negative examples from N EGAT ER results in statistically significant accuracy improvements of up to 1.90 % . Code and data are available at [URL_CITE] https://github.com/tsafavi/NegatER.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt\nCitation Context: B.1 Neural Machine Translation For the rich-resource WMT\u201916 En-De dataset, we use the pre-processed data from Ott et al. (2018) [URL_CITE] . For the low-resource datasets, we use byte-pair encoding (Sennrich et al., 2016) with 10,000 merge operations to build the vocabulary for the IWSLT (\u201914, \u201915, \u201916) datasets.\nFootnote or Reference text (if exists): 7 https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D11-1014\nCitation Context: Association for Computa-tional Linguistics, Edinburgh, Scotland, UK., pages 151\u2013161. [URL_CITE] http://www.aclweb.org/anthology/D11-1014.ord.2016. Bilingual learning of multi-sense embeddings with discrete autoencoders.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1231\nCitation Context: Andor et al. (2016) [URL_CITE] developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective ad-dresses the label bias problem and makes global training effective in the transition-based parsing setting.\nFootnote or Reference text (if exists): Daniel Andor, Chris Alberti, David Weiss, Aliak-sei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Glob-ally normalized transition-based neural networks. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 2442\u20132452. http://www.aclweb.org/anthology/P16-1231.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics\nCitation Context: Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://commoncrawl.org/\nCitation Context: is limited even when combined. This holds back research on open-domain CDCR systems which [URL_CITE] Fundamentals could increase the (currently limited) applicability\nFootnote or Reference text (if exists): 2 https://commoncrawl.org/ to expert-annotated CDCR corpora.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.keenage.com\nCitation Context: Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely. Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) and HowNet (Dong, 2000) [URL_CITE] (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).\nFootnote or Reference text (if exists): Zhendong Dong. 2000. HowNet Chinese-English Con-ceptual Database. Technical Report Online Software Database, Released at ACL. http://www.keenage.com.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freelang.net/dictionary\nCitation Context: These dictionaries provided high-quality manually edited word translations. In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site ( [URL_CITE] http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium). Compiled without professional editing, these bilingual word lists contain errors and inaccurate translations, and hence they introduced noise into the mapping pro-cess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Efraim\nCitation Context: If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map \u201cEphraim Sneh\u201d to [URL_CITE] http://en.wikipedia.org/wiki/Efraim Sneh and to assign it the Israeli nationality, male gender, and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Wikipedia as Knowledge', 'Named Entities with Wikipedia Disambiguation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://snips.ai/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai [URL_CITE] , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 6 https://snips.ai/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: In order to facilitate the adaptation of NLP tools for use in gun violence research, we introduce the Gun Violence Database [URL_CITE] (GVDB), a dataset for training and evaluating the performance of NLP systems in the domain of gun violence. The GVDB is the result of a large crowdsourced annotation effort. This an-notation is ongoing, and the GVDB will be regularly updated with new data and new layers of annotation, making it an interesting and challenging data set on which to evaluate state-of-the-art NLP tools.\nFootnote or Reference text (if exists): 2 http://gun-violence.org/\nSection Titles (if exists):  ['3 The Gun Violence Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://hackingsemantics.xyz/2019/leaderboards/\nCitation Context: While it has helped create more accurate models, we argued that this has been at the expense of fairness, efficiency, and robust-ness, among other desiderata. We were not the first to criticize NLP leaderboards (Rogers, 2019 [URL_CITE] ; Crane, 2018; Linzen, 2020), but we were the first to do so under a framework of utility, which we used to study the divergence between what is incentivized by leaderboards and what is valued by practitioners. Given the diversity of NLP practitioners, there is no one-size-fits-all solution; rather, leaderboards should demand transparency, requiring the report-ing of statistics that may be of practical concern.\nFootnote or Reference text (if exists): Anna Rogers. 2019. How the transformers broke NLP leaderboards. https://hackingsemantics.xyz/2019/leaderboards/. Accessed: 2020-05- 20.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~avaswani/giza-pp-l0.html\nCitation Context: The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) [URL_CITE] and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem.\nFootnote or Reference text (if exists): 11 http://www.isi.edu/\u02dcavaswani/giza-pp-l0.html\nSection Titles (if exists):  ['4 Experiments', '4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser\nCitation Context: As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [URL_CITE]\nFootnote or Reference text (if exists): 3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.\nSection Titles (if exists):  ['3 Consensus and Minimum Bayes Risk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dennybritz/cnn-text-classification-tf\nCitation Context: The embedding matrix W covers the subset of a vocabulary of interests for a particular NLP task, note that the value of V is known to be very large due to the rich variations in human languages. Here we showcase the em-bedding matrix size of a popular text classification model [URL_CITE] on AG-news dataset (Zhang et al., 2015) in Table 1. From which we can easily observe that the embedding matrix is commonly occupy-ing most of the parameter capacity, which could be the bottleneck in many real-world applications with limited computation resources.\nFootnote or Reference text (if exists): 2 https://github.com/dennybritz/ cnn-text-classification-tf\nSection Titles (if exists):  ['2 Vocabulary Selection', '2.1 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ir-china.org.cn/coae2008.html\nCitation Context: (2011), which contains English review sets on two do-mains (MP3 and Hotel) . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [URL_CITE] , where two review sets (Camera and Car) are selected. Xu et al.\nFootnote or Reference text (if exists): 6 http://ir-china.org.cn/coae2008.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuzhaopeng/NMT-Coverage\nCitation Context: Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-dent answers (A s ,A t ) such that grade(A i ,A s ) > grade(A i ,A t ), where A i is the corresponding in-structor answer. We use the SVMLight [URL_CITE] implemen-tation of SVMRank with tuned parameters.\nFootnote or Reference text (if exists): 8 http://svmlight.joachims.org/\nSection Titles (if exists):  ['6 http://Infomap-nlp.sourceforge.net/']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JiaweiSheng/FAAN\nCitation Context: Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [URL_CITE] https://github.com/JiaweiSheng/FAAN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://msnews.github.io/index.html\nCitation Context: We conduct extensive experiments on two real-world datasets to evaluate the effectiveness of Hi-eRec. The first one is the public MIND dataset (Wu et al., 2020d) [URL_CITE] . It is constructed by user behavior data collected from Microsoft News from October 12 to November 22, 2019 (six weeks), where user data in the first four weeks was used to construct users\u2019 reading history, user data in the penultimate week was used for model training and user data in the last week was used for evaluation.\nFootnote or Reference text (if exists): 1 We use the small version of MIND for quick experiments. This dataset is at https://msnews.github.io/index.html\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental Datasets and Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007) [URL_CITE] , SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!)). Such predicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over all c\u2019s) and are prevalent in NLP applications.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['5 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://reddit.com\nCitation Context: \u2022 Reddit - Reddit [URL_CITE] is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag \u2018/s\u2019 which are marked by the authors themselves.\nFootnote or Reference text (if exists): 3 https://reddit.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge\nCitation Context: We used the delexicalization script pro-vided by the organizers in the TGen repository [URL_CITE] https://github.com/UFAL-DSG/tgen/tree/master/e2e-challenge. This module replaces the names of restaurants which appeared in the Name and Near attributes with a generic value, X-Name and X-Near.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.verify-sy.com\nCitation Context: Claim Extraction We consider two websites as the source of our claims. V ERIFY [URL_CITE] is a project that was established to expose false claims made about the war in Syria and other related Middle Eastern issues. It is an independent platform that debunks claims made by all parties to the conflict.\nFootnote or Reference text (if exists): 4 http://www.verify-sy.com\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: In this paper we focus on verification of textual claims against textual sources. When compared to textual entailment (TE)/natural language infer-ence (Dagan et al., 2009 [URL_CITE] ; Bowman et al., 2015), the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veri-fication systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to han-dle large-scale resources such as Wikipedia (Chen et al., 2017).\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sunlab-osu/covid-faq\nCitation Context: We analyze COUGH by testing different FAQ re-trieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge pre-sented by COUGH and encouraging future re-search for further improvement. Our COUGH dataset is available at [URL_CITE] https://github.com/sunlab-osu/covid-faq.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Appendix B Aggregation Schemes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.unicode.org/udhr/\nCitation Context: Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights [URL_CITE] , non-English Wikipedias , the Jehovah\u2019s Witnesses website , and the Rosetta project (Lands-bergen, 1989).\nFootnote or Reference text (if exists): 2 The Universal Declaration of Human Rights is a document created by the United Nations and translated into many lan-guages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/\nSection Titles (if exists):  ['3 Task Definition', '3.4 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChunchuanLv/Iterative_Inference\nCitation Context: Furthermore, we provide analysis on con-straint violations and errors on the English test set. [URL_CITE]\nFootnote or Reference text (if exists): 2 The code and experiment settings can be ac-cessed at https://github.com/ChunchuanLv/ Iterative_Inference\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://multiwordnet.itc.it\nCitation Context: MultiWordNet [URL_CITE] . It is a multilingual computa-tional lexicon, conceived to be strictly aligned with the Princeton WordNet.\nFootnote or Reference text (if exists): 4 Available at http://multiwordnet.itc.it.\nSection Titles (if exists):  ['4 Exploiting Bilingual Dictionaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger\nCitation Context: Since then, the system, resp. its reimple-mentation by Heilman [URL_CITE] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013).\nFootnote or Reference text (if exists): 2 https://github.com/kutschkem/SmithHeilmann_fork/tree/master/ MIRATagger\nSection Titles (if exists):  ['2 Related Work', '2.2 Supersense Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: ELI5 (Fan et al., 2019b) [URL_CITE] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4).\nFootnote or Reference text (if exists): 8 https://yjernite.github.io/lfqa.html\nSection Titles (if exists):  ['3 Tasks', '3.4 Open Domain Question Answering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://worksheets.codalab.org/worksheets/0x593676a278fc4e5abe2d8bac1e3df486/\nCitation Context: Reproducibility. All code, data, and experiments for this paper are avail-able on the CodaLab platform at [URL_CITE] https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/ .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Discussion and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [URL_CITE] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov\nCitation Context: \u201cmetabolic diseases(C18.452)\u201d node is root of the subtree, and the subtree consists of 436 disease names which are target terms of specificity measuring. A set of journal abstracts was extracted from MEDLINE [URL_CITE] database using the disease names as quires. Therefore, all the abstracts are related to some of the disease names.\nFootnote or Reference text (if exists): 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine, USA. (http://www.nlm.nih.gov)\nSection Titles (if exists):  ['4. Experiment and Evaluation', '4.1. Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/giza/GIZA++.html\nCitation Context: As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [URL_CITE] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.\nFootnote or Reference text (if exists): 12 http://www.statmt.org/moses/giza/GIZA++.html\nSection Titles (if exists):  ['8 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dl.acm.org/citation.cfm?id=601016\nCitation Context: Consequently, by the non-negativity and joint convexity of the f-divergence (Csiszar and Korner, 1982) [URL_CITE] , we have: Corollary 1. The optimum of J D is N logN when using NLL loss, and 0 for the F s L2 loss. The optimum value above is achieved if and only if P 1 = P 2 = \u00b7 \u00b7 \u00b7 = P N = P for either loss.\nFootnote or Reference text (if exists): Imre Csiszar and Janos Korner. 1982. Infor-mation Theory: Coding Theorems for Discrete Memoryless Systems. Academic Press, Inc., Orlando, FL, USA. https://dl.acm.org/citation.cfm?id=601016.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mayoclinic.org/tests-procedures\nCitation Context: These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [URL_CITE] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped.\nFootnote or Reference text (if exists): 15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nCitation Context: With our work, we free CDCR research from depending on costly human-annotated training data and open up possibili-ties for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and model available at https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nCitation Context: The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [URL_CITE] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:\nFootnote or Reference text (if exists): 1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ghpaetzold/questplusplus\nCitation Context: The source code for the framework, the datasets and extra resources can be downloaded from [URL_CITE] https://github.com/ghpaetzold/questplusplus.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Remarks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.icsi.berkeley.edu/~framenet/\nCitation Context: This section presents the semantic role paradigm and the role-annotated corpus on which the present study is based. FrameNet [URL_CITE] is a lexical resource based on Fillmore\u2019s Frame Semantics (Fillmore, 1985). It de-scribes frames, representations of prototypical situa-tions.\nFootnote or Reference text (if exists): 1 http://www.icsi.berkeley.edu/~framenet/\nSection Titles (if exists):  ['2 FrameNet']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: C.2 Data Following Devlin et al. (2019), we use the English Wikipedia and BookCorpus (Zhu et al., 2015) data (WikiBooks) downloaded from the datasets library [URL_CITE] . We remove headers for the English Wikipedia and extract training samples with a max-imum length of 512.\nFootnote or Reference text (if exists): 8 https://github.com/huggingface/ datasets\nSection Titles (if exists):  ['C Experimental Setup', 'C.1 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1027\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [URL_CITE] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with LSTMs. In NAACL. Association for Computational Linguistics, San Diego, California, pages 232\u2013237. http://www.aclweb.org/anthology/N16-1027.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://accent.gmu.edu\nCitation Context: These differences are based upon a se-lectable set of binary phonetic features such as [voice], [continuant], [high], etc. STAT was initially designed to provide sets of phonological speech patterns in the compari-sons of various English accents found in the Speech Accent Archive [URL_CITE] http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, foren-sic linguistics, and speech recognition.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Abstract']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/nni\nCitation Context: To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [URL_CITE] Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the pre-processing procedures of logical forms.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/nni\nSection Titles (if exists):  ['4 Experimental Setup', '4.2 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://help.bet365.com/en/privacy-policy\nCitation Context: Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [URL_CITE] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers.\nFootnote or Reference text (if exists): 2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit , we use their released model as baselines. For the End-to-end model, we also use their re-leased code [URL_CITE] .\nFootnote or Reference text (if exists): 7 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 The Experiment', '5.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html\nCitation Context: We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset [URL_CITE] labeled for coref-erence, in order to assess accuracy. We also include \u223c5 million mentions from DBLP.\nFootnote or Reference text (if exists): 4 http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html\nSection Titles (if exists):  ['4 Experiments: Author Coreference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 dis-tinct supertags (2165 occur once) while the CCG-annotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [URL_CITE] ).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nCitation Context: First we implemented the standard linear SVM [URL_CITE] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.\nFootnote or Reference text (if exists): 6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nSection Titles (if exists):  ['3 Experiments and results', '3.1 Basic SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://labs.google.com/sets\nCitation Context: Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [URL_CITE] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:\nFootnote or Reference text (if exists): 1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openie.allenai.org/\nCitation Context: We utilize an off-the-shelf toolbox of OpenIE [URL_CITE] to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in dif-ferent granularities from one sentence.\nFootnote or Reference text (if exists): 1 http://openie.allenai.org/\nSection Titles (if exists):  ['2 Framework Description', '2.2 Answer-relevant Relation Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nCitation Context: For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/srparser.shtml\nCitation Context: In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014) [URL_CITE] ) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on dis-continuous structures, such as a new feature set.\nFootnote or Reference text (if exists): John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/srparser.shtml.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The score of a cluster is the sum of the indi-vidual scores of its senses (except for IMS which provides only one answer per word); the cluster with the highest score was selected. Our source code and the results of our evaluation are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 10 http://people.mpi-inf.mpg.de/\u02dccorrogg/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nusnlp/m2scorer\nCitation Context: Following recent work in English GEC, we conduct experiments in the same setting with the restricted track of the BEA-2019 GEC shared task (Bryant et al., 2019), using FCE (Yannakoudakis et al., 2011), Lang-8 Corpus of Learner English (Mizu-moto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Granger, 1998; Bryant et al., 2019) as training data. We use CoNLL-2013 test set as the dev set to choose the best-performing models, and evaluate on the well-known GEC benchmark datasets: CoNLL-2014 (Ng et al., 2014) and BEA-2019 test set with the official evaluation scripts (m2scorer [URL_CITE] for CoNLL-14, ERRANT for BEA-19). As previous work (Grundkiewicz et al., 2019) trained with synthetic data, we synthesize\nFootnote or Reference text (if exists): 2 https://github.com/nusnlp/m2scorer\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.urbandictionary.com/\nCitation Context: Considering situations where many slang terms have literal meanings, it may be unfair to re-trieve target terms from such machine translators by solely inputing slang terms without specific contexts. Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban Dictionary [URL_CITE] ). The following example shows how we obtain the target translation terms for the slang word \u201cfruitcake\u201d (an insane person):\nFootnote or Reference text (if exists): 11 http://www.urbandictionary.com/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf\nCitation Context: The following mainstream models are adopted as baselines to evaluate our model: SVM (Cui and Qi, 2017) [URL_CITE] and XGBoost (Tadesse et al., 2018): Support vector machine (SVM) or XGBoost is utilized as the classifier with features extracted by TF-IDF and LIWC from all posts.\nFootnote or Reference text (if exists): Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).\nSection Titles (if exists):  ['4 Experiments', '4.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/sphinx/tutorial.html\nCitation Context: We divided the test set roughly equally into two sets, and used one for parameter tuning and the other for rescoring test. The recognizer used for this data set was based on Sphinx-3 [URL_CITE] . The context-dependent triphone acoustic models with 32 Gaus-sian mixtures were trained using the training par-tition of the BU corpus described above, together with the broadcast new data.\nFootnote or Reference text (if exists): 2 CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html\nSection Titles (if exists):  ['5 Data and Baseline Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.markwatson.com/opensource/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [URL_CITE] etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 9 http://www.markwatson.com/opensource/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bert-as-service.github.com/hanxiao/bert-as-service\nCitation Context: Note that for BERT we evaluated all pooling strategies available in bert-as-service (Xiao, 2018) [URL_CITE] applied to either the last or second-to-last layers and report results for the best-performing combi-nation, which was mean-pooling on the last layer for both model sizes. Our results are presented in Table 1.\nFootnote or Reference text (if exists): Han Xiao. 2018. https://bert-as-service.github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.di.unito.it/~tutreeb/\nCitation Context: Meanwhile, motivated by different syntactic theories and practices, major languages in the world often possess multiple large-scale hetero-geneous treebanks, e.g., Tiger (Brants et al., 2002) and T\u00fcBa-D/Z (Telljohann et al., 2004) treebanks for German, Talbanken (Einarsson, 1976) and Syntag (J\u00e4rborg, 1986) treebanks for Swedish, ISST (Montemagni et al., 2003) and TUT [URL_CITE] treebanks for Italian, etc. Ta-ble 1 lists several large-scale Chinese tree-banks.\nFootnote or Reference text (if exists): 1 http://www.di.unito.it/~tutreeb/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/segtok/\nCitation Context: Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok [URL_CITE] , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing.\nFootnote or Reference text (if exists): 8 https://pypi.org/project/segtok/\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/20717\nCitation Context: F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://competitions.codalab.org/competitions/20717\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/anjalief/unsupervised_gender_bias\nCitation Context: Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-trained models are available at https: //github.com/anjalief/unsupervised_ gender_bias\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/mlm-scoring\nCitation Context: (2020) for computing pseudo-likelihood. [URL_CITE] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects.\nFootnote or Reference text (if exists): 8 https://github.com/awslabs/ mlm-scoring\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/faiss\nCitation Context: As for the evalu-ation network R, we use a multilayer perceptron with two hidden layers of size 512. For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [URL_CITE] . We calculate the similarity of sentences in each episode instead of each batch for computa-tional efficiency.\nFootnote or Reference text (if exists): 2 https://github.com/facebookresearch/ faiss\nSection Titles (if exists):  ['4 Experiments', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/aclwiki/index.php?title=Textual_Entailment_Resource_Pool\nCitation Context: To date, the primary sources of annotated NLI cor-pora have been the Recognizing Textual Entail-ment (RTE) challenge tasks. [URL_CITE] These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical mod-els of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed rep-resentations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowl-edge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al.\nFootnote or Reference text (if exists): 1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool\nSection Titles (if exists):  ['2 A new corpus for NLI']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleyaligner/\nCitation Context: We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [URL_CITE] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/berkeleyaligner/\nSection Titles (if exists):  ['6 Experiments', '6.3 Performance in Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/label_rationale_association\nCitation Context: Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/allenai/label_rationale_association.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax\nCitation Context: We have used Flax, an open-source library [URL_CITE] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone.\nFootnote or Reference text (if exists): 8 https://github.com/google/flax\nSection Titles (if exists):  ['8 Experiments And Results', '8.2 Language Models Trained on One-Billion Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everytrail.com/best/\nCitation Context: In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., [URL_CITE] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ex-periment with both type-level word embeddings (GloVe.6B.300d; Pennington et al., 2014) and contextual embeddings (BERT large; Devlin et al., 2019). [URL_CITE] In both cases, we keep the embeddings fixed, so the vast majority of the learnable pa-rameters are in the WFSAs. We train mod-els using GloVe embeddings on all datasets.\nFootnote or Reference text (if exists): 8 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mayoclinic.org/\nCitation Context: These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [URL_CITE] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped.\nFootnote or Reference text (if exists): 15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/sabine/chunklink/\nCitation Context: 1 Software available at [URL_CITE] http://ilk.uvt.nl/sabine/chunklink/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Unsupervised Relation Extraction Problem', '2.1 Context Vector and Feature Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/\nCitation Context: Here is an screen cast of the relevant anno-tation interface interface: [URL_CITE] https://youtu.be/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=BJJsrmfCZ\nCitation Context: More details of us-ing CNN for text classification can be found in the original paper (Kim, 2014). MAN is implemented using PyTorch (Paszke et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Adam Paszke, Sam Gross, Soumith Chintala, Gre-gory Chanan, Edward Yang, Zachary DeVito, Zem-ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in py-torch. NIPS 2017 Autodiff Workshop https://openreview.net/pdf?id=BJJsrmfCZ.\nSection Titles (if exists):  ['5 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nCitation Context: (2019), who publishes a manually labeled sentence-level test set. [URL_CITE] Besides, they also publish a test set for evaluating noise-filtering ability. Details of the adopted dataset are shown in Table 1.\nFootnote or Reference text (if exists): 4 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cia.gov/cia/publications/factbook\nCitation Context: We used BBN\u2019s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [URL_CITE] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset.\nFootnote or Reference text (if exists): 1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.\nSection Titles (if exists):  ['2 References to people', '2.3 Automatic semantic tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://linear-transformers.com\nCitation Context: We re-implement the Transformer and use the orig-inal implementation of the LT. [URL_CITE] All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model .\nFootnote or Reference text (if exists): 1 https://linear-transformers.com\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://math-qa.github.io/math-QA/\nCitation Context: The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at: [URL_CITE] https: //math-qa.github.io/math-QA/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use the 100-dimensional GloVe vectors [URL_CITE] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Experiments', '4.1 Answer Selection', '4.1.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gnu.org/s/wdiff/\nCitation Context: The HOO test data consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [URL_CITE] to extract system edits. We obtain each system\u2019s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation.\nFootnote or Reference text (if exists): 4 http://www.gnu.org/s/wdiff/\nSection Titles (if exists):  ['3 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nCitation Context: In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [URL_CITE] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007).\nFootnote or Reference text (if exists): 4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/higher\nCitation Context: In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [URL_CITE] (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ higher\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1146/annurev.neuro.26.041002.131047\nCitation Context: We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015) [URL_CITE] , sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.\nFootnote or Reference text (if exists): Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml\nCitation Context: Furthermore, we demonstrate how the new data can be exploited to train an ensemble\u2013 based method, that relies on these methods as un-derlying features. We believe that the new TR9856 benchmark, which is freely available for research purposes, [URL_CITE] along with the reported results, will contribute to the development of novel term relat-edness methods.\nFootnote or Reference text (if exists): 1 https://www.research.ibm.com/haifa/dept/vst/mlta_data.shtml\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Material and Supplement)\nTYPE: Mixed (Dataset and Paper)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/20717\nCitation Context: F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://competitions.codalab.org/competitions/20717\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://godel.iis.sinica.edu.tw/ROCLING\nCitation Context: Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): 2 See http://godel.iis.sinica.edu.tw/ROCLING.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dzonesoftware.com/products/open-source-question-answer-software/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [URL_CITE] etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 7 http://www.dzonesoftware.com/products/open-source-question-answer-software/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Tool, Website)\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs.stanford.edu/\nCitation Context: We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [URL_CITE] To this end, it would be instructive to understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality.\nFootnote or Reference text (if exists): 11 To facilitate future work, all of our models are publicly available at http://cs.stanford.edu/ \u223c valentin/.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sussex.ac.uk/Users/drh21/\nCitation Context: A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1\u201320 in our experi-ments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004), [URL_CITE] which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled.\nFootnote or Reference text (if exists): 8 http://www.sussex.ac.uk/Users/drh21/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/tree/master/research/lm_1b\nCitation Context: (2016) that was trained on the One Billion Word Benchmark. [URL_CITE] Hyper-parameter tun-ing is based on validation set perplexity.\nFootnote or Reference text (if exists): 4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/tree/master/research/lm_1b.\nSection Titles (if exists):  ['2 Number Agreement with LSTM Language Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Shakespeare%27ssonnets\nCitation Context: The example in the third row of Figure 1 comes from Shakespeare\u2019s \u201cSonnet 116\u201d (Shake-speare, 2000), the first four sentences. Usually, the rhyming schemes of Shakespeare\u2019s Sonnets is \u201cABAB CDCD EFEF GG\u201d [URL_CITE] . In the example, the rhyming words in scheme \u201cABAB\u201d are \u201cminds\u201d, \u201clove\u201d, \u201cfinds\u201d, and \u201cremove\u201d.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/Shakespeare%27s sonnets\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases\nCitation Context: PPMI creates embeddings by first building a positive pointwise mutual information word-context matrix, and then reducing the dimension-ality of this matrix using SVD (Bullinaria and Levy, 2007). A more recent word embedding al-gorithm, word2vec (skip-gram model) (Mikolov et al., 2013b) [URL_CITE] uses a shallow neural network to learn word embeddings by predicting context words. Another recent method for creating word embeddings, GloVe, is based on factoring a matrix of ratios of co-occurrence probabilities (Penning-ton et al., 2014).\nFootnote or Reference text (if exists): Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Informa-tion Processing Systems (NIPS). pages 3111\u20133119. http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases.\nSection Titles (if exists):  ['4 Factors Influencing Stability', '4.4 Algorithm Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: The input for each word is represented via con-catenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morpho-logical information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016 [URL_CITE] ). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word.\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We use the implementation of CRF with GE cri-teria from MALLET (McCallum, 2002) [URL_CITE] , which uses a gradient descent algorithm to optimize the objec-tive function. (Mann and McCallum, 2008; Druck, 2011)\nFootnote or Reference text (if exists): Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['5 Methods', '5.1 Conditional Random Field Model trained with Generalized Expectation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://roman.urdu.co/\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog [URL_CITE] . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service .\nFootnote or Reference text (if exists): 8 http://roman.urdu.co/\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nCitation Context: Consider the publicly available EHR note [URL_CITE] in Figure 1. The note is divided into the 10 sections found in that EHR (Problems, Medications, His-tory, etc.).\nFootnote or Reference text (if exists): 1 http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/valentinhofmann/dagobert\nCitation Context: We also publish the largest dataset of derivatives in context to date. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our code and data publicly available at https: //github.com/valentinhofmann/dagobert.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: Lan-guage models, and especially transformer-based language models, have shown to contain com-monsense and factual knowledge (Petroni et al., 2019; Jiang et al., 2019). We adopt this direc-tion in our model and build an encoder, pre-trained with Roberta (Liu et al., 2019c), which has been pre-trained on the huge language corpora (e.g., BooksCorpus (Zhu et al., 2015), Wikipedia (Remy, 2002) [URL_CITE] ) to capture implicit knowledge. We tokenize a description Q using WordPiece (Wu et al., 2016) as in BERT (Devlin et al., 2019a), giving us a se-quence of |Q| tokens and embed them with the pre-trained Roberta embeddings and append Roberta\u2019s positional encoding, giving us a sequence of d-dimensional token representation x Q1 , ..., x Q|Q| .\nFootnote or Reference text (if exists): M. Remy. 2002. Wikipedia: The free encyclope-dia200214wikipedia: The free encyclopedia. 2001 \u2013 updated daily. gratis http://www.wikipedia.com.\nSection Titles (if exists):  ['3 Methodology', '3.3 Pre-trained Knowledge Encoder']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html\nCitation Context: (1) was 23. Japanese word segmentation was performed by using JUMAN [URL_CITE] in our experiments.\nFootnote or Reference text (if exists): 4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', 'Translation Quality']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kbqademo.gstore.cn\nCitation Context: Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset with such strat-egy is also published to promote further re-search. An online demo of NAMER [URL_CITE] is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion of NAMER is also available.\nFootnote or Reference text (if exists): 2 http://kbqademo.gstore.cn\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bzhangXMU/transformer-aan\nCitation Context: We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/bzhangXMU/transformer-aan.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://air.jd.com/people2.html\nCitation Context: He received several awards including the Outstanding Paper Award at ACL 2015. Website: [URL_CITE] http://air.jd.com/people2.html\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Information About the Presenters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (providing information about the presenter)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhcross/span-parser\nCitation Context: The Penn Treebank dataset and splits are available at [URL_CITE] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.3 Syntactic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paraphrase.org\nCitation Context: In this work, we release version 1.0 of the Para-Phrase DataBase PPDB, [URL_CITE] a collection of ranked En-glish and Spanish paraphrases derived by:\nFootnote or Reference text (if exists): 1 Freely available at http://paraphrase.org.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt/\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): 6 http://www.phontron.com/kftt/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.uniroma1.it/wsdeval/training-data\nCitation Context: Section 3), keeping only the substitutes whose similarity to the target is higher than \u03bb. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [URL_CITE] . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense dis-tribution is also beneficial for lexical substitution.\nFootnote or Reference text (if exists): 4 We use the version released at http://nlp.uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0.\nSection Titles (if exists):  ['5 Dataset Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://prolific.ac/\nCitation Context: We employ the Prolific Academic (PA) crowdsourc-ing platform, [URL_CITE] an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.\nFootnote or Reference text (if exists): 11 https://prolific.ac/ (We chose PA for logistic reasons.)\nSection Titles (if exists):  ['4 Word Pair Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/chaitanyamalaviya/lang-reps\nCitation Context: Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phy-logenetic neighbors. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org/\nCitation Context: Our English-Portuguese training corpus consists of 28.5 Million segments from the Europarl, JRC-Aquis (Steinberger et al., 2006) and OpenSubti-tles [URL_CITE] corpora.\nFootnote or Reference text (if exists): 5 http://www.opensubtitles.org/\nSection Titles (if exists):  ['A.3 English-Portuguese']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/parser-faq.shtml\nCitation Context: The Stanford parser score is not designed to measure grammat-icality, however, it will generally prefer a gram-matical parse to a non-grammatical one. [URL_CITE] Thus, lower parser scores indicate less grammatically-correct utterances. In future work, we aim to use specifically designed grammar-scoring functions, e.g.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/parser-faq.shtml\nSection Titles (if exists):  ['4 Metrics', '4.2 Grammar-based metrics (GBMs)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tartarus.org/~martin/PorterStemmer\nCitation Context: To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [URL_CITE] . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy.\nFootnote or Reference text (if exists): 1 http://www.tartarus.org/\u02dcmartin/PorterStemmer\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/libsvm/\nCitation Context: This paper selects LIBSVM toolkit [URL_CITE] to implement the SVM classifier. The kernel is the radical basis function with the parameter \u03b3 = 0.001 in the exper-iments.\nFootnote or Reference text (if exists): 1 http://www.csie.ntu.edu.tw/cjlin/libsvm/\nSection Titles (if exists):  ['3 The U-SVM', '3.2 Classifying Question']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhclark/ducttape\nCitation Context: The toolkit that seems to be most similar to our approach is Ducttape [URL_CITE] , the successor of LonnyBin (Clark and Lavie, 2010). It is well designed and covers many useful points.\nFootnote or Reference text (if exists): 2 https://github.com/jhclark/ducttape\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Table 1: MDTC results on the Amazon dataset. Mod-els in bold are ours while the performance of the rest is taken from Wu and Huang (2015) [URL_CITE] . Numbers in paren-theses indicate standard errors, calculated based on 5 runs.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: We use the well-known 20 Newsgroups dataset (20 NEWS ) used in previous interactive topic mod-eling work: 18,846 Usenet postings from 20 dif-ferent newgroups in the early 1990s. [URL_CITE] We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes. We then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.\nFootnote or Reference text (if exists): 1 http://qwone.com/\u02dcjason/20Newsgroups/\nSection Titles (if exists):  ['3 High Water Mark for Tandem Anchors', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/budzianowski/multiwoz\nCitation Context: In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [URL_CITE]\nFootnote or Reference text (if exists): 5 The model is publicly available at https://github.com/budzianowski/multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.tencent.com/ailab/nlp/dialogue/datasets/grayscale_data_release.zip\nCitation Context: To facilitate further research, we have made our col-lected grayscale data publicly available. [URL_CITE] During training, we use these grayscale responses in a way adaptive to the training matching model. At each training epoch, ten different grayscale responses are used: the top 5 retrieval responses ranked by the current matching model and all 5 seq2seq gen-eration responses.\nFootnote or Reference text (if exists): 1 Related resources can be found at https: //ai.tencent.com/ailab/nlp/dialogue/ datasets/grayscale_data_release.zip\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ehealthmd.com/library/acltears\nCitation Context: Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. [URL_CITE] http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Alex-Fabbri/\nCitation Context: Datasets can be obtained by running the data scripts at [URL_CITE] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.2 Semantic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nCitation Context: We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [URL_CITE] . Model and Hyper-parameters.\nFootnote or Reference text (if exists): 6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/INK-USC/RE-Net\nCitation Context: We implement Distmult in PyTorch and use the binary cross-entropy loss for learning parameters. We use the official implementation of TuckER , COMPGCN , and RE-Net [URL_CITE] . For a fair comparison, we choose to use the variant of RE-Net with ground truth history during multi-step inference, and thus the model knows all the interactions before the time for testing.\nFootnote or Reference text (if exists): 4 https://github.com/INK-USC/RE-Net\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://pub.hal3.name\\\\\nCitation Context: To identify speaker errors on the sentence level, we consider and combine a collection of features into a single framework using a maximum entropy model (implemented with the Daume\u0301 III (2004) [URL_CITE] MEGA Model toolkit).\nFootnote or Reference text (if exists): Hal Daume\u0301 III. 2004. Notes on CG and LM-BFGS optimization of logistic regression. Paper available at http://pub.hal3.name\\\\ #daume04cg-bfgs, implementation available at http://hal3.name/megam/,August.\nSection Titles (if exists): ['3 Identifying poor constructions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.casmacat.eu/\nCitation Context: Such data has been made available by the CAS - MACAT project [URL_CITE] . In the project\u2019s first field trial , professional translators corrected machine transla-tions of news stories from a competitive English\u2013 Spanish machine translation system (Koehn and Haddow, 2012). This test set consists of 24,444 word predictions and 141,662 letter predictions.\nFootnote or Reference text (if exists): 1 http://www.casmacat.eu/\nSection Titles (if exists):  ['3 Properties of Core Algorithm', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SussexCompSem/learninghypernyms\nCitation Context: For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [URL_CITE] from the BLESS dataset (Baroni and Lenci, 2011). These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs.\nFootnote or Reference text (if exists): 2 https://github.com/SussexCompSem/ learninghypernyms\nSection Titles (if exists):  ['5 Evaluation', '5.1 Hyponymy with Word2Vec Vectors']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/havakv/pycox\nCitation Context: The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [URL_CITE] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.\nFootnote or Reference text (if exists): 2 https://github.com/havakv/pycox\nSection Titles (if exists):  ['4 Experiments', '4.3 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain\nCitation Context: Our web interface, hosted at [URL_CITE] http: //hierplane.allenai.org/explain, has the architecture shown in Figure 6. The front-end accepts text input from the user.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/i-machine-think/machine-tasks/tree/master/LongLookupTables\nCitation Context: Based on this observation, we evaluate on a compositionality-specific artificial task, lookup ta-bles (Liska et al., 2018), but extend it to better quantify extrapolation. [URL_CITE] This task is especially interesting to us, as there is a clear notion of what a good attention pattern should look like, making it easy to qualitatively and quantitatively analyze attentive models. It is a well-controlled task, which allows us to uncover challenges that prevent models from extrapolating on real-world data.\nFootnote or Reference text (if exists): 6 The extended datasets as well as scripts to gener-ate them can be found at https://github.com/i-machine-think/machine-tasks/tree/master/LongLookupTables\nSection Titles (if exists):  ['6 Experiments', '6.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nCitation Context: Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { \u22121,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [URL_CITE] :\nFootnote or Reference text (if exists): Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/syxu828/CSRL_dataset\nCitation Context: Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [URL_CITE] https: //github.com/syxu828/CSRL_dataset.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/McGill-NLP/contextual-nmn\nCitation Context: We significantly outperform the baselines by as much as +10.4% absolute ac-curacy on CC-Ref+, illustrating the generaliza-tion skills of our approach. Our dataset is pub-licly available at [URL_CITE] https://github.com/McGill-NLP/contextual-nmn.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/jdpacorpus/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity or entity [URL_CITE] depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 2 Available from http://verbs.colorado.edu/jdpacorpus/ \u2013 we ignore cars batch 009 where no arguments of comparative predicates are annotated.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ofai.at/\nCitation Context: For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [URL_CITE] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels.\nFootnote or Reference text (if exists): 2 We used the Matlab script downloaded from http://www.ofai.at/ \u223c dominik.schnitzer/mp/.\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We ran initial experiments with exemplar-based clas-sification and experimented with different cluster (Word2Vec) and topic (LDA) sizes of 100, 500, 1000, 2000 on the training set. For LDA (McCal-lum, 2002) [URL_CITE] , we also varied the threshold to assign a topic only to a text when it covered 1%, 2%, or 5% of the text. The best results were obtained with 100 topics and a proportion threshold of 1%.\nFootnote or Reference text (if exists): Andrew McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/docs/en/crawl-vectors.html\nCitation Context: We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [URL_CITE] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf.\nFootnote or Reference text (if exists): 7 https://fasttext.cc/docs/en/crawl-vectors.html\nSection Titles (if exists):  ['6 Language Modeling Analysis', '6.1 Predicting Insertion Locations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nCitation Context: \u2022 We use Comet\u2019s Bayes algorithm [URL_CITE] , which bal- Learning rate\nFootnote or Reference text (if exists): 6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/chrisjbryant/errant\nCitation Context: To identify incorrect spans, we use a binary se-quence tagging model in which tag 0 means the token is in a correct span; while tag 1 means the to-ken is in a grammatically incorrect span that needs to be edited, as shown in Figure 1(a). In order to train the tagging model, we align [URL_CITE] tokens across the source and target sentence in training data. With token alignment, we can identify the text spans that are edited and thus can annotate the edited text spans in the original sentences as erroneous spans.\nFootnote or Reference text (if exists): 1 Alignment can be solved by dynamic programming like Levenshtein distance. We here use ERRANT (https://github.com/chrisjbryant/errant) for alignment.\nSection Titles (if exists):  ['3 Erroneous Span Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Tiiiger/bert_score\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [URL_CITE]\nFootnote or Reference text (if exists): 13 https://github.com/Tiiiger/bert_score\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: Kiperwasser and Goldberg (2016) incor-porated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation. Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) [URL_CITE] proposed graph-based parsers that have ac-cess to rich feature representations obtained from BiLSTMs.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\nCitation Context: This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012) [URL_CITE] , which we denote as symDelete(\u00b7,\u00b7,\u00b7). This al-gorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical af-ter k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair moderno-modern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.\nFootnote or Reference text (if exists): Wolf Garbe. 2012. 1000x faster spelling correc-tion algorithm. http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/. Accessed: 2018-02-12.\nSection Titles (if exists):  ['4 Orthographic Similarity Adjustment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/webis-de/acl20-editorials-style-persuasive-effect\nCitation Context: (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [URL_CITE]\nFootnote or Reference text (if exists): 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asiddhant/Active-NLP\nCitation Context: In this paper, we present a large-scale study [URL_CITE] , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul-tiple acquisition functions.\nFootnote or Reference text (if exists): 1 Code for all of our models and for running active learn-ing experiments can be found at https://github.com/asiddhant/Active-NLP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.senseval.org/senseval3\nCitation Context: Conversely, researchers interested in producing richer semantic outputs have concen-trated on two-stage systems, where the semantic la-belling task is performed on the output of a parser, in a pipeline architecture divided in several stages (Gildea and Jurafsky, 2002). See also the common task of (CoNLL, 2004 2005; Senseval, 2004 [URL_CITE] ).\nFootnote or Reference text (if exists): Senseval. 2004. Third international workshop on the evalua-tion of systems for the semantic analysis of text (acl 2004). http://www.senseval.org/senseval3.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/twitter-customer-care-document-prediction\nCitation Context: Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [URL_CITE] . We summarize the statistics of the three datasets in Table 2.\nFootnote or Reference text (if exists): 3 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/srparser.shtml\nCitation Context: Our parser architecture follows previous work, particularly Zhu et al. (2013) and Bauer (2014) [URL_CITE] .\nFootnote or Reference text (if exists): John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/srparser.shtml.\nSection Titles (if exists):  ['2 Discontinuous Shift-Reduce Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/tkbc\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [URL_CITE] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 6 https://github.com/facebookresearch/tkbc\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We implement our TrigNet in Pytorch [URL_CITE] and train it on four NVIDIA RTX 2080Ti GPUs. Adam (Kingma and Ba, 2014) is utilized as the optimizer, with the learning rate of BERT set to 2e-5 and of other components set to 1e-3.\nFootnote or Reference text (if exists): 8 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments', '4.3 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.yahoo.com/\nCitation Context: For external evidence, only one external macro context feature f [URL_CITE] , as shown in Table 5, is captured in our model. f 4 is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.).\nFootnote or Reference text (if exists): 4 http://www.yahoo.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.2 External Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml/\nCitation Context: Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at [URL_CITE] http://archive.ics.uci.edu/ml/. The properties of the datasets are summarized in Ta-ble 2.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Experiments', '7.2 Document classification', '7.2.1 Task and dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cnap/grammaticality-metrics\nCitation Context: To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [URL_CITE]\nFootnote or Reference text (if exists): 9 https://github.com/cnap/ grammaticality-metrics\nSection Titles (if exists):  ['6 Summary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip\nCitation Context: We open source SB-TOP in the follow-ing link: [URL_CITE] http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip. More information about the dataset can be found in the Table ?\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments', '4.1 Session Based Task Oriented Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mdelhoneux/uuparser-composition\nCitation Context: (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [URL_CITE] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009).\nFootnote or Reference text (if exists): 4 The code can be found at https://github.com/mdelhoneux/uuparser-composition\nSection Titles (if exists):  ['4 Composition in a K&G Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Code, Paper\nFUNCTION: Extend, Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nsdl.org\nCitation Context: Our research is motivated by the challenges we encountered in working with the National Science Digital Library (NSDL) collection. [URL_CITE] Each item in the collection is a scientific resource, such as a re-search paper, an educational video, or perhaps an entire website. In addition to its main content, each resource is annotated with metadata, which provides information such as the author or creator of the re-source, its subject area, format (text/image/video) and intended audience \u2013 in all over 90 distinct fields (though some are very related).\nFootnote or Reference text (if exists): 1 http://www.nsdl.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cambridgeenglish.org/exams-and-tests/cefr/\nCitation Context: Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yan-nakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR). [URL_CITE] The W&I corpus contains 3, 600 texts across 3 different CEFR levels \u2013 A (be-ginner), B (intermediate), and C (advanced) \u2013 that have been annotated for errors.\nFootnote or Reference text (if exists): 11 https://www.cambridgeenglish.org/exams-and-tests/cefr/\nSection Titles (if exists):  ['3 GEC Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/30sGEbi\nCitation Context: We train all models on a single NVIDIA Titan X GPU, with the excep-tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat-edly query the NVIDIA System Management In-terface [URL_CITE] to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel\u2019s Running Average Power Limit interface.\nFootnote or Reference text (if exists): 2 nvidia-smi: https://bit.ly/30sGEbi\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/jdpacorpus/\nCitation Context: To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora [URL_CITE] (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1.\nFootnote or Reference text (if exists): 7 http://verbs.colorado.edu/jdpacorpus/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/35hKwhY\nCitation Context: As previously men-tioned, this could be attributed to the fact that MPAA\u2019s ratings are particularly sensitive to sex-ual content (Thompson and Yokota, 2004). In fact, filmmakers are advised to avoid the repeated usage of sexually-derived words\u2014either as an expletive or in a sexual context\u2014as to avoid a non family-friendly rating (Myers, 2018) [URL_CITE] . Thus, they might refer to sexual acts through the use of euphemisms or innuendos, which the model seems unable to pick up on.\nFootnote or Reference text (if exists): Scott Myers. 2018. Reader question: Is there a rule as to how many \u201ccuss words\u201d can be used in a script? https://bit.ly/35hKwhY. Accessed: 04/09/2020.\nSection Titles (if exists):  ['6 Results', '6.1 Classification Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/sw2v\nCitation Context: For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V [URL_CITE] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al.\nFootnote or Reference text (if exists): 13 http://lcl.uniroma1.it/sw2v\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasperGuo/Unimer\nCitation Context: We plan to cover more domains and more MRs in U NIMER . We have made U NIMER along with the execution engines publicly available. [URL_CITE] We be-lieve that U NIMER can provide fertile soil for ex-ploring MRs and addressing challenges in semantic parsing.\nFootnote or Reference text (if exists): 3 Our benchmark, execution engines and and our im-plementation can be found on: https://github.com/JasperGuo/Unimer\nSection Titles (if exists):  ['3 Benchmark']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/cnn.git\nCitation Context: For the Adam optimizer we use the default settings in the CNN neural network library. [URL_CITE] Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncer-tainty under the ensemble.\nFootnote or Reference text (if exists): 8 https://github.com/clab/cnn.git\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: To diminish the effects of randomness in training, the results of our model are averaged with 5 random initializa-tions. For data preprocessing, we use the tok-enizer [URL_CITE] from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework .\nFootnote or Reference text (if exists): 6 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: For instance: [URL_CITE] http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.biography.com/search/index.jsp\nCitation Context: We download from infoplease.com and biogra-phy.com [URL_CITE] two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 3 http://www.biography.com/search/index.jsp\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/arc-etrr-code\nCitation Context: The model is trained for 100 epochs. Our code is released at [URL_CITE] https://github.com/nijianmo/arc-etrr-code.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/trescal\nCitation Context: NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set [URL_CITE] . The data statistics of the training set are summarized in Table 1.\nFootnote or Reference text (if exists): 4 http://bit.ly/trescal\nSection Titles (if exists):  ['5 Experiments', '5.1 Knowledge Base Completion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sequencecomparison.github.io\nCitation Context: Language-specific ap-proaches, on the other hand, are based on pre-viously identified recurring correspondences be-tween the languages from which the words are taken (List 2014b: 48-50) and may differ across languages. An example for language-specific similarity measures is the LexStat algorithm, first proposed in List (2012a) and later refined in List (2014b) [URL_CITE] . As a general rule, language-specific ap-proaches outperform language-independent ones, provided the sample size is large enough (List 2014a).\nFootnote or Reference text (if exists): Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.\nSection Titles (if exists):  ['3 Methods', '3.1 Sequence Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.translationautomation.com/\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.translationautomation.com/\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL citation is used to provide additional information about the source of the data, but it is not clear how the website itself was used in the research.)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://git.io/MqyoIg\nCitation Context: So, for example, awe#n has a predominant weight in INSPIRED (0.38), comical#a has a predominant weight in AMUSED (0.51), while kill#v has a predomi-nant weight in AFRAID , ANGRY and SAD (0.23, 0.21 and 0.27 respectively). This matrix, that we call DepecheMood 2 , represents our emotion lex-icon, it contains 37k entries and is freely available for research purposes at [URL_CITE] http://git.io/MqyoIg.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Emotion Lexicon Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P/P16/P16-1013.pdf\nCitation Context: Word embeddings are low-dimensional, dense vector representations that capture semantic prop-erties of words. Recently, they have gained tremendous popularity in Natural Language Pro-cessing (NLP) and have been used in tasks as diverse as text similarity (Kenter and De Rijke, 2015), part-of-speech tagging (Tsvetkov et al., 2016) [URL_CITE] , sentiment analysis (Faruqui et al., 2015), and machine translation (Mikolov et al., 2013a). Although word embeddings are widely used across NLP, their stability has not yet been fully evaluated and understood.\nFootnote or Reference text (if exists): Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney, and Chris Dyer. 2016. Learning the curriculum with Bayesian optimization for task-specific word representation learning. Associa-tion for Computational Linguistics (ACL) pages 130\u2013139. http://anthology.aclweb.org/P/P16/P16-1013.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nickvosk/acl2015-\nCitation Context: Five human annotators provided relevance judg-ments, manually judging sentences based on how well they describe the relationship for an entity pair, for which we use a five-level graded rele-vance scale (perfect, excellent, good, fair, bad). [URL_CITE] Of all relevance grades 8.1% is perfect, 15.69% excellent, 19.98% good, 8.05% fair, and 48.15% bad. Out of 1 476 entity pairs, 1 093 have at least one sentence annotated as fair.\nFootnote or Reference text (if exists): 5 https://github.com/nickvosk/acl2015- dataset-learning-to-explain-entity- relationships\nSection Titles (if exists):  ['5 Experimental setup', '5.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: Word embedding layer (W E \u2208 R N\u00d7300 ): Sup-posing the word number of the vocabulary is N, this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Neural Sparse Topical Coding', '3.3 Neural Sparse Topical Coding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/International_English_Language_Testing_System\nCitation Context: Their learning progress is com-monly evaluated with classroom tests prepared by language instructors, quizzes in language learn-ing software such as Duolingo and Rosetta Stone, and by official standardized language proficiency tests such as TOEFL, IELTS, MET and others. In \u201chigh stakes\u201d scenarios, official language profi-ciency tests are the de-facto standards for language assessment; they are accepted by educational and professional institutions, and are taken by millions of language learners every year (for example, in 2016 over three million people took the IELTS test (IELTS, 2017) [URL_CITE] ). These tests probe language profi-ciency based on performance on various linguistic tasks, including grammar and vocabulary exams, reading and listening comprehension questions, as well as essay writing and speaking assignments.\nFootnote or Reference text (if exists): IELTS. 2017. International English language testing system \u2014 Wikipedia, the free encyclopedia. On-line; accessed November 2017. https://en.wikipedia.org/wiki/International_English_Language_Testing_System.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.shigeku.com/\nCitation Context: One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website [URL_CITE] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training.\nFootnote or Reference text (if exists): 2 http://www.shigeku.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Setups']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.psychpark.org\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [URL_CITE] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics\nCitation Context: Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/UKPLab/emnlp2021-prompt-ft-heuristics\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anserini.io/\nCitation Context: At inference time, we retrieve k text segments (one of the above conditions) using the question as a \u201cbag of words\u201d query. We use a post-v0.3.0 branch of Anserini, [URL_CITE] with BM25 as the ranking function (Anserini\u2019s default parameters).\nFootnote or Reference text (if exists): 1 http://anserini.io/\nSection Titles (if exists):  ['3 System Architecture', '3.1 Anserini Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip\nCitation Context: Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [URL_CITE] http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1 Wikipedia Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hatespeech.di.unito.it/resources.html\nCitation Context: HurtLex is a multilingual lexicon of hate words, originally built from 1,082 Italian hate words compiled in a manual fashion by the linguist Tullio De Mauro (De Mauro, 2016). This lexi-con is semi-automatically extended and translated into 53 languages by using BabelNet (Navigli and Ponzetto, 2012), and the lexical items are divided into 17 categories such as homophobic slurs, eth-nic slurs, genitalia, cognitive and physical disabil-ities, animals and more [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://hatespeech.di.unito.it/resources.html\nSection Titles (if exists):  ['4 Cross-domain Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Irina_Dunn\nCitation Context: However, this simplifi-cation (of representing sarcasm merely as positive verbs followed by negative situation) may not capture difficult forms of context incongruity. Consider the sarcastic sen-tence \u2018A woman needs a man like a fish needs bicycle\u2019 [URL_CITE] . The sarcasm in this sentence is understood from the fact that a fish does not need bicycle - and hence, the sentence ridicules the target \u2018a man\u2019.\nFootnote or Reference text (if exists): 2 This quote is attributed to Irina Dunn, an Australian writer (https://en.wikipedia.org/wiki/Irina_Dunn\nSection Titles (if exists):  ['2 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/treelstm\nCitation Context: Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks. Implementations of our mod-els and experiments are available at [URL_CITE] https://github.com/stanfordnlp/treelstm.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/about\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [URL_CITE] On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 1 http://stackexchange.com/about\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Pourdamghani et al. (2016) linearize input graphs by breadth-first traversal, and then use a phrase-based machine translation system [URL_CITE] to gen-erate results by translating linearized sequences.\nFootnote or Reference text (if exists): 2 http://www.statmt.org/moses/\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.cam.ac.uk/~ah433/mcrae-quantified-majority.txt\nCitation Context: In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vec-tors \u2013 giving us a common representation across for-malisms. Using a publicly available dataset of feature norms annotated with quantifiers [URL_CITE] (Herbelot and Vec-chi, 2015), we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data.\nFootnote or Reference text (if exists): 1 Data available at http://www.cl.cam.ac.uk/\u02dcah433/mcrae-quantified-majority.txt\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: 2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ).\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.3 Joint Modeling']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/skip-thoughts/\nCitation Context: The encoder combines the sentence em-beddings to produce 4800-dimensional combine-skip vectors with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model. This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments [URL_CITE] . For training of the STGAN, the Skip-Thought encoder produces sentence embed-ding vectors which are labelled as real samples for GAN discriminator.\nFootnote or Reference text (if exists): 1 https://github.com/ryankiros/skip-thoughts/\nSection Titles (if exists):  ['3 Skip-Thought Generative Adversarial Network', '3.3 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: Training Details Word embeddings were pre-trained with the Gensim [URL_CITE] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact). The di-mensionality of the word embeddings was set to 50.\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents\nCitation Context: To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007) [URL_CITE] . Articles are in English and come from a variety of sources.\nFootnote or Reference text (if exists): AQUAINT-2 (2007). URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.\nSection Titles (if exists):  ['4 Evaluation', '4.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ShyamSubramanian/HESM\nCitation Context: Our ex-perimental results show that HESM outper-forms 7 state-of-the-art methods for fact ex-traction and claim verification. Our source code is available at [URL_CITE] https://github.com/ShyamSubramanian/HESM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org/wsd\nCitation Context: A live demo featuring several dis-ambiguation models is available online. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://jobimtext.org/wsd\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (provides a live demo, not used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/INK-USC/KagNet\nCitation Context: Using ConceptNet as the only external resource for B ERT -based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning. We open-source our code [URL_CITE] to the community for future research in knowledge-aware commonsense reasoning.\nFootnote or Reference text (if exists): 1 https://github.com/INK-USC/KagNet\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/~mpalmer/projects/ace.html\nCitation Context: \u2022 semantic roles in the form of PropBank [URL_CITE] style (e.g. say.01-A0-boy for \u201c(the) boy said\u201d)\nFootnote or Reference text (if exists): 3 http://verbs.colorado.edu/\u02dcmpalmer/projects/ace.html\nSection Titles (if exists):  ['3 Models for Short Answer Scoring', '3.1 Response-based']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [URL_CITE] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 4 WMT16: http://www.statmt.org/wmt16/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nCitation Context: The supplemental material is available at our web site [URL_CITE] that provides proofs of the theorems, pseudo-codes of the algorithms, and more experiment re-sults with examples.\nFootnote or Reference text (if exists): 9 http://www-bigdata.ist.osaka-u.ac.jp/arase/pj/phrase-alignment/\nSection Titles (if exists):  ['Supplemental Material']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Supplement"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt17/\nCitation Context: Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [URL_CITE] . All sentences are longer than 20, making the dataset appropriate for testing the exposure bias problem.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt17/\nSection Titles (if exists):  ['4 Experiments', '4.3 Neural Language Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.gsk.or.jp/catalog/gsk2014-a/\nCitation Context: Dataset We use the Extended Named Entity Cor-pus for English and Japanese. [URL_CITE] fine-grained NER (Mai et al., 2018) In this dataset, each NE is assigned one of 200 entity labels defined in the Extended Named Entity Hierarchy (Sekine et al., 2002). For the English dataset, we follow the train-ing/development/test split defined by Mai et al.\nFootnote or Reference text (if exists): 3 https://www.gsk.or.jp/catalog/gsk2014-a/\nSection Titles (if exists):  ['3 Experiments', '3.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/bFokI\nCitation Context: We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English. [URL_CITE] We predict semantic similarity as cosine similarity. We make a prediction for a word pair if both words are repre-sented in the semantic space and their vectors have a non-zero similarity.\nFootnote or Reference text (if exists): 2 Downloadable from: http://goo.gl/bFokI\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wuch15/NRHUB\nCitation Context: We conducted experiments on a real-world news recommendation dataset [URL_CITE] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html\nCitation Context: (1) was 23. Japanese word segmentation was performed by using JUMAN [URL_CITE] in our experiments.\nFootnote or Reference text (if exists): 4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', 'Translation Quality']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nCitation Context: Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/cgibin/cmudict\nCitation Context: We also consider the number of syllables in a word, determined using the CMU Pronuncing Dic-tionary (Weide, 1998) [URL_CITE] . If the word is not present in the dictionary, then this is set to zero.\nFootnote or Reference text (if exists): Robert L Weide. 1998. The CMU pronouncing dictionary http://www.speech.cs.cmu.edu/cgibin/cmudict.\nSection Titles (if exists):  ['4 Factors Influencing Stability', '4.2 Word Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sentiwordnet.isti.cnr.it/\nCitation Context: The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet. [URL_CITE] An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008). Our ap-proach uses a Roget-like thesaurus, and it does not use any supervised classifiers.\nFootnote or Reference text (if exists): 2 http://sentiwordnet.isti.cnr.it/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/daiquocnguyen/R-MeN\nCitation Context: In future work, we plan to extend R-MeN for multi-hop knowledge graph reasoning. Our code is available at: [URL_CITE] https://github.com/daiquocnguyen/R-MeN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhclark/ducttape\nCitation Context: The toolkit that seems to be most similar to our approach is Ducttape [URL_CITE] , the successor of LonnyBin (Clark and Lavie, 2010). It is well designed and covers many useful points.\nFootnote or Reference text (if exists): 2 https://github.com/jhclark/ducttape\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://readium.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [URL_CITE] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 4 https://readium.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/International_English_Language_Testing_System\nCitation Context: Their learning progress is com-monly evaluated with classroom tests prepared by language instructors, quizzes in language learn-ing software such as Duolingo and Rosetta Stone, and by official standardized language proficiency tests such as TOEFL, IELTS, MET and others. In \u201chigh stakes\u201d scenarios, official language profi-ciency tests are the de-facto standards for language assessment; they are accepted by educational and professional institutions, and are taken by millions of language learners every year (for example, in 2016 over three million people took the IELTS test (IELTS, 2017) [URL_CITE] ). These tests probe language profi-ciency based on performance on various linguistic tasks, including grammar and vocabulary exams, reading and listening comprehension questions, as well as essay writing and speaking assignments.\nFootnote or Reference text (if exists): IELTS. 2017. International English language testing system \u2014 Wikipedia, the free encyclopedia. On-line; accessed November 2017. https://en.wikipedia.org/wiki/International_English_Language_Testing_System.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cluebenchmarks.com/\nCitation Context: CLUENER (Xu et al., 2020) In addition to the medical NER datasets, we also use a conven-tional NER dataset CLUENER released by CLUE organization [URL_CITE] , which is a well-defined and fine-grained dataset for named entity recognition in Chi-nese, including 10 categories like Person Name, Organzation, Book, etc. We extract 12090 avail-able instances from this dataset.\nFootnote or Reference text (if exists): 3 https://www.cluebenchmarks.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OceanskySun/\nCitation Context: For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; [URL_CITE] how-ever, we retrieve data with a simpler process, as described in Section 4.4.\nFootnote or Reference text (if exists): 6 https://github.com/OceanskySun/\nSection Titles (if exists):  ['4 Experiments and Results', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: In Universal Dependencies, Dozat et al. (2017) [URL_CITE] empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/N10-1138\nCitation Context: Baseline: Kang et al. (2017) extract relevant variables based on a semantic parser - SEMAFOR (Das et al., 2010) [URL_CITE] by filtering causation related frames from news corpora, topics and sentiments from tweets. To overcome the problem of low re-call, they adopt a topic-based knowledge base ex-pansion.\nFootnote or Reference text (if exists): Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948\u2013956. https://www.aclweb.org/anthology/N10-1138\nSection Titles (if exists):  ['5 Results', '5.2 Prediction Performance of PCG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nCitation Context: Relevance (R) We use the Lucene implementa-tion [URL_CITE] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.\nFootnote or Reference text (if exists): 4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nSection Titles (if exists):  ['3 Approach', '3.1 Ranking of Candidate Justification Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flan.cs.pitt.edu/~hwa/align/align.html\nCitation Context: In summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags for Chinese. The system is in place and the annotation process is underway. [URL_CITE]\nFootnote or Reference text (if exists): 4 The annotation interface is open to public. Please visit http://flan.cs.pitt.edu/~hwa/align/align.html\nSection Titles (if exists):  ['4 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.crowdflower.com\nCitation Context: We used CrowdFlower, [URL_CITE] where annotators were randomly presented test questions for quality con-trol. We collected labels from three annotators for each entity pair, and considered labels when at least two agreed. The resulting annotation con-tains total 2,995 labels on 914 documents, 682 positive, 836 negative and 474 without sentiment, which we discarded.\nFootnote or Reference text (if exists): 6 http://www.crowdflower.com\nSection Titles (if exists):  ['4 Data', '4.2 Sentiment Data Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/checklist\nCitation Context: changing names), we ex-pect that collaborative test creation will result in evaluation of NLP models that is much more ro-bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at [URL_CITE] https://github.com/marcotcr/checklist.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rikdz/GraphWriter\nCitation Context: We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [URL_CITE] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.\nFootnote or Reference text (if exists): 12 https://github.com/rikdz/GraphWriter\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: This concatenation af-ter each layer differs from Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanford-qa.com\nCitation Context: F1 measures the overlap between the prediction and ground truth answers which takes the max-imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi-cial script [URL_CITE] . Since the test set is hidden, we are re-quired to submit the model to Stanford NLP group to obtain the test scores.\nFootnote or Reference text (if exists): 3 Downloaded from http://stanford-qa.com\nSection Titles (if exists):  ['4 Experiment', '4.2 Main Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryanzhumich/editsql\nCitation Context: EditSQL (Zhang et al., 2019): EditSQL (with BERT) is the highest-performing model on the Spi-der dataset that also provides an open-source im-plementation along with a downloadable trained model. [URL_CITE] The model was built for edit-based multi-turn parsing tasks, but can also be used as a single-turn parser for Spider or KaggleDBQA. It employs a sequence-to-sequence model with a question-table co-attention encoder for schema encoding.\nFootnote or Reference text (if exists): 3 https://github.com/ryanzhumich/ editsql\nSection Titles (if exists):  ['4 Experiments', '4.1 Baseline Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Adaxry/GCDT\nCitation Context: Experiments on two standard se-quence labeling tasks show that, given only training data and the ubiquitous word embed-dings (Glove), our GCDT achieves 91.96 F 1 on the CoNLL03 NER task and 95.43 F 1 on the CoNLL2000 Chunking task, which outper-forms the best reported results under the same settings. Furthermore, by leveraging BERT as an additional resource, we establish new state-of-the-art results with 93.47 F 1 on NER and 97.30 F 1 on Chunking [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/Adaxry/GCDT.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.csai.tsinghua.edu.cn/~ly/systems/TsinghuaAligner/TsinghuaAligner.html\nCitation Context: For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set. [URL_CITE] The evalu-ation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets.\nFootnote or Reference text (if exists): 1 http://nlp.csai.tsinghua.edu.cn/\u02dcly/systems/TsinghuaAligner/TsinghuaAligner.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://liblinear.bwaldvogel.de/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: Parse trees are obtained using the Stanford Parser [URL_CITE] , and each clause is treated as an EDU. For a given parent p in the tree and its two children c 1 (associated with vector representation h c 1 ) and c 2 (associated with vector representation h c 2 ), stan-dard recursive networks calculate the vector for parent p as follows: where [h c 1 , h c 2 ] denotes the concatenating vector for children representations h c 1 and h c 2 ; W is a K \u00d7 2K matrix and b is the 1 \u00d7 K bias vector; and f(\u00b7) is the function tanh. Recursive neural models compute parent vectors iteratively until the root node\u2019s representation is obtained, and use the root embedding to represent the whole sentence.\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 EDU Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/i-machine-think/emergent_grammar_induction\nCitation Context: We argue that while the extent to which syntax develops in different types of referential games is an interesting question in its own right, a better understanding of the syntactic structure of emer-gent languages could also provide pivotal in better understanding their semantics, especially if this is considered from a compositional point of view. To facilitate such analysis, we bundled our tests in a comprehensive and easily usable evaluation frame-work. [URL_CITE] We hope to have inspired other researchers to apply syntactic analysis techniques and encour-age them to use our code to evaluate new emergent languages trained in other scenarios.\nFootnote or Reference text (if exists): 9 https://github.com/i-machine-think/ emergent_grammar_induction\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method & Supplement)\nTYPE: Mixed (Code & Document)\nFUNCTION: Produce & Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://reddit.com\nCitation Context: \u2022 Reddit - Reddit [URL_CITE] is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag \u2018/s\u2019 which are marked by the authors themselves.\nFootnote or Reference text (if exists): 3 https://reddit.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://personalitycafe.com/forum\nCitation Context: We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [URL_CITE] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts.\nFootnote or Reference text (if exists): 5 http://personalitycafe.com/forum\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.im2.ch\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Project)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleyparser/\nCitation Context: Finally, all the sentences in the corpus are divided into words using a Chinese word segmentation tool ( ICTCLAS ) with all entities annotated in the corpus kept. We use Berkeley Parser [URL_CITE] and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.\nFootnote or Reference text (if exists): 2 http://code.google.com/p/berkeleyparser/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by (Diaz et al., 2016). The implementation of Word2Vec [URL_CITE] from (Mikolov et al., 2013) is employed. In par-ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 \u22123 .\nFootnote or Reference text (if exists): 1 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Evaluation', '3.1 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions\nCitation Context: Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, \"AG\") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017 [URL_CITE] ) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) for ancient literary texts.\nFootnote or Reference text (if exists): Laura Gawlinski. 2017. Review: Packard hu-manities institute\u2019s searchable greek inscrip-tions. https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions. Accessed on 2019-08-26.\nSection Titles (if exists): [\\'3 Generating PHI-ML\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: The candidate se-lector uses a rule-based lemmatizer without part-of-speech (POS) information. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://spacy.io/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lxucs/multilingual-sl\nCitation Context: We implement three different settings for the baseline. [URL_CITE] BL-Direct is the direct zero-shot trans-fer without utilizing unlabeled data of TLs. BL-Single trains gold data of English and silver data of only one TL per model; it simply selects predic-tions of all unlabeled data as silver labels, without considering any uncertainties.\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/lxucs/multilingual-sl.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) \u2013 a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [URL_CITE] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.\nFootnote or Reference text (if exists): 10 https://spacy.io\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/pretrained_models.html\nCitation Context: LMs examined in this paper share similar archi-tecture and pre-training setting as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), but are trained on different corpora. We provide the shortcut name of each LM in the Hugging-Face\u2019s Transformer library ( [URL_CITE] https://huggingface.co/transformers/pretrained_models.html) and their training corpora in Tab. 8, from which you can find more information.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['D Details of Pre-trained LMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [URL_CITE] (d = 768). We imple-mented the HAN following Yang et al.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2015/task3/\nCitation Context: The corpus contains data from the Qatar Living forum, and is publicly available on the task\u2019s website. [URL_CITE] The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description.\nFootnote or Reference text (if exists): 2 http://alt.qcri.org/semeval2015/task3/\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0677528/data.html\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [URL_CITE] It contains 3,361 articles that have been downloaded from the BBC News website . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 2 Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ymcui/Chinese-BERT-wwm\nCitation Context: \u2022 ROBERTa, a robust BERT (Liu et al., 2019). We used ROBERTa-wwm-est-large. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ymcui/Chinese-BERT-wwm\nSection Titles (if exists):  ['5 Experiments', '5.1 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://feedly.uservoice.com/forums/192636-suggestions\nCitation Context: 1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app [URL_CITE] , and Windows App studio . We crawled the suggestion for improvement posts for these products, and labeled only a subset of them due to the annotation costs.\nFootnote or Reference text (if exists): 2 https://feedly.uservoice.com/forums/192636- suggestions\nSection Titles (if exists):  ['3 Research Methodology', '3.2 RQ2: Suggestion Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [URL_CITE] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps.\nFootnote or Reference text (if exists): 3 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www3.interscience.wiley.com/cgi-bin/jhome/31248\nCitation Context: We have obtained the digital publications of 9474 Journal of Comparative Neurology (JCN) [URL_CITE] articles from 1982 to 2005. We have converted the PDF format into plain text, maintaining paragraph breaks (some errors still occur though).\nFootnote or Reference text (if exists): 1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tau-nlp.org/commonsenseqa\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Category:Clinical_medicine\nCitation Context: The medical literature dataset consists of passages from textbooks, guidelines, and a subset of med-ically relevant Wikipedia articles [URL_CITE] . The number of sentences per source type is shown in Table 1. In total there are four sources in this dataset: Wikipedia and licensed content from DynaMed, Elsevier, and Wiley publishers.\nFootnote or Reference text (if exists): 2 Articles under the \u2018Clinical Medicine\u2019 category (https://en.wikipedia.org/wiki/Category:Clinical_medicine).\nSection Titles (if exists):  ['3 Data', '3.1 Medical Literature (MedLit)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained re-triever. We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020) [URL_CITE] . Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT (Devlin et al., 2019), we use a sparse-attention variant of the Transformer to allow modeling over longer sequences.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: (2019) to run the experiments 5 times with random initialization and report the averaged per-formance. We denote the original PWCN with rel-ative position information as PWCN-Pos, and that utilizes dependency trees constructed by SpaCy [URL_CITE] as PWCN-Dep. SpaCy has reported an UAS of 94.5 on English PTB and so it can serve as a good reference for human-designed dependency schema.\nFootnote or Reference text (if exists): 5 https://spacy.io/\nSection Titles (if exists):  ['6 BERT-based Trees VS Parser-provided Trees']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/TDT5/\nCitation Context: We evaluate the TW detection task on the bench-mark TDT-5 dataset [URL_CITE] , which consists of 250 news events spread over a time period of half a year, and 9,812 documents in English, Arabic and Chinese (translated to English), annotated by their relation-ship to those events. The largest event in TDT-5 dataset (#55106, titled \u201cBombing in Riyadh, Saudi Arabia\u201d) has 1,144 documents, while 66 out of the 250 events have only one document each.\nFootnote or Reference text (if exists): 8 http://projects.ldc.upenn.edu/TDT5/\nSection Titles (if exists):  ['5 Experimentation', '5.2 Detecting the topic of the week']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/syxu828/CSRL_dataset\nCitation Context: Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [URL_CITE] https: //github.com/syxu828/CSRL_dataset.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.babelnet.org/\nCitation Context: Throughout the paper, by a concept we mean a tuple b = (p, s) where p is a Wikipedia page and s is the corresponding WordNet synset. As a bridge between the two resources we use the synset-to-article mappings provided by BabelNet [URL_CITE] (Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipedia and WordNet. Note that the concept b can also con-tain a Wikipedia page or a WordNet synset only, if a mapping is not provided by BabelNet.\nFootnote or Reference text (if exists): 1 http://www.babelnet.org/\nSection Titles (if exists):  ['2 Semantic Representation of Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: We modify the experiments of Cheng et al. (2016) [URL_CITE] slightly, and assume that the user only pro-vides sequences of up to three words which are missing from the hypothesis. To simulate user interaction, at each iteration we chose a phrase of up to three tokens from the reference transla-tion which does not appear in the current MT hy-potheses.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisisnlp.qcri.org/\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [URL_CITE] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex.\nFootnote or Reference text (if exists): 2 https://crisisnlp.qcri.org/\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/\nCitation Context: In to-tal, we estimate that our resource contains 5.6 mil-lion correct rules. For comparison, Snow\u2019s pub-lished extension to WordNet [URL_CITE] , which covers simi-lar types of terms but is restricted to synonyms and hyponyms, includes 400,000 relations.\nFootnote or Reference text (if exists): 3 http://ai.stanford.edu/ \u223c rion/swn/\nSection Titles (if exists):  ['4 Extraction Methods Analysis', '4.1 Judging Rule Correctness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openie.cs.washington.edu/\nCitation Context: M ATCHER uses an API for the ReVerb Open IE system [URL_CITE] (Fader et al., 2011) to collect I(r T ), for each r T . The API for ReVerb allows for rela-tional queries in which some subset of the entity strings, entity categories, and relation string are specified.\nFootnote or Reference text (if exists): 1 http://openie.cs.washington.edu/\nSection Titles (if exists):  ['3 Textual Schema Matching', '3.4 Comparing database relations with extracted relations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://clic.cimec.unitn.it/composes\nCitation Context: The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [URL_CITE]\nFootnote or Reference text (if exists): 8 This dataset is available at http://clic.cimec.unitn.it/composes\nSection Titles (if exists):  ['5 Noun phrase generation', '5.2 Recursive decomposition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.metaversum.com/\nCitation Context: The KomParse system, described in this paper, provides NPCs for a virtual online world named Twinity, a product of the Berlin startup company Metaversum [URL_CITE] . The KomParse NPCs offer vari-ous services through conversation with game users using question-answering and dialog functional-ity. The utilization of Semantic Web technology with RDF-encoded generic and domain-specific ontologies furthermore enables semantic search and inference.\nFootnote or Reference text (if exists): 1 http://www.metaversum.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/newspaper\nCitation Context: We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [URL_CITE] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/newspaper\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flowacldemo.appspot.com\nCitation Context: This paper presents FLOW [URL_CITE] (Figure 1), an interactive system for assisting EFL writers in composing and revising writing. Different from existing tools, its context-sensitive and first-language-oriented features enable EFL writers to concentrate on their ideas and thoughts without being hampered by the limited lexical resources.\nFootnote or Reference text (if exists): 1 FLOW: http://flowacldemo.appspot.com\nSection Titles (if exists):  ['1. Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://research.microsoft.com/users/silviu/WebAssistant/TestData\nCitation Context: We evaluated the system in two ways: on a set of Wikipedia articles, by comparing the system out-put with the references created by human contribu-tors, and on a set of news stories, by doing a post-hoc evaluation of the system output. The evalua-tion data can be downloaded from [URL_CITE] http://research.microsoft.com/users/silviu/WebAssistant/TestData. In both settings, we computed a disambiguation baseline in the following manner: for each surface form, if there was an entity page or redirect page whose title matches exactly the surface form then we chose the corresponding entity as the baseline disambiguation; otherwise, we chose the entity most frequently mentioned in Wikipedia using that surface form.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://super.gluebenchmark.com/leaderboard/\nCitation Context: The human performance on B OOL Q and M ULTI RC are directly reported from SuperGLUE (Wang et al., 2019) leaderboard. [URL_CITE] Here are the key observations in this table:\nFootnote or Reference text (if exists): 5 https://super.gluebenchmark.com/leaderboard/\nSection Titles (if exists):  ['C Performances Across Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org/wiki/Wiktionary:Idioms_that_survived_RFD\nCitation Context: Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are com-positional (e.g. from the Wiktionary guidelines for idiom inclusion, [URL_CITE] computer chess, basketball player, telephone box).\nFootnote or Reference text (if exists): 3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.neunet.2010.10.005\nCitation Context: Reweighting for covariate shift. Although they are not typically thought of in the context of es-timating proportions, several methods have been proposed to deal directly with the problem of co-variate shift, including kernel mean matching and its extensions (Huang et al., 2006; Sugiyama et al., 2011 [URL_CITE] ). Here, we consider the two-stage method from Bickel et al.\nFootnote or Reference text (if exists): Masashi Sugiyama, Makoto Yamada, Paul von Bu\u0308nau, Taiji Suzuki, Takafumi Kanamori, and Motoaki Kawanabe. 2011. Direct density-ratio estima-tion with dimensionality reduction via least-squares hetero-distributional subspace search. Neural Net-works 24(2). https://doi.org/10.1016/j.neunet.2010.10.005.\nSection Titles (if exists):  ['3 Methods', '3.2 Existing methods appropriate for extrinsic labels']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xwhan/One-shot-Relational-Learning\nCitation Context: We conduct experiments on two public benchmark datasets: NELL and Wiki [URL_CITE] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks.\nFootnote or Reference text (if exists): 1 https://github.com/xwhan/ One-shot-Relational-Learning\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [URL_CITE]\nFootnote or Reference text (if exists): 5 CRF experiments used the CRF++ package http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['4 From MaxEnt to CRFs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/easonnie/combine-FEVER-NSMN\nCitation Context: For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [URL_CITE] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input.\nFootnote or Reference text (if exists): 22 available at https://github.com/easonnie/ combine-FEVER-NSMN\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: For all other hyper-parameters, we use the default values of previous implementations. [URL_CITE] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.\nFootnote or Reference text (if exists): 6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bzhangXMU/transformer-aan\nCitation Context: We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/bzhangXMU/transformer-aan.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: For each sentence, the annotation indi-cates whether adverse effects of a drug are be-ing described (\u2018Positive\u2019) or not (\u2018Negative\u2019). The original release of the data does not contain the document context, which we reconstructed from PubMed [URL_CITE] . Sentences for which the full abstract could not be found were removed, resulting in 20,040 labelled sentences, with average sentence length 21 and average document length 129.\nFootnote or Reference text (if exists): 4 https://www.ncbi.nlm.nih.gov/pubmed/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with [URL_CITE] random initializa-tions. For data preprocessing, we use the tok-enizer 6 from BERT to preprocess the sentences.\nFootnote or Reference text (if exists): 5 Code for metrics: https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md\nCitation Context: We use an annotated corpus of 1154 English Elec-tronic Health Records from cancer patients. Each note was annotated [URL_CITE] by two annotators who label clinical entities into several categories. These cate-gories can be broadly divided into two groups, Clin-ical Events and Attributes.\nFootnote or Reference text (if exists): 2 The annotation guidelines can be found at https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/yblcb5q5\nCitation Context: From a fact-checkers\u2019 point of view, the abun-dance of previously fact-checked claims increases the likelihood that the next claim that needs to be checked would have been fact-checked already by some trusted organization. Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [URL_CITE] Thus, before spending hours fact-checking a claim manually, it is worth first making sure that nobody has done it already.\nFootnote or Reference text (if exists): 7 President Trump has repeated one claim over 80 times: http://tinyurl.com/yblcb5q5.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [URL_CITE] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/liushulinle/event\nCitation Context: We publish our code for further study by the NLP community. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/liushulinle/event detection without triggers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://api.syntagnet.org/\nCitation Context: SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/ , and a RESTful endpoint to query the system programmatically (accessible at [URL_CITE] http://api.syntagnet.org/).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, DataSource)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: Our classifier forms an encoder-labeler struc-ture, which consists of the above encoder and a la-beler that predicts a sentiment label from the con-text. Note that the encoder of the classifier is fine-tuned with labeled data, as in (Dai and Le, 2015) [URL_CITE] . The main difference between their approach and ours is that we examine paired (dialog) data for pretraining, while they only showed the usefulness of pretraining with unpaired data.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/multilingual.md\nCitation Context: We used Wikipedia as the main data source for all our experiments. Multilingual BERT [URL_CITE] was trained on the 104 languages with the largest Wikipedias \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.\nFootnote or Reference text (if exists): 10 Information about multilingual BERT can be found in: https://github.com/google-research/bert/blob/master/multilingual.md\nSection Titles (if exists):  ['6 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlpir.nist.gov/projects/duc/\nCitation Context: English document summarization experiments are run using the summarization test collection provided in the framework of the Document Understanding Con-ference (DUC). In particular, we use the data set of 567 news articles made available during the DUC 2002 evaluations (DUC, 2002) [URL_CITE] , and the correspond-ing 100-word summaries generated for each of these documents. This is the single document summariza-tion task undertaken by other systems participating in\nFootnote or Reference text (if exists): DUC. 2002. Document understanding conference 2002. http://www-nlpir.nist.gov/projects/duc/.\nSection Titles (if exists):  ['3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atschneid/DebugSL\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at [URL_CITE] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sec.gov/edgar.shtml\nCitation Context: These reports are available to the public and pub-lished on the SEC\u2019s web site. [URL_CITE] The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the period 1996\u20132006 from 10,492 different companies.\nFootnote or Reference text (if exists): 3 http://www.sec.gov/edgar.shtml\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ai-systems/premise\nCitation Context: For the experiments using BERT and the pro-posed approach, we split the dataset using a 50/20/30 (train/dev/test) split. We run all exper-iments ten times, evaluating on the test set, and report the average Precision, Recall and F1-score. All evaluation data, as well as the experimental pipeline, can be found online [URL_CITE] for reproducibility purposes.\nFootnote or Reference text (if exists): 2 https://github.com/ai-systems/premise selection graph\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2Hw0xWc\nCitation Context: Whereas a decade ago most mon NLP models, compared to familiar consumption. [URL_CITE] NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.\nFootnote or Reference text (if exists): 1 Sources: (1) Air travel and per-capita consumption: https://bit.ly/2Hw0xWc; (2) car lifetime: https: //bit.ly/2Qbr0w1.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Support"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://toolbox.google.com/factcheck/explorer\nCitation Context: In an industrial setting, Google has developed Fact Check Explorer, [URL_CITE] which is an exploration tool that allows users to search a number of fact-checking websites (those that use ClaimReview from schema.org ) for the mentions of a topic, a person, etc. However, the tool cannot handle a complex claim, as it runs Google search, which is not optimized for semantic matching of long claims. While this might change in the future, as there have been reports that Google has started using BERT in its search, at the time of writing, the tool could not handle a long claim as an input.\nFootnote or Reference text (if exists): 9 http://toolbox.google.com/factcheck/ explorer\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in [URL_CITE] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 8 https://github.com/tensorflow/tensor2tensor\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cluebenchmarks.com/\nCitation Context: CLUENER (Xu et al., 2020) In addition to the medical NER datasets, we also use a conven-tional NER dataset CLUENER released by CLUE organization [URL_CITE] , which is a well-defined and fine-grained dataset for named entity recognition in Chi-nese, including 10 categories like Person Name, Organzation, Book, etc. We extract 12090 avail-able instances from this dataset.\nFootnote or Reference text (if exists): 3 https://www.cluebenchmarks.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [URL_CITE] on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident.\nFootnote or Reference text (if exists): 3 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Experiments', '3.2 Chinese SRL Performance']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.cornell.edu/people/pabo/movie-review-data/\nCitation Context: \u2022 MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://www.cs.cornell.edu/people/pabo/movie-review-data/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.5281/zenodo.4432958\nCitation Context: TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at [URL_CITE] https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.luis.ai/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis [URL_CITE] , wit.ai , snips.ai , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 4 https://www.luis.ai/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: We used equal numbers of sentences from three different genres [URL_CITE] :\nFootnote or Reference text (if exists): 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['3 Linguistic acceptability study', '3.1 Dataset creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (as it's used as a source to obtain data, but not directly used or compared in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/Zero-Shot-DST\nCitation Context: , R t\u22121 , U t }, we add the \"user:\" and \"system:\" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U [URL_CITE] . .\nFootnote or Reference text (if exists): 1 Source code is available in https://github.com/facebookresearch/Zero-Shot-DST\nSection Titles (if exists): ['3 Methodology', '3.1 T5DST']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/antonisa/at\nCitation Context: In Following this simple triangulation approach, we cre-the case of feature mismatch (for instance, Greek uses 2 ate 4,704 new dictionaries over pairs between the 50 numbers, 4 cases and 3 genders while Italian has 2 num-languages of the MUSE dictionaries. [URL_CITE] For consistency, bers, 2 genders, and no cases) or if we only find a partial we keep the same train and test splits as with MUSE, so tag match over a feature subset, we filter out transla-that the source-side types are equal across all dictionar-tions with disagreeing tags. We ignore the grammatical ies with the same source language.\nFootnote or Reference text (if exists): 9 Available unable to filter dictionaries in the following 11 languages: aze,https://github.com/antonisa/atembeddings. bel, ben, bos, lit, mkd, msa, sqi, tam, tha, tel.\nSection Titles (if exists):  ['3 New LI Evaluation Dictionaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tweetcongress.org/members/\nCitation Context: It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012 [URL_CITE] ). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average.\nFootnote or Reference text (if exists): TweetCongress. (2012). Congress Members on Twitter Retrieved Mar 18, 2012, from http://tweetcongress.org/members/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QAML/S3QACoreFramework\nCitation Context: Our cQA pipeline is available for download [URL_CITE] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled.\nFootnote or Reference text (if exists): 1 https://github.com/QAML/ S3QACoreFramework\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ethnologue.com\nCitation Context: Language families include Indo-European, Austronesian and Niger-Congo, and gen-era within Indo-European include Germanic, In-dic and Slavic. For more detailed trees, we used hierarchical classifications provided by Ethno-logue (Lewis et al., 2014) [URL_CITE] . The mapping between WALS and Ethnologue was done using ISO 639-3 language codes.\nFootnote or Reference text (if exists): M. Paul Lewis, Gary F. Simons, and Charles D. Fen-nig, editors. 2014. Ethnologue: Languages of the World, 17th Edition. SIL International. Online ver-sion: http://www.ethnologue.com.\nSection Titles (if exists):  ['3 Data and Preprocessing', '3.1 Typology Database and Phylogenetic Trees']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.apple.com/legal/privacy/en-ww/\nCitation Context: Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [URL_CITE] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers.\nFootnote or Reference text (if exists): 2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.cornell.edu/people/pabo/movie-review-data/\nCitation Context: Pang and Lee (2004) demonstrate that the subjec-tivity detection can be a useful input for a sen-timent classifier. They compose a publicly avail-able dataset [URL_CITE] of 5000 subjective and 5000 objec-tive sentences, classifying them with a reported accuracy of 90-92% and further show that predict-ing this information improves the end-level sen-timent classification on a movie review dataset. Kim (2014) and Wang and Manning (2013) fur-ther improve the performance through different machine learning methods.\nFootnote or Reference text (if exists): 9 https://www.cs.cornell.edu/people/pabo/movie-review-data/\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.3 Subjectivity Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/PepsiChallenge\nCitation Context: In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge [URL_CITE] , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola\u2019s branding has made it more popular. Even now, Microsoft uses similar blind taste tests to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing\u2019s results, Google\u2019s brand might have strengthened over the years.\nFootnote or Reference text (if exists): 1 http://en.wikipedia.org/wiki/Pepsi Challenge\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/r/WritingPrompts/\nCitation Context: For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [URL_CITE] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox.\nFootnote or Reference text (if exists): 5 https://www.reddit.com/r/WritingPrompts/\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.codeproject.com/KB/recipes/englishparsing.aspx\nCitation Context: Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the \u201copenNLP\u201d software (Northedge, 2005) [URL_CITE] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters.\nFootnote or Reference text (if exists): R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx\nSection Titles (if exists):  ['4 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://knowitall.github.io/ollie/\nCitation Context: We extract relational triples from each sentence in a large corpus using the OLLIE Open IE system (Mausam et al., 2012). [URL_CITE] This provides relational tu-ples in the format (Arg1, Relation, Arg2) where each tuple element is a phrase from the sentence. The sentence \u201cHe cited a new study that was released by UCLA in 2008.\u201d produces three tuples:\nFootnote or Reference text (if exists): 4 Available at: http://knowitall.github.io/ollie/\nSection Titles (if exists):  ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.whlib.ac.cn/sjk/bkqs.htm\nCitation Context: We used the labeled sentences as test data and the remaining sentences as unclassified data in English. We also used the sentences in the Great Encyclopedia [URL_CITE] Chinese corpus as unclassified data in Chinese. We defined, for each translation, a seed word in English as a classified example (cf., Table 5).\nFootnote or Reference text (if exists): 5 http://www.whlib.ac.cn/sjk/bkqs.htm\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset and Code)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/vilbert-multi-task\nCitation Context: Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [URL_CITE] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\".\nFootnote or Reference text (if exists): 5 https://github.com/facebookresearch/ vilbert-multi-task\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nCitation Context: Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [URL_CITE]\nFootnote or Reference text (if exists): 3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nSection Titles (if exists):  ['6 Experiments', '6.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/yHWeJp\nCitation Context: In practice, exact values of percent-upvoted are not directly available; the site adds \u201cvote fuzzing\u201d to fight vote manipulation. [URL_CITE] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.\nFootnote or Reference text (if exists): 6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can \u201cactually be trusted;\u201d https://goo.gl/yHWeJp\nSection Titles (if exists):  ['2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://glaros.dtc.umn.edu/gkhome/views/cluto\nCitation Context: The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package [URL_CITE] for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others.\nFootnote or Reference text (if exists): 2 http://glaros.dtc.umn.edu/gkhome/views/cluto\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EducationalTestingService/skll\nCitation Context: Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [URL_CITE]\nFootnote or Reference text (if exists): 2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.\nSection Titles (if exists):  ['3 Models for Short Answer Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/WSDTutorial.html\nCitation Context: Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [URL_CITE] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.\nFootnote or Reference text (if exists): Ted Pedersen and Rada. F. Mihalcea, \u201cAdvances in Word Sense Disambiguation,\u201d tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.\nSection Titles (if exists):  ['2 The proposal']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/master/examples/run\nCitation Context: Most of the hyperparmeters used for fine-tuning are the default ones from Wolf et al. (2019) [URL_CITE] , except for learning rate for BART, which we set to 1e \u2212 5.\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/transformers/blob/master/examples/run lm finetuning.py\nSection Titles (if exists):  ['3 Experiments and Discussion', '3.2 Language Model Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq [URL_CITE] (Ott et al., 2019) to imple-ment our algorithms. We adopt the Transformer-base (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT\u201914 De-En.\nFootnote or Reference text (if exists): 5 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://towcenter.org/research/lies-damn-lies-and-viral-content/\nCitation Context: It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015) [URL_CITE] , and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article.\nFootnote or Reference text (if exists): Craig Silverman. 2015. Lies, Damn Lies and Viral Content. http: //towcenter.org/research/ lies-damn-lies-and-viral-content/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/pkumod/gAnswer\nCitation Context: Baselines We compare our results with the top ranking team \"jchl\" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [URL_CITE] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module.\nFootnote or Reference text (if exists): 7 https://github.com/pkumod/gAnswer\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Experimental Setup\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xijiz/cfgen\nCitation Context: As a result, our method eliminates part of the spu-rious correlations between context representa-tion and output labels. The code is available at [URL_CITE] https://github.com/xijiz/cfgen.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/International_English_Language_Testing_System\nCitation Context: Their learning progress is com-monly evaluated with classroom tests prepared by language instructors, quizzes in language learn-ing software such as Duolingo and Rosetta Stone, and by official standardized language proficiency tests such as TOEFL, IELTS, MET and others. In \u201chigh stakes\u201d scenarios, official language profi-ciency tests are the de-facto standards for language assessment; they are accepted by educational and professional institutions, and are taken by millions of language learners every year (for example, in 2016 over three million people took the IELTS test (IELTS, 2017) [URL_CITE] ). These tests probe language profi-ciency based on performance on various linguistic tasks, including grammar and vocabulary exams, reading and listening comprehension questions, as well as essay writing and speaking assignments.\nFootnote or Reference text (if exists): IELTS. 2017. International English language testing system \u2014 Wikipedia, the free encyclopedia. On-line; accessed November 2017. https://en.wikipedia.org/wiki/International_English_Language_Testing_System.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://snap.stanford.edu/data,June\nCitation Context: For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014) [URL_CITE] . It has over 7.9 million reviews for 250,000 movies.\nFootnote or Reference text (if exists): Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data,June.\nSection Titles (if exists):  ['4 Human Subject Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: The Fake News challenge (Pomerleau and Rao, 2017) [URL_CITE] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain\nCitation Context: In this section, we describe our key features for fa-cilitating a rapid understanding of linguistic struc-ture. As a companion to this section, we invite readers to use our web demonstration at [URL_CITE] http://hierplane.allenai.org/explain to in-teractively explore examples. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Key Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm\nCitation Context: Excerpt from Text View, with Search and Text Tabs covered by the KB, the user can load natural lan-guage text into SconeEdit from a file or the system clipboard. We take an article from Xinhuanet News Service (Xinhuanet, 2006) [URL_CITE] as an example. Figure 2 shows an excerpt of this text after it has been loaded.\nFootnote or Reference text (if exists): Xinhuanet. 2006. US accused of blocking approval of new UN human rights body. http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm.\nSection Titles (if exists):  ['3 Architecture', '3.1 Text View']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: For instance: [URL_CITE] http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1.\nFootnote or Reference text (if exists): 3 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers [URL_CITE] (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively.\nFootnote or Reference text (if exists): 10 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adalmia96/Cluster-Analysis\nCitation Context: 5 Experimental Setup Our implementation is freely available online. [URL_CITE] 5.1 Datasets We use the 20 newsgroup dataset (20NG) which contains around 18000 documents and 20 cate-gories, 5 and a subset of Reuters21578 6 which con-tains around 10000 documents. 5.2 Evaluation (Topic Coherence) We adopt a standard 60-40 train-test split for 20NG and 70-30 for Reuters.\nFootnote or Reference text (if exists): 4 https://github.com/adalmia96/ Cluster-Analysis\nSection Titles (if exists):  ['4 Computational Complexity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/dataset-factoid-webquestions\nCitation Context: WebQuestions [URL_CITE] (Berant et al., 2013b) is de-signed for answering questions from the Free-base knowledge base, which is built by crawl-ing questions through the Google Suggest API and the paragraphs are retrieved from the English Wikipedia using .\nFootnote or Reference text (if exists): 5 https://github.com/brmson/ dataset-factoid-webquestions\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nCitation Context: Figure 1: RNN (a) and LSTM (b) [URL_CITE]\nFootnote or Reference text (if exists): 1 This figure referred to http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nCitation Context: The full implementation can be found in an online tuto-rial. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nSection Titles (if exists):  ['4 Implementing a game']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [URL_CITE] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models.\nFootnote or Reference text (if exists): 3 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/timvdc/poetry\nCitation Context: In order to facilitate reproduction of the results and encourage further research, the poetry genera-tion system is made available as open source soft-ware. The current version can be downloaded at [URL_CITE] https://github.com/timvdc/poetry.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/\nCitation Context: monolingual word embedding. We adopt a similar model as in cross-domain classi-fication where we use machine translation (Google Translate ) to translate training data from source to target language. In this model, we use pre-trained word embedding from FastText [URL_CITE] .\nFootnote or Reference text (if exists): 6 https://fasttext.cc/\nSection Titles (if exists):  ['5 Cross-lingual Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/data.html\nCitation Context: We first applied BB, MB-B, and MB-D to translation of the English words \u2018line\u2019 and \u2018interest\u2019 using a benchmark data [URL_CITE] . The data mainly consists of articles in the Wall Street Journal and it is designed for conducting Word\nFootnote or Reference text (if exists): 2 http://www.d.umn.edu/~tpederse/data.html.\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.1 Experiment 1: WSD Benchmark Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://conceptnet.io/\nCitation Context: 2. Retrieval (RTRVL): We also experiment with a retrieval approach where we retrieve a VEHICLE from ConceptNet (Speer et al., 2017) having the highest HasProperty rela-tion w.r.t our input (i.e., an adjective or adverb at the end of literal sentence) [URL_CITE] . For the in-put The city was beautiful we query Concept-Net with beautiful, which returns sunset as the VEHICLE having the highest weight for HasProperty beautiful.\nFootnote or Reference text (if exists): 10 ConceptNet is a weighted graph with multiple relations as can be viewed here http://conceptnet.io/. We use \u2018has property\u201d for our work. There are multiple edges for objects with their properties. We choose the edge with the highest weight with the title Sunset and Car Accident. We replace the literal sentences with generated similes from SCOPE.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Baseline Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We also extend the models in Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016) by allowing highway con-nections between LSTM layers.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [URL_CITE] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 5 http://pytorch.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [URL_CITE] as our proposal distribution. As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity de-spite substantially worse performance than RN-NGs in number agreement with multiple attrac-tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success.\nFootnote or Reference text (if exists): 11 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.3 Further Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.allenai.org/paper-appendix/emnlp2016-p3\nCitation Context: Code, data and supplementary material for this paper are available at: [URL_CITE] http://www.allenai.org/paper-appendix/emnlp2016-p3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://tizirinagh.github.io/acl2019/\nCitation Context: We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [URL_CITE] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2.\nFootnote or Reference text (if exists): 3 https://tizirinagh.github.io/acl2019/\nSection Titles (if exists):  ['5 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with [URL_CITE] random initializa-tions. For data preprocessing, we use the tok-enizer 6 from BERT to preprocess the sentences.\nFootnote or Reference text (if exists): 5 Code for metrics: https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2041\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [URL_CITE] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Wenduan Xu, Michael Auli, and Stephen Clark. 2015. CCG supertagging with a recurrent neural network. In ACL. Association for Computational Linguistics, Beijing, China, pages 250\u2013255. http://www.aclweb.org/anthology/P15-2041.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.cam.ac.uk/~ah433/material/herbelot_iwcs13_data.txt\nCitation Context: In order to remedy data sparsity, we consider the use of additional data in the form of the animal dataset from Herbelot (2013) (henceforth AD). AD [URL_CITE] is a set of 72 animal concepts with quantification annotations along 54 features. The main differences between QMR and AD are as follows:\nFootnote or Reference text (if exists): 3 Data available at http://www.cl.cam.ac.uk/\u02dcah433/material/herbelot_iwcs13_data.txt.\nSection Titles (if exists):  ['3 Annotated datasets', '3.2 Additional animal data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://framenet.icsi.berkeley.edu/\nCitation Context: We use FrameNet-style (Fillmore, 1976) se-mantic frames as our main source of weak supervision. [URL_CITE] We also explore named entity recognition (NER).\nFootnote or Reference text (if exists): 1 See http://framenet.icsi.berkeley.edu/\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/sp2016.11-731/tree/master/hw4/data\nCitation Context: Language generation is performed on a dataset comprising simple English sentences referred to as CMU-SE [URL_CITE] (Rajeswar et al., 2017). The CMU-SE dataset consists of 44,016 sentences with a vocabulary of 3,122 words.\nFootnote or Reference text (if exists): 2 https://github.com/clab/sp2016.11-731/tree/master/hw4/data\nSection Titles (if exists):  ['4 Results and Discussion', '4.2 Language Generation.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/zrRIBxMUYJ\nCitation Context: Tweet Officer Wilson will be returned to active duty if no indictment, says #Ferguson Police Chief [URL_CITE] http://t.co/zrRIBxMUYJ\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [URL_CITE] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/data/glove.840B.300d.zip\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [URL_CITE] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia.\nFootnote or Reference text (if exists): 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)', '3.1 Inducing Crosslingually Constrained Word Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2005T23\nCitation Context: To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. [URL_CITE] We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2005T23\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politicalforum.com\nCitation Context: We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [URL_CITE] . All the posts are in English.\nFootnote or Reference text (if exists): 2 http://www.politicalforum.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl [URL_CITE] as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task.\nFootnote or Reference text (if exists): 6 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: As Kernel Methods are the state-of-the-art su-pervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to per-form all our experiments, in particular Support Vector Machines [URL_CITE] . Taking into account the exter-nal knowledge provided by a MDM it is possible estimate the topic similarity among two texts ex-pressed in different languages, with the following kernel: where D is defined as in equation 1.\nFootnote or Reference text (if exists): 3 We adopted the efficient implementation freely available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['3 Exploiting Comparable Corpora', '3.1 Multilingual Domain Model', 'w il evaluated in the corpus T l .']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Victor0118/cross_domain_embedding/\nCitation Context: Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code and left unchanged in RegFreq [URL_CITE] (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly.\nFootnote or Reference text (if exists): 8 https://github.com/Victor0118/cross_domain_embedding/\nSection Titles (if exists):  ['Topics and tasks', '4.2 Epochs vs. regularization results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/conll/software.html\nCitation Context: Parsing Quality The parsing quality was evalu-ated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script [URL_CITE] . Due to this conversion, the accuracy on the relation labels is less important.\nFootnote or Reference text (if exists): 6 http://ilk.uvt.nl/conll/software.html\nSection Titles (if exists):  ['6 Evaluation', '6.3 Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [URL_CITE] framework.\nFootnote or Reference text (if exists): 1 https://pytorch.org/\nSection Titles (if exists):  ['3 Methodology', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.conexor.com\nCitation Context: We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004) [URL_CITE] to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences.\nFootnote or Reference text (if exists): Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com\nSection Titles (if exists):  ['2. Information for Term Specificity', '2.2. Contextual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Using all available features, we compare four MAL-LET (McCallum, 2002) [URL_CITE] classifiers: logistic regres-sion, na\u0131\u0308ve Bayes, decision tree, and Winnow2. Fig-ure 1 shows the learning curves for each classifier as the number of sampled words comprising each train-ing example is varied from 10 to 1000.\nFootnote or Reference text (if exists): Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Word-level Language Classification', '4.2 Classifiers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mturk.com\nCitation Context: In order to study how entrainment in various dimen-sions correlated with perceived social behaviors of our subjects, we asked Amazon Mechanical Turk [URL_CITE] annotators to label the 168 Objects games in our cor-pus for an array of social behaviors perceived for each of the speakers, which we term here \u201csocial variables.\u201d\nFootnote or Reference text (if exists): 1 http://www.mturk.com\nSection Titles (if exists):  ['3 Annotation of Social Variables']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lifengjin/dimi_emnlp18\nCitation Context: Specifically, \u03c4each tree is a set {\u03c4 , \u03c4 , \u03c4 [URL_CITE] , \u03c4 11 , \u03c4 12 , \u03c4 21 , ...} of category node labels \u03c4 \u03b7 where \u03b7 \u2208 {1,2} \u2217 defines a path of left or right branches from the root to that node. Category labels for every pair of left and right children \u03c4 \u03b71 ,\u03c4 \u03b72 are drawn from a multinomial distribution defined by the grammar G and the category of the parent \u03c4 \u03b7 : where \u03b4 x is a Kronecker delta function equal to 1 at value x and 0 elsewhere, and terminals have null expansions P G (a b | w) = P G (a b | \u22a5) = ~a, b=\u22a5, \u22a5\u007f for w \u2208 W. 4\nFootnote or Reference text (if exists): 2 https://github.com/lifengjin/dimi_emnlp18.\nSection Titles (if exists):  ['5 Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.utexas.edu/ml/nldata/jobquery.html\nCitation Context: We directly use Prolog logical forms provided on the web-site. [URL_CITE] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance.\nFootnote or Reference text (if exists): 9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"\nSection Titles (if exists): [\\'-\\', \\'A.1 Details of Benchmark Construction\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/Downloads2015-04\nCitation Context: \u2022 TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen-tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus [URL_CITE] to im-prove the original algorithm.\nFootnote or Reference text (if exists): 4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04.\nSection Titles (if exists):  ['4 Evaluation', '4.3 End-to-End Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jsplumb.org\nCitation Context: Since the actual positions of mention span el-ements on the user\u2019s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb [URL_CITE] , a highly cus-tomizable library for line drawing in HTML doc-uments.\nFootnote or Reference text (if exists): 10 http://www.jsplumb.org\nSection Titles (if exists):  ['4 Implementation', '4.1 Design Decisions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis).\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['3 Defining Stability']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0460084/data/\nCitation Context: We also present results on Clarke and Lapata\u2019s (2006a) Broadcast News corpus. [URL_CITE] This corpus was created manually (annotators were asked to produce com-pressions for 50 Broadcast news stories) and poses more of a challenge than Ziff-Davis. Being a speech corpus, it often contains incomplete and ungram-matical utterances and speech artefacts such as dis-fluencies, false starts and hesitations.\nFootnote or Reference text (if exists): 4 The corpus can be downloaded from http://homepages.inf.ed.ac.uk/s0460084/data/.\nSection Titles (if exists):  ['4 Evaluation Set-up']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Ecco is open-source software [URL_CITE] and contributions are welcome. Acknowledgments This work was improved thanks to feedback pro-vided by Abdullah Almaatouq, Anfal Alatawi, Christopher Olah, Fahd Alhazmi, Hadeel Al-Negheimish, Hend Al-Khalifa, Isabelle Augen-stein, Jasmijn Bastings, Najwa Alghamdi, Pepa Atanasova, and Sebastian Gehrmann.\nFootnote or Reference text (if exists): 5 https://github.com/jalammar/ecco\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Document)\nFUNCTION: Introduce, Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://simple.wikipedia.org\nCitation Context: Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [URL_CITE] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each.\nFootnote or Reference text (if exists): 10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com\nSection Titles (if exists):  ['5 Experiment Details', 'Details of Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/software/sutime.shtml\nCitation Context: Temporal expressions are automatically identified with SU-Time (Chang and Manning, 2012), an off-the-shelf temporal expression detector. [URL_CITE] We keep only the dialogs with more than 3 temporal expressions and at least one expression that contains numerals like \u201ctwo weeks\u201d (as opposed to non-numeric spans, like \u201csummer\u201d, \u201cright now\u201d, and \u201clater\u201d). In our initial experiment, we observe that language models can often correctly predict these non-numerical tempo-ral phrases.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/software/sutime.shtml\nSection Titles (if exists):  ['3 Dataset: T IME D IAL', '3.1 Data Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://onlineslangdictionary.com/word-list/\nCitation Context: Table 5: ACS Sum Results of Slang Translation list from OnlineSlangDictionary [URL_CITE] with explana-tions and downsample the list to 200 terms.\nFootnote or Reference text (if exists): 9 http://onlineslangdictionary.com/word-list/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.1 Ground Truth']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wordvectors.org/\nCitation Context: We use the datasets summarized by Faruqui and Dyer (Faruqui and Dyer, 2014): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999. [URL_CITE] We compute the similarities between pairs of words and check the Spearman\u2019s rank correlation coeffi-cient (Myers and Well., 1995) between the com-puter and the human labeled ranks.\nFootnote or Reference text (if exists): 2 http://www.wordvectors.org/\nSection Titles (if exists):  ['4 Experiments on Word/Phrase Similarity', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lexalytics.com/webdemo\nCitation Context: In the internet, we can find many systems and companies related with sentiment analysis. For example, the company Lexalytics has in its website an available demo [URL_CITE] for sentiment detection. This demo shows an interface which highlights positive and negative words in the text.\nFootnote or Reference text (if exists): 1 http://www.lexalytics.com/webdemo\nSection Titles (if exists):  ['2 Analyzing Sentiment in Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) [URL_CITE] . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 3 https://www.statmt.org/wmt14/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.org\nCitation Context: The UD (universal dependencies) project [URL_CITE] releases a more detailed language-generic guideline to facilitate cross-linguistically consistent annotation, containing 37 relation labels. However, after in-depth study, we find that the UD guideline is very useful and comprehensive, but may not be completely compact for realistic annotation of Chinese-specific syntax.\nFootnote or Reference text (if exists): 3 http://universaldependencies.org\nSection Titles (if exists):  ['2 Annotation of Bi-tree Aligned Data', '2.1 Data Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora and the StackExchange [URL_CITE] family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 3 http://stackexchange.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gutenberg.org\nCitation Context: We examine the proposed priors using three commonly sized English corpora for textual anal-ysis within CSSDH: the top 100 list of books in Project Gutenberg (2019) [URL_CITE] , a sample from Twitter (Go et al., 2009) and the U.S. Congress Speeches 1981-2016 (Gentzkow et al., 2018). The num-ber of tokens ranges from 1.8M to 40M after pre-processing (see Sec.\nFootnote or Reference text (if exists): Project Gutenberg. 2019. http://www.gutenberg.org. Accessed: 2019-04-24.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scopesys.com/\nCitation Context: This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [URL_CITE] . Gazetters have been widely used in NER systems to improve performance.\nFootnote or Reference text (if exists): 5 http://www.scopesys.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.1 Internal Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017), and Fried-man et al. (2017) [URL_CITE] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:Requests_for_adminship\nCitation Context: Ad-mins have a higher status than common editors (non-admins), and this distinction seems to be widely acknowledged by the community (Burke and Kraut, 2008b; Leskovec et al., 2010; Danescu-Niculescu-Mizil et al., 2012). Aspiring editors become admins through public elections, [URL_CITE] so we know when the status change from non-admin to admins occurred and can study users\u2019 language use in relation to that time.\nFootnote or Reference text (if exists): 8 http://en.wikipedia.org/wiki/ Wikipedia:Requests_for_adminship\nSection Titles (if exists):  ['5 Relation to social factors', '5.1 Relation to social outcome']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nCitation Context: To identify the replaceable entities, we apply named entity recognition (NER) and POS tag-ging using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [URL_CITE] . This improves the likelihood that the terms are familiar to both headline editors and humor judges.\nFootnote or Reference text (if exists): 1 http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nSection Titles (if exists):  ['2 The Humor Dataset', '2.1 Task Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at [URL_CITE] https://aka.ms/KaggleDBQA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion & Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/amoudgl/short-jokes-dataset\nCitation Context: Formality NTERPERS GYAFC (Rao and Tetreault, 2018) 224k given 2 formal (50%), informal (50%) Y web N clsf. Politeness I StanfPolite (Danescu et al., 2013) 10k given 2 polite (49.6%), impolite (50.3%) Humor ShortHumor (CrowdTruth, 2016) 44k random 2 humor (50%), non-humor (50%) ShortJoke (Moudgil, 2017) [URL_CITE] 463k random 2 humor (50%), non-humor (50%) IGURATIVE Sarcasm SarcGhosh (Ghosh and Veale, 2016) 43k given 2 sarcastic (45%), non-sarcastic (55%) EmoBank valence (Buechel and Hahn, 2017) 10k random 1 negative, positive EmoBank arousal (Buechel and Hahn, 2017) 10k random 1 calm, excited EmoBank dominance (Buechel and Hahn, 2017) 10k random 1 being_controlled, being_in_control FFECTIVE DailyDialog (Li et al., 2017) 102k given 7 noemotion(83%), happy(12%).. Offense HateOffensive (Davidson et al., 2017) 24k given 3 hate(6.8%), offensive(76.3%).. A Romance ShortRomance 2k random 2 romantic (50%), non-romantic (50%) Sentiment SentiBank (Socher et al., 2013) 239k given 2 positive (54.6%), negative (45.4%) Gender PASTEL (Kang et al., 2019) 41k given 3 Female (61.2%), Male (38.0%).. 41k given 8 35-44 (15.3%), 25-34 (42.1%).. ERSONAL Age PASTEL (Kang et al., 2019)\nFootnote or Reference text (if exists): Abhinav Moudgil. 2017. short jokes dataset. https://github.com/amoudgl/short-jokes-dataset. [On-line; accessed 1-Oct-2019].\nSection Titles (if exists):  ['2 Related Work', '3.1 Style selection and groupings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sodawater/SentenceOrdering\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset [URL_CITE] (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets.\nFootnote or Reference text (if exists): 4 https://github.com/sodawater/SentenceOrdering\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.org/download/\nCitation Context: ; \u03c6). All intermediate pre-training is done with input sequence length of 128, batch size of 2048, learning rate of 0.0001, up to a total num-ber of 100, 000 updates, using Wikipedia snapshot from December 20, 2018 [URL_CITE] .\nFootnote or Reference text (if exists): 1 The snapshot is available at https://archive.org/download/enwiki-20181220. Wikipedia is licensed under CC BY-SA 3.0.\nSection Titles (if exists):  ['3 Analysis Setup', '3.1 Experiment Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/research/umls/\nCitation Context: Clinical narratives, unlike newswire data, provide a domain with significant knowledge that can be ex-ploited systematically to improve the accuracy of the prediction task. Knowledge in this domain can be thought of as belonging to two categories: (1) Background Knowledge captured in medical ontolo-gies like UMLS (Url1, 2013) [URL_CITE] , MeSH and SNOMED CT and (2) Discourse Knowledge driven by the fact that the narratives adhere to a specific writing style. While the former can be used by generating more expressive knowledge-rich features, the lat-ter is more interesting from our current perspective, since it provides global constraints on what output structures are likely and what are not.\nFootnote or Reference text (if exists): Url1. 2013. Umls: Unified medical language system (http://www.nlm.nih.gov/research/umls/) (ac-cessed july 1, 2013).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: [URL_CITE] https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JiaweiSheng/FAAN\nCitation Context: Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [URL_CITE] https://github.com/JiaweiSheng/FAAN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wangcunxiang/Sen-Making-and-Explanation\nCitation Context: Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [URL_CITE] https://github.com/wangcunxiang/Sen-Making-and-Explanation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlpviz.bpodgursky.com\nCitation Context: Figure 1 was generated from (Podgursky, 2015) [URL_CITE] , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom).\nFootnote or Reference text (if exists): Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/\nCitation Context: To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [URL_CITE] . We split the origi-nal training set into a dev set (top 1,000 sent.)\nFootnote or Reference text (if exists): 4 http://www.cnts.ua.ac.be/conll2000/chunking/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: The third dataset (wikipedia, henceforth) comes from the Wikipedia [URL_CITE] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it.\nFootnote or Reference text (if exists): 4 http://www.wikipedia.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/qingsongma/percentage-refBias\nCitation Context: Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [URL_CITE] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME \u00d7 DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences.\nFootnote or Reference text (if exists): 7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias\nSection Titles (if exists):  ['2 Background', '2.1 Measures of Central Tendency']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gnu.org/s/wdiff/\nCitation Context: The HOO test data consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff [URL_CITE] to extract system edits. We obtain each system\u2019s output and segment it at the sentence level accord-ing to the gold standard sentence segmentation.\nFootnote or Reference text (if exists): 4 http://www.gnu.org/s/wdiff/\nSection Titles (if exists):  ['3 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.2200/S00429ED1V01Y201207AIM018\nCitation Context: Finally, this work also relates to the problem of active learning, where the goal is to interac-tively choose instances to be labeled, in a way that maximizes accuracy while minimizing the to-tal cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012 [URL_CITE] ). This is an interesting area that might be productively combined with the ideas in this paper. In general, however, the use of active learning involves additional logistical complica-tions and does not always work better than ran-dom sampling in practice (Attenberg and Provost, 2011).\nFootnote or Reference text (if exists): Burr Settles. 2012. Active Learning. Morgan & Claypool. https://doi.org/10.2200/S00429ED1V01Y201207AIM018.\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/\nCitation Context: All data is freely available for public use under a Creative Commons Sharealike license and is avail-able at: [URL_CITE] https://github.com/dbamman/ litbank; code to support this work can be found at: https://github.com/dbamman/NAACL2019-literary-entities.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.elastic.co/products/elasticsearch\nCitation Context: ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [URL_CITE] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers.\nFootnote or Reference text (if exists): 3 https://www.elastic.co/products/ elasticsearch\nSection Titles (if exists):  ['2 System Architecture and Components', '2.1 System Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/and\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wiktionary.org\nCitation Context: Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary [URL_CITE] , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al.\nFootnote or Reference text (if exists): 6 https://en.wiktionary.org\nSection Titles (if exists):  ['2 MWE-aware Dependency Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/\nCitation Context: Wei Lu is an Assistant Professor at the Sin-gapore University of Technology and Design (SUTD), directing the StatNLP research group ( [URL_CITE] http://statnlp.org/). He received his Ph.D. from the National University of Singapore (NUS) in 2009.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['About the Instructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Baroni et al. develop the word embeddings using word2vec [URL_CITE] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013).\nFootnote or Reference text (if exists): 6 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['4 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: We also thank the students of the University of Pennsylvania\u2019s crowdsourcing class (NETS 213) for their involvement in building and testing a useful crowdsourcing pipeline for in-formation extraction. Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [URL_CITE] http://gun-violence.org/. Amit Singhal.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://msdn.microsoft.com/en-us/library/hh361572\nCitation Context: Several commercial speech recognition systems offer high-level Application Programming Inter-faces (APIs) that make it extremely simple to add voice interfaces to an application, requiring very little general technical expertise and virtually no knowledge of the inner workings of the recogni-tion engine. If the target language is supported by the system \u2013 the Microsoft Speech Platform, [URL_CITE] for example, supports over 20 languages \u2013 this makes it very easy to create speech-driven applications.\nFootnote or Reference text (if exists): 2 http://msdn.microsoft.com/en-us/library/hh361572\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools [URL_CITE] ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Leven-shtein, 1966) over tokens (for a total of 49,208 sen-tences and 1,173,750 gold-standard tokens).\nFootnote or Reference text (if exists): 5 See http://nlp.stanford.edu/software/corenlp.shtml, run in \u2018strictTreebank3\u2019 mode.\nSection Titles (if exists):  ['3 A Contrastive Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html\nCitation Context: We use an array of features in CRFs which are ei-ther derived or borrowed from the taxonomy that a Japanese tokenizer called JUMAN and KNP , [URL_CITE] a Japanese dependency parser (aka Kurohashi-Nagao Parser), make use of in characterizing the output they produce: both JUMAN and KNP are part of the compression model we build.\nFootnote or Reference text (if exists): 6 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html\nSection Titles (if exists):  ['3 Features in CRFs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [URL_CITE] . For Twitter and Weibo datasets, we pre-train embeddings on large-scale external data with 99M tweets and 467M Weibo messages, respectively.\nFootnote or Reference text (if exists): 11 http://nlp.stanford.edu/data/glove.6B.zip (200d)\nSection Titles (if exists):  ['3 Experiment Setup', '3.2 Model Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub [URL_CITE] . Detailed documentation and tutorials are available on Hugging Face\u2019s website .\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/segmenter.shtml\nCitation Context: For a given full form, we first segment it us-ing a standard word segmenter to get a coarse-grained segmentation result. Here we use the Stan-ford Chinese Word Segmenter [URL_CITE] . Then we use the MSU set to segment each word using the strategy of \u201cMaximum Forward Matching\u201d to get the fine-grained MSU segmentation result.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/software/segmenter.shtml\nSection Titles (if exists):  ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.1 Minimum Semantic Unit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://demo.patrickpantel.com/\nCitation Context: The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error \u201casteroid\u201d, a distributional similarity thesaurus such as (Lin 1998) [URL_CITE] can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are also similar to Earth .\nFootnote or Reference text (if exists): 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://rnnlm.org\nCitation Context: In-put bytes were encoded using one-hot representa-tion. We modified the RNNLM toolkit (Mikolov, 2012a) [URL_CITE] to record the activations of the hidden layer and ran it with the default learning rate schedule. Given that training SRNs on large amounts of text takes a considerable amount of time we did not vary the size of the hidden layer.\nFootnote or Reference text (if exists): Toma\u0301s\u030c Mikolov. 2012a. Recurrent neural network lan-guage models. http://rnnlm.org.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Unlabeled tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://edition.cnn.com/specials/world/fast-facts\nCitation Context: Ground-Truth Timelines: We obtain ground-truth timelines from CNN Fast Facts [URL_CITE] , which has a collection of several hundred timelines grouped in categories, e.g., \u2018people\u2019 or \u2018disasters\u2019. We pick all timelines of the \u2018people\u2019 category and a small number from other categories.\nFootnote or Reference text (if exists): 6 http://edition.cnn.com/specials/world/fast-facts\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/BLINK\nCitation Context: We don\u2019t evaluate multi-hop specific baselines on KILT but the current best F1-score for HotpotQA is 75.43 according to the official lead-earboard , that is quite far from what achieved by our general solutions. BLINK results are in line with what reported in the GitHub repository [URL_CITE] for all three entity linking datasets. The Tranformer MemNet of Dinan et al.\nFootnote or Reference text (if exists): 20 https://github.com/facebookresearch/ BLINK\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE [URL_CITE] and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 2 https://lucene.apache.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/reggaeton.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: A i and B j denote the word vector of the i th word in A and j th word in B respec-tively. The embeddings used in ACS computations are pre-trained GloVe word vectors [URL_CITE] and thus the computation is fair among different methods.\nFootnote or Reference text (if exists): 13 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.3 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jmcauley.ucsd.edu/data/amazon/\nCitation Context: Besides the above datasets, we also use a Yelp dataset and an Amazon Electronics dataset (He and McAuley, 2016) [URL_CITE] as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings.\nFootnote or Reference text (if exists): 2 http://jmcauley.ucsd.edu/data/amazon/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ankara.lti.cs.cmu.edu/side/video.swf\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the word-pair embeddings (Section 3.1), which is our pro-posed method. The word embeddings in the def-inition encoder were initialized by the Google Word2Vec vectors [URL_CITE] . Google Word2Vec vectors and GloVe were used as the other baselines.\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://timex2.mitre.org/tern.html\nCitation Context: Time chunk label: The value of this feature is a B-I-O label indicating whether or not this word is contained in a temporal annotation. The temporal annotations are produced by a word-chunking SVM-based system trained on the temporal ex-pressions (TIMEX2 annotations) in the TERN 2004 data [URL_CITE] . In addition to identifying expres-sions like Monday and this year, the TERN data identifies event-containing expressions like the time she arrived at her doctor's office.\nFootnote or Reference text (if exists): 4 http://timex2.mitre.org/tern.html\nSection Titles (if exists):  ['5 Classifier Features', '5.6 Temporal features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\nCitation Context: We selected the Text Categorization task on the RCV1 dataset (Lewis et al., 2004) with the setting that can be found on the LibLinear website [URL_CITE] . In this version of the dataset, CCAT and ECAT are collapsed into a positive class, while GCAT and MCAT are the negative class, resulting in a dataset composed by 20, 242 examples. As shown in Ta-ble 2, we applied the LibLinear, Pegasos and Lin-ear Passive-Aggressive implementations, comput-ing the accuracy and the standard deviation with respect to a 5-fold cross validation strategy.\nFootnote or Reference text (if exists): 10 http://www.csie.ntu.edu.tw/\u223ccjlin/libsvmtools/datasets/\nSection Titles (if exists):  ['3 Case Studies in NLP', '3.2 Text Categorization']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/chakki-works/seqeval\nCitation Context: The metrics were calculated using seqeval (Nakayama, 2018) [URL_CITE] . \u2191 (\u2193) indicate whether the higher (or lower) the better from the attack perspective.\nFootnote or Reference text (if exists): Hiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.\nSection Titles (if exists):  ['4 Experiments', '4.2 Adversarial training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Oneplus/tamr\nCitation Context: Our code and the alignments for LDC2014T12 dataset are publicly available at [URL_CITE] https://github.com/Oneplus/tamr\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlds.soe.ucsc.edu/sarcasm1\nCitation Context: \u2022 Debates - We use two datasets [URL_CITE] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets.\nFootnote or Reference text (if exists): 4 https://nlds.soe.ucsc.edu/sarcasm1\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/tkbc\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE , TNTComplEx [URL_CITE] , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 6 https://github.com/facebookresearch/tkbc\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freebase.com/\nCitation Context: Given a structured knowledge base, such as Free-base [URL_CITE] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel. Then we can get hQ,reli pairs.\nFootnote or Reference text (if exists): 5 http://www.freebase.com/\nSection Titles (if exists):  ['4 Response Ranking', '4.5 Relation-level Feature']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.urbandictionary.com/define.php?term=Hoe\nCitation Context: The second example shows, instead, a problem related to context and disambiguation issues. In-deed, the word \u201choe\u201d here is used informally in its derogatory sense, meaning \u201cA woman who engages in sexual intercourse for money\u201d (syn-onyms: slut, whore, floozy) [URL_CITE] . But, disregarding the context, it is translated in Spanish by relying on a different conventional meaning (hoe as agri-cultural and horticultural hand tool).\nFootnote or Reference text (if exists): 8 https://www.urbandictionary.com/define.php?term=Hoe\nSection Titles (if exists):  ['6 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We ran initial experiments with exemplar-based clas-sification and experimented with different cluster (Word2Vec) and topic (LDA) sizes of 100, 500, 1000, 2000 on the training set. For LDA (McCal-lum, 2002) [URL_CITE] , we also varied the threshold to assign a topic only to a text when it covered 1%, 2%, or 5% of the text. The best results were obtained with 100 topics and a proportion threshold of 1%.\nFootnote or Reference text (if exists): Andrew McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPIR-team/NLPIR\nCitation Context: If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [URL_CITE] messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).\nFootnote or Reference text (if exists): 10 https://github.com/NLPIR-team/NLPIR\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ruidan/IMN-E2E-ABSA\nCitation Context: We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [URL_CITE] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models.\nFootnote or Reference text (if exists): 6 IMN code available at https://github.com/ruidan/IMN-E2E-ABSA.\nSection Titles (if exists):  ['4 Modeling', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/europarl/\nCitation Context: We took the 1996 documents from the Europarl cor-pus (Koehn, 2005) [URL_CITE] using three bridges: GermaNet, dictionary, and the uninformative flat matching. The model is unaware that the translations of documents in one language are present in the other language.\nFootnote or Reference text (if exists): Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit. http://www.statmt.org/europarl/.\nSection Titles (if exists):  ['4 Experiments', '4.1 Matching on Multilingual Topics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nCitation Context: We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [URL_CITE] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1).\nFootnote or Reference text (if exists): 4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nSection Titles (if exists):  ['4 Experiments', '4.4 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://new.qq.com/cmsn/20140605/20140605002796\nCitation Context: This event is about a Child drew all over his dads passport and made his dad stuck in South Korea. During the manual annota-tion process, we found out that it is a real event confirmed by official accounts according to one news article from Chinese social media [URL_CITE] , while CCMR Twitter labeled such tweets as fake. Since Transfer is pre-trained using Twitter dataset, it is not surprising that Transfer achieves 0 in F1-score on this event.\nFootnote or Reference text (if exists): 3 http://new.qq.com/cmsn/20140605/20140605002796\nSection Titles (if exists):  ['8 Low-resource Rumor Verification via Transfer Learning', '8.2 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.comp.nus.edu.sg/software/\nCitation Context: Experi-ments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M 2 scorer is available for download at [URL_CITE] http://nlp.comp.nus.edu.sg/software/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andy194673/nlg-sclstm-multiwoz\nCitation Context: The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [URL_CITE]\nFootnote or Reference text (if exists): 6 The model is publicly available at https://github.com/andy194673/ nlg-sclstm-multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.3 Dialogue-Act-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: The parser is fast enough for large-scale appli-cation to unrestricted texts, and it delivers depen-dency relations which are a suitable base for a range of applications. We have used it to parse the entire 100 million words British National Corpus ( [URL_CITE] http://www.natcorp.ox.ac.uk) and similar amounts of biomedical texts. Its parsing speed is about 500,000 words per hour.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Pro3Gres and its Design Policy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/qa.html\nCitation Context: We are now studying open-domain question an-swering systems [URL_CITE] , and we expect QA systems to benefit from anaphora resolution. Typical QA sys-tems try to answer a user\u2019s question by finding rel-evant phrases from large corpora.\nFootnote or Reference text (if exists): 1 http://trec.nist.gov/data/qa.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3\nCitation Context: To make the disambiguation task more challenging, we also ensure that at least one other distinct personal name is present in the text in a position before the masked occurrence. We instantiate our method on English Wikipedia and generate the Wikipedia Co-REferences Masked (W IKI CREM) dataset with 2.4M examples, which we make publicly available for further usage [URL_CITE] . We show its value by using it to fine-tune the B ERT language model (Devlin et al., 2018) for pronoun resolution.\nFootnote or Reference text (if exists): 1 The code can be found at https://github.com/vid-koci/bert-commonsense. The dataset and the models can be obtained from https://ora.ox.ac.uk/objects/uuid: c83e94bb-7584-41a1-aef9-85b0e764d9e3\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sary.sourceforge.net\nCitation Context: The system was implemented in Java, however it handled the suffix arrays through an external C li-brary called Sary. [URL_CITE] All experiments were conducted on a 2 GHz Core2Duo T7200 machine with 2 GB RAM.\nFootnote or Reference text (if exists): 1 http://sary.sourceforge.net\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/AIPHES/\nCitation Context: We release our metrics under the name XMover-Score publicly: [URL_CITE] https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~cyl/ROUGE/\nCitation Context: Table 2 shows the results obtained on these two data sets for different graph settings. The table also lists baseline results, obtained on summaries generated by 1 R OUGE is available at [URL_CITE] http://www.isi.edu/\u02dccyl/ROUGE/. taking the first sentences in each document.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Evaluation', '-', 'the DUC 2002 document summarization evaluations.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://osf.io/z89vn/\nCitation Context: Results on reading and fMRI measures show substantial generalization im-provements from CDRNN over baselines, along with detailed insights about the underlying dynam-ics that cannot easily be obtained from existing methods. [URL_CITE]\nFootnote or Reference text (if exists): 1 Because of page constraints, additional replication details and synthetic results are provided in an external supplement, available here: https://osf.io/z89vn/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [URL_CITE] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: The Amazon dataset contains 2000 samples for each of the four domains: book, DVD, electron-ics, and kitchen, with binary labels (positive, neg-ative). Following Wu and Huang (2015) [URL_CITE] , we con-duct 5-way cross validation. Three out of the five folds are treated as the training set, one serves as the validation set, while the remaining is the test set.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multi-Domain Text Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: Answers dataset from Yahoo! Labs Webscope [URL_CITE] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc.\nFootnote or Reference text (if exists): 11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/tpu\nCitation Context: Using the proposed T IME D IAL challenge set, we next conduct extensive experiments and analyses on the different model variants and context settings. We use either 4x4 or 8x8 Cloud TPUs V3 pod slices [URL_CITE] for fine-tuning and one V100 GPU for infer-ence. We provide more details of the experiment configurations in the appendix.\nFootnote or Reference text (if exists): 5 https://cloud.google.com/tpu\nSection Titles (if exists):  ['5 Experiments and Analyses']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.replysms.com/\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS [URL_CITE] and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service .\nFootnote or Reference text (if exists): 7 http://www.replysms.com/\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=rke2P1BFwS\nCitation Context: We take three static KG models as the static baselines, including Distmult (Yang et al., 2014), TuckER (Balazevic et al., 2019), and COMPGCN (Vashishth et al., 2019). For tKG baselines, we report the performance of TTransE (Leblay and Chekol, 2018), TA-Distmult (Garc\u00eda-Dur\u00e1n et al., 2018), CyGNet (Zhu et al., 2020), DE-SimplE (Goel et al., 2020), TNTComplEx (Lacroix et al., 2020) [URL_CITE] , and RE-Net (Jin et al., 2019). We provide implementation details of baselines and TANGO in Appendix C.\nFootnote or Reference text (if exists): Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.\nSection Titles (if exists):  ['4 Experiments', '4.1.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.delph-in.net/crawler/\nCitation Context: All numbers reported here reflect this frozen system. [URL_CITE]\nFootnote or Reference text (if exists): 10 The code and data are available from http://www.delph-in.net/crawler/ , for replicability (Fokkens et al., 2013).\nSection Titles (if exists):  ['4 Experiments', '4.1 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, DataSource)\nFUNCTION: Use, Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [URL_CITE] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.paracrawl.eu/\nCitation Context: Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [URL_CITE] We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English.\nFootnote or Reference text (if exists): 4 https://www.paracrawl.eu/\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data and Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TemporalKGTeam/TANGO\nCitation Context: We evaluate our model by performing future link prediction on five tKG datasets [URL_CITE] . We compare TANGO\u2019s performance with several existing meth-ods and evaluate its potential with inductive link prediction and long horizontal link forecasting.\nFootnote or Reference text (if exists): 1 Code and datasets are available at https://github.com/TemporalKGTeam/TANGO. filtered MRR (%) on ICEWS05-15 with regard to dif-ferent \u2206t. preprint arXiv:2012.08492.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xinyadu/nqg\nCitation Context: The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) and (Du et al., 2017) [URL_CITE] . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%).\nFootnote or Reference text (if exists): 3 https://github.com/xinyadu/nqg\nSection Titles (if exists):  ['3 Experimental Setting', '3.1 Dataset & Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/QA/QC/\nCitation Context: We call this QClassification. Next, we also use a question/answer classification [URL_CITE] from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought.\nFootnote or Reference text (if exists): 7 http://cogcomp.cs.illinois.edu/Data/QA/QC/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pilehvar.github.io/wic/\nCitation Context: To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno-tations curated by experts, for generic evalua-tion of context-sensitive representations. WiC is released in [URL_CITE] https://pilehvar.github.io/wic/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [URL_CITE] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Method and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ehealthmd.com/library/acltears\nCitation Context: Second, the method feeds them to the machine learner, and the classification program can deter-mine the correct definition according to the context information around the acronym in question. [URL_CITE] http://www.ehealthmd.com/library/acltears She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.playframework.com\nCitation Context: This back-end is implemented as a RESTful API using the Play framework. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://www.playframework.com\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC [URL_CITE] corpus and com-pared against the same baseline system as for Japanese-English. The corpora contain 3M parallel sentences for Japanese-English and 670K for Japanese-Chinese.\nFootnote or Reference text (if exists): 5 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss\nCitation Context: In the experiment, we have explored two cases for the ReWE loss : the minimum square error (MSE) [URL_CITE] and the cosine embedding loss (CEL) . Finally, the NLL loss and the ReW E loss are com-bined to form the training objective using a posi-tive trade-off coefficient, \u03bb:\nFootnote or Reference text (if exists): 1 https://pytorch.org/docs/stable/nn.html#torch.nn. MSELoss\nSection Titles (if exists):  ['3 Model', '3.2 ReWE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~nbach/papers/\nCitation Context: Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007) [URL_CITE] .\nFootnote or Reference text (if exists): Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. Unpublished. http: //www.cs.cmu.edu/\u02dcnbach/papers/ A-survey-on-Relation-Extraction. pdf.\nSection Titles (if exists):  ['9 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lynx.invisible-island.net\nCitation Context: Some Amazon reviews contain HTML markup. We used Lynx [URL_CITE] to render the reviews as UTF-8 plain-text.\nFootnote or Reference text (if exists): 3 https://lynx.invisible-island.net\nSection Titles (if exists):  ['2 Data preparation', '2.2 Data Processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://newsela.com\nCitation Context: Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [URL_CITE] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each.\nFootnote or Reference text (if exists): 10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com\nSection Titles (if exists):  ['5 Experiment Details', 'Details of Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gluebenchmark.com/\nCitation Context: Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark [URL_CITE] . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA).\nFootnote or Reference text (if exists): 10 https://gluebenchmark.com/\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.globalWordnet.org/gwa/Wordnet_table.html\nCitation Context: It is also free. Wordnets in many languages are being constructed and developed [URL_CITE] . However, only a few of these Wordnets are of high quality and free for downloading.\nFootnote or Reference text (if exists): 2 http://www.globalWordnet.org/gwa/Wordnet_table.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Publicly available Wordnets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/ACE/\nCitation Context: Our proposed graph-based method is evaluated on the ACE corpus [URL_CITE] , which contains 519 files from sources including broadcast, newswire, and news-paper. A break-down of the tagged data by different relation subtypes is given in Table 1.\nFootnote or Reference text (if exists): 1 http://www.ldc.upenn.edu/Projects/ACE/\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/penumbra\nCitation Context: In contrast, the use of penumbra in (2) is more creative and novel. Its literal meaning is \u201can area covered by the outer part of a shadow.\u201d [URL_CITE] Its metaphoric meaning is seldom encountered: Shadows follow objects that cast them, and espe-cially penumbras can be perceived as having fuzzy outlines; attributes which are picked up by the metaphorical sense of a rather unspecified group of people following someone in differing vicinity.\nFootnote or Reference text (if exists): 1 https://www.macmillandictionary.com/dictionary/american/penumbra\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Define"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-cleaning\nCitation Context: This is enabled by a set of regular expres-sions released by Dus\u030cek et al. (2019a) [URL_CITE] . The reg-ular expressions capture the entire phrase used to express an attribute-value pair, focusing on the con-tent words and attempting as much as possible to leave out the function words, e.g.\nFootnote or Reference text (if exists): 1 https://github.com/tuetschek/ e2e-cleaning\nSection Titles (if exists):  ['2 Frequency of Surface Forms']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.google.com\nCitation Context: In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google [URL_CITE] search engine for simple text queries to define various similarity scores.\nFootnote or Reference text (if exists): 3 http://www.google.com\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/HeidelTime/heideltime\nCitation Context: We obtain these from (i) the publication dates of all articles in A and (ii) tex-tual references of dates in sentences in A, such as \u2019last Monday\u2019, or \u201912 April\u2019. We use the tool Hei-delTime [URL_CITE] (Stro\u0308tgen and Gertz, 2013) to detect and resolve textual mentions of dates. Date Selection Next, we select the l most important dates.\nFootnote or Reference text (if exists): 2 https://github.com/HeidelTime/ heideltime\nSection Titles (if exists):  ['3 Strategies for Timeline Summarization', 'Defining the Set of Dates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2017T01\nCitation Context: We construct our corpus by extending Kato et al. (2016)\u2019s corpus [URL_CITE] , which is itself built on a corpus by Shigeto et al. (2013).\nFootnote or Reference text (if exists): 5 https://catalog.ldc.upenn.edu/LDC2017T01\nSection Titles (if exists):  ['2 MWE-aware Dependency Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://proofwiki.org/\nCitation Context: In order to evaluate the premise selection, we used a corpus extracted from ProofWiki [URL_CITE] . ProofWiki is an online compendium of mathemati-cal proofs, with a goal to collect and classify math-ematical proofs.\nFootnote or Reference text (if exists): 1 http://proofwiki.org/\nSection Titles (if exists):  ['3 The Natural Language Premise Selection task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2018-novel-metaphors\nCitation Context: We make the annota-tions and scripts to embed them into the original corpus publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/UKPLab/ emnlp2018-novel-metaphors\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2016-supersense-embeddings\nCitation Context: \u2022 We are the first to provide a joint word-and supersense-embedding model, which we make publicly available [URL_CITE] for the research com-munity. This provides an insight into the word and supersense positions in the vector space\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2016-supersense-embeddings\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/birch\nCitation Context: The final score used by Birch for re-reranking is: \u03b3\u00b7S BM25 +(1\u2212\u03b3)\u00b7P i \u03ba i \u00b7S(x q , x i ) where S BM25 is the baseline BM25 score and \u03b3, \u03ba i are weight coefficients. We use the published mod-els [URL_CITE] and coefficient values for evaluation.\nFootnote or Reference text (if exists): 3 https://github.com/castorini/birch\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/pkumod/gAnswer\nCitation Context: Baselines We compare our results with the top ranking team \"jchl\" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [URL_CITE] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module.\nFootnote or Reference text (if exists): 7 https://github.com/pkumod/gAnswer\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Experimental Setup\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/\nCitation Context: We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 [URL_CITE] test sets, being assisted by statistical machine translation systems trained on data created under the GALE research program.\nFootnote or Reference text (if exists): 1 http://www.itl.nist.gov/iad/mig/tests/mt/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.delectable.com\nCitation Context: These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino or Delectable [URL_CITE] work with metadata and user-based filtering, i.e. the principle of \u2018other users also bought .\nFootnote or Reference text (if exists): 3 Delectable: http://www.delectable.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ifttt.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://labelstud.io/\nCitation Context: IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [URL_CITE] to create a new medical NER dataset. We collect 12127 health record images from the hospital, which are converted into text paragraphs by optical character recognition (OCR).\nFootnote or Reference text (if exists): 2 https://labelstud.io/\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pan.webis.de\nCitation Context: For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [URL_CITE] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).\nFootnote or Reference text (if exists): 3 http://pan.webis.de\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [URL_CITE] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2011T13\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thompsonb/prism\nCitation Context: \u2022 Outperforms or ties with prior metrics and language pairs (Ganitkevitch and Callison-Burch, several contrastive neural methods on the 2014). Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [URL_CITE] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,\nFootnote or Reference text (if exists): 1 Except for Gujarati, where we had no training data. 2 https://github.com/thompsonb/prism\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/\nCitation Context: Preprocessing and Hyperparameter Setting. To preprocess the training corpus D, we use the word tokenizer provided by NLTK [URL_CITE] . We also use a phrase mining tool, AutoPhrase (Shang et al., 2017), to extract meaningful phrases such as \u201cgreat wine\u201d and \u201cnumeric keypad\u201d such that they can capture complicated semantics in a single text unit.\nFootnote or Reference text (if exists): 3 https://www.nltk.org/\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org\nCitation Context: Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [URL_CITE] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each.\nFootnote or Reference text (if exists): 10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com\nSection Titles (if exists):  ['5 Experiment Details', 'Details of Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-baselines\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [URL_CITE] to stimulate further research on verification.\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/ fever-baselines\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Table 6 shows the results. Our joint parser outperforms the other parsers, including the neu-ral network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] . Our data-driven parsers yield relatively low performance in the ObQ and RNR constructions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/bible-uedin.php\nCitation Context: For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set. We sample these exam-ples either only from News Commentary v13 of WMT2018 or from both News Commentary and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS [URL_CITE] , because half of in-domain Chr-En data is the Bible. Whenever we sample from Bible-uedin, we keep the sample size as 6K and sample the rest from News Commentary.\nFootnote or Reference text (if exists): 20 http://opus.nlpl.eu/bible-uedin.php\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gutenberg.spiegel.de/\nCitation Context: We identified 115 novels among the texts pro-vided by Project Gutenberg (English) and Project Gutenberg-DE (German) that were available in both languages, with a total of 0.5M sentences per lan-guage. [URL_CITE] Examples include Dickens\u2019 David Copper-field or Tolstoy\u2019s Anna Karenina. We decided to exclude plays and poems as they often include partial sentences and structures that are difficult to align.\nFootnote or Reference text (if exists): 1 http://www.gutenberg.org and http://gutenberg.spiegel.de/\nSection Titles (if exists):  ['2 A Parallel Corpus of Literary Texts', '2.1 Data Selection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/\nCitation Context: Binomial adversarial nets are known to have theoretical connections to the minimization of various f-divergences between two distribu-tions (Nowozin et al., 2016) [URL_CITE] . However, for ad-versarial training among multiple distributions, no theoretical justifications have been provided to our best knowledge, despite that this idea has recently been explored empirically (Liu et al., 2017).\nFootnote or Reference text (if exists): Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271\u2013 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ). However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [URL_CITE] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://search.carrot2.org/stable/search\nCitation Context: With respect to implementation, we used the Carrot2 APIs [URL_CITE] which are freely available for STC, LINGO and the classical BIK. It is worth notic-ing that all implementations in Carrot2 are tuned to extract exactly 10 clusters.\nFootnote or Reference text (if exists): 4 http://search.carrot2.org/stable/search [Last access: 15/05/2013].\nSection Titles (if exists):  ['4 Evaluation', '4.3 Comparative Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/search\nCitation Context: Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API [URL_CITE] us-ing a query composed of the ambiguous word and its hypernym, e.g. \u201cjaguar animal\u201d.\nFootnote or Reference text (if exists): 9 https://azure.microsoft.com/en-us/services/cognitive-services/search\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EdoFrank/emnlp2019_morio_egawa\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nCitation Context: VN is or-ganised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class mem-bers. According to the official VerbNet guidelines, [URL_CITE] \u201cVerb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.\u201d For instance, all verbs from the top-level Class 9 are labelled \u201cVerbs of Putting\u201d, all verbs from Class 30 are labelled \u201cVerbs of Perception\u201d, while Class 39 contains \u201cVerbs of Ingesting\u201d.\nFootnote or Reference text (if exists): 5 http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [URL_CITE] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everyzing.com/\nCitation Context: We\u2019ve taken the rather nar-row view that systems might only expose the list of scores they assigned to retrieved documents, a plau-sible scenario considering the many systems now emerging which are effectively doing this already. Some examples include EveryZing, [URL_CITE] the MIT Lec-ture Browser, 2 and Comcast\u2019s video search. 3 This trend is likely to continue as the underlying repre-sentations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks).\nFootnote or Reference text (if exists): 1 http://www.everyzing.com/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nlm.nih.gov/databases/download/pubmed_medline.html\nCitation Context: For minimally supervised extraction of n-gram patterns, we use structured abstracts in which the authors describe different aspects of their work under targeted headings. We retrieved the headings and associated sections automatically from abstracts in XML format (downloaded from PubMed [URL_CITE] ). In general abstracts are structured id-iosyncratically (often as Introduction, Methods, Results, Discussion).\nFootnote or Reference text (if exists): 2 https://www.nlm.nih.gov/databases/download/pubmed_medline.html\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html\nCitation Context: Each sen-tence in texts is processed by the morphological an-alyzer JUMAN and the dependency parser KNP. [URL_CITE] JUMAN consults a hand-crafted dictionary and an automatically constructed dictionary. KNP is used to form a phrasal unit called bunsetsu by chunking morphemes.\nFootnote or Reference text (if exists): 2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html\nSection Titles (if exists):  ['3 Lexicon Acquisition', '3.2 System Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/miso-belica/jusText\nCitation Context: Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [URL_CITE] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences.\nFootnote or Reference text (if exists): 5 https://github.com/miso-belica/ jusText\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nCitation Context: ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [URL_CITE] . The output of this model is a 2048-dimensional vector, and we refer to these representations as \u201cResNeXt WSL\" features.\nFootnote or Reference text (if exists): 4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.xerox.fr/people/grenoble/hull/papers/sigir96.ps\nCitation Context: In Pro-ceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Informa-tion Retrieval. [URL_CITE] http://www.xerox.fr/people/grenoble/hull/papers/sigir96.ps.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/VNTC\nCitation Context: As backtranslation is a widely-used data augmenta-tion method by utilizing monolingual data to gener-ate new parallel pairs, we show how TCWR can be used with backtranslation. To perform backtransla-tion, we use the monolingual sequences from News Crawl 2017, News Crawl 2010 and VNTC [URL_CITE] for En-Tr, En-De and En-Vi, respectively. Then we per-form data augmentation on both training data and backtranslated data.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/VNTC\nSection Titles (if exists):  ['5 Experiments', '5.6 Backtranslation Result']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-\nCitation Context: For any fixed F s , with either NLL or L2 loss, the optimum domain discriminator D \u21e4 is: grangian Multiplier to solve the minimum value of J D , and the details can be found in the Ap-pendix. We then have the following main theo-rems for the domain loss for F s : to its optimality, if D adopts the NLL loss: where JSD(\u00b7) is the generalized Jensen-Shannon Divergence (Lin, 1991) among multiple distribu-tions, defined as the average Kullback-Leibler di-vergence of each P i to the centroid P (Aslam and Pavlu, 2007) [URL_CITE] .\nFootnote or Reference text (if exists): Javed A. Aslam and Virgil Pavlu. 2007. Query hardness estimation using jensen-shannon di-vergence among multiple scoring functions. In Advances in Information Retrieval. Springer Berlin Heidelberg, Berlin, Heidelberg, pages 198\u2013209. https://doi.org/10.1007/978-3-540- 71496-5 20.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11372/LRT-1632\nCitation Context: We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016 [URL_CITE] ), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain. Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.\nFootnote or Reference text (if exists): Marco Turchi, Rajen Chatterjee, and Matteo Negri. 2016. WMT16 APE shared task data. LIN-DAT/CLARIN digital library at the Institute of For-mal and Applied Linguistics, Charles University in Prague. http://hdl.handle.net/11372/LRT-1632.\nSection Titles (if exists):  ['4 Experiments', '4.2 Automatic MT Post-editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bing.com/\nCitation Context: p \u03b8 SYN (c) denotes the synonym similarity between translated words and the contexts. This is necessary since the sense of translated words and the contexts are expected to be similar; and p \u03b8 PMI (c) presents the PMI simi-larity between translated words and the contexts, while the PMI score is calculated by the individ-ual and co-occurred hit count between translated words and contexts from the search engine [URL_CITE] (Tur-ney, 2002). This is to ensure that the translated words are highly associated with the contexts.\nFootnote or Reference text (if exists): 4 We use BING.com as the search engine for PMI: http://www.bing.com/\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.1 Bilingual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhaozj89/TensorEmbeddingNLP\nCitation Context: The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [URL_CITE] https://github.com/zhaozj89/TensorEmbeddingNLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhanglu-cst/ClassKG\nCitation Context: Our code has already been released. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/zhanglu-cst/ ClassKG\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.luis.ai/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis [URL_CITE] , wit.ai , snips.ai , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 4 https://www.luis.ai/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/noutenki/vex\nCitation Context: VEX is available as free, open-source soft-ware for download at [URL_CITE] http://github.com/noutenki/vex and as a web service at http://cosyne.h-its.org/vex.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stepforwardpak.com/\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news [URL_CITE] , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service .\nFootnote or Reference text (if exists): 5 http://www.shashca.com,http://stepforwardpak.com/\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/naist-cl-parsing/\nCitation Context: To pursue this direction further, we construct a corpus such that dependency structures are consis-tent with MWEs, by extending Kato et al. (2016)\u2019s corpus [URL_CITE] . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware de-pendency structure from our corpus (Figure 1b).\nFootnote or Reference text (if exists): 2 We release our dependency corpus at https: //github.com/naist-cl-parsing/ mwe-aware-dependency. MWE-aware phrase struc-tures will be distributed from LDC as a part of LDC2017T01. Computational Linguistics (Short Papers), pages 427\u2013432\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: The approach is in theory applicable to any language, but there are some obstacles: first, it relies on the availability of the target sense in the target language Wiktionary. For many of the top 30 languages in FNWKxx, the Wiktionary edi-tions seem sufficiently large to provide targets for translation disambiguation, [URL_CITE] and they are contin-uously extended. Second, our approach requires access to the target language Wiktionary, but the data format across Wiktionary language editions is not standardized.\nFootnote or Reference text (if exists): 4 see overview table at http://www.ukp.tu-darmstadt.de/fnwkde/.\nSection Titles (if exists):  ['9 Discussion: a Multilingual FrameNet']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [URL_CITE] (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cuitools.sourceforge.net\nCitation Context: Our experiments were conducted using CuiTools v0.15, which is freely available from [URL_CITE] http://cuitools.sourceforge.net.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Word2vec We average word embeddings trained with word2vec. [URL_CITE] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.\nFootnote or Reference text (if exists): 2 The code is available from https://code.google.com/archive/p/word2vec/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.paracrawl.eu/\nCitation Context: Unlabeled-domain For additionalData unlabeled-domain data, we use web-crawled bitext from the Paracrawl project. [URL_CITE] We filter the data using the Zipporah cleaning tool (Xu and Koehn, 2017), with a threshold score of 1. After filtering, we have around 13.6 million Paracrawl sentences available for German-English and 3.7 million Paracrawl sentences available for Russian-English.\nFootnote or Reference text (if exists): 4 https://www.paracrawl.eu/\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data and Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/docs/datasets/master/#\nCitation Context: We train each model on a NVIDIA RTX 8000 GPU (48GB memory) for maximum 200 epochs with a batch size of 64 and a learning rate linearly decaying from 5e-5. Training ends if the validation set loss has not decreased for [URL_CITE] epochs. Early stopping occurs within 15 epochs for most mod-els.\nFootnote or Reference text (if exists): 10 https://huggingface.co/docs/datasets/master/#\nSection Titles (if exists):  ['A Additional Information', 'A.4 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.davidsilver.uk/teaching/\nCitation Context: The agent reacts to its envi-ronment by accounting for past predictions and getting rewarded along the way, while its state is defined by the partial sequence generated so far. This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015 [URL_CITE] ). More precisely, a sequence gen-eration task can be recast as a Markov Decision Process (MDP) where the agent behavior follows a policy \u03c0(a t |s t ).\nFootnote or Reference text (if exists): David Silver. 2015. Lectures on reinforcement learn-ing. URL : https://www.davidsilver.uk/teaching/.\nSection Titles (if exists):  ['3 Models', '3.1 Reinforcement Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/king-kong-1933.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: A survey of recent work of several researchers on producing animations of sign language with fa-cial expressions appears in (Kacorri, 2015) [URL_CITE] . There is recent interest in data-driven approaches using facial motion-capture of human performances to generate sign language animations: For example, (Schmidt et al., 2013) used clustering techniques to select facial expressions that co-occur with indi-vidual lexical items, and (Gibet et al., 2011) stud-ied how to map facial motion-capture data to ani-mation controls.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['1 Introduction and Motivation', '1.2 Prior Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.thestocksonar.com/\nCitation Context: The Stock Sonar [URL_CITE] has a timeline chart as the main interface. In this timeline, both positive and negative sentiments are displayed throughout time.\nFootnote or Reference text (if exists): 3 http://www.thestocksonar.com/\nSection Titles (if exists):  ['2 Analyzing Sentiment in Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Tool)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws\nCitation Context: We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS and Stan-ford Chinese word segmenter [URL_CITE] , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws\nSection Titles (if exists): ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ziyi-yang/GEM\nCitation Context: Finally, we summarize our work in Section 5. Our implemen-tation is available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/ziyi-yang/GEM\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE\nCitation Context: Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [URL_CITE] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.\nFootnote or Reference text (if exists): 5 https://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.is.cs.cmu.edu/mie\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://perso.limsi.fr/aufrant\nCitation Context: For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source [URL_CITE] implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).\nFootnote or Reference text (if exists): 5 https://perso.limsi.fr/aufrant\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://chywang.github.io/data/emnlp17.zip\nCitation Context: In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://chywang.github.io/data/emnlp17.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/diegma/neural-dep-srl\nCitation Context: When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/diegma/neural-dep-srl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/jax\nCitation Context: We have implemented the proposed hierarchical attention using Jax, an open source library [URL_CITE] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax.\nFootnote or Reference text (if exists): 5 https://github.com/google/jax\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mynlp/jigg\nCitation Context: We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [URL_CITE] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.\nFootnote or Reference text (if exists): 9 https://github.com/mynlp/jigg\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=rke2P1BFwS\nCitation Context: A tKG represents a temporal fact as a quadruple (s, r, o, t) by extending a static triple with time t, describing that this fact is valid at time t. In recent years, several sizable temporal knowledge graphs, such as ICEWS (Boschee et al., 2015), have been developed that provide widespread availability of such data and enable reasoning on temporal KGs. While lots of work (Garc\u00eda-Dur\u00e1n et al., 2018; Goel et al., 2020; Lacroix et al., 2020 [URL_CITE] ) focus on the tem-poral KG completion task and predict missing links at observed timestamps, recent work (Jin et al., 2019; Trivedi et al., 2017) paid attention to forecast future links of temporal KGs. In this work, we focus on the temporal KG forecasting task, which is more challenging than the completion task.\nFootnote or Reference text (if exists): Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.\nSection Titles (if exists):  ['\u2020 Corresponding author.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/cabocha/\nCitation Context: For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [URL_CITE] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai).\nFootnote or Reference text (if exists): 11 http://taku910.github.io/cabocha/\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nju-websoft/GLRE\nCitation Context: We implemented our GLRE with PyTorch 1.5. The source code and datasets are available online. [URL_CITE] In this section, we report our experimental results.\nFootnote or Reference text (if exists): 1 https://github.com/nju-websoft/GLRE\nSection Titles (if exists):  ['4 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/cos-e/issues/2\nCitation Context: CoS-E v1.11 has some noise in its annotations (Narang et al., 2020). [URL_CITE] This is our primary motivation for reporting on v1.0 as well, which we observe does not have these issues. A.3 Details of T5 The T5 model (Raffel et al., 2020) is pretrained on a multi-task mixture of unsupervised and supervised tasks, including machine translation, question an-swering, abstractive summarization, and text classi-fication.\nFootnote or Reference text (if exists): 9 https://github.com/salesforce/cos-e/issues/2\nSection Titles (if exists):  ['A Additional Information', 'A.2 Details of Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/Multilingual-Model-Transfer\nCitation Context: Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at in-stance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target lan-guage and each individual source language [URL_CITE] . This enables our model to learn effectively what to share between various languages in the multilingual setup.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/microsoft/Multilingual-Model-Transfer.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016) and Kasai et al. (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 512 units each.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.artint.2008.12.002\nCitation Context: The task of generating descriptions from time-series or structured data has been tackled in vari-ous domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009 [URL_CITE] ; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used hand-crafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005).\nFootnote or Reference text (if exists): Fran\u00e7ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter, Somayajulu Sripada, Yvonne Freer, and Cindy Sykes. 2009. Automatic generation of textual sum-maries from neonatal intensive care data. Artificial Intelligence 173(7-8):789\u2013816. https://doi.org/10.1016/j.artint.2008.12.002.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://reddit.com\nCitation Context: \u2022 Reddit - Reddit [URL_CITE] is a highly popular social forum and community. Similar to Tweets, sarcastic posts are obtained via the tag \u2018/s\u2019 which are marked by the authors themselves.\nFootnote or Reference text (if exists): 3 https://reddit.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md\nCitation Context: We compare our TAA method with the following representative baselines: Back Translation (BT) (Sennrich et al., 2016; Imamura et al., 2018). We utilize WMT\u201919 English-German translation models (Ng et al., 2019) based on Transformer (Vaswani et al., 2017) to translate the text from English to German, [URL_CITE] and then trans-late it back to English. We use random sampling for decoding as recommended by (Xie et al., 2020; Edunov et al., 2018), and set the temperature to 0.8 to generate more diverse paraphrases.\nFootnote or Reference text (if exists): 4 https://github.com/pytorch/fairseq/blob/master/examples/translation/README. md\nSection Titles (if exists):  ['3 Experiments', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.jhu.edu/~jason/tutorials/variational.html\nCitation Context: Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth-ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables which we refer to as the marginal energy. Follow-ing the approach described for undirected models by Eisner (2011) [URL_CITE] , we approximate these quantities and their gradients with respect to the model pa-rameters \u03b8 as we will now describe (thread-level embeddings are omitted in this section for clarity).\nFootnote or Reference text (if exists): Jason Eisner. 2011. High-level explanation of varia-tional inference. https://www.cs.jhu.edu/~jason/tutorials/variational.html. Accessed: 2018-02-22.\nSection Titles (if exists):  ['3 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: (2009) we use asymmetric \u03b1 and symmetric \u03b2; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [URL_CITE] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations).\nFootnote or Reference text (if exists): 2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.\nSection Titles (if exists):  ['3 Models', '3.1 Latent variable context models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://naudio.codeplex.com/\nCitation Context: The recorder, built using the open-source library NAudio, [URL_CITE] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).\nFootnote or Reference text (if exists): 8 http://naudio.codeplex.com/\nSection Titles (if exists):  ['5 User interface', '5.1 Audio input and recording']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [URL_CITE] . Table 9 gives the number of parameters for each introduced model.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendix']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnetcode.princeton.edu/glosstag.shtml\nCitation Context: Princeton WordNet Gloss Corpus (PWNG) is the semantically-annotated gloss corpus made available by WordNet since its 3.0 release. [URL_CITE] Glosses are short definitions providing proper meanings for synsets, and in PWNG they have been tagged according to the senses in WordNet. Following Agirre et al.\nFootnote or Reference text (if exists): 2 http://wordnetcode.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Preliminaries', '2.1 Lexical Knowledge Bases']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/EGG\nCitation Context: EGG is implemented in PyTorch (Paszke et al., 2017) and it is licensed under the MIT license. EGG can be installed from [URL_CITE] https://github.com/facebookresearch/EGG.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cogsci.princeton.edu/~wn\nCitation Context: The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English mor-phological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English (LDOCE) 3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998) [URL_CITE] , and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. 4\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].\nSection Titles (if exists):  ['3 Building the CatVar']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/qiangning/MATRES\nCitation Context: The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal order-ing of events. [URL_CITE] It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and 20 more for evaluation) from TimeBank 2 (Puste-jovsky et al., 2003), AQUAINT 3 (Graff, 2002) and Platinum (UzZaman et al., 2013).\nFootnote or Reference text (if exists): 1 https://github.com/qiangning/MATRES\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/\nCitation Context: Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia [URL_CITE] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thompsonb/prism\nCitation Context: \u2022 Outperforms or ties with prior metrics and language pairs (Ganitkevitch and Callison-Burch, several contrastive neural methods on the 2014). Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [URL_CITE] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,\nFootnote or Reference text (if exists): 1 Except for Gujarati, where we had no training data. 2 https://github.com/thompsonb/prism\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/knowitall/openie\nCitation Context: While the relation phrase is similar to a relation in a concept map, many arguments in these tuples represent useful concepts. We used Open IE 4 [URL_CITE] , a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences. After removing du-plicates, we obtained 4137 tuples per topic.\nFootnote or Reference text (if exists): 7 https://github.com/knowitall/openie\nSection Titles (if exists):  ['5 Corpus Creation', '5.2 Proposition Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Our experiments are based on the publicly avail-able word2vec [URL_CITE] and GloVe packages. We mod-ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009.\nFootnote or Reference text (if exists): 1 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['4 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com/en/basics/what-is-twitter/\nCitation Context: Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012) [URL_CITE] . It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011).\nFootnote or Reference text (if exists): Twitter. (2012). What is Twitter Retrieved Mar 18, 2012, from https://business.twitter.com/en/basics/what-is-twitter/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: \u2022 Genericembeddingswordusedembeddingsare GloVe: Generic from wordboth Wikipedia and common crawl and the word2vec (Skip-gram) embeddings [URL_CITE] . These generic embeddings will be denoted as Glv, GlvCC and w2v.\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ccsasuke/man\nCitation Context: Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [URL_CITE] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions.\nFootnote or Reference text (if exists): 1 The source code of MAN is available at https://github.com/ccsasuke/man.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-cleaning\nCitation Context: 2. To calculate semantic accuracy we use a minimally modified version of the slot error.py module from [URL_CITE] https: //github.com/tuetschek/e2e-cleaning . We noticed it was incorrectly grouped together attributes in a small number of cases (we saw less than 5).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: Based on the above setup, we present three nega-tive empirical results, which challenge the claim that the methods presented in \u00a72.3 are learning a rela-tion between x and y. In addition to our setup, these results were also reproduced in preliminary exper-iments by applying the JoBimText framework [URL_CITE] for scalable distributional thesauri (Biemann and Riedl, 2013) using Google\u2019s syntactic N-grams (Goldberg and Orwant, 2013) as a corpus.\nFootnote or Reference text (if exists): 4 http://jobimtext.org\nSection Titles (if exists):  ['3 Negative Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/translation\nCitation Context: We follow the scripts in Ott et al. (2019) [URL_CITE] for other pre-processing steps.\nFootnote or Reference text (if exists): 8 https://github.com/pytorch/fairseq/tree/master/examples/translation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sites.google.com/site/trecliveqa2015/\nCitation Context: Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. [URL_CITE] The Yahoo! Answers man-ner question dataset prepared by Jansen et al.\nFootnote or Reference text (if exists): 5 https://sites.google.com/site/trecliveqa2015/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1161\nCitation Context: Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [URL_CITE] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.\nFootnote or Reference text (if exists): Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/mt-para\nCitation Context: We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [URL_CITE]\nFootnote or Reference text (if exists): 4 The data is available at http://bit.ly/mt-para.\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atschneid/DebugSL\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at [URL_CITE] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Thus our method some-times acts as an efficient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour-aged by the Wikipedia community, [URL_CITE] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.\nFootnote or Reference text (if exists): 8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations\nSection Titles (if exists):  ['6 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fever.ai\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [URL_CITE] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 1 http://fever.ai\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2108\nCitation Context: At a higher level of granularity, Tan et al. (2015) [URL_CITE] analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia).\nFootnote or Reference text (if exists): Luchen Tan, Haotian Zhang, Charles LA Clarke, and Mark D Smucker. 2015. Lexical compari-son between Wikipedia and Twitter corpora by us-ing word embeddings. In Assocation for Computa-tional Linguistics (ACL). pages 657\u2013661. http: //www.aclweb.org/anthology/P15-2108.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/phrase2vec\nCitation Context: More concretely, we train our n-gram embed-dings using phrase2vec [URL_CITE] , a simple extension of skip-gram that applies the standard negative sam-pling loss of Mikolov et al. (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs.\nFootnote or Reference text (if exists): 1 https://github.com/artetxem/ phrase2vec\nSection Titles (if exists):  ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jbnc.sourceforge.net\nCitation Context: To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, [URL_CITE] http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We initialized our word representations using publicly available 300-dimensional Glove vec-tors [URL_CITE] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.\nFootnote or Reference text (if exists): 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['5 Experiments', '5.3 Hyperparameters and Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N/N09/N09-2047\nCitation Context: Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 dis-tinct supertags (2165 occur once) while the CCG-annotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Ban-galore et al., 2009 [URL_CITE] ; Chung et al., 2016; Kasai et al., 2017).\nFootnote or Reference text (if exists): Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Beno\u0131\u0302t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185\u2013188. http://www.aclweb.org/anthology/N/N09/N09-2047.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/iterative-search-semparse\nCitation Context: Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at [URL_CITE] https://github.com/allenai/iterative-search-semparse.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Experiments', '6.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mecab/files/\nCitation Context: Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [URL_CITE] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task.\nFootnote or Reference text (if exists): 4 http://sourceforge.net/projects/mecab/files/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ccks2019.cn/?pageid=62\nCitation Context: CNER [URL_CITE] CNER is a Chinese clinical NER dataset in the CCKS-2019 challenge, including anatomy, disease, imaging examination, laboratory examina-tion, drug, and operation. We extract 1650 avail-able medical records from CNER, which contains entities of the disease type only.\nFootnote or Reference text (if exists): 1 http://www.ccks2019.cn/?page id=62\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paraphrase.org\nCitation Context: For instance, the word foolishness is simpler than its meaning-preserving substitution folly even though foolish-ness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006). In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [URL_CITE] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.\nFootnote or Reference text (if exists): 2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http://paraphrase.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/projects/morpho/\nCitation Context: Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005) [URL_CITE] as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained.\nFootnote or Reference text (if exists): Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.4 Turkish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nCitation Context: The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [URL_CITE] and annotated as described in (Bethard et al., 2012). In this section we illustrate the main annotation princi-ples for coherent temporal annotation. As an example story, consider:\nFootnote or Reference text (if exists): 1 Data available at http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nCitation Context: One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [URL_CITE] .\nFootnote or Reference text (if exists): 6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nSection Titles (if exists):  ['5 Experiments', '5.3 Further experiments on the Europarl corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: To evaluate WP with lengthy stories, we fine-tune pretrained Longformer-base model with the learning rate of 2e-5 and batch size 3 by encoding texts with at most 1024 tokens for three epochs. [URL_CITE]\nFootnote or Reference text (if exists): 3 We fine-tune RoBERTa and Longformer mod-els using https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 Experiments', '5.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2JTbGnI\nCitation Context: Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [URL_CITE] and Germany (Burger, 2019).\nFootnote or Reference text (if exists): 5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~cyl/ROUGE\nCitation Context: For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [URL_CITE] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence.\nFootnote or Reference text (if exists): 1 http://www.isi.edu/\u02dccyl/ROUGE\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.1 DUC 2004 data and ROUGE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: This motivates approaches that directly detect hypernymy in the cross-lingual setting by extend-ing distributional methods for detecting monolin-gual hypernymy, as in our prior work (Vyas and Carpuat, 2016) [URL_CITE] . State-of-the-art distributional ap-proaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy re-quire syntactic analysis (eg.\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.conexor.com\nCitation Context: We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004) [URL_CITE] to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences.\nFootnote or Reference text (if exists): Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com\nSection Titles (if exists):  ['2. Information for Term Specificity', '2.2. Contextual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zxshamson/dy-conv-rec\nCitation Context: For experiments [URL_CITE] , we collect Reddit conversa-tions from three subreddits \u2014 \u201ctechnology\u201d, \u201cto-dayilearned\u201d, and \u201cfunny\u201d, each exhibiting differ-ent data statistics, discussion topics, and language styles. An absolute date is used to separate training data (before the date) from test and validation data (after the date).\nFootnote or Reference text (if exists): 1 The datasets and codes are available at: https://github.com/zxshamson/dy-conv-rec\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf\nCitation Context: \u2022 T I MBL (Tilburg Memory-Based Learner), (Daelemans et al., 2003) [URL_CITE] , which implements a memory-based learning algorithm ( IB 1) which pre-dicts the class of a test data point by looking at its distance to all examples from the training data, us-ing some distance metric. In our experiments, we have used the weighted-overlap method, which as-signs weights to all features. timation.\nFootnote or Reference text (if exists): Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. TiMBL: Tilburg memory based learner, version 5.0, reference guide. ILC Technical Report 03-10, Induction of Linguis-tic Knowledge; Tilburg University. Available from http://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [URL_CITE]\nFootnote or Reference text (if exists): 5 Available at https://github.com/Anna146/ CHARM\nSection Titles (if exists):  ['4 Dataset', '4.2 Document collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.texar.io.1\nCitation Context: The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [URL_CITE] https: //www.texar.io. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp-semparse\nCitation Context: IS-SP is a semantic parsing based model trained on WikiTa-blesQuestions (Pasupat and Liang, 2015) dataset (See Section 2 for the details of this work). For building their model we used the code provided in (Gardner et al., 2020) [URL_CITE] . For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset.\nFootnote or Reference text (if exists): Matt Gardner, Dirk Groeneveld, Brendan Roof, Pradeep Dasigi, Michael Schmitz, and Nitish Gupta. 2020. allennlp-semparse. https://github.com/allenai/allennlp-semparse.\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Metaphone\nCitation Context: Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caver-phone (Wang, 2009) and Double Metaphone. [URL_CITE] These schemes encode words based on their sound which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Metaphone\nSection Titles (if exists):  ['3 Our Method', '3.1 UrduPhone']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.elastic.co/products/elasticsearch\nCitation Context: For a question with q words Q = {w tQ } qt=1 along with its N answer choices C = {C n } Nn=1 where C n = {w Ct } ct=1 , the essential-term selector chooses a subset of essential terms E \u2282 Q, which are then concatenated with each C n to formulate a query. The query for each answer choice, E+C n , is sent to the retriever (e.g. Elastic Search [URL_CITE] ), and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w tP } tK=1 .\nFootnote or Reference text (if exists): 2 https://www.elastic.co/products/elasticsearch\nSection Titles (if exists):  ['3 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://compling.hss.ntu.edu.sg/omw/\nCitation Context: For the IWND approach, we use all [URL_CITE] Wordnets as intermediate resources. The number of Wordnet synsets we create using the IWND approach are presented in Table 4. We only construct Wordnet synsets for ajz, asm and dis using the IWND ap-proach because these languages are not supported by MT.\nFootnote or Reference text (if exists): 4 http://compling.hss.ntu.edu.sg/omw/\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental results and discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/MultiHopKG\nCitation Context: (2) The patterns of InferWiki is more evenly distributed. Note that the patterns 7 https://github.com/ibalazevic/TuckER hop [URL_CITE] , and AnyBURL . Because we utilize various\nFootnote or Reference text (if exists): 8 https://github.com/salesforce/ MultiHopKG\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://discussions.apple.com\nCitation Context: We use a crawl of 140k threads from Apple Dis-cussion forums [URL_CITE] . Out of these, 300 threads (com-prising 1440 posts) were randomly chosen and each post was manually tagged as either solution or non-solution by the authors of (Catherine et al., 2013) (who were kind enough to share the data with us) with an inter-annotator agreement of 0.71.\nFootnote or Reference text (if exists): 10 http://discussions.apple.com\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://propaganda.qcri.org/emnlp20-tutorial\nCitation Context: Here is an outline of the tutorial. More informa-tion and materials are available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://propaganda.qcri.org/ emnlp20-tutorial\nSection Titles (if exists):  ['2 Outline of the Tutorial']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/\nCitation Context: The increasing popularity of social media has drastically changed how our daily news are pro-duced, disseminated and consumed. [URL_CITE] Without sys-tematic moderation, a large volume of informa-tion based on false or unverified claims (e.g., fake news, rumours, propagandas, etc.) can proliferate online.\nFootnote or Reference text (if exists): 1 The latest Pew Research statistics show that 68% American adults at least occasionally get news on social media. http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/jweb1t/\nCitation Context: We use the Google Web1T to generate n-gram features as we expect this open domain resource to have consid-erable coverage for most specific domains as well. For accessing the resource, we use JWeb1T [URL_CITE] (Giuliano et al., 2007).\nFootnote or Reference text (if exists): 5 https://code.google.com/p/jweb1t/\nSection Titles (if exists):  ['4 Resources', '4.3 Google Web1T']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vikas95/AutoROCC\nCitation Context: The ROCC system and the codes for generat-ing all the analysis are provided here - [URL_CITE] https: //github.com/vikas95/AutoROCC .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.lighttag.io/\nCitation Context: Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [URL_CITE] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.\nFootnote or Reference text (if exists): 4 https://www.lighttag.io/ Demonstrations, pages 99\u2013105\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.shigeku.com/\nCitation Context: One is a modern Chinese poetry dataset, while the other is a modern Chinese lyrics dataset. We collected the modern Chinese poetry dataset from an online poetry website [URL_CITE] and crawled about 100,000 Chi-nese song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training.\nFootnote or Reference text (if exists): 2 http://www.shigeku.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Setups']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: GPE. Geo-political entities are single units that contain a population, government, physical loca-tion, and political boundaries (LDC, 2005) [URL_CITE] . In literary data, this includes not only cities that have known geographical locations within the real world (London, New York), or nations (England, the United States), but also both named and com-mon imagined entities as well (the town, the vil-lage).\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['3 Annotation', '3.1 Entity types']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/reggaeton.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/heartexlabs/label-studio#try-out-label-studio\nCitation Context: The annotator is required to select, if there are any, all the substitutes that are not suitable replacements for the target in the given context. We select three annotators with certified proficiency in English and previous experi-ence in linguistic annotation tasks and present them with a sample of 322 test instances drawn from the LST dataset [URL_CITE] . The annotators are asked to select the inappropriate substitutes from an anonymized shuffled set of three gold and three generated substi-tutes, obtained with G ENE S IS trained on CT T .\nFootnote or Reference text (if exists): 8 The sample size is significant with respect to the source dataset with confidence level of 95% and a margin error of \u00b1 5. The annotation interface was developed through Label Studio https://github.com/heartexlabs/label-studio#try-out-label-studio. layer dropout, when keeping dropout fixed at 0.1. The chosen value is 0.6.\nSection Titles (if exists):  ['9 Qualitative Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/eladsegal/tag-based-multi-span-extraction\nCitation Context: In addition, integrating our multi-span architecture into ex-isting models further improves performance on DROP, as is evident from the leading models on DROP\u2019s leaderboard. Our code can be downloaded from [URL_CITE] https://github.com/eladsegal/tag-based-multi-span-extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: The raw data consists of 86 game transcripts col-lected by Barnwell (Barnwell, 2012) [URL_CITE] . The tran-scripts have an average length of 205 messages per transcript, including judge comments.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: We classify these selected tweets by Random Forest classifier (Breiman, 2001) implemented in sklearn [URL_CITE] python module with 10-fold cross valida-tion. Using accuracy as the evaluation metric, we report the classification performance of different topic models in Figure 2.\nFootnote or Reference text (if exists): 2 See http://scikit-learn.org/stable/\nSection Titles (if exists):  ['4 Experiments', '4.2 Document Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bethard.info/data/fables-100-temporal-dependency.xml\nCitation Context: Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [URL_CITE] could be used to train a temporal dependency parsing model.\nFootnote or Reference text (if exists): 2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.procyclingstats.com/team/bahrain-merida-2019\nCitation Context: \u2022 Professional websites: To annotate the triple (Bahrain-Merida 2019, hasPart, Carlos Betancur), you may search the person in professional websites, such as [URL_CITE] https://www.procyclingstats.com/team/bahrain-merida-2019. Since there is no Carlos Betancur listed in that website, please choose false.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Literature Review']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.infoplease.com/people.html\nCitation Context: We download from infoplease.com [URL_CITE] and biogra-phy.com two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 2 http://www.infoplease.com/people.html\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: There were 8 men and 9 women of ages 19-29 (average age 22.8). In prior work, we (Kacorri et al., 2015) [URL_CITE] have advocated that participants in studies eval-uating sign language animation complete a two standardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici-pants, to enable comparison across studies. In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.cs.nyu.edu/evalb\nCitation Context: In this paper, POS tags are removed and not used as input features in both training and testing in CTB 5.1, following the previous work in (Zhang et al., 2020). Standard precision, recall and F1-measure are em-ployed as evaluation metrics, where the EVALB [URL_CITE] tool is employed in the single task. The hyper-parameters in the implementation are shown in Ta-ble.\nFootnote or Reference text (if exists): 1 https://nlp.cs.nyu.edu/evalb\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/downloads.html\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [URL_CITE]\nFootnote or Reference text (if exists): 5 http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://FN.icsi.berkeley.edu/fndrupal/\nCitation Context: \u2020 See the details in [URL_CITE] https://FN.icsi.berkeley.edu/fndrupal/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [URL_CITE] for infer-ence.\nFootnote or Reference text (if exists): 3 http://www.gurobi.com/\nSection Titles (if exists):  ['6 Empirical Evaluation', '6.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/fever\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [URL_CITE] and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 2 https://github.com/awslabs/fever\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.eccox.io/\nCitation Context: More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntac-tic information. Ecco is available at [URL_CITE] https: //www.eccox.io/ . 2\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/\u223ctpederse/data.html\nCitation Context: For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d [URL_CITE] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3.\nFootnote or Reference text (if exists): 1 Available at http://www.d.umn.edu/\u223ctpederse/data.html\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aclweb.org/anthology/D16-1264\nCitation Context: It has been hypothesized that difficult MRC problems re-quire some form of multi-step synthesis and rea-soning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) [URL_CITE] illustrates the need for synthesis of information across sentences and multiple steps of reasoning:\nFootnote or Reference text (if exists): Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr\nCitation Context: One is described in (Hahn et al., 2009) and uses a margin based criterion for probabilities estimation. The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [URL_CITE] . The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:\nFootnote or Reference text (if exists): 1 available at http://wapiti.limsi.fr\nSection Titles (if exists):  ['3 Reranking Framework', '3.1 Conditional Random Fields']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt\nCitation Context: The 2012 JHU workshop on Domain Adapta-tion for MT [URL_CITE] proposed phrase sense disambiguation (PSD) for translation model adaptation. In this ap-proach, the context of a phrase helps the system to find the appropriate translation.\nFootnote or Reference text (if exists): 4 http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/KILT\nCitation Context: We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, out-performing more tailor-made approaches for fact checking, open-domain question answer-ing and dialogue, and yielding competitive re-sults on entity linking and slot filling, by gen-erating disambiguated text. KILT data and code are available at [URL_CITE] https://github.com/facebookresearch/KILT. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset, Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/jacana/\nCitation Context: In the following we first provide background on the TED model, going on to evaluate our implemen-tation against prior work in the context of question answer sentence ranking (QASR), achieving state of the art in that task. We then describe how we cou-ple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and fi-nally experimental results on an extraction dataset we make public (together with the software) to the community. [URL_CITE] Related prior work is interspersed throughout the paper.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/jacana/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OnlpLab/morphodetection\nCitation Context: We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available on https://github.com/OnlpLab/morphodetection.\nSection Titles (if exists):  ['table, given a few forms of the same lexeme. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iwslt.org/\nCitation Context: Dataset statistics are summarized in Table 1. For the low-resource experiments, we use [URL_CITE] : English-Vietnamese from IWSLT\u201915, German-English from IWSLT\u201914, and French-English from IWSLT\u201916. For the rich-resource experiments, we use the English-German dataset from WMT\u201916, which contains about 4.5 million training samples.\nFootnote or Reference text (if exists): 4 https://iwslt.org/\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andreasvc/disco-dop\nCitation Context: We evaluate our parser on the test sets of the three treebanks. We report F-measure and discontinuous F-measure as computed using the disco-dop tool [URL_CITE] with standard parameters in Table 2. ants of our parsers produced exactly the same re-sults in all settings. This may be expected as their cover of the original treebanks are almost similar.\nFootnote or Reference text (if exists): 11 https://github.com/andreasvc/ disco-dop\nSection Titles (if exists):  ['4 Experiments', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 400 units each.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.amap.com\nCitation Context: How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap [URL_CITE] ) to complete the KG.\nFootnote or Reference text (if exists): 5 https://www.amap.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://protege.stanford.edu/\nCitation Context: OWL allows us to define concept hierarchies, re-lations between concepts, domains and ranges of these relations, as well as specific relation in-stances between instances of a concept. Our on-tologies are defined by the freely available ontol-ogy editor Prote\u0301ge\u0301 4.0 [URL_CITE] . The advantage of using an ontology for structuring the domain knowledge is the modular non-redundant encoding.\nFootnote or Reference text (if exists): 2 http://protege.stanford.edu/, as accessed 27 Oct 2009\nSection Titles (if exists):  ['3 Knowledge Representation and Semantic Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/raylin1000/drop-bert\nCitation Context: We treat the second ref-erence as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT [URL_CITE] models for DROP.\nFootnote or Reference text (if exists): 2 https://github.com/raylin1000/drop-bert\nSection Titles (if exists):  ['2 A Description of MOCHA', '2.2 Collecting Candidates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cambridgeenglish.org/exams-and-tests/cefr/\nCitation Context: Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yan-nakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR). [URL_CITE] The W&I corpus contains 3, 600 texts across 3 different CEFR levels \u2013 A (be-ginner), B (intermediate), and C (advanced) \u2013 that have been annotated for errors.\nFootnote or Reference text (if exists): 11 https://www.cambridgeenglish.org/exams-and-tests/cefr/\nSection Titles (if exists):  ['3 GEC Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.newsreader-project.eu/results/data/the-ecb-corpus/\nCitation Context: In future, we intend to adapt this model to coreference across document streams and investigate alternatives to greedy prediction (e.g., beam search). A Implementation details The dataset is available here: [URL_CITE] http: //www.newsreader-project.eu/ results/data/the-ecb-corpus/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sweaglesw.org/linguistics/ace/\nCitation Context: To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [URL_CITE] . The created grammar-licensed analyses contain both a deriva-tion tree recording the used grammar rules and lex-ical entries, and the associated semantic represen-tation constructed compositionally via this deriva-tion (Bender et al., 2015).\nFootnote or Reference text (if exists): 2 http://sweaglesw.org/linguistics/ace/\nSection Titles (if exists):  ['3 Literal versus Intended Meaning', '3.3.2 SemBanking with ERG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [URL_CITE] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, Code)\nFUNCTION: Use, Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.gdeltproject.org/events/index.html\nCitation Context: We crawled and parsed the GDELT Even-t Database [URL_CITE] containing news articles published in May 2014. We manually annotated one-week da-ta containing 101,654 documents and identified 77 storylines for evaluation.\nFootnote or Reference text (if exists): 1 http://data.gdeltproject.org/events/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tapas\nCitation Context: For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models [URL_CITE] for Wik-iSQL and applied to the lookup subsets of the dev and test sets.\nFootnote or Reference text (if exists): 4 https://github.com/google-research/tapas\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cmusphinx.org\nCitation Context: Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [URL_CITE] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010).\nFootnote or Reference text (if exists): 3 http://www.cmusphinx.org\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nCitation Context: Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [URL_CITE] . People often provide the context in such posts, which gets repetitive over a large number of posts. Suggestion mining methods can identify the exact textual unit in the post where a suggestion is conveyed.\nFootnote or Reference text (if exists): 1 https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL citation is used to provide an example of a suggestion forum, but it's not clear how it was used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: Bousmalis et al. (2016) [URL_CITE] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abetusk/euclideanmst.js\nCitation Context: Since the actual positions of mention span el-ements on the user\u2019s screen depend on various user environment factors such as font size and browser window dimensions, the MSTs of dis-played entities are computed using a client-side JavaScript library [URL_CITE] and are automatically redrawn if the browser window is resized. Drawing of edges is performed via jsPlumb , a highly cus-tomizable library for line drawing in HTML doc-uments.\nFootnote or Reference text (if exists): 9 https://github.com/abetusk/euclideanmst.js. This library employs Kruskal\u2019s algorithm (Kruskal, 1956) for finding MSTs.\nSection Titles (if exists):  ['4 Implementation', '4.1 Design Decisions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.py\nCitation Context: We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [URL_CITE] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.\nFootnote or Reference text (if exists): 1 https://github.com/saffsd/langid.py\nSection Titles (if exists):  ['6 Data and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.leffingwell.com/top_10.htm\nCitation Context: The lack of vocabulary for smells and flavors contrasts starkly with the interest people in the West have for flavors and fragrances, and what they are willing to spend on such products. The flavor and fragrance industry is estimated to be worth over $20 billion in 2015 [URL_CITE] . In this context, experts\u2019 recommendations are used by the pub-lic in order to help them make decisions about purchases.\nFootnote or Reference text (if exists): 1 http://www.leffingwell.com/top_10.htm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Provide context"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf\nCitation Context: For transliterating the Urdu and Konkani to a com-mon script, we used the Indic Trans library (Bhat et al., 2014), and for the others, we used Indic NLP library (Kunchukuttan, 2020) [URL_CITE] (as Urdu and Konkani not supported). In addition, there is an exception with Urdu because it follows a right to left writing system and all other Indian languages follow left to right writing order. Hence, in the pro-cessing step, we also changed the order of Urdu to maintain consistency among all languages, and do-ing this also made our string similarity algorithms work more efficiently.\nFootnote or Reference text (if exists): Anoop Kunchukuttan. 2020. The IndicNLP Library. https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.1 Data Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LayneIns/Unsupervised_dialo_disentanglement\nCitation Context: Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [URL_CITE] In summary, our main contributions are three-fold:\nFootnote or Reference text (if exists): 3 Code will be publicly available at https: //github.com/LayneIns/Unsupervised_ dialo_disentanglement\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPIR-team/NLPIR\nCitation Context: Chinese texts are tokenized with Jieba (Jieba, 2017) or NLPIR (NLPIR, 2018) [URL_CITE] . 300-dimensional word vectors are trained on our insurance corpus.\nFootnote or Reference text (if exists): NLPIR. 2018. https://github.com/NLPIR-team/NLPIR. Accessed: 2018-05- 28.\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ymcui/Chinese-BERT-wwm\nCitation Context: Setup We adopt Chinese RoBERTa-large (Cui et al., 2020) in transformers library (Wolf et al., 2020) released by HFL [URL_CITE] as encoder and a 6-layer 8-head transformer as decoder. For our best results, we co-train the NE and QG models, remaining RE as a separate model.\nFootnote or Reference text (if exists): 8 Pretrained weights: https://github.com/ymcui/Chinese-BERT-wwm\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2011/03/numbers.html\nCitation Context: Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! [URL_CITE] Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in.\nFootnote or Reference text (if exists): 2 In fact, the peak record is 6,939 tweets per second, reported by http://blog.twitter.com/2011/03/numbers.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dhruvramani/C2AE-Multilabel-Classification\nCitation Context: Unfortunately, most of the related deep learning methods only target one or two as-pects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [URL_CITE] . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels.\nFootnote or Reference text (if exists): 1 https://github.com/dhruvramani/ C2AE-Multilabel-Classification\nSection Titles (if exists):  ['2 The Proposed Method (Rank-AE)', '2.3 L h and L ae Loss Functions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/intersun/LightningDOT\nCitation Context: In fact, Light-ningDOT achieves new state of the art across multiple ITR benchmarks such as Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that consume 1000\u00d7 mag-nitude of computational hours. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-training checkpoints are available at https://github.com/intersun/LightningDOT.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pku-wuwei/QCN\nCitation Context: \u2022 Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [URL_CITE]\nFootnote or Reference text (if exists): 3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/dbikel/software.html\nCitation Context: The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel\u2019s randomized parsing evaluation com-parator [URL_CITE] and found that both the precision and recall gains were significant at p \u2264 0.01.\nFootnote or Reference text (if exists): 6 Available at http://www.cis.upenn.edu/dbikel/software.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dailp.northeastern.edu/\nCitation Context: A Digital Archive for American Indian Languages Preservation and Per-severance (DAILP) has been developed for tran-scribing, translating, and contextualizing histori-cal Cherokee language documents (Bourns, 2019; Cushman, 2019). [URL_CITE] However, the translation be-tween Cherokee and English still can only be done by human translators. Given that only 2,000 flu-ent first-language speakers are left, and the major-ity of them are elders, it is important and urgent to have a machine translation system that could as-sist them with translation.\nFootnote or Reference text (if exists): 4 https://dailp.northeastern.edu/\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crowdflower.com\nCitation Context: While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [URL_CITE] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.\nFootnote or Reference text (if exists): 4 http://crowdflower.com\nSection Titles (if exists):  ['4 Crowd-Sourcing Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ebiquity.umbc.edu/resource/html/id/212/Splog-Blog-Dataset\nCitation Context: Spam filtering To estimate the spaminess of a blog, we take a simple approach. We train an SVM classifier on a labeled splog blog dataset (Kolari et al., 2006) [URL_CITE] using the top 1500 words for both spam and non-spam blogs as features. For each classified blog d we have a confidence value s(d).\nFootnote or Reference text (if exists): Kolari, P., Finin, T., Java, A., and Joshi, A. (2006). Splog blog dataset. URL: http: //ebiquity.umbc.edu/resource/html/ id/212/Splog-Blog-Dataset.\nSection Titles (if exists):  ['3 Modeling', '3.3.2 Blog level credibility indicators']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip\nCitation Context: We open source SB-TOP in the follow-ing link: [URL_CITE] http://www.dl.fbaipublicfiles.com/sbtop/SBTOP.zip. More information about the dataset can be found in the Table ?\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments', '4.1 Session Based Task Oriented Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlds.soe.ucsc.edu/sarcasm1\nCitation Context: \u2022 Debates - We use two datasets [URL_CITE] from the In-ternet Argument Corpus (IAC) (Lukin and Walker, 2017) which have been hand anno-tated for sarcasm. This dataset, unlike the first two, is mainly concerned with long text and provides a diverse comparison from the other datasets.\nFootnote or Reference text (if exists): 4 https://nlds.soe.ucsc.edu/sarcasm1\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/usage/linguistic-features#named-entities\nCitation Context: We used the Spacy Named Entity Recognition library [URL_CITE] to find the occurrences of names in the text. The resulting dataset consists of 2, 438, 897 samples.\nFootnote or Reference text (if exists): 3 https://spacy.io/usage/ linguistic-features#named-entities\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/mirrors.html\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/WeimingWen/CCRV\nCitation Context: We published our code and dataset on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/WeimingWen/CCRV\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Dataset)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lingpy.org\nCitation Context: The code was implemented in Python, as part of the LingPy library (Version 2.5, List and Forkel (2016), [URL_CITE] http://lingpy.org). The Igraph soft-ware package (Cs\u00e1rdi and Nepusz 2006) is needed to apply the Infomap algorithm.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Methods', '3.5 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Word2vec We average word embeddings trained with word2vec. [URL_CITE] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.\nFootnote or Reference text (if exists): 2 The code is available from https://code.google.com/archive/p/word2vec/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Microsoft/NeuronBlocks\nCitation Context: An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [URL_CITE] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files.\nFootnote or Reference text (if exists): 1 Code: https://github.com/Microsoft/NeuronBlocks\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/tree/master/examples/text-classification\nCitation Context: The example code to train the NLI-style models is also available. [URL_CITE] We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters.\nFootnote or Reference text (if exists): 11 https://github.com/huggingface/transformers/tree/master/examples/text-classification.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [URL_CITE] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word.\nFootnote or Reference text (if exists): 7 We use Jieba for segmentation: https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['3 Embeddings for Chinese Text']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mturk.com\nCitation Context: To look at more perceptual measures of dialogue quality, we used Amazon Mechanical Turk [URL_CITE] to an-notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants. Details on the annotation pro-cess can be found in (Gravano et al., 2011).\nFootnote or Reference text (if exists): 2 http://www.mturk.com\nSection Titles (if exists):  ['5 Entrainment and dialogue quality', '5.1 Entrainment correlates of dialogue charac-teristics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.org\nCitation Context: Verb pattern The ATB includes morphological analyses for each verb resulting from the Buckwal-ter [URL_CITE] analyzer. Semitic languages such as Arabic have a rich templatic morphology, and this analy-sis includes the root and pattern information of each verb. This feature is of particular scientific interest because it is unique to the Semitic languages, and has an interesting potential correlation with argu-ment structure.\nFootnote or Reference text (if exists): 2 http://www.ldc.org\nSection Titles (if exists):  ['4 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014) [URL_CITE] , domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/nmt\nCitation Context: We compare SFOT with a variety of meth-ods: MLE (Luong et al., 2017) [URL_CITE] , Scheudle Sam-pling (SS), TFOT and several RL-based models, i.e. RAML (Norouzi et al., 2016), SPG (Ding and Soricut, 2017), and MIXER (Ranzato et al., 2016b).\nFootnote or Reference text (if exists): Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. 2017. Neural machine translation (seq2seq) tutorial. https://github.com/tensorflow/nmt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org\nCitation Context: The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Scho\u0308lkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [URL_CITE]\nFootnote or Reference text (if exists): 2 Available at http://svmlight.joachims.org.\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ontotext.com/owlim/\nCitation Context: We use SwiftOwlim [URL_CITE] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data.\nFootnote or Reference text (if exists): 3 http://www.ontotext.com/owlim/\nSection Titles (if exists):  ['3 Knowledge Representation and Semantic Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jmcauley.ucsd.edu/data/amazon\nCitation Context: Table [URL_CITE] shows an example of the generation pro-cess. We initialize the template sequence X 0 as (universe, [MASK], .\nFootnote or Reference text (if exists): 4 http://jmcauley.ucsd.edu/data/amazon\nSection Titles (if exists):  ['3 Approach', '3.3 Aspect Conditional Masked Language Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stp.lingfil.uu.se/~nivre/research/Penn2Malt.html\nCitation Context: We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour-nal sections 2\u201321 for training, 22 for development testing and 23 for final testing. Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [URL_CITE] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011).\nFootnote or Reference text (if exists): 1 http://stp.lingfil.uu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/enwiki/\nCitation Context: In this section, we describe how we obtained W I - KI CREM. Starting from English Wikipedia [URL_CITE] , we search for sentences and pairs of sentences with the following properties: at least two distinct per-sonal names appear in the text, and one of them is repeated. We do not use pieces of text with more than two sentences to collect concise examples only.\nFootnote or Reference text (if exists): 2 https://dumps.wikimedia.org/enwiki/ dump id: enwiki-20181201\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: Our Wiktionary datasets and an open-source version of our code are available at [URL_CITE] http://eecs.berkeley.edu/~gdurrett\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We next analyze the induced vector representa-tions in the output projection matrices of our su-pertagger and joint parsers using the syntactic analogy framework (Kasai et al., 2017) [URL_CITE] . Consider, for instance, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a tran-sitive verb (t99) is to a subject relative headed by an intransitive verb (t109).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/IQMT\nCitation Context: This approach has proven very effective in recent eval-uation campaigns. Metric computation has been carried out using the IQ MT Framework for Auto-matic MT Evaluation (Gime\u0301nez, 2007) [URL_CITE] . The sim-plicity of this approach (with no training of the metric weighting scheme) ensures that the poten-tial advantages detected in our experiments are not due to overfitting effects.\nFootnote or Reference text (if exists): 1 http://www.lsi.upc.edu/\u02dcnlp/IQMT\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.1 Metric Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-30586-638\nCitation Context: Dimension of human ratings Study Sentence Planning this paper weak positive (\u03c1 = 0.33, WPS) Surface Realisation Domain weak negative (\u03c1 = 0. \u2212 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson\u2019s r = 0.96, NIST ) NLG, weather forecast (Stent et al., 2005) [URL_CITE] weak positive (\u03c1 = 0.47, LSA ) (Liu et al., 2016) weak positive (\u03c1 = 0.35, BLEU -4 ) (Elliott and Keller, 2014) positive (\u03c1 = 0.53, METEOR ) (Kilickaya et al., 2017) positive (\u03c1 = 0.64, SPICE ) (Cahill, 2009) N/A (Espinosa et al., 2010) weak positive (\u03c1 = 0.43, TER ) negative (\u03c1 = \u22120.56, NIST ) paraphrasing of news N/A dialogue/Twitter pairs N/A image caption N/A image caption negative (\u03c1 = \u22120.64, ROUGE ) NLG, German news texts positive (\u03c1 = 0.62, BLEU -4 ) NLG, news texts\nFootnote or Reference text (if exists): Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341\u2013351. https://doi.org/10.1007/978-3-540-30586-638.\nSection Titles (if exists):  ['7 Relation of Human and Automatic Metrics', '8.3 Example-based Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.research.att.com/sw/tools/fsm/\nCitation Context: The OOV rate on the TED dev2010 set is 1.1%. We used the AT&T FSM toolkit (Mohri et al., 1997) [URL_CITE] to train a trigram lan-guage model (LM) for each component (corpus). Fi-nally, the component language models were interpo-lated by minimizing the perplexity on the dev2010 set.\nFootnote or Reference text (if exists): M. Mohri, F. Pereira, and M. Riley. 1997. At&t general-purpose finite-state machine software tools, http://www.research.att.com/sw/tools/fsm/.\nSection Titles (if exists):  ['5 Speech Translation Models', '5.1 Acoustic and Language Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rikdz/GraphWriter\nCitation Context: We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [URL_CITE] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.\nFootnote or Reference text (if exists): 12 https://github.com/rikdz/GraphWriter\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code, data, and models publicly available at: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [URL_CITE] . This dataset provides pairs of English sentences with their agreement annota-tions in \u201cagree\u201d, \u201cdisagree\u201d, \u201cdiscuss\u201d and \u201cunre-lated\u201d.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/15StNc\u201d\nCitation Context: (11) \u201cTodd and Sarah Palin to divorce [URL_CITE] http://bit.ly/15StNc\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data', '4.1 Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/projects/morpho/\nCitation Context: Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005) [URL_CITE] as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained.\nFootnote or Reference text (if exists): Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.4 Turkish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://l10n.kde.org/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, [URL_CITE] movie subtitles, and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 5 http://l10n.kde.org/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries to extract etymology-related information and Google Trans-late [URL_CITE] to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well.\nFootnote or Reference text (if exists): 5 http://translate.google.com\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/gender-guesser/\nCitation Context: We analyze our dataset for gender bias. We use the Gender guesser li-brary [URL_CITE] to determine the gender of the candidates. To mimic the analysis of pronoun genders per-formed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we ob-serve the gender of the correct candidates only.\nFootnote or Reference text (if exists): 4 https://pypi.org/project/gender-guesser/\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/taka-yamakoshi/neural_constructions\nCitation Context: We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were \u201cabout the same\u201d (see Appendix A for details) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.\nSection Titles (if exists):  ['3 The DAIS dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2012T21\nCitation Context: To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [URL_CITE] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2012T21\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/complementizer/news-tls\nCitation Context: For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at [URL_CITE] https://github.com/complementizer/news-tls.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stats.wikimedia.org/EN/TablesDatabaseEdits.htm\nCitation Context: While the number of newly created articles in the online encyclopedia declined in the last few years (Suh et al., 2009), the number of edits in existing articles is rather stable. [URL_CITE] It is reasonable to assume that the latter will not change in the near future. One of the major reasons for the popular-ity of Wikipedia is its up-to-dateness (Keegan et al., 2013), which in turn requires constant editing activ-ity.\nFootnote or Reference text (if exists): 1 http://stats.wikimedia.org/EN/TablesDatabaseEdits.htm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/wtimkey/rogue-dimensions\nCitation Context: Taken together, we argue that accounting for rogue dimensions is essential when evaluating representa-tional similarity in transformer language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publically released at: http://github.com/wtimkey/rogue-dimensions\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T08\nCitation Context: Therefore, we prepared an evaluation data set that consists of a list of sen-tences containing at least one of the five temporal connectives since, betwen, from, before and after. To do this, we extract all sentences that satisfy the condition from 183 articles in the TimeBank 1.2 corpus [URL_CITE] . This results in a total of 486 sentences.\nFootnote or Reference text (if exists): 3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp? catalogId=LDC2006T08\nSection Titles (if exists):  ['3 Experimental Study', '3.1 Data Preparation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nCitation Context: We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [URL_CITE] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison.\nFootnote or Reference text (if exists): 10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2012/03/twitter-turns-six.html\nCitation Context: Twitter is perhaps the most popular such service with over 140 million active users as of 2012. [URL_CITE] Twitter enables users to send and read text-based posts of up to 140 characters, known as tweets. Twitter users follow others or are followed.\nFootnote or Reference text (if exists): 1 For details see http://blog.twitter.com/2012/03/twitter-turns-six.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Metaphone\nCitation Context: Several sound-based encoding schemes for words have been proposed in literature such as Soundex (Knuth, 1973; Hall and Dowling, 1980), NYSIIS (Taft, 1970), Metaphone (Philips, 1990), Caver-phone (Wang, 2009) and Double Metaphone. [URL_CITE] These schemes encode words based on their sound which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European languages and are limited when apply to other family of languages like Urdu.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Metaphone\nSection Titles (if exists):  ['3 Our Method', '3.1 UrduPhone']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Ci(poetry\nCitation Context: Ci is a type of lyric poetry in the tradition of Clas-sical Chinese poetry. [URL_CITE] , SongCi is the Ci created during Song dynasty), etc., and some examples are illustrated in Figure 1. The typical characteristics of these text can be categorized into three folds: (1) The assembling of text must comply fully with the predefined rigid formats.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Ci (poetry)\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb\nCitation Context: Table 2 primarily compares the accuracy of the Collins model 3 and RH parsers. The entries show the proportion of fully accurate parses, the f-score average of bracket precision and recall, and average crossing brackets, as obtained by EVALB (Sekine and Collins, 1997) [URL_CITE] . The RH f-score is currently somewhat lower, but the proportion of fully correct parses is significantly higher.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 1997. EvalB. Available at http://nlp.cs.nyu.edu/evalb\nSection Titles (if exists):  ['4 Indicative Current Results', '4.2 Comparative Acccuracy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/\nCitation Context: We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [URL_CITE] between the tests. The converted scores were then divided by two.\nFootnote or Reference text (if exists): 2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.\nSection Titles (if exists):  ['2 Experimental Setup', '2.3 Standardized English Tests']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: VEH. Literary texts include a number of vehi-cles defined as \u201ca physical device primarily de-signed to move an object from one location to an-other\u201d (LDC, 2005) [URL_CITE] ; ships, trains, and carriages dominate since nearly all texts were written before the rise of automobiles.\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['3 Annotation', '3.1 Entity types']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/Jf3nn\nCitation Context: For VG-NSL we run the authors\u2019 code. [URL_CITE] We re-implement C-PCFG using automatic differentiation (Eisner, 2016) to speed up training. Our VC-PCFG com-prises a parsing model and an image-text match-ing model.\nFootnote or Reference text (if exists): 5 https://git.io/Jf3nn.\nSection Titles (if exists):  ['4 Experiments', '4.2 Settings and hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ibm.com/cloud/watson-assistant/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant [URL_CITE] , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 3 https://www.ibm.com/cloud/watson-assistant/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/textblob/\nCitation Context: We use PoS (Part of Speech) information as a way of capturing error types of current word-level QE systems. First, the MT output from the training set of the WMT 2019 QE shared task is PoS tagged using the TextBlob library [URL_CITE] , which provides a sim-ple API for common natural language processing tasks . The PoS-tagged MT is then fed into the \u201cQEBrain\u201d model (Wang et al., 2018) to generate quality predictions.\nFootnote or Reference text (if exists): 1 https://pypi.org/project/textblob/\nSection Titles (if exists):  ['3 Concept and Implementation', '3.1.1 Pre-Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1002/9781444395037.ch5\nCitation Context: Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [URL_CITE] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport.\nFootnote or Reference text (if exists): Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181\u2013224. https://doi.org/10.1002/9781444395037.ch5.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008) [URL_CITE] . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['4 Results of Dependency to Phrase Structure Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [URL_CITE] https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf\nCitation Context: During annotation, we follow the defini-tion of events from the ACE annotation guideline. [URL_CITE] We use the manual annotation as the ground truth and compare it with the event detection model out-put to calculate the metrics (i.e., precision, recall and F1) in Table 3.\nFootnote or Reference text (if exists): 11 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf\nSection Titles (if exists):  ['A.3 Top Ten Extracted Events']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/mlqa\nCitation Context: We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [URL_CITE] MLQA contains QA instances in 7 languages, English, Ara-bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in-stances in English and 5K in each other lan-guage, with each instance parallel between 4 languages on average.\nFootnote or Reference text (if exists): 1 MLQA is publicly available at https://github.com/facebookresearch/mlqa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.org\nCitation Context: Verb pattern The ATB includes morphological analyses for each verb resulting from the Buckwal-ter [URL_CITE] analyzer. Semitic languages such as Arabic have a rich templatic morphology, and this analy-sis includes the root and pattern information of each verb. This feature is of particular scientific interest because it is unique to the Semitic languages, and has an interesting potential correlation with argu-ment structure.\nFootnote or Reference text (if exists): 2 http://www.ldc.org\nSection Titles (if exists):  ['4 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Pretraining Data: We pretrain all models on the English Wikipedia and BookCorpus (Zhu et al., 2015) (WikiBooks) using the datasets library. [URL_CITE] Implementation Details: We pretrain and fine-tune our models with two NVIDIA Tesla V100 (SXM2 - 32GB) with a batch size of 32 for BASE and 64 for MEDIUM and SMALL . We pretrain all our models for up to five days each due to limited access to computational resources and funds for running experiments.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ datasets\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) \u2013 a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [URL_CITE] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.\nFootnote or Reference text (if exists): 10 https://spacy.io\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com/\nCitation Context: Yahoo! An-swers, [URL_CITE] Answers.com, etc. ), which became very popular resources for question answering.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.acm.org/crossroads/xrds3-1/aied.html\nCitation Context: The architecture of iList includes a problem model, a constraint evaluator, a feedback manager, and a graphical user interface. Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996) [URL_CITE] , have not been implemented yet, and will be included in a future version. Currently, the system provides only simple negative feedback in response to students\u2019 mistakes, as customary in constraint-based ITSs.\nFootnote or Reference text (if exists): Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.\nSection Titles (if exists):  ['4 A tutoring system for linked lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ethnologue.com\nCitation Context: Language families include Indo-European, Austronesian and Niger-Congo, and gen-era within Indo-European include Germanic, In-dic and Slavic. For more detailed trees, we used hierarchical classifications provided by Ethno-logue (Lewis et al., 2014) [URL_CITE] . The mapping between WALS and Ethnologue was done using ISO 639-3 language codes.\nFootnote or Reference text (if exists): M. Paul Lewis, Gary F. Simons, and Charles D. Fen-nig, editors. 2014. Ethnologue: Languages of the World, 17th Edition. SIL International. Online ver-sion: http://www.ethnologue.com.\nSection Titles (if exists):  ['3 Data and Preprocessing', '3.1 Typology Database and Phylogenetic Trees']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets\nCitation Context: We obtain closed-book QA datasets from https://github.com/facebookresearch/DPR/blob/master/data/downloaddata.py, knowledge-intensive language tasks from https://github.com/facebookresearch/KILT/blob/master/scripts/donwload all kilt data.py. We obtain ROPES, WIQA and QuaRTz from hug-gingface datasets ( [URL_CITE] https://huggingface.co/datasets). For more details, see Table 6.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['D Reproducibility', 'D.1 Dataset Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/question-classification\nCitation Context: This analysis is done on the 10% split of the SQuAD training set. We consider two classifications of each ques-tion \u2013 one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [URL_CITE] adapted from (Li and Roth, 2002). We use the coarse grain labels namely Abbreviation (ABBR), Entity (ENTY), Descrip-tion (DESC), Human (HUM), Location (LOC), Numeric (NUM) trained on a Logistic Regres-sion classification system .\nFootnote or Reference text (if exists): 7 https://github.com/brmson/question-classification\nSection Titles (if exists):  ['4 Experiments & Results', '4.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Then we used the bilingual dictionary to compute the data uncer-tainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped them 38 into [URL_CITE] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual 36 data.\nFootnote or Reference text (if exists): 5 https://github.com/clab/fast_align\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nCitation Context: VN is or-ganised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class mem-bers. According to the official VerbNet guidelines, [URL_CITE] \u201cVerb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.\u201d For instance, all verbs from the top-level Class 9 are labelled \u201cVerbs of Putting\u201d, all verbs from Class 30 are labelled \u201cVerbs of Perception\u201d, while Class 39 contains \u201cVerbs of Ingesting\u201d.\nFootnote or Reference text (if exists): 5 http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.lingfil.uu.se/\nCitation Context: Adjective-noun translation dataset We ran-domly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie sub-titles, available at [URL_CITE] http://opus.lingfil.uu.se/ (OpenSubtitles2012, en-it) (Tiedemann, 2012). translation: phrases are composed in source lan-guage and decomposed in target language.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Noun phrase translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [URL_CITE] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 8 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lemurproject.org/clueweb12/\nCitation Context: Although some of the facts mentioned in QnA pairs can also be found in some other text documents, another part might be unique (e.g. in Clueweb [URL_CITE] about 10% of entity pairs with exist-ing Freebase relations mentioned in Yahoo!Answers documents cannot be found in other documents). There are certain limitations in applying existing re-lation extraction algorithms to CQA data, i.e., they typically consider sentences independently and ig-nore the discourse of QnA pair text.\nFootnote or Reference text (if exists): 3 http://www.lemurproject.org/clueweb12/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://AFINN.localhost/pubdb/p.php?6010\nCitation Context: We consider two semantic dimensions, gen-der, which is explored in SOTA, and sentiment, a dimension proven difficult to capture in stan-dard word embedding models (Tang et al., 2014). We follow SOTA when choosing prior anchors for gender, while using the AFINN dictionary (Nielsen, 2011) [URL_CITE] to find prior anchors for sentiment. To evaluate the effect of \u201cfew\u201d vs. \u201cmany\u201d prior anchors, we run experiments using between 2 and 276 words depending on the dimension of interest and the dataset at hand.\nFootnote or Reference text (if exists): F. A\u030a. Nielsen. 2011. http://AFINN.localhost/pubdb/p.php?6010.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LinyangLee/BERT-Attack\nCitation Context: We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack [URL_CITE] are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set.\nFootnote or Reference text (if exists): 12 https://github.com/LinyangLee/ BERT-Attack\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/schmid\nCitation Context: Besides the slash features, we used other fea-tures in order to improve the parsing accuracy of the PCFG, inspired by the work of Klein and Man-ning (2003). The most important ones of these features [URL_CITE] will now be described in detail. Sec-tion 4.3 shows the impact of these features on labeled bracketing accuracy and empty category prediction.\nFootnote or Reference text (if exists): 1 The complete annotation program is available from the author\u2019s home page at http://www.ims.uni-stuttgart.de/ schmid\nSection Titles (if exists):  ['2 Feature Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mlpc-ucsd/BERT_Convolutions\nCitation Context: Full pre-training and fine-tuning details are outlined in Appendix A.1. [URL_CITE] Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), gram-maticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set.\nFootnote or Reference text (if exists): 2 Code is available at https://github.com/mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).\nSection Titles (if exists):  ['3 Integrating lightweight convolutions', '3.2 Composite attention and lightweight convolution experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.procyclingstats.com/team/bahrain-merida-2019\nCitation Context: \u2022 Professional websites: To annotate the triple (Bahrain-Merida 2019, hasPart, Carlos Betancur), you may search the person in professional websites, such as [URL_CITE] https://www.procyclingstats.com/team/bahrain-merida-2019. Since there is no Carlos Betancur listed in that website, please choose false.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Literature Review']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/jax\nCitation Context: We have implemented the proposed hierarchical attention using Jax, an open source library [URL_CITE] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax.\nFootnote or Reference text (if exists): 5 https://github.com/google/jax\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Our three models are implemented in Python using TensorFlow [URL_CITE] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing.\nFootnote or Reference text (if exists): 9 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonDeDeyne/SWOWEN-2018\nCitation Context: Pre-Processing Our pre-processing steps are mostly based on De Deyne et al. (2018) [URL_CITE] . These procedures consist of spell-check, unifying cue forms, removing untrustworthy participants, and Americanize all non-American spellings.\nFootnote or Reference text (if exists): 3 Raw word association test results and pre-process scripts are available in https://github.com/SimonDeDeyne/SWOWEN-2018. We re-implement their R codes with Python.\nSection Titles (if exists):  ['3 Word Association Graph']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/withattardi/wikiextractor\nCitation Context: Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [URL_CITE] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset.\nFootnote or Reference text (if exists): 9 Downloaded https://github.com/withattardi/wikiextractor\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com\nCitation Context: We take advantage of the annotation platform in Figure Eight. [URL_CITE] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator\u2019s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.\nFootnote or Reference text (if exists): 5 https://www.figure-eight.com\nSection Titles (if exists):  ['4 Dataset', '4.1 Annotation using Crowd Workers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://casmacat.eu/corpus/\nCitation Context: Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [URL_CITE] EuroParl\nFootnote or Reference text (if exists): 5 http://casmacat.eu/corpus/ Bootstrap resampling (Koehn, 2004; Graham et al., global-voices.html 2014) is used to estimate confidence intervals for\nSection Titles (if exists):  ['1 Introduction', '4.2 Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008) [URL_CITE] , which ignores function tags and empty cate-gories. We return to this issue in the conclusion.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['2 Overview and Example', '2.1 Creation of Dependency Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/\nCitation Context: The third set of features that we use are stylis-tic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK [URL_CITE] (Loper and Bird, 2002).\nFootnote or Reference text (if exists): 7 http://www.nltk.org/\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/msgs\nCitation Context: Second, since models learn useful features early, there is hope that fu-ture advances could accelerate by reducing the amount of data needed to learn which features mat-ter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [URL_CITE] https://github.com/nyu-mll/msgs.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingc9/LinearTemplatesSRW\nCitation Context: Computationally, the search time increases with increasing sentence lengths. On a reasonably modern machine, our implementation generated the above sentences in about 150 seconds while using 2.2 GB of memory. [URL_CITE]\nFootnote or Reference text (if exists): 2 The code used for this research will be made available on https://github.com/tingc9/LinearTemplatesSRW\nSection Titles (if exists):  ['9 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N10-1090\nCitation Context: Zhang et al. (2010) [URL_CITE] use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.\nFootnote or Reference text (if exists): Yao-zhong Zhang, Takuya Matsuzaki, and Jun\u2019ichi Tsujii. 2010. A Simple Approach for HPSG Su-pertagging Using Dependency Information. In Hu-man Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Associ-ation for Computational Linguistics, pages 645\u2013648. http://aclweb.org/anthology/N10-1090.\nSection Titles (if exists):  ['7 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.c\nCitation Context: The langid.py model is 88.6\u201399.2% accurate (Lui and Baldwin, 2012). We tested the origi-nal Python, a Java implementation that \u201cshould be faster than anything else out there\u201d (Weiss, 2013), a C implementation (Lui, 2014) [URL_CITE] , and our replica in hardware. We also tested CLD2 (Sites, 2013) writ-ten in C++, which has a different model that was less accurate on 4 of 6 languages selected from Europarl (Koehn, 2005).\nFootnote or Reference text (if exists): Marco Lui. 2014. Pure C natural language iden-tifier with support for 97 languages. https://github.com/saffsd/langid.c.\nSection Titles (if exists):  ['6 Experiments', '6.1 Language Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://agtk.sourceforge.net/\nCitation Context: Bird and Liberman (2001) introduce an abstract general annotation approach, based on annotation graphs. [URL_CITE] The model is best suited for speech data, where time constraints are limited within an inter-val, but it is unnecessarily complex for supporting annotations on written text.\nFootnote or Reference text (if exists): 3 http://agtk.sourceforge.net/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Leechikara/Dialogue-Based-Anti-Fraud\nCitation Context: Furthermore, our learned dialogue strategies are interpretable and flexi-ble, which can help promote real-world appli-cations. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/Leechikara/ Dialogue-Based-Anti-Fraud\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: We thus conclude that, contrary to recent trend, future work in BLI should not focus exclusively in direct retrieval approaches, nor should BLI be the only evaluation task for cross-lingual embeddings. Our code is available at [URL_CITE] https://github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP\nCitation Context: We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b) [URL_CITE] . Since KNP internally has a flag that indicates either an \u201cobligatory case\u201d or an \u201cadjacent case\u201d, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression.\nFootnote or Reference text (if exists): Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Man-ual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.\nSection Titles (if exists):  ['5 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/3dxyZ1i\nCitation Context: We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset [URL_CITE] , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels).\nFootnote or Reference text (if exists): 3 https://bit.ly/3dxyZ1i\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Indexing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0\nCitation Context: Named entity counts and concreteness ratio. For each turn, we calculate the number of named entities in five coarse-grained groups constructed from the fine-grained entity types of OntoNotes [URL_CITE] (Hovy et al., 2006): (1) events, (2) numbers, (3) organizations/locations, (4) persons, and (5) prod-ucts. We also calculate (6) a concreteness ratio: the number of named entities in the turn divided by the total number of tokens in the turn.\nFootnote or Reference text (if exists): 6 Version 5, https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0. pdf Section 2.6.\nSection Titles (if exists):  ['4 Pragmatic correlations with analysts\u2019 pre-call judgments', '4.1 Pragmatic lexical features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf\nCitation Context: 2004. LIB-SVM: a Library for Support Vector Machines, [URL_CITE] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes\nCitation Context: Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [URL_CITE] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.\nFootnote or Reference text (if exists): 3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qwone.com/jason/20Newsgroups/\nCitation Context: \u2022 20Newsgroups: is comprised of 18775 newsgroup articles with 20 categories, and contains 60698 unique words [URL_CITE] .\nFootnote or Reference text (if exists): 2 http://www.qwone.com/jason/20Newsgroups/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.utexas.edu/ml/nldata/jobquery.html\nCitation Context: We directly use Prolog logical forms provided on the web-site. [URL_CITE] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance.\nFootnote or Reference text (if exists): 9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"\nSection Titles (if exists): [\\'-\\', \\'A.1 Details of Benchmark Construction\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research-lab.yahoo.co.jp/en/software/\nCitation Context: While redistribution of tweets is prohibited, we are planning to publi-cize tweet IDs of this dataset for reproducibility. [URL_CITE]\nFootnote or Reference text (if exists): 2 The tweet IDs will be provided from https://research-lab.yahoo.co.jp/en/software/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.python.org/3/library/difflib.html\nCitation Context: To restrain the vocab-ulary size and correct the typos, we use a default vocabulary of fixed size 42K words from spaCy. Each word in the dataset is then compared with the vocabulary using the difflib library [URL_CITE] in Python (algorithm based on the Levenshtein distance), and mapped to the most similar word in the vocabu-lary. If no word with more than 90% of similarity is found, the word is considered a rare word or a typo, and is mapped to the out-of-vocabulary (OOV) word.\nFootnote or Reference text (if exists): 2 https://docs.python.org/3/library/difflib.html\nSection Titles (if exists):  ['4 Model Training', '4.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/\nCitation Context: We used lp solve [URL_CITE] to solve our ILP optimization problems. We ran experiments on two datasets.\nFootnote or Reference text (if exists): 3 From http://lpsolve.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.watchtower.org\nCitation Context: Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights , non-English Wikipedias , the Jehovah\u2019s Witnesses website [URL_CITE] , and the Rosetta project (Lands-bergen, 1989).\nFootnote or Reference text (if exists): 4 As of February 2011, there were 310 versions of the site available at http://www.watchtower.org\nSection Titles (if exists):  ['3 Task Definition', '3.4 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/row-column-intersection\nCitation Context: Experi-ments on recent benchmarks prove that the pro-posed methods can effectively locate cell val-ues on tables (up to \u223c98% Hit@1 accuracy on WikiSQL lookup questions). Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving \u223c3.4% and \u223c18.86% additional pre-cision improvement on the standard WikiSQL benchmark [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code and the models we built are available at https://github.com/IBM/row-column-intersection.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UFAL-DSG/tgen\nCitation Context: \u2022 TG EN : [URL_CITE] The system by Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2015) learns to incrementally generate deep-syntax dependency trees of candidate sentence plans (i.e. which MR elements to mention and the overall sentence structure). Surface realisation is performed using a separate, domain-independent rule-based module.\nFootnote or Reference text (if exists): 3 https://github.com/UFAL-DSG/tgen\nSection Titles (if exists):  ['2 End-to-End NLG Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T07\nCitation Context: \u2022 Caciularu et al. (2021) \u2013 graph-based Bh2019 model using representations from a Long-2 [URL_CITE] https://catalog.ldc.upenn.edu/LDC2011T07 former (Beltagy et al., 2020) with cross-document attention during pre-training\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experimental Setup', '4.4 Baselines and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pemistahl/lingua\nCitation Context: Currently, A CTIVE A NNO has three inbuilt an-notation generator implementations. The first one automatically detects the language of the generator input using the language detection library Lingua [URL_CITE] . This is an example of a statistical and rule-based annotation generator as compared to a machine learning-based generator.\nFootnote or Reference text (if exists): 6 https://github.com/pemistahl/lingua\nSection Titles (if exists):  ['4 Annotation Generator']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com\nCitation Context: Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [URL_CITE] https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/malllabiisc/pra-oda\nCitation Context: Our experiments sug-gest that ODA provides better performance than (Gardner et al., 2013) and nearly the same pre-diction performance as provided by (Gardner et al., 2014), but in both cases with the added ad-vantage of faster running time and greater flex-ibility due to its online and on-demand nature. The code along with the results can be obtained at [URL_CITE] https://github.com/malllabiisc/pra-oda.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mpi-inf.mpg.de/\nCitation Context: Since the acquirement of rule knowledge is not our focus in this paper, we utilize AMIE+ (Gala\u0301rraga et al., 2015), a modern rule mining system, to effi-ciently find Horn rules from large-scale KG, such as marriedTo(x, y) \u2227 liveIn(x, z) \u21d2 liveIn(y, z). Its source code is available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://www.mpi-inf.mpg.de/\nSection Titles (if exists):  ['3 KG Completion', '3.1 Rule Inference and Transfer']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cindyxinyiwang/TrDec_pytorch\nCitation Context: Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/universal-tagger/\nCitation Context: We made our code are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://code.google.com/p/universal-tagger/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.copyright.gov/title17/92chap1.html#107\nCitation Context: How-ever, you could claim the concept of fair use which allows the use of copyrighted material without per-mission from the copyright holder when it is used for research, scholarship, and teaching\u201d. We also consulted Section 107 [URL_CITE] of U.S. Copyright Act and ensured that our collection action fell under fair use category.\nFootnote or Reference text (if exists): 9 https://www.copyright.gov/title17/92chap1.html#107\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Refer"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/typetagging/\nCitation Context: On several languages, we report performance exceeding that of more complex state-of-the art systems. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/antonisa/embeddings\nCitation Context: Second, we both expand a standard English-centered evaluation dictionary collection to in-clude all language pairs using triangulation, and create new dictionaries for under-represented languages. [URL_CITE] Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we iden-tify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.\nFootnote or Reference text (if exists): 1 Available at https://github.com/antonisa/embeddings.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip\nCitation Context: The proposed approach is evaluated on the NLP&CC 2013 cross-lingual opinion analysis (in short, NLP&CC) dataset [URL_CITE] . In the training set, there are 12,000 labeled English Amazon.com products reviews, denoted by Train_ENG, and 120 labeled Chinese product reviews, denoted as Train_CHN, from three categories, DVD, BOOK, MUSIC.\nFootnote or Reference text (if exists): 1 http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip\nSection Titles (if exists):  ['4 Experiment', '4.1 Experiment Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.netlib.org/ode/vode.f\nCitation Context: The CSLS scheme with a neighbourhood size of 10 (CSLS-10) is adopted to build synthetic dictio-naries via the input CLWEs. A variable-coefficient ordinary differential equation (VODE) solver [URL_CITE] was implemented for the system described in Eq. (7).\nFootnote or Reference text (if exists): 3 http://www.netlib.org/ode/vode.f\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation Details of Algorithm 1']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clg.ox.ac.uk/tedcorpus\nCitation Context: To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [URL_CITE] http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/madaan/thinkaboutit\nCitation Context: This result is significant as it il-lustrates that performance can be improved by guiding a system to \u201cthink about\u201d a question and explicitly model the scenario, rather than answering reflexively. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code located at https://github.com/madaan/thinkaboutit\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ualberta.ca/~lindek/minipar.htm\nCitation Context: These features attempt to include some simple dependency information from the surrounding words, using the dependency parses produced by Minipar [URL_CITE] . These features aim to identify events that are expressed as phrases or that require knowledge of the surrounding phrase to be iden-tified.\nFootnote or Reference text (if exists): 3 http://www.cs.ualberta.ca/~lindek/minipar.htm\nSection Titles (if exists):  ['5 Classifier Features', '5.5 Governing features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/gale/data/Catalog.html\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [URL_CITE] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.\nFootnote or Reference text (if exists): 2 http://projects.ldc.upenn.edu/gale/data/Catalog.html\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/pubs/\nCitation Context: Melamed et al. (2004) [URL_CITE] give more details about conversion to GCNF, as well as the full proof of our final theorem:\nFootnote or Reference text (if exists): I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.\nSection Titles (if exists):  ['6 Generalized Chomsky Normal Form', '6.2 Step 4: Eliminate \u2019s']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/svcca\nCitation Context: Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [URL_CITE] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).\nFootnote or Reference text (if exists): 4 https://github.com/google/svcca\nSection Titles (if exists):  ['6 System Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1220175.1220321\nCitation Context: Association for Computational Linguistics, Sydney, Australia, pages 1161\u2013 1168. [URL_CITE] https://doi.org/10.3115/1220175.1220321.2016. Word alignment without null words.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/TurboParser\nCitation Context: We then tagged and parsed the selected sentences with TurboParser. [URL_CITE] Our choice of a dependency parser was motivated by our will for a fast system; in par-ticular, TurboParser attains top accuracies at a rate of 1,200 words per second, keeping parsing times below 1 second for each summarization problem. Multi-Task Learning.\nFootnote or Reference text (if exists): 9 http://www.ark.cs.cmu.edu/TurboParser\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ontotext.com/owlim/\nCitation Context: We use SwiftOwlim [URL_CITE] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data.\nFootnote or Reference text (if exists): 3 http://www.ontotext.com/owlim/\nSection Titles (if exists):  ['3 Knowledge Representation and Semantic Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: Our Wiktionary datasets and an open-source version of our code are available at [URL_CITE] http://eecs.berkeley.edu/~gdurrett\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freelang.net/dictionary\nCitation Context: These dictionaries provided high-quality manually edited word translations. In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site ( [URL_CITE] http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium). Compiled without professional editing, these bilingual word lists contain errors and inaccurate translations, and hence they introduced noise into the mapping pro-cess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2010S01\nCitation Context: We used Fisher Spanish corpus (Graff et al., 2010) [URL_CITE] to perform Spanish speech to English text transla-tion. And we followed previous works (Inaguma et al., 2019) for pre-processing steps, and 40/160 hours of train set, standard dev-test are used for the experiments.\nFootnote or Reference text (if exists): David Graff, Shudong Huang, Ingrid Carta-gena, Kevin Walker, , and Christopher Cieri. 2010. Fisher spanish speech (ldc2010s01). https://catalog.ldc.upenn.edu/LDC2010S01.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://leaderboard.allenai.org/orb/submissions/public\nCitation Context: ORB Evaluation Finally, Table 4 shows that our model trained with dynamic sampling and het-erogeneous batches significantly outperforms the previous ORB state-of-the-art NABERT baseline model (submitted on 11/12/2019 on the leader-board site [URL_CITE] ).\nFootnote or Reference text (if exists): 4 https://leaderboard.allenai.org/orb/submissions/ public Language Processing (EMNLP).\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tdozat/Parser-v1\nCitation Context: (2016). On the Chinese CoNLL-2009 data, our parser achieves 85.80% in LAS, whereas the origi-nal tensorflow-based parser [URL_CITE] achieves 85.54% (85.38% reported in their paper) under the same parameter settings and external word embedding.\nFootnote or Reference text (if exists): 6 https://github.com/tdozat/Parser-v1\nSection Titles (if exists):  ['5 Experiments', '5.1 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/docs/en/language-identification.html\nCitation Context: It is surpris-ing that almost 6% of the data seems to have the wrong source or target language. [URL_CITE]\nFootnote or Reference text (if exists): 5 LID itself may also commit errors, we used https://fasttext.cc/docs/en/language-identification.html\nSection Titles (if exists):  ['5 Experimental evaluation: improving WMT\u201914 En-De NMT systems', '5.2 Filtering Common Crawl']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/AntNLP/gnn-dep-parsing\nCitation Context: The default setting for our final parser is a 2-layer GNN model that uses hd \u25b7 h (Equation 8) aggregating function and \u201cH-first\u201d asynchronous update method (Equation 9). [URL_CITE]\nFootnote or Reference text (if exists): 6 Our implementation is publicly available at: https: //github.com/AntNLP/gnn-dep-parsing\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html\nCitation Context: \u2022 M AX E NT , Zhang Le\u2019s C++ implementation [URL_CITE] of maximum entropy modelling (Berger et al., 1996). In our experiments, we used L-BFGS parameter es-\nFootnote or Reference text (if exists): 8 Available from http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.waikato.ac.nz/ml/weka/\nCitation Context: Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [URL_CITE] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).\nFootnote or Reference text (if exists): 2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/)\nSection Titles (if exists):  ['2 Related Work and Experimental Framework']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.vivino.com\nCitation Context: These systems are also a first step towards a recommender system for wines based on review content and flavor descriptions. Current recom-mender systems such as the mobile apps Vivino [URL_CITE] or Delectable work with metadata and user-based filtering, i.e. the principle of \u2018other users also bought .\nFootnote or Reference text (if exists): 2 Vivino: http://www.vivino.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation\nCitation Context: We make our MT evaluation code available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano/\nCitation Context: We implement our models and DeClarE with Theano [URL_CITE] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.\nFootnote or Reference text (if exists): 3 http://deeplearning.net/software/theano/\nSection Titles (if exists):  ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20\nCitation Context: Zhang et al. (2018) [URL_CITE] approach retweet prediction as a multi-class classification problem, and present a feature-weighted model, where weights are com-puted using information gain.\nFootnote or Reference text (if exists): Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model. http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20.\nSection Titles (if exists):  ['3 Related work', '3.2 Predicting tweet popularity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nCitation Context: For the experiments with external resources in the open setting, we utilize 1) word embed-dings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pre-trained on Wikipedia and Gigaword for English; and 2) ELMo [URL_CITE] (Peters et al., 2018) and BERT (Devlin et al., 2018), two recently proposed effec-tive deep contextualized word representations .\nFootnote or Reference text (if exists): 3 We use the released model on their website: https://github.com/allenai/allennlp/blob/master/tutorials/howto/elmo.md\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sfu.ca/~mtaboada\nCitation Context: This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [URL_CITE] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7\nFootnote or Reference text (if exists): 6 https://www.sfu.ca/\u02dcmtaboada\nSection Titles (if exists):  ['5 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/pull/1095\nCitation Context: Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accu-racies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code can be found at https://github.com/pytorch/fairseq/pull/1095\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://keras.io\nCitation Context: Our model was implemented in Keras [URL_CITE] . Although not common in most deep-learning approaches, we performed 10-fold cross-validation (CV) to obtain a more reliable estimation for our model\u2019s perfor-mance.\nFootnote or Reference text (if exists): 7 https://keras.io\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Model Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tedlab.mit.edu/\u223cdr/Tgrep2/\nCitation Context: (2005) for an overview and critical comparisons.) TGrep2 [URL_CITE] is a a grep-like utility for the Penn Treebank corpus of parsed Wall Street Journal texts. It allows Boolean expressions over nodes and regular expressions in-side nodes.\nFootnote or Reference text (if exists): 1 http://tedlab.mit.edu/\u223cdr/Tgrep2/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: The Dela was manually developed in the 90\u2019s by a team of linguists. We used the distribution freely available in the platform Unitex [URL_CITE] (Paumier, 2011). It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns).\nFootnote or Reference text (if exists): 3 http://igm.univ-mlv.fr/\u02dcunitex\nSection Titles (if exists):  ['4 Resources', '4.2 Lexical resources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fever.ai/\nCitation Context: A rapid increase in the spread of misinformation on the Internet has necessitated automated solu-tions to determine the validity of a given piece of information. To this end, the Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018a) [URL_CITE] introduced a dataset for evidence-based fact verification. Given a claim, the task in-volves extracting relevant evidence sentences from a given Wikipedia dump and assigning a label to the claim by reasoning over the extracted evidence (S UPPORTS / R EFUTES / N OT E NOUGH I NFO ).\nFootnote or Reference text (if exists): 1 https://fever.ai/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iptc.org\nCitation Context: We suggest the we adopt text categorization on top of topic detection and tracking, similar to Figure 3. There has been good results in text categorization (see, e.g., (Yang and Liu, 1999; Sebastiani, 2002)) The pre-defined categories would form the static hierarchy \u2013 the IPTC Subject Reference System [URL_CITE] , for example \u2013 on top of all event-based information organization, and the mod-els for the categories could be built on the basis of the test set.\nFootnote or Reference text (if exists): 2 International Press Telecommunications Council, http://www.iptc.org\nSection Titles (if exists):  ['5 Dynamic Hierarchies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://computing.open.ac.uk/coda/\nCitation Context: We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University\u2019s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [URL_CITE] http://computing.open.ac.uk/coda/) which was funded by the UK\u2019s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/go-through#go-through\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org\nCitation Context: To alleviate the data sparsity problem, EB is first trained on a very large corpus [URL_CITE] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R.\nFootnote or Reference text (if exists): 3 Wikipedia(http://www.wikipedia.org) is used in practice.\nSection Titles (if exists):  ['3 The Proposed Method', '3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph', '3.2.1 Learning Word Embedding for']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bastings/interpretable_predictions\nCitation Context: We use the same initial hyperpa-rameter settings used by the authors in their codebase. We use the Hard Kumaraswamy distribution as provided by the authors here [URL_CITE] https://github.com/bastings/interpretable_predictions.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.postgresql.org\nCitation Context: To enable fast access to the sense inventories and effective parallel predictions, the WSD models ob-tained at the previous step were indexed in a rela-tional database. [URL_CITE] In particular, each word sense is represented by its hypernyms, related words, and usage examples. Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format.\nFootnote or Reference text (if exists): 5 https://www.postgresql.org\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: In that way, the in-put is turned into a discourse tree, consisting of a set of hierarchically ordered and semantically interconnected sentences that present a simpli-fied syntax. Table 2 displays some examples of our transformation patterns, [URL_CITE] which are specified in terms of Tregex patterns.\nFootnote or Reference text (if exists): 2 For reproducibility purposes, the complete set of trans-formation patterns is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['3 Recursive Sentence Splitting', '3.1 Transformation Stage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ntu.edu.au/education/langs/ielex/IE-DATA1\nCitation Context: This information is obtained from manually curated cognate lists such as the data of Dyen et al. (1997) [URL_CITE] .\nFootnote or Reference text (if exists): I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.\nSection Titles (if exists):  ['1 Introduction', '1.1 Previous work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/mocha\nCitation Context: MOCHA presents a chal-lenging problem for developing accurate and robust generative reading comprehension met-rics. [URL_CITE]\nFootnote or Reference text (if exists): 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset, Code, Website)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ucasir/NPRF\nCitation Context: Unsupervised ranking models serve as baselines for comparisons. We use the open source Terrier platform\u2019s (Macdonald et al., 2012) implementa-tion of these ranking models: the DRMM and K-NRM models, and report re-sults for all six variants. Our implementation of the NPRF framework is available to enable fu-ture comparisons [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/ucasir/NPRF\nSection Titles (if exists):  ['3 Evaluation', '3.1 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kkma.snu.ac.kr/\nCitation Context: The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [URL_CITE] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.\nFootnote or Reference text (if exists): 4 http://kkma.snu.ac.kr/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html\nCitation Context: the Timebank and Aquaint annotations involving timex re-lations (timex-event, event-timex, timex-timex) (Ning et al., 2018; Goyal and Durrett, 2019). [URL_CITE] We expect the model to greatly benefit from being ex-posed to the timex relations in an MTL framework by learning about temporality in general and by adding specificity of the event-event temporal re-lations from the MATRES annotations. Figure 2 shows an example of the data annotated with an event-timex relation.\nFootnote or Reference text (if exists): 6 http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html.\nSection Titles (if exists):  ['3 Multi-task Learning', '3.2 Auxiliary Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [URL_CITE] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/arc-etrr-code\nCitation Context: The model is trained for 100 epochs. Our code is released at [URL_CITE] https://github.com/nijianmo/arc-etrr-code.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A62436.html\nCitation Context: Spearman\u2019s correlation coefficient [URL_CITE] is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson\u2019s correlation coefficient when the val-ues of data are converted into ranks before comput-ing the coefficient.\nFootnote or Reference text (if exists): 6 For a quick overview of the Spearman\u2019s coefficient, see: http://davidmlane.com/hyperstat/A62436.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Other (provides a quick overview, not used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dialogflow.com/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) [URL_CITE] , Waston Assistant , Luis , wit.ai , snips.ai , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 2 https://dialogflow.com/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.twitter.com/en/docs/tweets/optimize-with-cards\nCitation Context: A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter\u2019s card API [URL_CITE] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID.\nFootnote or Reference text (if exists): 5 https://developer.twitter.com/en/docs/tweets/optimize-with-cards\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: After finding concept boundaries, we determine the probability distribution for each concept over 4 possible types (TEST, TRE, PROB or NULL). These probability distributions are found using a multi-class SVM classifier (Chang and Lin, 2011). Fea-tures used for training this classifier include con-cept tokens, full text of concept, bi-grams, head-word, suffixes of headword, capitalization pattern, shallow parse constituent, Metamap type of concept, MetaMap type of headword, occurrence of concept in MeSH (Url4, 2013) [URL_CITE] and SNOMED CT (Url5, 2013), MeSH and SNOMED CT descriptors.\nFootnote or Reference text (if exists): Url4. 2013. Mesh: Medical subject headings (http://www.nlm.nih.gov/mesh/meshhome.html) (ac-cessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/bk42ZQ\u201d\nCitation Context: (doubtful) \u201cPresident Barack Obama\u2019s Religion: Christian, Muslim, or Agnostic? - The News of Today (Google): Share With Friend... [URL_CITE] http://bit.ly/bk42ZQ\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/jazzydicts\nCitation Context: For spell-checking, we use British and US-American English Jazzy dic-tionaries. [URL_CITE] Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011).\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/ jazzydicts\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pstadler1990/nmt_paper21_appendix\nCitation Context: \u2022 Epoch 39 (iteration 45): Highest BLEU score The complete dataset can be found online in the repository [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://github.com/pstadler1990/nmt_ paper21_appendix Computational Linguistics (ACL).\nSection Titles (if exists):  ['5 Results', '5.1.1 Snapshot selection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [URL_CITE] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2011T13\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://meta.wikimedia.org/wiki/User_classes\nCitation Context: Meta data features We consider the comment, author, time stamp or any other flag (\u201cminor change\u201d) of r v as meta data. The Wikimedia user group [URL_CITE] of an author specifies the edit permissions of this user (e.g. bot, administrator, blocked user).\nFootnote or Reference text (if exists): 4 http://meta.wikimedia.org/wiki/User_ classes\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N10-1089\nCitation Context: Korkontzelos and Manandhar (2010) [URL_CITE] reports an improvement in base-phrase chunking by pre-grouping MWEs as words-with-spaces. They fo-cus on compound nouns, adjective-noun construc-tions, and named entities.\nFootnote or Reference text (if exists): Ioannis Korkontzelos and Suresh Manandhar. 2010. Can recognising multiword expressions improve shallow parsing? In Human Language Technolo-gies: The 2010 Annual Conference of the North American Chapter of the Association for Compu-tational Linguistics. Association for Computational Linguistics, Los Angeles, California, pages 636\u2013 644. http://www.aclweb.org/anthology/N10-1089.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/N10-1138\nCitation Context: Key PCG factors for 2013: The causal links in PCG are more generic (Table 2) than the ones described in CGRAPH, supporting the hypothesis that latent word relationships do exist that go be-yond the scope of a single news article. The nodes of CGRAPH are tuples extracted from a seman-tic parser (SEMAFOR (Das et al., 2010) [URL_CITE] ) based on evidence of causality in a sentence. PCG poses no such restriction and derives topical (unfriended, FB) and inter-topical (healthcare, AMZN), sparse, latent and semantic relationships.\nFootnote or Reference text (if exists): Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948\u2013956. https://www.aclweb.org/anthology/N10-1138\nSection Titles (if exists):  ['5 Results', '5.2 Prediction Performance of PCG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.politifact.com\nCitation Context: We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [URL_CITE] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it.\nFootnote or Reference text (if exists): 7 https://www.politifact.com\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jkkummerfeld/berkeley-parser-analyser\nCitation Context: 3. The Berkeley Parser Analyser [URL_CITE] (Kummerfeld et al., 2012) to analyze parsing error types.\nFootnote or Reference text (if exists): 5 https://github.com/jkkummerfeld/berkeley-parser-analyser\nSection Titles (if exists):  ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nCitation Context: Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [URL_CITE] . This is similar to the removal of stop words widely used in information retrieval.\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nSection Titles (if exists):  ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: In this work, we only focus on the masked token prediction. We use the PyTorch implementation of B ERT [URL_CITE] and the pre-trained weights for B ERT -large released by Devlin et al. (2018).\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Model', '4.1 B ERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.tencent.com/ailab/nlp/dialogue/datasets/grayscale_data_release.zip\nCitation Context: To facilitate further research, we have made our col-lected grayscale data publicly available. [URL_CITE] During training, we use these grayscale responses in a way adaptive to the training matching model. At each training epoch, ten different grayscale responses are used: the top 5 retrieval responses ranked by the current matching model and all 5 seq2seq gen-eration responses.\nFootnote or Reference text (if exists): 1 Related resources can be found at https: //ai.tencent.com/ailab/nlp/dialogue/ datasets/grayscale_data_release.zip\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://blog.lawgeex.com/contractcosts/\nCitation Context: Cur-rently, legal review remains an arduous and expen-sive process. For instance, a procurement contract requires 5 hours of legal review on average, con-tributing to thousands of dollars in total cost (Cum-mins, 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Tim Cummins. 2017. Cost of processing a ba-sic contract soars to $6900 (accessed: 2021- 04-09). https://blog.lawgeex.com/contractcosts/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fred2008/TCMSA\nCitation Context: To our knowledge, we are the first to investigate graph NNs for tree senti-ment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST. We release our code and models at [URL_CITE] https://github.com/fred2008/TCMSA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ualberta.ca/~ab31/langid/\nCitation Context: We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. [URL_CITE] We discarded those names that fell into both categories (e.g. \u201cMaya\u201d) as well as those that we could not confidently classify.\nFootnote or Reference text (if exists): 1 Our tagged data are available online at http://www.cs.ualberta.ca/\u02dcab31/langid/.\nSection Titles (if exists):  ['5 Application to machine transliteration', '5.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iesl.cs.umass.edu/data/bibtex\nCitation Context: For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. [URL_CITE] We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset labeled for coref-erence, in order to assess accuracy.\nFootnote or Reference text (if exists): 3 http://www.iesl.cs.umass.edu/data/bibtex\nSection Titles (if exists):  ['4 Experiments: Author Coreference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.rulequest.com/Personal/\nCitation Context: Our results are compared against the FOIL algorithm [URL_CITE] , which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match.\nFootnote or Reference text (if exists): 3 http://www.rulequest.com/Personal/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jasonriesa.github.io/nile/\nCitation Context: Recently, a hierarchical alignment model (whose implementation is known as Nile [URL_CITE] ) (Riesa et al., 2011), which performs better than IBM models, has been proposed. In the hierarchi-cal alignment model, both source and target con-stituency trees are used for incorporating syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees.\nFootnote or Reference text (if exists): 2 http://jasonriesa.github.io/nile/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm\nCitation Context: Excerpt from Text View, with Search and Text Tabs covered by the KB, the user can load natural lan-guage text into SconeEdit from a file or the system clipboard. We take an article from Xinhuanet News Service (Xinhuanet, 2006) [URL_CITE] as an example. Figure 2 shows an excerpt of this text after it has been loaded.\nFootnote or Reference text (if exists): Xinhuanet. 2006. US accused of blocking approval of new UN human rights body. http://news.xinhuanet.com/english/2006-03/02/content_4247159.htm.\nSection Titles (if exists):  ['3 Architecture', '3.1 Text View']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://computing.open.ac.uk/coda/\nCitation Context: We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University\u2019s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [URL_CITE] http://computing.open.ac.uk/coda/) which was funded by the UK\u2019s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/pkumod/gAnswer\nCitation Context: Baselines We compare our results with the top ranking team \"jchl\" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [URL_CITE] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module.\nFootnote or Reference text (if exists): 7 https://github.com/pkumod/gAnswer\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Experimental Setup\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sshaar/That-is-a-Known-Lie\nCitation Context: \u2022 We create a specialized dataset, which we release to the research community. [URL_CITE] Un-like previous work in fact-checking, which used normalized claims from fact-checking datasets, we work with naturally occurring claims, e.g., in debates or in social media.\nFootnote or Reference text (if exists): 8 Data and code are available at the following URL: https://github.com/sshaar/ That-is-a-Known-Lie\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (both Method and Material)\nTYPE: Mixed (both Code and Dataset)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxue004cs/GCAE\nCitation Context: The experiments on SemEval datasets demon-strate the efficiency and effectiveness of our models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data is available at https://github.com/wxue004cs/GCAE\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kovvalsky/LangPro\nCitation Context: L ang P ro is an automated theorem prover for natural language. [URL_CITE] Given a set of premises and a hypothesis, it is able to prove semantic relations between them. The prover is based on a version of an-alytic tableau method specially designed for natural logic.\nFootnote or Reference text (if exists): 1 https://github.com/kovvalsky/LangPro\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/\nCitation Context: Most of the labels [URL_CITE] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in \u201cto have [a plan] CPHR \u201d, and DIR3 indicates destination.\nFootnote or Reference text (if exists): 2 http://ufal.mff.cuni.cz/ \u223c toman/pcedt/en/functors.html\nSection Titles (if exists):  ['5 Results', '5.2 Argument Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fjoch.com/mkcls.html\nCitation Context: Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [URL_CITE] has become a standard part of statis-tical machine translation systems.\nFootnote or Reference text (if exists): 1 Available from http://fjoch.com/mkcls.html.\nSection Titles (if exists):  ['2 Background']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/robertostling/eflomal/\nCitation Context: Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [URL_CITE] (\u00d6stling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary.\nFootnote or Reference text (if exists): 7 https://github.com/robertostling/eflomal/\nSection Titles (if exists):  ['3 AfroBART', '3.2 Dictionary Augmentation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/fnlp\nCitation Context: We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [URL_CITE] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times.\nFootnote or Reference text (if exists): 9 https://github.com/FudanNLP/fnlp\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/\nCitation Context: To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [URL_CITE] . We split the origi-nal training set into a dev set (top 1,000 sent.)\nFootnote or Reference text (if exists): 4 http://www.cnts.ua.ac.be/conll2000/chunking/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html\nCitation Context: We present the first parsing results for the Penn Parsed Corpus of Modern British English (PPCMBE) (Kroch et al., 2010) [URL_CITE] , showing that it can be parsed at a few points lower in F-score than the Penn Treebank (PTB) (Marcus et al., 1999). We discuss some of the differences in annotation style and source material that make a direct com-parison problematic. Some first steps at analysis of the parsing results indicate aspects of the anno-tation style that are difficult for the parser, and also show that the parser is creating structures that are not present in the training material.\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Dier-tani. 2010. Penn Parsed Corpus of Modern British English. http://www.ling.upenn.edu/hist-corpora/PPCMBE-RELEASE-1/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/\nCitation Context: One of the major development challenges is eval-uation. The conferences such as TREC [URL_CITE] , CLEF and NTCIR have provided valuable QA evaluation methods, and in addition produced and distributed corpora of questions, answers and corresponding documents. However, these conferences have fo-cused mainly on fact-based questions with short an-swers, so called factoid questions.\nFootnote or Reference text (if exists): 1 http://trec.nist.gov/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983\nCitation Context: Data We experiment on twelve languages with varying morphological typologies (Table 1) in the Universal Dependencies (UD) treebanks version 2.0 (Nivre et al., 2017) [URL_CITE] . Note that while Ara-bic and Hebrew follow a root & pattern typology, their datasets are unvocalized, which might reduce the observed effects of this typology.\nFootnote or Reference text (if exists): Joakim Nivre et al. 2017. Universal Dependencies 2.0. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://continual-vista.github.io/\nCitation Context: Evidence from child language acquisi-tion indicates that children learn Wh-questions before polar (Yes/No) questions (Moradlou and Ginzburg, 2016; Moradlou et al., 2018). Mo-tivated by this finding, we design a set of linguistically-informed experiments: i) to inves-tigate whether the order in which children ac-quire question types facilitates continual learning for computational models and, accordingly, the impact of task order on catastrophic forgetting; ii) to measure how far two well-known CL ap-proaches help to overcome the problem (Robins, 1995; Kirkpatrick et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code and data are available at the link http://continual-vista.github.io/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-atomic-edits\nCitation Context: \u2022 A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages (\u00a73 and \u00a74): [URL_CITE] http: //goo.gl/language/wiki-atomic-edits .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.acm.org/crossroads/xrds3-1/aied.html\nCitation Context: The architecture of iList includes a problem model, a constraint evaluator, a feedback manager, and a graphical user interface. Student model and pedagogical module, important components of a complete ITS (Beck et al., 1996) [URL_CITE] , have not been implemented yet, and will be included in a future version. Currently, the system provides only simple negative feedback in response to students\u2019 mistakes, as customary in constraint-based ITSs.\nFootnote or Reference text (if exists): Joseph Beck, Mia Stern, and Erik Haugsjaa. 1996. Applications of AI in education. ACM crossroads. http://www.acm.org/crossroads/xrds3-1/aied.html.\nSection Titles (if exists):  ['4 A tutoring system for linked lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/McGill-NLP/contextual-nmn\nCitation Context: We significantly outperform the baselines by as much as +10.4% absolute ac-curacy on CC-Ref+, illustrating the generaliza-tion skills of our approach. Our dataset is pub-licly available at [URL_CITE] https://github.com/McGill-NLP/contextual-nmn.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatik.uni-trier.de/\u223cley/db/\nCitation Context: We apply our method to the task of citation extraction. The input to our training algorithm is a set of matching DBLP [URL_CITE] -record/citation-text pairs and global GE criteria of the following two types: (1) alignment criteria that consider fea-tures of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word. In our experiments, the parallel record-text pairs are collected manually but this process can be auto-mated using systems that match text sequences to records in the DB (Michelson and Knoblock, 2005; Michelson and Knoblock, 2008).\nFootnote or Reference text (if exists): 2 http://www.informatik.uni-trier.de/\u223cley/db/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mdtux89/OpenNMT-py\nCitation Context: From the test split of LDC2017T10, we gener-ated 251 contrastive examples due to antecedent replacements, 912 due to pronoun type replace-ments, 1840 due to number replacements and 95 due to gender replacements. [URL_CITE] The results are shown in Table 6. The sequential encoder per-forms surprisingly well at this task, with better or on par performance with respect to the tree encoder.\nFootnote or Reference text (if exists): 5 The generated contrastive examples are available at https://github.com/mdtux89/OpenNMT-py.\nSection Titles (if exists):  ['5 Experiments', '5.1 Reentrancies', '5.1.2 Contrastive Pairs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.systransoft.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [URL_CITE] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 4 http://www.systransoft.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [URL_CITE] framework.\nFootnote or Reference text (if exists): 1 https://pytorch.org/\nSection Titles (if exists):  ['3 Methodology', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Adapter-Hub/adapter-transformers\nCitation Context: It is thus not ferring to other languages (Pires et al., 2019; Wu suited to excel at a specific language at inference and Dredze, 2019; Wu et al., 2020) and the cur-time. We propose a simple method to ameliorate 1 [URL_CITE] https://github.com/Adapter-Hub/adapter-transformers this issue by allowing the model to additionally adapt to the particular target language.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow [URL_CITE] , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 1 http://www.stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rowanzellers.com/hellaswag\nCitation Context: We study this question by introducing Hella-Swag, [URL_CITE] a new benchmark for commonsense NLI. We use Adversarial Filtering (AF), a data-collection paradigm in which a series of discrim-inators is used to select a challenging set of gen-erated wrong answers.\nFootnote or Reference text (if exists): 2 Short for Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations. Dataset and code at https://rowanzellers.com/hellaswag.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/natashamjaques/neural_chat/tree/master/BatchRL\nCitation Context: After chatting, users were asked to provide a Likert scale rating of the bot\u2019s conversation quality, fluency, diversity, contingency/relatedness, and empathy. The code for the RL models is available in open-source at [URL_CITE] https://github.com/natashamjaques/neural_chat/tree/master/BatchRL. Using the server, we collected a batch of human interaction data containing 46,061 pairs of user input and agent response.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dwadden/dygiepp\nCitation Context: For instance, propagat-ing span representations via predicted coref-erence links can enable the model to disam-biguate challenging entity mentions. Our code is publicly available at [URL_CITE] https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: KaggleDBQA We introduce KaggleDBQA, a new dataset and evaluation setting for text-to-SQL parsers to bridge the gap between SOTA DBQA research and its real-life deployment. [URL_CITE] It systemati-cally addresses three aforementioned challenges:\nFootnote or Reference text (if exists): 1 Available at https://aka.ms/KaggleDBQA.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/tools/\nCitation Context: These high scores allowed us to de-velop successful discourse similarity metrics. [URL_CITE]\nFootnote or Reference text (if exists): 2 The discourse parser is freely available from http://alt.qcri.org/tools/\nSection Titles (if exists):  ['3 Our Discourse-Based Measures', '3.1 Generating Discourse Trees']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [URL_CITE] . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data .\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nCitation Context: Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. [URL_CITE] This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in. A hypothetical tweet recommendation system could alleviate this acute information overload, e.g., by limiting the stream of tweets to those of interest to the user, or by discovering intriguing content outside the user\u2019s following network.\nFootnote or Reference text (if exists): 3 See http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dgaddy/silent_speech\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nCitation Context: BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45). [URL_CITE] The basic overview of the VerbNet structure already suggests that measur-ing verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.\nFootnote or Reference text (if exists): 6 https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt\nCitation Context: Some concepts (such as \u201chave-rel-role-91\u201d) in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list [URL_CITE] for concept rule generation. For rule \u201cVERBALIZE peacekeep-ing TO keep-01 :ARG1 peace\u201d, we will create a con-cept rule \u201c(k/keep-01 :ARG1 (p/peace)) ||| peace-keeping\u201d if the left-hand-side fragment appears in the target graph.\nFootnote or Reference text (if exists): 2 http://amr.isi.edu/download/lists/verbalization-list-v1.06.txt\nSection Titles (if exists):  ['2 Method', '2.3 Rule Acquisition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ]
]
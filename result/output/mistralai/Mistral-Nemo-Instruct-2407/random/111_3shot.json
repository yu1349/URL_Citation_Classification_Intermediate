[
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/\nCitation Context: Most of the labels [URL_CITE] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in \u201cto have [a plan] CPHR \u201d, and DIR3 indicates destination.\nFootnote or Reference text (if exists): 2 http://ufal.mff.cuni.cz/ \u223c toman/pcedt/en/functors.html\nSection Titles (if exists):  ['5 Results', '5.2 Argument Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lpl.univ-\nCitation Context: Multext-East References (Copernicus 106). [URL_CITE] http://www.lpl.univ-\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The Methodology', '3.5 Handling Unknown Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016 [URL_CITE] ), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salaniz/pycocoevalcap\nCitation Context: We use implementations of BLEU, METEOR, and ROUGE using Microsoft MS COCO evaluation scripts [URL_CITE] . We removed question marks, periods, and exclamation marks from references and candi-dates when evaluating with BLEU, METEOR, and ROUGE.\nFootnote or Reference text (if exists): 4 https://github.com/salaniz/pycocoevalcap\nSection Titles (if exists):  ['B Details on Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nCitation Context: This intuition is reflected in the bigram transition probabilities obtained from our corpus. [URL_CITE]\nFootnote or Reference text (if exists): 1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nSection Titles (if exists):  ['4 Training on Speech Acts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.uniroma1.it/wsdeval/training-data\nCitation Context: Section 3), keeping only the substitutes whose similarity to the target is higher than \u03bb. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [URL_CITE] . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense dis-tribution is also beneficial for lexical substitution.\nFootnote or Reference text (if exists): 4 We use the version released at http://nlp.uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0.\nSection Titles (if exists):  ['5 Dataset Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/vokenization\nCitation Context: Trained with these contextually gener-ated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-trained models publicly available at: https://github.com/airsplay/vokenization.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/syxu828/CSRL_dataset\nCitation Context: Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [URL_CITE] https: //github.com/syxu828/CSRL_dataset.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimental re-sults show that the transition-based linearization sys-tem runs an order of magnitude faster than a state-of-the-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at [URL_CITE] http://sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\nCitation Context: \u2022 The threshold for CFCS as classifica-tion experiments (Section 6.2 (1)) we calculated by tuning for best balanced accuaracy [URL_CITE] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['C Reproducibility Checklist', 'C.2 Neural Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit , we use their released model as baselines. For the End-to-end model, we also use their re-leased code [URL_CITE] .\nFootnote or Reference text (if exists): 7 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 The Experiment', '5.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://godel.iis.sinica.edu.tw/ROCLING\nCitation Context: Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): 2 See http://godel.iis.sinica.edu.tw/ROCLING.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://clic.cimec.unitn.it/composes\nCitation Context: The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [URL_CITE]\nFootnote or Reference text (if exists): 8 This dataset is available at http://clic.cimec.unitn.it/composes\nSection Titles (if exists):  ['5 Noun phrase generation', '5.2 Recursive decomposition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Network Training We implement our proposed method based on huggingface Transformers [URL_CITE] . Fol-lowing Wolf et al.\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mayoclinic.org/symptoms\nCitation Context: These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [URL_CITE] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped.\nFootnote or Reference text (if exists): 15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/data.php\nCitation Context: For our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manu-ally annotated them with comma information [URL_CITE] . This annotated corpus served as both training and test datasets (using cross-validation).\nFootnote or Reference text (if exists): 3 The guidelines and annotations are available at http://L2R.cs.uiuc.edu/\u02dccogcomp/data.php. 19: Remove all examples from p that are covered by r 20: end for 21: end for\nSection Titles (if exists):  ['4 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip\nCitation Context: We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [URL_CITE] Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above.\nFootnote or Reference text (if exists): 6 Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip. See Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective.\nSection Titles (if exists):  ['p = Bv (5)', '3.3 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luheng/lsgn\nCitation Context: Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and models: https://github.com/luheng/lsgn\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tsafavi/NegatER\nCitation Context: Most notably, training KB completion models with highly-ranked negative examples from N EGAT ER results in statistically significant accuracy improvements of up to 1.90 % . Code and data are available at [URL_CITE] https://github.com/tsafavi/NegatER.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/Multilingual-Model-Transfer\nCitation Context: Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at in-stance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target lan-guage and each individual source language [URL_CITE] . This enables our model to learn effectively what to share between various languages in the multilingual setup.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/microsoft/Multilingual-Model-Transfer.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [URL_CITE] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [URL_CITE] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset, Webcite)\nFUNCTION: Introduce, Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J11-2003\nCitation Context: Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [URL_CITE] ; Higashinaka and Isozaki, 2008). We primar-ily chose this architecture because it allows the user to directly relate the results of the system to the an-swer selection model.\nFootnote or Reference text (if exists): Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank an-swers to non-factoid questions from web collections. Computational Linguistics 37(2). http://aclweb.org/anthology/J11-2003.\nSection Titles (if exists):  ['2 System Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/question-classification\nCitation Context: This analysis is done on the 10% split of the SQuAD training set. We consider two classifications of each ques-tion \u2013 one determined on the first word (usually a wh-word) of the question (Figure 2 (bottom)) and one based on the output of a separate ques-tion type classifier [URL_CITE] adapted from (Li and Roth, 2002). We use the coarse grain labels namely Abbreviation (ABBR), Entity (ENTY), Descrip-tion (DESC), Human (HUM), Location (LOC), Numeric (NUM) trained on a Logistic Regres-sion classification system .\nFootnote or Reference text (if exists): 7 https://github.com/brmson/question-classification\nSection Titles (if exists):  ['4 Experiments & Results', '4.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/ttc-project\nCitation Context: The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-ried out using the TTC TermSuite [URL_CITE] that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/ttc-project\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tzutalin/labelImg\nCitation Context: Therefore, a novel dataset was created. We rendered PDF articles to JPEG image sets (using the ImageMagick package, at 72dpi), and used an open source utility (Tzutalin, 2015) [URL_CITE] to manually annotate regions. Version 1 of this dataset consists of region annotations for 100 scientific articles sampled from the PMC Open Access set.\nFootnote or Reference text (if exists): Tzutalin. 2015. Labelimg. https://github.com/tzutalin/labelImg.\nSection Titles (if exists):  ['4 Novel Labeled Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://labelstud.io/\nCitation Context: IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [URL_CITE] to create a new medical NER dataset. We collect 12127 health record images from the hospital, which are converted into text paragraphs by optical character recognition (OCR).\nFootnote or Reference text (if exists): 2 https://labelstud.io/\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://metamap.nlm.nih.gov/\nCitation Context: Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013 [URL_CITE] ), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of words in a window of size 3. We also use conjunctions of the features.\nFootnote or Reference text (if exists): Url2. 2013. Metamap (http://metamap.nlm.nih.gov/) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [URL_CITE] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.\nFootnote or Reference text (if exists): 2 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf\nCitation Context: Given unstructured transcribed CC, we extract en-tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [URL_CITE] :\nFootnote or Reference text (if exists): NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.\nSection Titles (if exists):  ['2 Approach', '2.1 Event Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nCitation Context: Keras demo [URL_CITE] , into which we incorporate the su-persense information. Figure 3 displays our net-work architecture.\nFootnote or Reference text (if exists): 7 https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nCitation Context: We generated translations using beam search with beam width 5, and followed Edunov et al. (2018) [URL_CITE] to filter the generated sentence pairs (See Ap-pendix A.1).\nFootnote or Reference text (if exists): 4 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [URL_CITE] . In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-of-speech) take specific values at the neighbor of an arc. One example of a feature assigned to a diag-onal arc at row i and column j of the alignment graph is where POS [i] denotes the part-of-speech of the ith word in a sentence.\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['2 Alignment-based coordinate structure analysis', '2.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wapo.st/2rucTq8\nCitation Context: While politicians may refrain themselves from making outright false claims to avoid being fact-checked, oftentimes they even double down after their false claims are debunked. [URL_CITE]\nFootnote or Reference text (if exists): 9 https://wapo.st/2rucTq8\nSection Titles (if exists):  ['2 System Architecture and Components', '2.5 Claim Matcher']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: For the former, we take all tokens excluding stop words. [URL_CITE] Together with gender attributes from Caliskan et al. (2017), we calculate and show the\nFootnote or Reference text (if exists): 8 We use spaCy (https://spacy.io/) to tokenize the corpus and remove stop words.\nSection Titles (if exists):  ['3 Detecting Gender Biases in Events']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P14-1006\nCitation Context: Association for Compu-tational Linguistics, Baltimore, Maryland, pages 58\u201368. [URL_CITE] http://www.aclweb.org/anthology/P14-1006.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cfmrp/mtool\nCitation Context: (2017). For UCCA parsing, we employ the official evaluation metric [URL_CITE] of the shared task, conceptually similar to the Smatch score.\nFootnote or Reference text (if exists): 15 https://github.com/cfmrp/mtool\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.waikato.ac.nz/ml/weka/\nCitation Context: Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [URL_CITE] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).\nFootnote or Reference text (if exists): 2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/)\nSection Titles (if exists):  ['2 Related Work and Experimental Framework']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://phontron.com/lader\nCitation Context: For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types of pre-ordering: original order with F 0 \u2190 F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latent derivations (lader). [URL_CITE] Except when stated oth-erwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 \u22123 (chosen through cross-validation).\nFootnote or Reference text (if exists): 7 Available open-source: http://phontron.com/lader\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html\nCitation Context: Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we\u2019ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [URL_CITE]\nFootnote or Reference text (if exists): 6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.\nSection Titles (if exists):  ['3 Corpus, Features, and Data Creation', '3.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [URL_CITE] systems in ELS task of SENSEVAL-3.\nFootnote or Reference text (if exists): 3 we SV M light ,used linear available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Experiment 1: LP vs. SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elsevier.com\nCitation Context: Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [URL_CITE] . The documents were taken from the medical domain within the sub-domain of \u201cbreast cancer\u201d.\nFootnote or Reference text (if exists): 2 http://www.elsevier.com\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json\nCitation Context: The example code to train the NLI-style models is also available. We use the roberta-base configuration [URL_CITE] for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters.\nFootnote or Reference text (if exists): 12 https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.engappai.2020.103641\nCitation Context: Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [URL_CITE] ), character-level perturbations are not expected to change the text\u2019s meaning in most cases. Therefore, they can be automatically produced and used for testing the robustness of NLP systems.\nFootnote or Reference text (if exists): Jincheng Xu, and Qingfeng Du. 2020. TextTricker: Loss-based and gradient-based adversarial attacks on text classification models. Engineering Applications of Artificial Intelligence, 92: 103641. https://doi.org/10.1016/j.engappai.2020.103641.\nSection Titles (if exists):  ['5 Experimental results', '5.1 Performance on perturbed inputs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/iterative-search-semparse\nCitation Context: Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at [URL_CITE] https://github.com/allenai/iterative-search-semparse.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Experiments', '6.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pan.webis.de\nCitation Context: For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [URL_CITE] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).\nFootnote or Reference text (if exists): 3 http://pan.webis.de\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wiktionary.org/\nCitation Context: BV 1 : P V 1 :: BV 2 : P V 2 such as klappern:abklappern::klopfen:abklopfen. We aimed for \u2248200 analogies per particle type, focusing on the four highly frequent particle types ab, an, auf, aus. The target selection was re-stricted to PV 1 /PV 2 combinations with identical particles, and where the two PVs were deemed (near-)synonyms according to the German stan-dard dictionary DUDEN or the German Wik-tionary [URL_CITE] , as we were interested in BV\u2013PV analo-gies with semantically highly similar PVs.\nFootnote or Reference text (if exists): 2 https://www.wiktionary.org/\nSection Titles (if exists):  ['2 A Collection of BV\u2013PV Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/h-shahidi/\nCitation Context: Experimen-tal results demonstrate that a basic attention-based seq2seq model trained with the expo-nential moving average technique achieves the state of the art in both tasks. Code is avail-able at [URL_CITE] https://github.com/h-shahidi/ 2birds-gen .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/segmenter.shtml\nCitation Context: The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [URL_CITE] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/segmenter.shtml\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.863data.org.cn/\nCitation Context: One type of data is constructed by arbi-trarily collecting sentence pairs in the source- and target-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation , NIST Machine Translation Evaluation , and HTRDP Evaluation [URL_CITE] were organized to sup-port the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conver-sation data for fair evaluation.\nFootnote or Reference text (if exists): 3 http://www.863data.org.cn/\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.comp.nus.edu.sg/software/\nCitation Context: Experi-ments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M 2 scorer is available for download at [URL_CITE] http://nlp.comp.nus.edu.sg/software/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxue004cs/GCAE\nCitation Context: The experiments on SemEval datasets demon-strate the efficiency and effectiveness of our models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data is available at https://github.com/wxue004cs/GCAE\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/S1351324909990209\nCitation Context: the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009) [URL_CITE] , the systems were provided with the sources to verify against, instead of having to retrieve them.\nFootnote or Reference text (if exists): Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2009. Recognizing textual entail-ment: Rational, evaluation and approaches. Natural Language Engineering 15(4):i\u2013xvii. https://doi.org/10.1017/S1351324909990209.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bitterlemons.org\nCitation Context: Our corpus consists of articles published on the bitterlemons website [URL_CITE] . The website is set up to \u201ccontribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas\u201d.\nFootnote or Reference text (if exists): 1 http://www.bitterlemons.org\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016). In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020) [URL_CITE] . Furthermore, adversarial perturbations should not be perceived by humans (Liang et al., 2018).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jeknov/EMNLP_17_submission\nCitation Context: \u2022 Make all associated code and data publicly avail-able, including detailed analysis results. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available for download at: https://github.com/jeknov/EMNLP_17_submission\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip\nCitation Context: Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [URL_CITE] http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1 Wikipedia Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/valentinhofmann/dga\nCitation Context: The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make all our code and data publicly available at https://github.com/valentinhofmann/dga.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iitp.ac.in/~ai-nlp-ml/resources.html\nCitation Context: For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [URL_CITE] The user-level tweets consist of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 4 All the developed resources are available at http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html\nSection Titles (if exists):  ['4 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.rulequest.com/Personal/\nCitation Context: Our results are compared against the FOIL algorithm [URL_CITE] , which learns first-order horn clauses. In order to evaluate FOIL using MAP, its candidate beliefs are first ranked by the number of FOIL rules they match.\nFootnote or Reference text (if exists): 3 http://www.rulequest.com/Personal/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/conll/software.html\nCitation Context: Parsing Quality The parsing quality was evalu-ated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script [URL_CITE] . Due to this conversion, the accuracy on the relation labels is less important.\nFootnote or Reference text (if exists): 6 http://ilk.uvt.nl/conll/software.html\nSection Titles (if exists):  ['6 Evaluation', '6.3 Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/usage/linguistic-features#named-entities\nCitation Context: We used the Spacy Named Entity Recognition library [URL_CITE] to find the occurrences of names in the text. The resulting dataset consists of 2, 438, 897 samples.\nFootnote or Reference text (if exists): 3 https://spacy.io/usage/ linguistic-features#named-entities\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: We use 70 percent of stories in ROC (ROC_LM) and WP (WP_LM) for fine-tuning GPT2 (Radford et al., 2019) lan-guage model with batch size of 4. [URL_CITE] After 3 epochs of fine-tuning, the perplexity on the validation set of ROC and WP datasets are 8.28 and 25.04, re-spectively.\nFootnote or Reference text (if exists): 2 We fine-tune GPT2 language model using https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://x-factr.github.io\nCitation Context: We run both the independent and confidence-based decoding methods with 3 M-LMs, and when avail-able 8 monolingual LMs, across 23 languages, [URL_CITE] with results shown in Fig. 3.\nFootnote or Reference text (if exists): 6 Check https://x-factr.github.io for latest results.\nSection Titles (if exists):  ['5 X-FACTR Benchmark Performance', '5.1 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cogcomp.cs.illinois.edu/page/publication_view/790\nCitation Context: Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re-quired by a factor of 16. The truncated word embeddings are available from the papers web page at [URL_CITE] https://cogcomp.cs.illinois.edu/page/publication_view/790.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: We used the multilingual BERT implementation from Huggingface. [URL_CITE] The design for these exper-iments is making a comparison between a cross validation within the TED-MDB and a zero-shot transfer learning from TED-CDB to TED-MDB. Due to the unbalanced distribution of senses in TED-MDB, using the method of Easy Ensemble (Liu, 2009), we divided the Expansion data of every language in the TED-MDB into 4 parts and then each part was added into the data of other types to become the training set.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['6 Transfer Learning via TED-CDB', '6.2 Same-Domain Cross-Language Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1002/9781444395037.ch5\nCitation Context: Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [URL_CITE] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport.\nFootnote or Reference text (if exists): Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181\u2013224. https://doi.org/10.1002/9781444395037.ch5.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ontotext.com/owlim/\nCitation Context: We use SwiftOwlim [URL_CITE] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data.\nFootnote or Reference text (if exists): 3 http://www.ontotext.com/owlim/\nSection Titles (if exists):  ['3 Knowledge Representation and Semantic Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/first20hours/google-10000-english\nCitation Context: measuring gen-der biases of a whole corpus). Therefore, we also consider two additional, larger target word lists: 4) the top 10,000 most frequent words of Google\u2019s trillion word corpus (Google10K) [URL_CITE] and 5) the full vocabulary of each corpus (Full).\nFootnote or Reference text (if exists): 7 https://github.com/first20hours/google-10000-english.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Word_Association\nCitation Context: Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being pre-sented with a word (the cue) [URL_CITE] . Table 1 lists some examples of the test.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Word_ Association\nSection Titles (if exists):  ['2 Word Association Test']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.pascal-network.org/Challenges/RTE\nCitation Context: Inferencing is a core requirement of systems that participate in the current PASCAL Recognizing Textual Entailment (RTE) challenge (see [URL_CITE] http://www.pascal-network.org/Challenges/RTE and .../RTE2). It is also used in at least two of the more visible end-to-end QA systems of the present day.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rikdz/GraphWriter\nCitation Context: We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [URL_CITE] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.\nFootnote or Reference text (if exists): 12 https://github.com/rikdz/GraphWriter\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/parsing/preprocessing.html\nCitation Context: As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [URL_CITE] and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016).\nFootnote or Reference text (if exists): 10 https://radimrehurek.com/gensim/parsing/preprocessing.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.gsk.or.jp/en/catalog//\nCitation Context: 6. We have released a part of it to the public on the web [URL_CITE] .\nFootnote or Reference text (if exists): 4 The corpus data are available at https://www.gsk.or.jp/en/catalog//.\nSection Titles (if exists):  ['4 Data Development']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\nCitation Context: \u2022 The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017): [URL_CITE] https: //github.com/facebookresearch/ fastText/blob/master/ pretrained-vectors.md\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset\nCitation Context: The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [URL_CITE] .\nFootnote or Reference text (if exists): 1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nCitation Context: Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [URL_CITE]\nFootnote or Reference text (if exists): 3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nSection Titles (if exists):  ['6 Experiments', '6.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nCitation Context: Relevance (R) We use the Lucene implementa-tion [URL_CITE] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.\nFootnote or Reference text (if exists): 4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nSection Titles (if exists):  ['3 Approach', '3.1 Ranking of Candidate Justification Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: 5. CorrNet + MT: Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [URL_CITE] using Z 2 . For the task of retrieving images given a French\nFootnote or Reference text (if exists): 4 http://www.statmt.org/moses/\nSection Titles (if exists):  ['6 Experiment 2: Cross modal access using a pivot language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zxshamson/dy-conv-rec\nCitation Context: For experiments [URL_CITE] , we collect Reddit conversa-tions from three subreddits \u2014 \u201ctechnology\u201d, \u201cto-dayilearned\u201d, and \u201cfunny\u201d, each exhibiting differ-ent data statistics, discussion topics, and language styles. An absolute date is used to separate training data (before the date) from test and validation data (after the date).\nFootnote or Reference text (if exists): 1 The datasets and codes are available at: https://github.com/zxshamson/dy-conv-rec\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Material and Method)\nTYPE: Dataset and Code\nFUNCTION: Use and Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cindyxinyiwang/TrDec_pytorch\nCitation Context: Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/idirlab/factframe\nCitation Context: The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/idirlab/factframe\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1073083.1073126\nCitation Context: Association for Compu-tational Linguistics, Philadelphia, Pennsylva-nia, USA, pages 255\u2013262. [URL_CITE] https://doi.org/10.3115/1073083.1073126.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/natural-language/docs/analyzing-entities\nCitation Context: We analyze four QA tasks: NQ , [URL_CITE] SQuAD (Rajpurkar et al., 2016), QB (Boyd-Graber et al., 2012) and Trivia QA (Joshi et al., 2017). Google CLOUD - NL 2 finds and links entity mentions in QA examples.\nFootnote or Reference text (if exists): 1 For NQ , we only consider questions with short answers. 2https://cloud.google.com/natural-language/docs/ analyzing-entities\nSection Titles (if exists):  ['2 Mapping Questions to Entities']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: Answers dataset from Yahoo! Labs Webscope [URL_CITE] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc.\nFootnote or Reference text (if exists): 11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tylin/coco-caption\nCitation Context: We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image cap-tioning evaluation tool [URL_CITE] which provides evalua-tion on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedan-tam et al., 2015).\nFootnote or Reference text (if exists): 4 https://github.com/tylin/coco-caption\nSection Titles (if exists):  ['4 Experiments', '4.4 Quantitative Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.playframework.com\nCitation Context: This back-end is implemented as a RESTful API using the Play framework. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://www.playframework.com\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ankara.lti.cs.cmu.edu/side/video.swf\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/multilingual\nCitation Context: We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and [URL_CITE] https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/zoloft.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL is cited to provide an example of a Lycos Retriever page, but it's not clear how it was used in the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/Anna146/CHARM\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability\nCitation Context: \u2022 Notgood,quantifiablebetter, best etcgradable. usually [URL_CITE] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability\nSection Titles (if exists):  ['3 Annotation Scheme']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/noa/naacl2021\nCitation Context: Our code, data splits, and scripts to repro-duce our experiments are available at [URL_CITE] http://github.com/noa/naacl2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cqa.iyas.qcri.org/cQA-Arabic-Demo\nCitation Context: A second example is a cQA demo [URL_CITE] in Arabic, which retrieves data from multiple medical forums from middle-east. In this case physicians answer to patients\u2019 questions: the left panel shows a ques-tion from a user and the right panel similar ques-tions with the answers from the expert. In general there is only one (good) answer from the doctor, so this is mostly a question re-ranking task.\nFootnote or Reference text (if exists): 4 http://cqa.iyas.qcri.org/ cQA-Arabic-Demo\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016). In order to generate effective adversarial examples, most attack methods need to have access to the NLP model structure, internal weights, and hyperparameters, which may not be possible in every testing scenario (Zhang et al., 2020) [URL_CITE] . Furthermore, adversarial perturbations should not be perceived by humans (Liang et al., 2018).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008) [URL_CITE] . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['4 Results of Dependency to Phrase Structure Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nCitation Context: Consider the publicly available EHR note [URL_CITE] in Figure 1. The note is divided into the 10 sections found in that EHR (Problems, Medications, His-tory, etc.).\nFootnote or Reference text (if exists): 1 http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: More closely related is our prior work (Vyas and Carpuat, 2016) [URL_CITE] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (\u00a76.1), and show that the improvements hold even when exposed to data scarce settings (\u00a76.3).\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lemurproject.org/\nCitation Context: Finally, using the Lemur Toolkit software [URL_CITE] , we employ a KL divergence retrieval model with Dirichlet smoothing (KL). In this model, documents are ranked according to the negation of the diver-gence of query and document language models. More formally, given the query language model \u03b8 Q , and the document language model \u03b8 D , the docu-ments are ranked by \u2212D(\u03b8 Q ||\u03b8 D ), where D is the KL-divergence between the two models.\nFootnote or Reference text (if exists): 3 http://www.lemurproject.org/\nSection Titles (if exists):  ['6 Experiments', '6.1 Rumor Retrieval', '6.1.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/\nCitation Context: Text Span Extraction. Sentences are ex-tracted from the documents using the sentence tokenizer from the nltk Python package [URL_CITE] (Bird et al., 2009).\nFootnote or Reference text (if exists): 6 https://www.nltk.org/\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.chinasmack.com/glossary\nCitation Context: Slang Terms. We collect the Chinese slang terms from an online Chinese slang glossary [URL_CITE] consisting of 200 popular slang terms with English expla-nations. For English, we resort to a slang word Truth Sets.\nFootnote or Reference text (if exists): 8 https://www.chinasmack.com/glossary\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.1 Ground Truth']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nCitation Context: In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [URL_CITE] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007).\nFootnote or Reference text (if exists): 4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nCitation Context: \u2022 We use Comet\u2019s Bayes algorithm [URL_CITE] , which bal- Learning rate\nFootnote or Reference text (if exists): 6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ualberta.ca/~ab31/langid/\nCitation Context: We manually classified these names as being of ei-ther Indian or non-Indian origin, occasionally resort-ing to web searches to help disambiguate them. [URL_CITE] We discarded those names that fell into both categories (e.g. \u201cMaya\u201d) as well as those that we could not confidently classify.\nFootnote or Reference text (if exists): 1 Our tagged data are available online at http://www.cs.ualberta.ca/\u02dcab31/langid/.\nSection Titles (if exists):  ['5 Application to machine transliteration', '5.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/bigquery\nCitation Context: We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [URL_CITE] . Each of these posts is a headline from a news source.\nFootnote or Reference text (if exists): 2 https://cloud.google.com/bigquery\nSection Titles (if exists):  ['2 The Humor Dataset', '2.2 Collecting Headlines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://continual-vista.github.io/\nCitation Context: Evidence from child language acquisi-tion indicates that children learn Wh-questions before polar (Yes/No) questions (Moradlou and Ginzburg, 2016; Moradlou et al., 2018). Mo-tivated by this finding, we design a set of linguistically-informed experiments: i) to inves-tigate whether the order in which children ac-quire question types facilitates continual learning for computational models and, accordingly, the impact of task order on catastrophic forgetting; ii) to measure how far two well-known CL ap-proaches help to overcome the problem (Robins, 1995; Kirkpatrick et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code and data are available at the link http://continual-vista.github.io/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/projects/morpho/\nCitation Context: The approach presented in this paper relies on a data-driven algorithm called Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005 [URL_CITE] ) which is a language independent unsupervised machine learn-ing method to find morpheme-like units (called sta-tistical morphs) from a large text corpus. This method has several advantages over the rule-based grammatical morphemes, e.g.\nFootnote or Reference text (if exists): Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2108\nCitation Context: At a higher level of granularity, Tan et al. (2015) [URL_CITE] analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia).\nFootnote or Reference text (if exists): Luchen Tan, Haotian Zhang, Charles LA Clarke, and Mark D Smucker. 2015. Lexical compari-son between Wikipedia and Twitter corpora by us-ing word embeddings. In Assocation for Computa-tional Linguistics (ACL). pages 657\u2013661. http: //www.aclweb.org/anthology/P15-2108.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thu-coai/OpenMEVA\nCitation Context: We also provide an open-source toolkit which implements various metrics, and therefore supports the comparison and analysis of metrics. In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [URL_CITE] .\nFootnote or Reference text (if exists): 1 All the tools, data, and evaluation scripts are available at https://github.com/thu-coai/OpenMEVA\nSection Titles (if exists):  ['\u2217 Corresponding author']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [URL_CITE] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://files.pushshift.io/reddit/comments/\nCitation Context: (2017) use Wikipedia, we extract the dataset from Reddit. [URL_CITE] Since productively formed derivatives are not part of the language norm ini-tially (Bauer, 2001), social media is a particularly fertile ground for our study.\nFootnote or Reference text (if exists): 3 We draw upon the entire Baumgartner Reddit Corpus, a collection of all public Reddit posts available at https: //files.pushshift.io/reddit/comments/.\nSection Titles (if exists):  ['3 Dataset of Derivatives']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: One state-of-the-art system for MDTC, the CMSC system of Wu and Huang (2015), com-bines a classifier that is shared across all do-mains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge. This paradigm is sometimes known as the Shared-Private model (Bousmalis et al., 2016) [URL_CITE] . CMSC, however, lacks an explicit mech-anism to ensure that the shared classifier captures only domain-independent knowledge: the shared classifier may well also acquire some domain-specific features that are useful for a subset of the domains.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, [URL_CITE] and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 5 http://www.snopes.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org/wiki/Wiktionary:Idioms_that_survived_RFD\nCitation Context: Second, the baseline method assumes that for any non-compositional MWE, all components must be equally non-compositional, despite the wealth of MWEs where one or more components are com-positional (e.g. from the Wiktionary guidelines for idiom inclusion, [URL_CITE] computer chess, basketball player, telephone box).\nFootnote or Reference text (if exists): 3 http://en.wiktionary.org/wiki/ Wiktionary:Idioms_that_survived_RFD\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use the third-party tool jieba [URL_CITE] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data. Specifically, the dictionary provided by the tool is inter-sected with a user-specified dictionary.\nFootnote or Reference text (if exists): 3 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Processing steps', '5.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [URL_CITE] random seeds.\nFootnote or Reference text (if exists): 5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html\nSection Titles (if exists):  ['4 Computational Complexity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: We train a multi-class logistic regression model [URL_CITE] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Identifying Simplification Rules', '2.2 Lexical Simplification Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The dataset and baseline models will be freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [URL_CITE] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/emorynlp/reading-comprehension\nCitation Context: For the future work, we will expand the annotation for more entity types and automatically link men-tions with respect to their entities using an entity linker. All our resources including the annotated corpus and source codes of the models are avail-able at: [URL_CITE] https://github.com/emorynlp/reading-comprehension.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sinantie/NeuralAmr\nCitation Context: We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [URL_CITE] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017).\nFootnote or Reference text (if exists): 1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Model Implementation. All pretrained mod-els and victim models based on RoBERTa and BERT base are implemented with Hugging Face transformers [URL_CITE] (Wolf et al., 2019) based on Py-Torch (Paszke et al., 2019). RoBERTa distill , RoBERTa base and uncase BERT base models have 82M, 125M and 110M parameters, respectively.\nFootnote or Reference text (if exists): 10 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\nCitation Context: Saleh et al. (2019) incor-porate a Toxicity Classifier trained with data from the Toxic Comment Classification Challenge [URL_CITE] as a reward in the training hierarchical RL dialog mod-els. We compute Toxicity reward scores using this classifier as Bot Toxicity (e.g.\nFootnote or Reference text (if exists): 3 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge\nSection Titles (if exists):  ['E Implicit Rewards Details', 'E.7 Toxicity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nCitation Context: Our re-sults demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/pubs/\nCitation Context: For lack of space, some proofs are only sketched, or entirely omitted when relatively intuitive: Melamed et al. (2004) [URL_CITE] provide more details. where ( 0( represents some grouping into strings of all and only the variables appearing in the left-hand side, possibly with some additional termi-nal symbols.\nFootnote or Reference text (if exists): I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.\nSection Titles (if exists):  ['4 Generative Capacity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Word_Association\nCitation Context: Word association test is a simple and sometimes entertaining game in which participants are asked to respond with the first several words that come out in their mind (the response) after being pre-sented with a word (the cue) [URL_CITE] . Table 1 lists some examples of the test.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Word_ Association\nSection Titles (if exists):  ['2 Word Association Test']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis).\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['3 Defining Stability']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/emnlp2013.zip\nCitation Context: The coffee shop dataset is freely available [URL_CITE] for re-search purposes.\nFootnote or Reference text (if exists): 6 http://www.cs.cmu.edu/\u02dcyww/data/emnlp2013.zip\nSection Titles (if exists):  ['Coffee Shops Reviews']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://luululu.com/tweet/\nCitation Context: We use a \u201csoft\u201d normalization strategy to all of the experimental datasets by translating word to-kens of user mentions and web/url links into spe-cial tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corre-sponding strings. We also apply a \u201chard\u201d strategy by further applying lexical normalization dictio-naries (Aramaki, 2010 [URL_CITE] ; Liu et al., 2012; Han et al., 2012) to normalize word tokens in Tweets.\nFootnote or Reference text (if exists): Eiji Aramaki. 2010. TYPO CORPUS. http://luululu.com/tweet/.\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [URL_CITE] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation.\nFootnote or Reference text (if exists): 4 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 MT Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: For the ease of reference, we call the corpus so produced \u2018NICOM\u2019 for the rest of the paper. A part of our sys-tem makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006 [URL_CITE] ). Throughout the experiments, we call our approach \u2018Generic Sen-tence Trimmer\u2019 or GST.\nFootnote or Reference text (if exists): Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['5 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/glampouras/JLOLS_NLG\nCitation Context: \u2022 LOLS : [URL_CITE] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.\nFootnote or Reference text (if exists): 4 https://github.com/glampouras/JLOLS_ NLG\nSection Titles (if exists):  ['2 End-to-End NLG Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://contrib.scikit-learn.org/lightning/\nCitation Context: 16 We cross-validate regularization strength 10\u02c6(-100,-5,- 4,-3,-2,-1,0,1), model type (SVM vs. Logistic L1 vs. Logistic L2 vs. Logistic L1/L2), and whether or not to apply feature standardization for each feature set and cross-validation split separately. These are trained using lightning ( [URL_CITE] http: //contrib.scikit-learn.org/lightning/). W2V.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Early Prediction of Controversy', '4.1 Comparing Text Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/\nCitation Context: (2016), existing methods often use language and task-specific rules and strate-gies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code re-trieval (Wei et al., 2015 [URL_CITE] ; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).\nFootnote or Reference text (if exists): Yi Wei, Nirupama Chandrasekaran, Sumit Gul-wani, and Youssef Hamadi. 2015. Build-ing bing developer assistant. Techni-cal report. https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/semi-supervised-qa\nCitation Context: To enable fair comparison, we re-implement their approach using their publicly available data, and train a variant with BERT-Large. [URL_CITE] Their approach also uses cloze questions, but without translation, and heavily relies on the structure of wikipedia ar-ticles.\nFootnote or Reference text (if exists): 6 http://bit.ly/semi-supervised-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jhlee.sakura.ne.jp/JEV.html\nCitation Context: Our work defines complex words as \u201cHigh Level\u201d words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [URL_CITE] The word level is calculated by five teachers of Japanese, based on their experience and intuition. There were 7,940 high-level words out of 17,921 words in the lex-icon.\nFootnote or Reference text (if exists): 4 http://jhlee.sakura.ne.jp/JEV.html\nSection Titles (if exists):  ['4 Balanced dataset for evaluation of Japanese lexical simplification', '4.1 Extracting sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors (Penning-ton et al., 2014), downloaded from [URL_CITE] http://nlp.stanford.edu/projects/glove/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\nCitation Context: Depending on the method we use D T , D S , or u S to train topic-specific embeddings and eval-uate them as-is on two tasks that train task-specific layers on top of these fixed embeddings. The first is an unsupervised language modeling task where we train a LSTM [URL_CITE] on the adapted embed-dings (which are pinned) and report perplexity on Df T . The second is a Duplicate question detec-tion task.\nFootnote or Reference text (if exists): 4 https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nCitation Context: To the best of our knowledge, this is the first work in attempting to generate simi-les. By framing the task as a style-transfer problem we make three contributions: [URL_CITE]\nFootnote or Reference text (if exists): 4 Code & Data at https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.politifact.com\nCitation Context: We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [URL_CITE] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it.\nFootnote or Reference text (if exists): 7 https://www.politifact.com\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/2623330.2623709\nCitation Context: (Amodeo et al., 2011) proposed a hybrid model consisting of time-series analysis, to predict future events using the New York Times corpus. FBLG (Cheng et al., 2014) [URL_CITE] focused on discovering temporal dependency from time series data and applied it to a Twitter dataset mention-ing the Haiti earthquake. Similar work by Luo et al.\nFootnote or Reference text (if exists): Dehua Cheng, Mohammad Taha Bahadori, and Yan Liu. 2014. FBLG: A Simple and Effective Ap-proach for Temporal Dependence Discovery from Time Series Data (KDD \u201914). 382\u2013391. https: //doi.org/10.1145/2623330.2623709\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: For instance: [URL_CITE] http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: 3 [URL_CITE] https://spacy.io is set to 0, probes with one hidden layer and 40 hidden neurons are better in both criteria.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments', '4.2 The \u201cgood probes\u201d are good for both']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://timan.cs.uiuc.edu/downloads.html\nCitation Context: The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) [URL_CITE] . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) , where two review sets (Camera and Car) are selected.\nFootnote or Reference text (if exists): 5 http://timan.cs.uiuc.edu/downloads.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://math-qa.github.io/math-QA/\nCitation Context: The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at: [URL_CITE] https: //math-qa.github.io/math-QA/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.washington.edu/triviaqa/\nCitation Context: TriviaQA [URL_CITE] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions.\nFootnote or Reference text (if exists): 3 http://nlp.cs.washington.edu/triviaqa/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kaggle.com/c/asap-sas\nCitation Context: To our knowledge, there is little if any research investigat-ing the value of stacking for NLP applications such as automated scoring. [URL_CITE]\nFootnote or Reference text (if exists): 1 Some applications have used stacking but not analyzed its value. For example, many participants used stacking in the ASAP2 competition http://http://www.kaggle.com/c/asap-sas. Also, Heilman and Madnani (2013) used stacking for Task 7 of SemEval 2013.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jhlee.sakura.ne.jp/JEV.html\nCitation Context: Our work defines complex words as \u201cHigh Level\u201d words in the Lexicon for Japanese Language Edu-cation (Sunakawa et al., 2012). [URL_CITE] The word level is calculated by five teachers of Japanese, based on their experience and intuition. There were 7,940 high-level words out of 17,921 words in the lex-icon.\nFootnote or Reference text (if exists): 4 http://jhlee.sakura.ne.jp/JEV.html\nSection Titles (if exists):  ['4 Balanced dataset for evaluation of Japanese lexical simplification', '4.1 Extracting sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ccsasuke/man\nCitation Context: Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [URL_CITE] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions.\nFootnote or Reference text (if exists): 1 The source code of MAN is available at https://github.com/ccsasuke/man.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum\nCitation Context: WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [URL_CITE] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al.\nFootnote or Reference text (if exists): 3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://haonanyu.com/research/acl2013/\nCitation Context: The sentences were constrained to conform to the grammar in Table 1. Our corpus of 159 training samples pairs some videos with more than one sentence and some sen-tences with more than one video, with an average of 2.6 sentences per video [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our code, videos, and sentential annotations are available at http://haonanyu.com/research/acl2013/.\nSection Titles (if exists):  ['6 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The au-thors would also like to thank Thang Minh Luong for his support for this project ad Nikola Mrks\u030cic\u0301 and anonymous reviewers for their constructive feedback. The data is avail-able at [URL_CITE] http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/opendialkg\nCitation Context: The OpenDialKG dataset [URL_CITE] : a knowledge graph enhanced conversation dataset in English proposed by Moon et al. (2019).\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/opendialkg\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asahi417/kex\nCitation Context: In this section, we explain our keyword extrac-tion experimental setting. All our experiments are run on a 16-core Ubuntu computer equipped with 3.8GHz i7 core and 64GiB memory. [URL_CITE]\nFootnote or Reference text (if exists): 5 All the details to reproduce our experiments are available at https://github.com/asahi417/kex\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: The third dataset (wikipedia, henceforth) comes from the Wikipedia [URL_CITE] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it.\nFootnote or Reference text (if exists): 4 http://www.wikipedia.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [URL_CITE] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/data/glove.840B.300d.zip\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LIAAD/yake\nCitation Context: To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation [URL_CITE] (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) within each dataset.\nFootnote or Reference text (if exists): 10 https://github.com/LIAAD/yake\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.conexor.com\nCitation Context: We use Conexor functional depend- t k , by estimating p(x k ). We describe three estimat-ency parser (Conexor, 2004) [URL_CITE] to analyze the struc- ing methods of p(x k ) in following sections. ture of sentences.\nFootnote or Reference text (if exists): Conexor. 2004. Conexor Functional Dependency Grammar Parser. http://www.conexor.com\nSection Titles (if exists):  ['2. Information for Term Specificity', '2.2. Contextual Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jvking/reddit-RL-simulator\nCitation Context: There are two main contributions in this paper. First, we propose a novel reinforcement learning task with both states and combinatorial actions de-fined by natural language, [URL_CITE] which is introduced in section 2. This task, which is based on comment popularity prediction using data from the Reddit dis-cussion forum, can serve as a benchmark in social media recommendation and trend spotting.\nFootnote or Reference text (if exists): 1 Simulator code and Reddit discussion identifiers are re-leased at https://github.com/jvking/reddit-RL-simulator\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Sections 0 and 1 of the PDTB were used for de-velopment of the features described in the previous section. Here we report results using a maximum entropy classifier [URL_CITE] using ten-fold cross-validation over sections 2-22.\nFootnote or Reference text (if exists): 2 http://mallet.cs.umass.edu\nSection Titles (if exists):  ['3 Discourse vs. non-discourse usage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: Given a dependency based co-occurrence matrix described in the previous section(s), we generate B I S PARSE -D EP embeddings using the framework from our prior work (Vyas and Carpuat, 2016) [URL_CITE] , which we henceforth call B I S PARSE . B I S PARSE generates sparse, bilingual word embeddings us-ing a dictionary learning objective with a spar-sity inducing l 1 penalty. We give a brief overview of this approach, the full details of which can be found in our prior work.\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['3 Our Approach \u2013 B I S PARSE -D EP', '3.3 Bilingual Sparse Coding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/checklist\nCitation Context: In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [URL_CITE] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks.\nFootnote or Reference text (if exists): 1 https://github.com/marcotcr/checklist\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org/\nCitation Context: Our English-Portuguese training corpus consists of 28.5 Million segments from the Europarl, JRC-Aquis (Steinberger et al., 2006) and OpenSubti-tles [URL_CITE] corpora.\nFootnote or Reference text (if exists): 5 http://www.opensubtitles.org/\nSection Titles (if exists):  ['A.3 English-Portuguese']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cninfo.com.cn/new/index\nCitation Context: Data Collection with Event Labeling. We uti-lize ten years (2008-2018) ChFinAnn [URL_CITE] documents and human-summarized event knowledge bases to conduct the DS-based event labeling. We focus on five event types: Equity Freeze (EF), Equity Re-purchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value.\nFootnote or Reference text (if exists): 4 Crawling from http://www.cninfo.com.cn/new/index\nSection Titles (if exists):  ['6 Experiments', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bastings/interpretable_predictions\nCitation Context: We use the same initial hyperpa-rameter settings used by the authors in their codebase. We use the Hard Kumaraswamy distribution as provided by the authors here [URL_CITE] https://github.com/bastings/interpretable_predictions.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlpado.de/~sebastian/sigf.html\nCitation Context: We calculate statistical significance of performance differences using strati-fied shuffling (Yeh, 2000). [URL_CITE]\nFootnote or Reference text (if exists): 5 We use the software package available at http://www.nlpado.de/\u02dcsebastian/sigf.html.\nSection Titles (if exists):  ['5 Experiment 2: Lexical substitution', '5.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pan.webis.de\nCitation Context: For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [URL_CITE] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).\nFootnote or Reference text (if exists): 3 http://pan.webis.de\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/vecmap\nCitation Context: (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [URL_CITE] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction.\nFootnote or Reference text (if exists): 3 https://github.com/artetxem/vecmap\nSection Titles (if exists):  ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser/\nCitation Context: Parsing the English test set required 39.97 sec. For comparison, we tested the MST parser version 0.4.3 (Mstparser, 2007) [URL_CITE] , configured for second-order, on the same data: training took 73.9 minutes to perform 10 iterations and parsing took 97.5 sec. MST parser achieved:\nFootnote or Reference text (if exists): Mstparser 0.4.3. 2007. http://sourceforge.net/projects/mstparser/\nSection Titles (if exists):  ['8 Performance']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jayelm/lsl\nCitation Context: Code, data, and experiments are available at [URL_CITE] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://onlinebooks.library.upenn.edu\nCitation Context: Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). [URL_CITE] The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed sep-arately on each of the twelve parts, with the task of correctly identifying the chapter and section bound-aries.\nFootnote or Reference text (if exists): 2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. Malioutov, and Masao Utiyama for making their topic segmentation systems publicly available, and to the anonymous reviewers for useful feedback. This research is supported by the Beckman Postdoctoral Fellowship.\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/e-bug/nmt-difficulty\nCitation Context: We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at [URL_CITE] https://github.com/e-bug/nmt-difficulty.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-01.ibm.com/software/data/infosphere/streams/\nCitation Context: For accuracy and speed, we built our real-time data processing infrastructure on the IBM\u2019s InfoSphere Streams platform (IBM, 2012) [URL_CITE] , which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by distributing processing tasks.\nFootnote or Reference text (if exists): IBM. (2012). InfoSphere Streams, from http://www-01.ibm.com/software/data/infosphere/streams/\nSection Titles (if exists):  ['3 The System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D11-1014\nCitation Context: Association for Computa-tional Linguistics, Edinburgh, Scotland, UK., pages 151\u2013161. [URL_CITE] http://www.aclweb.org/anthology/D11-1014.ord.2016. Bilingual learning of multi-sense embeddings with discrete autoencoders.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NP-NET-research/Recursive-Semi-Markov-Model\nCitation Context: Our code is released at [URL_CITE] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mutedfaith.com/funny/life.htm\nCitation Context: The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, http://www.berro.com/Jokes or [URL_CITE] http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pku-wuwei/QCN\nCitation Context: \u2022 Our proposed Question Condensing Net-works (QCN) achieves the state-of-the-art performance on two SemEval CQA datasets, outperforming all exisiting SOTA models by a large margin, which demonstrates the effec-tiveness of our model. [URL_CITE]\nFootnote or Reference text (if exists): 3 An implementation of our model is available at https: //github.com/pku-wuwei/QCN.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/streaming/\nCitation Context: The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [URL_CITE] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018 [URL_CITE] ). It includes 2,593 dialogues (each with a weakly connected DAG discourse structure), 12,588 \u201clinguistic\u201d DUs, 31,811 \u201cnon-linguistic\u201d DUs and 31,251 semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/knowitall/openie\nCitation Context: While the relation phrase is similar to a relation in a concept map, many arguments in these tuples represent useful concepts. We used Open IE 4 [URL_CITE] , a state-of-the-art system (Stanovsky and Dagan, 2016) to process all sentences. After removing du-plicates, we obtained 4137 tuples per topic.\nFootnote or Reference text (if exists): 7 https://github.com/knowitall/openie\nSection Titles (if exists):  ['5 Corpus Creation', '5.2 Proposition Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.im2.ch\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Project)\nFUNCTION: Other (Providing context and related projects)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisisnlp.qcri.org/\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [URL_CITE] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex.\nFootnote or Reference text (if exists): 2 https://crisisnlp.qcri.org/\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mayoclinic.org/\nCitation Context: These websites are NHS Health A-Z, 10 Everyday Health, 11 Medline Plus, Think Local, Act Personal, National Careers Healthcare Job, and the Mayo Clinic. [URL_CITE] Additional words added the health lexicon. The following are the extra words added to lexicon which we did not scraped.\nFootnote or Reference text (if exists): 15 https://www.mayoclinic.org/ diseases-conditions, https://www.mayoclinic.org/symptoms, https://www.mayoclinic.org/tests-procedures, https://www.mayoclinic.org/drugs-supplements\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lit.eecs.umich.edu/downloads.html\nCitation Context: The code used in the experiments described in this paper is publicly available from [URL_CITE] http: //lit.eecs.umich.edu/downloads.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion and Recommendations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ifttt.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE/tree/repro\nCitation Context: ERNIE baselines. E RNIE [URL_CITE] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER. E RNIE +F UR +E NT is a stronger baseline, which uses the same entity dictionary as ours for entity-level masking and further pre-trains E RNIE on the same raw text as ours.\nFootnote or Reference text (if exists): 5 https://github.com/PaddlePaddle/ERNIE/tree/repro\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elsevier.com\nCitation Context: Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [URL_CITE] . The documents were taken from the medical domain within the sub-domain of \u201cbreast cancer\u201d.\nFootnote or Reference text (if exists): 2 http://www.elsevier.com\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.py\nCitation Context: We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [URL_CITE] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.\nFootnote or Reference text (if exists): 1 https://github.com/saffsd/langid.py\nSection Titles (if exists):  ['6 Data and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/nlp/dpo3/\nCitation Context: 3. A free distribution of our implementation. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://groups.csail.mit.edu/nlp/dpo3/\nSection Titles (if exists):  ['3. A free distribution of our implementation. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: For the pairwise classifier, we use a class-weighted linear SVM. [URL_CITE] We include annotated pairs, and randomly sample negative examples from pairs without a label in the crowd-sourced training dataset. We made two versions of pair-wise classifiers by tuning weight on polarized classes and negative sampling ratio by grid search.\nFootnote or Reference text (if exists): 12 http://scikit-learn.org/\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/king-kong-1933.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~treebank/tokenizer.sed\nCitation Context: Content was lowercased, then processed by regu-lar expression to collapse the following into respec-tive single symbols: emoticons; URLs; usernames (@mentions); and hashtags. Any content deemed to be a retweet (following the characters RT) was removed. Text was then tokenized according to a modified version of the Penn TreeBank tokenization standard [URL_CITE] that was less English-centric.\nFootnote or Reference text (if exists): 6 Such as codified in http://www.cis.upenn.edu/\u02dctreebank/tokenizer.sed\nSection Titles (if exists):  ['5 Real World Stream: Twitter', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fever.ai/\nCitation Context: A rapid increase in the spread of misinformation on the Internet has necessitated automated solu-tions to determine the validity of a given piece of information. To this end, the Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018a) [URL_CITE] introduced a dataset for evidence-based fact verification. Given a claim, the task in-volves extracting relevant evidence sentences from a given Wikipedia dump and assigning a label to the claim by reasoning over the extracted evidence (S UPPORTS / R EFUTES / N OT E NOUGH I NFO ).\nFootnote or Reference text (if exists): 1 https://fever.ai/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf\nCitation Context: BBN\u2019s SERIF tool 8 (Boschee et al., 2005) provides a considerable suite of document annotations that are an excellent basis for building a knowledge base. The functions SERIF can provide are based largely on the NIST ACE specification, [URL_CITE] and include: (a) identifying named-entities and classifying them by type and subtype; (b) performing intra-document co-reference analysis, including named mentions, as well as co-referential nominal and pronominal mentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detect-ing certain types of events.\nFootnote or Reference text (if exists): 9 The principal types of ACE named-entities are per-sons, organizations, and geo-political entities (GPEs). GPEs are inhabited locations with a government. See http://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf.\nSection Titles (if exists):  ['3 Pipeline Components', '3.1 SERIF']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.rochester.edu/~gildea/Verbs/\nCitation Context: Slot structures are similar to verb frames, which are manually created for the PropBank annota-tion (Palmer et al., 2005). [URL_CITE] An example of the PropBank frame for the verb to kill is:\nFootnote or Reference text (if exists): 6 http://www.cs.rochester.edu/\u02dcgildea/Verbs/\nSection Titles (if exists):  ['5 Creating Templates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: The texts are tokenized using SpaCy [URL_CITE] and au-tomatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017).\nFootnote or Reference text (if exists): 9 https://spacy.io/\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [URL_CITE] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, Website)\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nCitation Context: Consider the publicly available EHR note [URL_CITE] in Figure 1. The note is divided into the 10 sections found in that EHR (Problems, Medications, His-tory, etc.).\nFootnote or Reference text (if exists): 1 http://www.ahrq.gov/professionals/prevention-chronic-care/improve/system/pfhandbook/mod8appbmonicalatte.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geniaproject.org/genia-corpus/treebank\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [URL_CITE] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 3 http://www.geniaproject.org/genia-corpus/treebank\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\nCitation Context: Fully fine-tuned RoBERTa and the current state-of-the-art method (DeBERT) score higher by 6.5 and 9.2 points, respectively. Linear Classifier 64.2 78.1 74.9 59.2 88.4 WARP 0 70.9 78.8 77.1 72.2 89.8 WARP 1 83.9 87.6 81.6 72.6 93.8 WARP 2 85.4 88.0 81.5 69.7 94.3 WARP [URL_CITE] 86.9 92.4 83.1 68.2 95.9 WARP 8 87.6 93.0 83.8 72.9 95.4 WARP init 86.8 90.4 83.6 80.1 96.0 WARP 20 88.2 93.5 84.5 75.8 96.0 WARP MNLI 86.3\nFootnote or Reference text (if exists): 4 https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\nSection Titles (if exists):  ['4 Experiments on GLUE', '4.1 Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/sentence-transformers\nCitation Context: Additional, we have tried different aggregation methods like taking the result with the maximum confidence and we did not find a meaningful improvement. For selective demonstra-tions, we take roberta-large-nli-stsb mean-tokens [URL_CITE] from Reimers and Gurevych (2019) as our sentence embedding model.\nFootnote or Reference text (if exists): 14 https://github.com/UKPLab/ sentence-transformers\nSection Titles (if exists):  ['C Experimental Details', '-', 'C.3 Fine-tuning with demonstrations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/UnsupervisedQA\nCitation Context: To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [URL_CITE]\nFootnote or Reference text (if exists): 1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: One state-of-the-art system for MDTC, the CMSC system of Wu and Huang (2015), com-bines a classifier that is shared across all do-mains (for learning domain-invariant knowledge) with a set of classifiers, one per domain, each of which captures domain-specific text classification knowledge. This paradigm is sometimes known as the Shared-Private model (Bousmalis et al., 2016) [URL_CITE] . CMSC, however, lacks an explicit mech-anism to ensure that the shared classifier captures only domain-independent knowledge: the shared classifier may well also acquire some domain-specific features that are useful for a subset of the domains.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: 14 The ELI5 demo from Jernite (2020) [URL_CITE] also retrieves the top- 1 similar training set question. Qualitatively, we found many validation examples had near-identical train paraphrases.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['3 Analysis', '3.2 Training / Validation Overlap']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.facebook.com/research/-babi/\nCitation Context: Bordes et al. (2015) released the SimpleQuestions data set [URL_CITE] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?)\nFootnote or Reference text (if exists): 11 https://research.facebook.com/research/-babi/\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://spark.apache.org\nCitation Context: As one may observe, no human labor is used to learn interpretable sense representa-tions and the corresponding disambiguation mod-els. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework [URL_CITE] , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps.\nFootnote or Reference text (if exists): 4 http://spark.apache.org\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.1 Induction of the WSD Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\nCitation Context: The encoder-decoder model is a conditional lan-guage model that predicts a correct output se-quence from an input sequence (Sutskever et al., 2014) [URL_CITE] . This model consists of two RNNs: an en-coder and decoder. The encoder extracts a context of the input sequence as a real-valued vector, and the decoder predicts the output sequences from the context individually.\nFootnote or Reference text (if exists): Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural In-formation Processing Systems 27 (NIPS 2014), Curran Associates, Inc., pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf.\nSection Titles (if exists):  ['2 Proposed Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isomap.stanford.edu/\nCitation Context: For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word \u201cuse\u201d in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [URL_CITE] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space.\nFootnote or Reference text (if exists): 5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.4 An Example: Word \u201cuse\u201d']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hub.microsofttranslator.com\nCitation Context: To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [URL_CITE] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.\nFootnote or Reference text (if exists): 2 http://hub.microsofttranslator.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://arxiv.org/abs/1803.07416\nCitation Context: Appendix A: Implementation Details Global-Local Attention Implementation This appendix provides further details on the TPU/GPU-friendly implementation of global-local attention. Our implementation of sliding win-dow local attention is similar to the approach in the local attention 1d layer in Ten-sor2Tensor [URL_CITE] , but with the addition of flexible mask-ing, relative position encoding, and global tokens as side keys/values. We use a simple example to describe the internal blocking logic.\nFootnote or Reference text (if exists): 8 https://arxiv.org/abs/1803.07416\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cond.org/schbase.html\nCitation Context: We further show how keyphrases can be placed into a semantically-meaningful \u201cphylogenetic\u201d structure and describe key features of this structure. The com-plete SCHB ASE dataset is available at: [URL_CITE] http://cond.org/schbase.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses [URL_CITE] as well as the sense-annotated SemCor corpus (Landes et al., 1998).\nFootnote or Reference text (if exists): 4 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jasonriesa.github.io/nile/\nCitation Context: Recently, a hierarchical alignment model (whose implementation is known as Nile [URL_CITE] ) (Riesa et al., 2011), which performs better than IBM models, has been proposed. In the hierarchi-cal alignment model, both source and target con-stituency trees are used for incorporating syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees.\nFootnote or Reference text (if exists): 2 http://jasonriesa.github.io/nile/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://inprotk.sourceforge.net\nCitation Context: After a discussion of related work (Section 2), we describe the basic elements of our iSS component (Section 3) and some demonstrator applications that we created which showcase certain abilities. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code of the toolkit and its iSS component and the demo applications discussed below have been released as open-source at http://inprotk.sourceforge.net.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryanzhumich/editsql\nCitation Context: EditSQL (Zhang et al., 2019): EditSQL (with BERT) is the highest-performing model on the Spi-der dataset that also provides an open-source im-plementation along with a downloadable trained model. [URL_CITE] The model was built for edit-based multi-turn parsing tasks, but can also be used as a single-turn parser for Spider or KaggleDBQA. It employs a sequence-to-sequence model with a question-table co-attention encoder for schema encoding.\nFootnote or Reference text (if exists): 3 https://github.com/ryanzhumich/ editsql\nSection Titles (if exists):  ['4 Experiments', '4.1 Baseline Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nCitation Context: The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [URL_CITE] and annotated as described in (Bethard et al., 2012). In this section we illustrate the main annotation princi-ples for coherent temporal annotation. As an example story, consider:\nFootnote or Reference text (if exists): 1 Data available at http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mscoco.org/dataset/\nCitation Context: The MSCOCO dataset [URL_CITE] contains images and their English captions. On an average there are 5 cap-tions per image.\nFootnote or Reference text (if exists): 2 http://mscoco.org/dataset/\nSection Titles (if exists):  ['4 Datasets', '4.2 Multilingual Image Caption dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at [URL_CITE] https://aka.ms/KaggleDBQA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion & Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [URL_CITE] . This dataset provides pairs of English sentences with their agreement annota-tions in \u201cagree\u201d, \u201cdisagree\u201d, \u201cdiscuss\u201d and \u201cunre-lated\u201d.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experimental Framework', '3.3 Tools and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jind11/TextFooler\nCitation Context: We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler [URL_CITE] and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set.\nFootnote or Reference text (if exists): 11 https://github.com/jind11/TextFooler\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.englishbaby.com/lessons/4349/slang/fruitcake\nCitation Context: She is a total fruitcake. [URL_CITE]\nFootnote or Reference text (if exists): 12 http://www.englishbaby.com/lessons/4349/slang/ fruitcake\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents\nCitation Context: To estimate the timeliness and semantic cred-ibility indicators, we use AQUAINT-2, a set of newswire articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007) [URL_CITE] . Articles are in English and come from a variety of sources.\nFootnote or Reference text (if exists): AQUAINT-2 (2007). URL: http://trec.nist.gov/data/qa/2007_qadata/qa.07.guidelines.html#documents.\nSection Titles (if exists):  ['4 Evaluation', '4.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/text/_modules/torchtext/data/utils.html\nCitation Context: We normalize the text by lower casing, removing special characters, etc. [URL_CITE] For each task, we construct separate 1-Gram vo-cabulary (U) and initialize a trainable randomly sampled token embedding (U \u00d7 d e ) from N (0, 1). Similarly, we randomly initialize a (d s-max \u00d7 d e ) positional embedding.\nFootnote or Reference text (if exists): 9 https://pytorch.org/text/_modules/torchtext/data/utils.html\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor\nCitation Context: The language clustering based on language family is shown in Figure 2, which results in [URL_CITE] groups given our 23 languages. All the language names and their corresponding ISO-639-1 code can be found in Supplementary Materials (Section 2).\nFootnote or Reference text (if exists): 8 https://github.com/tensorflow/tensor2tensor\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nCitation Context: The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [URL_CITE] , containing about 28 millions of Chinese words. Since punc-tuations are rarely used to construct collocations, they were removed from the corpora. To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations.\nFootnote or Reference text (if exists): 2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nSection Titles (if exists):  ['3 Initial Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: For both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The NPMI scores presented in Table 1 are averaged across cluster centers initialized using [URL_CITE] random seeds.\nFootnote or Reference text (if exists): 5 http://qwone.com/\u02dcjason/20Newsgroups/ 6 https://www.nltk.org/book/ch02.html\nSection Titles (if exists):  ['4 Computational Complexity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://visagetechnologies.com/products-and-services/visagesdk/facetrack\nCitation Context: To extract face and head movement information from the video, a face-tracker (Visage, 2016) [URL_CITE] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation.\nFootnote or Reference text (if exists): Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.\nSection Titles (if exists):  ['2 Method', '2.1 Dataset and Feature Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ownthink.com\nCitation Context: There are four types of personal information in a Chinese loan application form: \u201cSchool\u201d, \u201cCom-pany\u201d, \u201cResidence\u201d and \u201cBirthPlace\u201d. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [URL_CITE] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked.\nFootnote or Reference text (if exists): 3 https://www.ownthink.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sussex.ac.uk/Users/drh21/\nCitation Context: A sense is output only if the overlap exceeds some threshold; we used thresholds in the range of 1\u201320 in our experi-ments. There are many subtleties and details in the implementation of SimpleExtLesk so we used two different libraries: a Java implementation of WordNet::Similarity (Pedersen et al., 2004), [URL_CITE] which we modified to accept a context string, and DKPro-WSD (Miller et al., 2013) version 1.1.0, with lemmatization, removal of stop words, paired overlap enabled and normalization disabled.\nFootnote or Reference text (if exists): 8 http://www.sussex.ac.uk/Users/drh21/\nSection Titles (if exists):  ['7 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J07-4004\nCitation Context: In the CCG parsing literature, some work op-timizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007 [URL_CITE] ; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric.\nFootnote or Reference text (if exists): Stephen Clark and James R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Lin-guistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.\nSection Titles (if exists):  ['7 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geohive.com/\nCitation Context: This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive [URL_CITE] , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems . Gazetters have been widely used in NER systems to improve performance.\nFootnote or Reference text (if exists): 3 http://www.geohive.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.1 Internal Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dr97531/SparseSCVB0\nCitation Context: In this section we study the performance of our SparseSCVB0 [URL_CITE] algorithm, on small as well as large corpora to validate the proposed method for topic models such as LDA and MMSGTM, and to compare with other state-of-the-art algorithms.\nFootnote or Reference text (if exists): 1 Code implementing SparseSCVB0 can be found at https://github.com/dr97531/SparseSCVB0.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Other"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf\nCitation Context: USAS semantic categories [URL_CITE] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1\nFootnote or Reference text (if exists): 1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf\nSection Titles (if exists):  ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Other"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sle.sharp.co.uk/senseval2/\nCitation Context: We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [URL_CITE] We set the NB model parameters in several ways. We optimized JL (using the RFE s).\nFootnote or Reference text (if exists): 2 http://www.sle.sharp.co.uk/senseval2/\nSection Titles (if exists):  ['2 Objective Functions: Naive-Bayes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [URL_CITE] (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/litbank\nCitation Context: In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [URL_CITE] https://github.com/dbamman/litbank.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability\nCitation Context: \u2022 Notgood,quantifiablebetter, best etcgradable. usually [URL_CITE] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability\nSection Titles (if exists):  ['3 Annotation Scheme']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology. [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://eecs.berkeley.edu/~gdurrett for our datasets and code.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://skylion007.github.io/OpenWebTextCorpus/\nCitation Context: Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus [URL_CITE] dataset, an open source ef-fort to reproduce OpenAI\u2019s WebText dataset. Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents.\nFootnote or Reference text (if exists): 9 https://skylion007.github.io/OpenWebTextCorpus/\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.sourceforge.net/\nCitation Context: The Instance Extractor applies the patterns gener-ated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package ( [URL_CITE] http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sen-tence. These phrases are matched to the slots of the patterns.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Description of SRES', '3.2 Instance Extractor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rkadlec/ubuntu-ranking-dataset-creator\nCitation Context: We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus [URL_CITE] extracted from Ubuntu question-answering forum, named Ubuntu (Lowe et al., 2015). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012.\nFootnote or Reference text (if exists): 1 https://github.com/rkadlec/ubuntu-ranking-dataset- creator\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: In this paper we follow the protocol of Petroni et al. (2019)\u2019s English-language LAMA bench-mark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata [URL_CITE] curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for sub-jects and objects (e.g.\nFootnote or Reference text (if exists): 1 https://www.wikidata.org/\nSection Titles (if exists):  ['2 Retrieving Facts from LMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seatgeek/fuzzywuzzy\nCitation Context: Figure 7: Average number of citations per paper that are 15 years or older, by venue of publication. tance ratio [URL_CITE] is \u2264 95%.\nFootnote or Reference text (if exists): 12 As implemented by https://github.com/seatgeek/fuzzywuzzy.\nSection Titles (if exists):  ['A Fuzzy paper matching']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983\nCitation Context: Data We experiment on twelve languages with varying morphological typologies (Table 1) in the Universal Dependencies (UD) treebanks version 2.0 (Nivre et al., 2017) [URL_CITE] . Note that while Ara-bic and Hebrew follow a root & pattern typology, their datasets are unvocalized, which might reduce the observed effects of this typology.\nFootnote or Reference text (if exists): Joakim Nivre et al. 2017. Universal Dependencies 2.0. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1027\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [URL_CITE] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with LSTMs. In NAACL. Association for Computational Linguistics, San Diego, California, pages 232\u2013237. http://www.aclweb.org/anthology/N16-1027.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LayneIns/Unsupervised_dialo_disentanglement\nCitation Context: Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [URL_CITE] In summary, our main contributions are three-fold:\nFootnote or Reference text (if exists): 3 Code will be publicly available at https: //github.com/LayneIns/Unsupervised_ dialo_disentanglement\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://linear-transformers.com\nCitation Context: We re-implement the Transformer and use the orig-inal implementation of the LT. [URL_CITE] All models are trained to minimise cross-entropy with the AdamW optimiser (Loshchilov and Hutter, 2019). We use 300-D GloVe embeddings (Pennington et al., 2014) which are passed through a linear projection layer with size d model .\nFootnote or Reference text (if exists): 1 https://linear-transformers.com\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/pyenchant/\nCitation Context: The fifth set of features that we use are lan-guage modeling features. We use the count of words that are absent in Google News word vec-tors and misspelled words using the PyEnchant [URL_CITE] library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979).\nFootnote or Reference text (if exists): 9 https://pypi.python.org/pypi/pyenchant/\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: CRF++ [URL_CITE] , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 7 http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dhruvramani/C2AE-Multilabel-Classification\nCitation Context: Unfortunately, most of the related deep learning methods only target one or two as-pects. C2AE attempts to minimize the number of misclassified pairs between relevant and irrelevant labels, as a result its computational complexity is quadratic with number of labels in the worst case; also it fails to scale well on large number of in-put features or labels due to its inefficient imple-mentation [URL_CITE] . XML-CNN (Liu et al., 2017) achieves computational efficiency by training a deep neural network with hidden layers much smaller than the output layer with binary cross-entropy loss (BCE), which has linear complexity in number of labels.\nFootnote or Reference text (if exists): 1 https://github.com/dhruvramani/ C2AE-Multilabel-Classification\nSection Titles (if exists):  ['2 The Proposed Method (Rank-AE)', '2.3 L h and L ae Loss Functions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/moving-qatar/posts/can-i-obtain-driving-license-my-qid-written-employee\nCitation Context: A simplified example is shown in Figure 1, [URL_CITE] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad. In this paper, we focus on a 2-class variant of the above Sub-task A, which is closer to a real CQA application.\nFootnote or Reference text (if exists): 3 http://www.qatarliving.com/moving-qatar/posts/can-i- obtain-driving-license-my-qid-written-employee\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see [URL_CITE] http://www.ukp.tu-darmstadt.de/fnwkde/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['10 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [URL_CITE] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps.\nFootnote or Reference text (if exists): 3 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.crowdflower.com\nCitation Context: Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower [URL_CITE] for three BBN datasets and two Syrian datasets:\nFootnote or Reference text (if exists): 3 http://www.crowdflower.com\nSection Titles (if exists):  ['5 Creating sentiment labeled data in Arabic and English']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285\nCitation Context: To get a coarse-grained representation of the polysemy of the word, we consider the number of different POS present. For a finer-grained repre-sentation, we use the number of different Word-Net senses associated with the word (Miller, 1995; Fellbaum, 1998 [URL_CITE] ).\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet. Wiley Online Library. https://onlinelibrary.wiley.com/doi/full/10.1002/9781405198431.wbeal1285.\nSection Titles (if exists):  ['4 Factors Influencing Stability', '4.2 Word Properties']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [URL_CITE] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2011T13\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano/\nCitation Context: We implement our models and DeClarE with Theano [URL_CITE] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.\nFootnote or Reference text (if exists): 3 http://deeplearning.net/software/theano/\nSection Titles (if exists):  ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NVIDIA/nv-wavenet\nCitation Context: Our full synthesis model consists of a bidirectional LSTM of 512 dimen-sions, a linear projection down to 128 dimensions, and finally the WaveNet decoder which generates samples at 16 kHz. We use a WaveNet implementa-tion from NVIDIA [URL_CITE] which provides efficient GPU inference. WaveNet hyperparameters can be found in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/NVIDIA/nv-wavenet\nSection Titles (if exists):  ['\u03b4 CCA [i, j] = P S E 0S [i] \u2212 P V E V0 [j]', '3.3 WaveNet Synthesis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/easonnie/combine-FEVER-NSMN\nCitation Context: For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [URL_CITE] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input.\nFootnote or Reference text (if exists): 22 available at https://github.com/easonnie/ combine-FEVER-NSMN\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP\nCitation Context: A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha (Kudo et al., 2002) and KNP [URL_CITE] (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications.\nFootnote or Reference text (if exists): 2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.epinions.com\nCitation Context: The files retrieved from the web are converted in xml format, which allows preserving the metadata information. As an example, Figure 1 shows the organization of this xml file from a review retrieved from the website epinions.com ( [URL_CITE] http://www.epinions.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The System and its Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk\nCitation Context: \u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [URL_CITE]\nFootnote or Reference text (if exists): 9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JulySinceAndrew/HieRec\nCitation Context: All hyper-parameters of HieRec and baseline methods are manually tuned on the valida-tion set. [URL_CITE] Following Wu et al. (2019e), we use four ranking metrics, i.e., AUC, MRR, nDCG@5, and nDCG@10, for performance evaluation.\nFootnote or Reference text (if exists): 2 https://github.com/JulySinceAndrew/HieRec\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental Datasets and Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cordova.apache.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova [URL_CITE] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 3 https://cordova.apache.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jknack/handlebars.java\nCitation Context: The output component employs a tem-plate engine [URL_CITE] to convert the data collected by the processing component into HTML and JavaScript for handling display and user interac-tion in the web browser.\nFootnote or Reference text (if exists): 7 https://github.com/jknack/handlebars. java\nSection Titles (if exists):  ['4 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/budzianowski/multiwoz\nCitation Context: In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [URL_CITE]\nFootnote or Reference text (if exists): 5 The model is publicly available at https://github.com/budzianowski/multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/software/srparser.html\nCitation Context: We chunk the source and target sentences using constituent parsing (Bauer, 2014) [URL_CITE] . We consider all nodes with phrase-level tags (XP) to be con-stituents.\nFootnote or Reference text (if exists): John Bauer. 2014. Shift-reduce constituency parser. https://nlp.stanford.edu/software/srparser.html. Accessed: 2018-11-30.\nSection Titles (if exists):  ['3 Models', '3.1 Chunkings and Chunk Embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2017). For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) [URL_CITE] during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IntelLabs/academic-budget-bert\nCitation Context: We demonstrate that through a combination of software optimiza-tions, design choices, and hyperparameter tun-ing, it is possible to produce models that are competitive with BERT BASE on GLUE tasks at a fraction of the original pretraining cost. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at: https://github.com/IntelLabs/academic-budget-bert\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yftah89/PBLM-Cross-language-Cross-domain\nCitation Context: In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms pre-vious models even when it is trained in the lazy setup and previous models are trained in the full setup. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/yftah89/ PBLM-Cross-language-Cross-domain\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:User_pages\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [URL_CITE] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Wikipedia:User_pages\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Webcite\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pld/BMM_labels/\nCitation Context: To facilitate the search and avoid local minima, several heuris-tics and a look-ahead procedure are used to im-prove the performance of the algorithm. We use the BMM implementation provided by Borensztajn and Zuidema (2007) [URL_CITE] .\nFootnote or Reference text (if exists): 5 https://github.com/pld/BMM_labels/\nSection Titles (if exists):  ['3 Method', '3.3.2 Constituency labelling']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deansong/contextLSTMCNN\nCitation Context: In many cases, however, the context available is of significant size. We therefore introduce a new method, Context-LSTM-CNN [URL_CITE] , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence. The method consis-tently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time.\nFootnote or Reference text (if exists): 1 The code is publicly available at https://github.com/deansong/contextLSTMCNN\nSection Titles (if exists):  ['angus.roberts@kcl.ac.uk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcsa-exam.com/2006/02/02/effective-permissions.html\nCitation Context: (http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html) Definition 2 Access Control List http://en.wikipedia.org/wiki Calculating a user\u2019s effective permissions requires more than simply looking up that user\u2019s name in the ACL. ( [URL_CITE] http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym \u201cACL\u201d without its definition in three different meanings found in blogs\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [URL_CITE] . All evaluation code is from the PyTorch Transformers as well.\nFootnote or Reference text (if exists): 11 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at [URL_CITE] http://www.cs.cmu.edu/ \u223c pmichel1/sated/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mpi-sws.org/~cristian/Politeness.html\nCitation Context: Politeness annotation Computational studies of politeness, or indeed any aspect of linguistic prag-matics, demand richly labeled data. We there-fore label a large portion of our request data (over 10,000 utterances) using Amazon Mechan-ical Turk (AMT), creating the largest corpus with politeness annotations (see Table 1 for details). [URL_CITE]\nFootnote or Reference text (if exists): 3 Publicly available at http://www.mpi-sws.org/\u02dccristian/Politeness.html\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver [URL_CITE] is used to construct our global argument inference model.\nFootnote or Reference text (if exists): 5 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/tree/master/examples/text-classification\nCitation Context: The example code to train the NLI-style models is also available. [URL_CITE] We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments. All the model parameters including the RoBERTa parameters are updated during all the fine-tuning processes, where we use the AdamW (Loshchilov and Hutter, 2017) optimizer with a weight decay coefficient of 0.01 for all the non-bias parameters.\nFootnote or Reference text (if exists): 11 https://github.com/huggingface/transformers/tree/master/examples/text-classification.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets?search=kilt\nCitation Context: KILT data and code are available at https://github.com/facebookresearch/KILT. [URL_CITE]\nFootnote or Reference text (if exists): 1 and at https://huggingface.co/datasets? search=kilt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.2200/S00429ED1V01Y201207AIM018\nCitation Context: Finally, this work also relates to the problem of active learning, where the goal is to interac-tively choose instances to be labeled, in a way that maximizes accuracy while minimizing the to-tal cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012 [URL_CITE] ). This is an interesting area that might be productively combined with the ideas in this paper. In general, however, the use of active learning involves additional logistical complica-tions and does not always work better than ran-dom sampling in practice (Attenberg and Provost, 2011).\nFootnote or Reference text (if exists): Burr Settles. 2012. Active Learning. Morgan & Claypool. https://doi.org/10.2200/S00429ED1V01Y201207AIM018.\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa/\nCitation Context: For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [URL_CITE] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases.\nFootnote or Reference text (if exists): 6 http://www.cs.pitt.edu/mpqa/\nSection Titles (if exists):  ['3 Experimental Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sweaglesw.org/linguistics/ace/\nCitation Context: To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [URL_CITE] . The created grammar-licensed analyses contain both a deriva-tion tree recording the used grammar rules and lex-ical entries, and the associated semantic represen-tation constructed compositionally via this deriva-tion (Bender et al., 2015).\nFootnote or Reference text (if exists): 2 http://sweaglesw.org/linguistics/ace/\nSection Titles (if exists):  ['3 Literal versus Intended Meaning', '3.3.2 SemBanking with ERG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICCV.2017.304\nCitation Context: For D, we consider two variants of MAN. The first one is to use the NLL loss same as C which suits the classification task; while an-other option is to use the Least-Square (L2) loss that was shown to be able to alleviate the gradient vanishing problem when using the NLL loss in the adversarial setting (Mao et al., 2017) [URL_CITE] : where d is the domain index of some sample and d\u02c6is the prediction. Without loss of generality, we\nFootnote or Reference text (if exists): Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, and Stephen Paul Smolley. 2017. Least squares generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV). pages 2813\u20132821. https://doi.org/10.1109/ICCV.2017.304.\nSection Titles (if exists):  ['2 Model', '2.2 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://textessence.github.io\nCitation Context: A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k. All associated resources for TextEssence may be found at [URL_CITE] https://textessence.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Human eval-uation shows significant advantages of CLARE in terms of textual similarity, fluency and gram-maticality. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A34739.html\nCitation Context: The Pearson\u2019s correlation coefficient [URL_CITE] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.\nFootnote or Reference text (if exists): 5 For a quick overview of the Pearson\u2019s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepmipt/ner\nCitation Context: We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [URL_CITE] The output layer of the tagger network gives softmax proba-bility distributions over possible tags. To further increase recall, we add an inference-time rule \u2013 if slot candidate is split \u2013 it is just ranked for relevance multiple times (with respect to multiple contexts).\nFootnote or Reference text (if exists): 7 https://github.com/deepmipt/ner\nSection Titles (if exists):  ['3 Method', '3.3 Slot Tagger Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiesutd/SubwordEncoding-CWS\nCitation Context: In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [URL_CITE] is based on NCRF++ (Yang and Zhang, 2018).\nFootnote or Reference text (if exists): 1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Implementation details: The spaCy tool [URL_CITE] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing.\nFootnote or Reference text (if exists): 2 https://spacy.io\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Justin1904/Low-rank-Multimodal-Fusion\nCitation Context: In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [URL_CITE] for extracting unimodal rep-resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni-modal input features x a , x v , x l .\nFootnote or Reference text (if exists): 2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion\nSection Titles (if exists):  ['4 Experimental Methodology', '4.3 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.chinasmack.com/glossary\nCitation Context: Slang Terms. We collect the Chinese slang terms from an online Chinese slang glossary [URL_CITE] consisting of 200 popular slang terms with English expla-nations. For English, we resort to a slang word Truth Sets.\nFootnote or Reference text (if exists): 8 https://www.chinasmack.com/glossary\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.1 Ground Truth']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: We also thank the students of the University of Pennsylvania\u2019s crowdsourcing class (NETS 213) for their involvement in building and testing a useful crowdsourcing pipeline for in-formation extraction. Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [URL_CITE] http://gun-violence.org/. Amit Singhal.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sec.gov/edgar.shtml\nCitation Context: These reports are available to the public and pub-lished on the SEC\u2019s web site. [URL_CITE] The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the period 1996\u20132006 from 10,492 different companies.\nFootnote or Reference text (if exists): 3 http://www.sec.gov/edgar.shtml\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/KILT\nCitation Context: We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, out-performing more tailor-made approaches for fact checking, open-domain question answer-ing and dialogue, and yielding competitive re-sults on entity linking and slot filling, by gen-erating disambiguated text. KILT data and code are available at [URL_CITE] https://github.com/facebookresearch/KILT. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Code, Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/isobs\nCitation Context: We evaluate different character recognition mod-els on self-created dataset. The results show that our implementation of prototypical network can achieve stable and competitive results. The datasets and source code can be found from [URL_CITE] https: //github.com/thunlp/isobs .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiment and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/fever\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [URL_CITE] and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 2 https://github.com/awslabs/fever\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea\nCitation Context: If a review contains a token that doesn\u2019t occur in at least 20 other reviews, then the review is excluded from the dataset. We used Jieba [URL_CITE] for Chi-nese and KyTea 2 for Japanese word segmentation. The segmenters were only used during the filtering process, and the text provided in the dataset is not segmented or tokenized.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba 2 http://www.phontron.com/kytea\nSection Titles (if exists):  ['2 Data preparation', '2.2 Data Processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/viczong/measuring_forecasting_skill_from_text\nCitation Context: This could potentially be useful for identifying ac-curate predictions or potentially skilled fore-casters earlier. [URL_CITE]\nFootnote or Reference text (if exists): 1 We provide our code and dataset descriptions at: https://github.com/viczong/measuring_forecasting_skill_from_text.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webdatacommons.org/webtables\nCitation Context: WDC WebTable Corpus (Lehmberg et al., 2016) is a large collection of Web tables extracted from the Common Crawl Web scrape [URL_CITE] . We use its 2015 English-language relational subset, which consists of 50.8 million relational tables and their surrounding NL contexts.\nFootnote or Reference text (if exists): 10 http://webdatacommons.org/webtables\nSection Titles (if exists):  ['A Pretraining Details', 'A.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/jdpacorpus/\nCitation Context: To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora [URL_CITE] (JDPA) (Kessler et al., 2010). The statistics of these bench-mark datasets are shown in Table 1.\nFootnote or Reference text (if exists): 7 http://verbs.colorado.edu/jdpacorpus/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.languagetool.org/\nCitation Context: \u2022 Grammar error (GErr): the absolute num-ber of increased grammatical errors in the suc-cessful adversarial example, compared to the original text. Following (Zang et al., 2020; Morris et al., 2020b), we calculate this by the LanguageTool (Naber et al., 2003). [URL_CITE]\nFootnote or Reference text (if exists): 7 https://www.languagetool.org/\nSection Titles (if exists):  ['3 Experiments', '3.2 Datasets and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: We make both resources publicly available in the standardized format UBY-LMF (Eckle-Kohler et al., 2012), which supports automatic processing of the resources via the UBY Java API, see [URL_CITE] http://www.ukp.tu-darmstadt.de/fnwkde/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['10 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/docs/streaming-api/methods\nCitation Context: All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [URL_CITE] From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O\u2019Connor et al. (2010)).\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/docs/streaming-api/methods\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: To diminish the effects of randomness in training, the results of our model are averaged with 5 random initializa-tions. For data preprocessing, we use the tok-enizer [URL_CITE] from BERT to preprocess the sentences. The experiments are conducted on GeForce GTX 1080Ti GPU with PyTorch framework .\nFootnote or Reference text (if exists): 6 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cambridgeltl/BioNLP-2016\nCitation Context: The experiment setup for policy transfer and un-derlying model is kept the same as in the NER ex-periments in Section 4.2. For the word embed-ding, we use the pre-trained English BioNLP em-bedding [URL_CITE] (Chiu et al., 2016) with 200 dimension. Vocabulary size is set to 20,000.\nFootnote or Reference text (if exists): 4 https://github.com/cambridgeltl/ BioNLP-2016\nSection Titles (if exists):  ['4 Experiments', '4.3 Biomedical Named Entity Recognition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nCitation Context: Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { \u22121,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [URL_CITE] :\nFootnote or Reference text (if exists): Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Alex-Fabbri/\nCitation Context: Datasets can be obtained by running the data scripts at [URL_CITE] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.2 Semantic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Dataset)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [URL_CITE] Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: 1. Number of Parameters: For BART we use the BART large checkpoint (400M parame-ters) and use the implementation by FAIRSEQ (Ott et al., 2019). [URL_CITE]\nFootnote or Reference text (if exists): 12 https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ElliottYan/Multi\nCitation Context: These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/ElliottYan/Multi Unit Transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scalanlp.org\nCitation Context: First, the text is processed with a part-of-speech and a named entity taggers. [URL_CITE] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3.\nFootnote or Reference text (if exists): 10 http://www.scalanlp.org\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/pretrained_models.html\nCitation Context: LMs examined in this paper share similar archi-tecture and pre-training setting as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), but are trained on different corpora. We provide the shortcut name of each LM in the Hugging-Face\u2019s Transformer library ( [URL_CITE] https://huggingface.co/transformers/pretrained_models.html) and their training corpora in Tab. 8, from which you can find more information.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['D Details of Pre-trained LMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://super.gluebenchmark.com/leaderboard/\nCitation Context: The human performance on B OOL Q and M ULTI RC are directly reported from SuperGLUE (Wang et al., 2019) leaderboard. [URL_CITE] Here are the key observations in this table:\nFootnote or Reference text (if exists): 5 https://super.gluebenchmark.com/leaderboard/\nSection Titles (if exists):  ['C Performances Across Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [URL_CITE] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seraphinatarrant/plan-write-revise\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/K17-3014\nCitation Context: Nguyen et al. (2017) [URL_CITE] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.\nFootnote or Reference text (if exists): Dat Quoc Nguyen, Mark Dras, and Mark John-son. 2017. A novel neural network model for joint pos tagging and graph-based dependency pars-ing. In CoNLL 2017 Shared Task: Multilin-gual Parsing from Raw Text to Universal Depen-dencies. Association for Computational Linguis-tics, pages 134\u2013142. https://doi.org/10.18653/v1/K17-3014.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.org\nCitation Context: We sampled 1127 paragraphs from 271 articles from the online encyclopedia Wikipedia [URL_CITE] and labeled a to-tal of 4701 relation instances. In addition to a large set of person-to-person relations, we also included links between people and organizations, as well as biographical facts such as birthday and jobTitle . In all, there are 53 labels in the training data (Table 1).\nFootnote or Reference text (if exists): 1 http://www.wikipedia.org\nSection Titles (if exists):  ['5 Experiments', '5.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2012T21\nCitation Context: To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [URL_CITE] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2012T21\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fairseq-py\nCitation Context: For C ONV S2S [URL_CITE] and T-C ONV S2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings. We trained our convolutional models with Nesterov\u2019s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.99 and renormalized gra-dients if their norm exceeded 0.1 (Pascanu et al., 2013).\nFootnote or Reference text (if exists): 3 We used the code available at https://github.com/facebookresearch/fairseq-py.\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.uniroma1.it/wsdeval/training-data\nCitation Context: Section 3), keeping only the substitutes whose similarity to the target is higher than \u03bb. As source dataset C we exploit SemCor (Miller et al., 1993), a manually annotated corpus where instances are sense-tagged according to the WordNet sense in-ventory [URL_CITE] . While it is typically used as a training corpus for English Word Sense Disambiguation (WSD), as we show, its manually-curated sense dis-tribution is also beneficial for lexical substitution.\nFootnote or Reference text (if exists): 4 We use the version released at http://nlp.uniroma1.it/wsdeval/training-data, with sense annotations that leverage WordNet 3.0.\nSection Titles (if exists):  ['5 Dataset Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/lsl_acl20\nCitation Context: Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [URL_CITE] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ara.reuters.com\nCitation Context: After extracting the false claims from V ERIFY , we collected the true claims of our corpus from R EUTERS [URL_CITE] by extracting headlines of news docu-ments. We used a list of manually selected key-words to extract claims with the same topics as those extracted from V ERIFY . Then, we manually excluded claims that contained political rhetorical statements (see example 3 be-low), multiple facts, accusations or denials, and ultimately we only kept those claims that discuss factual events, i.e., that can be verified.\nFootnote or Reference text (if exists): 5 http://ara.reuters.com\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://coretex.itc.it/\nCitation Context: Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX ( [URL_CITE] http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt17/translation-task/preprocessed/\nCitation Context: All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://data.statmt.org/wmt17/translation-task/preprocessed/\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/S10-1009\nCitation Context: Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010) [URL_CITE] . The task was intended to evaluate syn-tactic parsers across different formalisms, focus-ing on entailments that could be determined en-tirely on the basis of the syntactic representa-tions of the sentences that are involved, with-out recourse to lexical semantics, logical reason-ing, or world knowledge.\nFootnote or Reference text (if exists): Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51\u2013 56. http://www.aclweb.org/anthology/S10-1009.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cogsci.princeton.edu/~wn\nCitation Context: Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely. Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) [URL_CITE] and HowNet (Dong, 2000) (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.github.io\nCitation Context: \u2022 We convert the representation of coordination structures into a format inspired by Universal Dependencies. [URL_CITE] In the original Treex anno-tation style, the conjunction heads both con-juncts, whereas in our modification, the first conjunct is at the top, heading the coordina-tion and the second conjunct (see Figure 4).\nFootnote or Reference text (if exists): 8 http://universaldependencies.github.io\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nCitation Context: One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [URL_CITE] .\nFootnote or Reference text (if exists): 6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nSection Titles (if exists):  ['5 Experiments', '5.3 Further experiments on the Europarl corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stanfordnlp.github.io/CoreNLP/\nCitation Context: As a result, we obtain passage-question-answer (P, Q, A) triples (Table 1 shows an example). As a post-processing step, we prune out (P,Q,A) triples where the word overlap between the question (Q) and pas-sage (P) is less than [URL_CITE] words (after excluding the stop words).\nFootnote or Reference text (if exists): 2 https://stanfordnlp.github.io/CoreNLP/\nSection Titles (if exists):  ['3 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Table 6 shows the results. Our joint parser outperforms the other parsers, including the neu-ral network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] . Our data-driven parsers yield relatively low performance in the ObQ and RNR constructions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/truprecht/rustomata-eval\nCitation Context: We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglo\u0308f, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. [URL_CITE] Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus.\nFootnote or Reference text (if exists): 6 The evaluation scripts are available on https://github.com/truprecht/rustomata-eval.\nSection Titles (if exists):  ['5 Evaluation and Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip\nCitation Context: We applied our annotation scheme to the product re-view dataset [URL_CITE] released by Hu and Liu (2004). We annotated the data for 3 out of 5 products.\nFootnote or Reference text (if exists): 4 http://www.cs.uic.edu/\u02dcliub/FBS/CustomerReviewData.zip\nSection Titles (if exists):  ['5 Data and Annotation Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/S10-1009\nCitation Context: Finally, we apply our new parsers to the down-stream tasks of Parsing Evaluation using Tex-tual Entailements (PETE, Yuret et al. (2010) [URL_CITE] ) and Unbounded Dependency Recovery (Rimell et al., 2009). We demonstrate that our end-to-end parser outperforms the best results in both tasks.\nFootnote or Reference text (if exists): Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51\u2013 56. http://www.aclweb.org/anthology/S10-1009.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). http://hierplane.allenai.org/explain/ with [URL_CITE] http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/antonisa/unimorph_inflect\nCitation Context: Once all the morphological features have been specified as detailed above, we use the unimorph_inflect package (Anastasopoulos and Neubig, 2019) to generate the appropriately inflected surface form of the bracketed words. [URL_CITE] We note that the target entity ([Y]) might also need to be inflected, as in the above Russian example, in which case we require the model\u2019s predictions to match the inflected target forms.\nFootnote or Reference text (if exists): 3 https://github.com/antonisa/unimorph_inflect\nSection Titles (if exists):  ['3 Multilingual Multi-token Factual Retrieval Benchmark', '3.3 Prompts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://textessence.github.io\nCitation Context: A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at [URL_CITE] https: //textessence.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Multiple general machine learning frame-works, such as SCIKIT - LEARN (Pedregosa et al., 2011), P Y T ORCH , T HEANO (Theano Develop-ment Team, 2016) and T ENSOR F LOW (Abadi et al., 2015) [URL_CITE] , among others, enable quick proto-typing and deployment of ML models. However, unlike J ACK , they do not offer a simple framework for defining and evaluating MR models.\nFootnote or Reference text (if exists): Mart\u00edn Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/bk42ZQ\u201d\nCitation Context: (doubtful) \u201cPresident Barack Obama\u2019s Religion: Christian, Muslim, or Agnostic? - The News of Today (Google): Share With Friend... [URL_CITE] http://bit.ly/bk42ZQ\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hitochan777/mt-tools/releases/tag/1.0.1\nCitation Context: We converted con-stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. [URL_CITE] We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model.\nFootnote or Reference text (if exists): 3 The conversion program is available at https://github.com/hitochan777/mt-tools/releases/tag/1.0.1\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/parser-faq.shtml\nCitation Context: The Stanford parser score is not designed to measure grammat-icality, however, it will generally prefer a gram-matical parse to a non-grammatical one. [URL_CITE] Thus, lower parser scores indicate less grammatically-correct utterances. In future work, we aim to use specifically designed grammar-scoring functions, e.g.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/parser-faq.shtml\nSection Titles (if exists):  ['4 Metrics', '4.2 Grammar-based metrics (GBMs)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/havakv/pycox\nCitation Context: The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [URL_CITE] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.\nFootnote or Reference text (if exists): 2 https://github.com/havakv/pycox\nSection Titles (if exists):  ['4 Experiments', '4.3 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\nCitation Context: Suppose p i , i = 1,..., n represent the training set of persons, and the classes for classifications are C ={c ,c } (for simplicity, we represent the 1 2 classes with C = { \u22121,1 } ). Then the classification task requires the solution of the following optimi-zation problem (Hsu et al., 2003) [URL_CITE] :\nFootnote or Reference text (if exists): Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-tical Guide to Support Vector Classification. Avail-able at: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf.\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.readabilityformulas.com/articles/dale-chall-readability-word-list.php\nCitation Context: 6. Dale-Chall readability score (Dale and Chall, 1948, 1995): DCRS \u201c 0.1579p DWR \u00a8 100q ` 0.0496 WPS , where DWR is the ratio of difficult words [URL_CITE] and WPS is the average words per sen-\nFootnote or Reference text (if exists): 2 Words not on a list of 3,000 familiar words at https: //www.readabilityformulas.com/articles/ dale-chall-readability-word-list.php\nSection Titles (if exists):  ['-', '8.1 Definitions of Complexity Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf\nCitation Context: During annotation, we follow the defini-tion of events from the ACE annotation guideline. [URL_CITE] We use the manual annotation as the ground truth and compare it with the event detection model out-put to calculate the metrics (i.e., precision, recall and F1) in Table 3.\nFootnote or Reference text (if exists): 11 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf\nSection Titles (if exists):  ['A.3 Top Ten Extracted Events']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, DataSource, Code)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://inklab.usc.edu/XCSR/\nCitation Context: To improve the performance be-yond English, we propose a simple yet effec-tive method \u2014 multilingual contrastive pre-training (MCP). It significantly enhances sen-tence representations, yielding a large perfor-mance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R L ) [URL_CITE] .\nFootnote or Reference text (if exists): 1 We release our code and data at the project website: https://inklab.usc.edu/XCSR/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politicalforum.com\nCitation Context: We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [URL_CITE] . All the posts are in English.\nFootnote or Reference text (if exists): 2 http://www.politicalforum.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Multi-Domain Text Classification The MDTC task was first examined by Li and Zong (2008), who proposed to fuse the training data from multi-ple domains either at the feature level or the classi-fier level. The prior art of MDTC (Wu and Huang, 2015) [URL_CITE] decomposes the text classifier into a gen-eral one and a set of domain-specific ones. How-ever, the general classifier is learned by param-eter sharing and domain-specific knowledge may sneak into it.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mynlp/jigg\nCitation Context: We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [URL_CITE] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.\nFootnote or Reference text (if exists): 9 https://github.com/mynlp/jigg\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jmcauley.ucsd.edu/data/amazon/links.html\nCitation Context: For parameter learning of the joint model RNCRF, we implement SGD with a de-caying learning rate initialized at 0.02. We also try with varying context window size, and use 3 for the laptop domain and [URL_CITE] for the restaurant domain, re-spectively. All parameters are chosen by cross vali-dation.\nFootnote or Reference text (if exists): 5 http://jmcauley.ucsd.edu/data/amazon/links.html\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/jacobe/naacl09.html\nCitation Context: Source code is available at [URL_CITE] http://people.csail.mit.edu/jacobe/naacl09.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/downloads.html\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [URL_CITE]\nFootnote or Reference text (if exists): 5 http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nCitation Context: We use hierarchical clustering (Rokach and Maimon, 2005) [URL_CITE] method to group the languages based on language embeddings. We use the el-bow method (Thorndike, 1953) to automatically decide the optimal number of clusters K. Note that we have tried to extract the language embed-dings from multiple model checkpoints randomly chosen in the later training process, and found that the clustering results based on these language em-beddings are stable.\nFootnote or Reference text (if exists): 10 https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nCitation Context: Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [URL_CITE] (Isozaki et al., 2010). Follow-ing Cho et al.\nFootnote or Reference text (if exists): 7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dcs.shef.ac.uk/nlp/meter\nCitation Context: A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [URL_CITE] and will continue to be enhanced. helpful comments on earlier drafts.\nFootnote or Reference text (if exists): 4 See http://www.dcs.shef.ac.uk/nlp/meter.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [URL_CITE] , trained on approxi-mately 1.7 million sentence pairs. For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the pos-teriors using DeNero and Klein\u2019s (2007) soft union method.\nFootnote or Reference text (if exists): 6 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2006/\nCitation Context: The Chinese word \u201cHE ZHANG,\u201d denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [URL_CITE] . The resulting alignment is shown in Figure 1(b).\nFootnote or Reference text (if exists): 2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [URL_CITE] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 8 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DancingSoul/NQ_BERT-DM\nCitation Context: We will release our code and models at [URL_CITE] https: //github.com/DancingSoul/NQ_BERT-DM . Figure 2: System overview.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn/tree/master/data\nCitation Context: (2019). [URL_CITE] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets.\nFootnote or Reference text (if exists): 10 https://github.com/sameenmaruf/selective-attn/tree/master/data\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~ytsvetko/metaphor/datasets.zip\nCitation Context: Tsvetkov et al. (2013) pursue this work further by constructing and publishing a dataset of 985 literal and 985 methaphorical adjective-noun pairs [URL_CITE] and classify them. Gersh-man et al.\nFootnote or Reference text (if exists): 10 http://www.cs.cmu.edu/\u02dcytsvetko/metaphor/datasets.zip\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.4 Metaphor Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) 7 ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea [URL_CITE] ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.\nFootnote or Reference text (if exists): 8 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/williamFalcon/\nCitation Context: For all models we train using Adam optimizer with 0.0001 as the learning rate, early stopping till vali-dation loss doesn\u2019t improve. For the sake of easy reproducibility, we use Pytorch Lightning (Falcon, 2019) [URL_CITE] framework. We used 8 Nvidia-TitanX GPUs on a DGX Server Workstation to train faster using Pytorch Distributed Data Parallel (DDP).\nFootnote or Reference text (if exists): W.A. Falcon. 2019. Pytorch lightning. https://github.com/williamFalcon/ pytorch-lightning .\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/\nCitation Context: For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [URL_CITE] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014).\nFootnote or Reference text (if exists): 3 http://opus.nlpl.eu/\nSection Titles (if exists):  ['2 A FRO MT benchmark', '2.2 Data Sources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/un33k/python-slugify\nCitation Context: Figure [URL_CITE] shows the average number of citations per paper, analogous to Figure 7, but for a larger number of citation ages. citations of 40 Number citations of 40 Number\nFootnote or Reference text (if exists): 10 We achieve this by using https://github.com/un33k/python-slugify.\nSection Titles (if exists):  ['A Fuzzy paper matching']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://facebook.github.io/react\nCitation Context: The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [URL_CITE] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:\nFootnote or Reference text (if exists): 8 https://facebook.github.io/react\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/\nCitation Context: We used mteval-v13a.pl for calculating BLEU scores. [URL_CITE]\nFootnote or Reference text (if exists): 1 It is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/\nSection Titles (if exists):  ['4 Experiments', '4.1 Common settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/MiuLab/CLUSE\nCitation Context: The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and dataset are available at http://github.com/MiuLab/CLUSE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code and Dataset)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sequencecomparison.github.io\nCitation Context: Language-specific ap-proaches, on the other hand, are based on pre-viously identified recurring correspondences be-tween the languages from which the words are taken (List 2014b: 48-50) and may differ across languages. An example for language-specific similarity measures is the LexStat algorithm, first proposed in List (2012a) and later refined in List (2014b) [URL_CITE] . As a general rule, language-specific ap-proaches outperform language-independent ones, provided the sample size is large enough (List 2014a).\nFootnote or Reference text (if exists): Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.\nSection Titles (if exists):  ['3 Methods', '3.1 Sequence Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2w7nUQK\nCitation Context: We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google\u2019s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [URL_CITE] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences.\nFootnote or Reference text (if exists): 2 This model can be downloaded at https://bit.ly/2w7nUQK.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm\nCitation Context: In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) [URL_CITE] package for the RBF and sigmoid kernels. We discarded any peri-ods and parentheses, but kept apostrophes and hy-phens, and we converted all letters to lower case.\nFootnote or Reference text (if exists): C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-brary for support vector machines. Software available at http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm.\nSection Titles (if exists):  ['3 Language identification with SVMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: \u2022 Logistic regression classifier: [URL_CITE] http://scikit-learn.org/stable/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (URL provided as a reference without specific citation reason)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors (Penning-ton et al., 2014), downloaded from [URL_CITE] http://nlp.stanford.edu/projects/glove/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lancers.jp/\nCitation Context: We use a crowdsourcing application, Lancers, [URL_CITE] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers.\nFootnote or Reference text (if exists): 3 http://www.lancers.jp/\nSection Titles (if exists):  ['4 Balanced dataset for evaluation of Japanese lexical simplification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PlusLabNLP/ee-wiki-bias\nCitation Context: To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [URL_CITE] We first detect events in the collected corpus using a state-of-the-art event extraction model (Han et al., 2019). Then, we extract gender-distinct events with a higher chance to occur for one group than the other.\nFootnote or Reference text (if exists): 2 https://github.com/PlusLabNLP/ ee-wiki-bias\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu\nCitation Context: The second fea-ture is invoked if two heads are identical or a head is a substring of another. The third feature is in-voked if two heads are synonyms or derivations that are extracted from the WordNet [URL_CITE] . The fourth feature is invoked if the cosine similarity between word embeddings of two heads is larger than a threshold.\nFootnote or Reference text (if exists): 4 http://wordnet.princeton.edu\nSection Titles (if exists):  ['5 Parameterization', '5.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/JU0JJ\nCitation Context: It also sub-stantially outperforms the previous grounded model, with largest improvements on more \u2018abstract\u2019 categories (e.g., +55.1% recall on VPs). [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://git.io/JU0JJ.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.nict.go.jp/r/r312/EDR/index.html\nCitation Context: We applied our improved model and Shimbo and Hara\u2019s original model to the EDR corpus (EDR, 1995) [URL_CITE] . We also ran the Kurohashi-Nagao parser (KNP) 2.0 , a widely-used Japanese dependency parser to which Kurohashi and Nagao\u2019s (1994) rule-based coordination analysis method is built in.\nFootnote or Reference text (if exists): EDR, 1995. The EDR dictionary. NICT. http://www2.nict.go.jp/r/r312/EDR/index.html.\nSection Titles (if exists):  ['4 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nCitation Context: Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. [URL_CITE] This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in. A hypothetical tweet recommendation system could alleviate this acute information overload, e.g., by limiting the stream of tweets to those of interest to the user, or by discovering intriguing content outside the user\u2019s following network.\nFootnote or Reference text (if exists): 3 See http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/lexicons/subjlexicon/\nCitation Context: In this paper, both Chinese and English [URL_CITE] sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).\nFootnote or Reference text (if exists): 6 English sentiment lexicon is uti-lized from MPQA Subjectivity Lexicon: http://mpqa.cs.pitt.edu/lexicons/subjlexicon/\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.2 Sentimental Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/liblinear\nCitation Context: 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver [URL_CITE] . Our MIRA solves the problem in Equation 13 in line 7 of Alg.\nFootnote or Reference text (if exists): 5 We used liblinear (Fan et al., 2008) at http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear with the solver type of 3.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf\nCitation Context: Traditional studies on this problem generally re-sort to feature-engineering methods, which first ex-tracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical fea-tures by the bag-of-words model (Zhang et al., 2010). These features are then fed into a classi-fier such as SVM (Cui and Qi, 2017) [URL_CITE] and XGBoost (Tadesse et al., 2018) to predict the personality traits. Despite interpretable features that can be expected, feature engineering has such limitations as it relies heavily on manually designed features.\nFootnote or Reference text (if exists): Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).\nSection Titles (if exists):  ['2 Related Work', '2.1 Personality Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [URL_CITE] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.\nFootnote or Reference text (if exists): 3 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.psychpark.org\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [URL_CITE] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/YerevaNN/WARP\nCitation Context: In this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming [URL_CITE] . The method is inspired by adversarial reprogramming (Elsayed et al., 2019) \u2014 a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to perform classification on a task other than the one it was originally trained for.\nFootnote or Reference text (if exists): 1 Our implementation is publicly available at: https: //github.com/YerevaNN/WARP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [URL_CITE] ). Ganin et al.\nFootnote or Reference text (if exists): Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence genera-tive adversarial nets with policy gradient. In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Thus our method some-times acts as an efficient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour-aged by the Wikipedia community, [URL_CITE] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.\nFootnote or Reference text (if exists): 8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations\nSection Titles (if exists):  ['6 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/BLINK\nCitation Context: We don\u2019t evaluate multi-hop specific baselines on KILT but the current best F1-score for HotpotQA is 75.43 according to the official lead-earboard , that is quite far from what achieved by our general solutions. BLINK results are in line with what reported in the GitHub repository [URL_CITE] for all three entity linking datasets. The Tranformer MemNet of Dinan et al.\nFootnote or Reference text (if exists): 20 https://github.com/facebookresearch/ BLINK\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html\nCitation Context: There are 35 different frames in to-tal. [URL_CITE] We manually assigned a set of clause types to each frame (e.g., SVO to frame \u201csomebody verb something\u201d). Table 1 shows an example frame for each of the seven clause types.\nFootnote or Reference text (if exists): 3 See http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html.\nSection Titles (if exists):  ['4 Syntactic Pruning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://demo.patrickpantel.com/\nCitation Context: Consider the set of seed terms S and an errone-ous expanded instance e. In the SIM method of Section 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed. The Feature Modification Method (FMM) instead tries to identify the subset of features of the error e which represent the unintended sense of the seed terms S. For example, let S = {Minerva, Neptune, Baccus, Juno, Apollo}. Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. In Wikipedia articles we see contexts (i.e., features) such as [URL_CITE] :\nFootnote or Reference text (if exists): 4 The full feature vector for these and all other terms in Wiki-pedia can be found at http://demo.patrickpantel.com/..\nSection Titles (if exists):  ['3 Dynamic Similarity Modeling', '3.2 Feature Modification Method (FMM)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wndomains.itc.it\nCitation Context: To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [URL_CITE] all the synsets labeled with the domain S EXUALITY . The list is further processed by removing all words with high polysemy (\u2265 4).\nFootnote or Reference text (if exists): 3 W ORD N ET D OMAINS assigns each synset in W ORD N ET with one or more \u201cdomain\u201d labels, such as S PORT , M EDICINE , E CONOMY . See http://wndomains.itc.it.\nSection Titles (if exists):  ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/newspaper\nCitation Context: We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [URL_CITE] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/newspaper\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/bigquery\nCitation Context: We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [URL_CITE] . Each of these posts is a headline from a news source.\nFootnote or Reference text (if exists): 2 https://cloud.google.com/bigquery\nSection Titles (if exists):  ['2 The Humor Dataset', '2.2 Collecting Headlines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pltrdy/files2rouge\nCitation Context: Actors say satirical Western\u2019s script is insulting to Native Americans and women . used files2rouge [URL_CITE] . Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT-GEN+COV (See et al., 2017), B ERT S UM E XT A BS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al.\nFootnote or Reference text (if exists): 3 files2rouge usage : https://github.com/pltrdy/files2rouge\nSection Titles (if exists):  ['4 Experiments', '[Gold Summary]']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://farm2.user.srcf.net/research/bagel/\nCitation Context: (2010), which fits our usage scenario in a spoken dialogue sys-tem and is freely available. [URL_CITE] It contains a to-tal of 404 sentences from a restaurant informa-tion domain (describing the restaurant location, food type, etc. ), which correspond to 202 dia-logue acts, i.e., each dialogue act has two para-phrases.\nFootnote or Reference text (if exists): 7 Available for download at: http://farm2.user.srcf.net/research/bagel/.\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [URL_CITE] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/RikVN/AMR\nCitation Context: For both phases, we use the scripts released by van Noord and Bos (2017). [URL_CITE]\nFootnote or Reference text (if exists): 6 https://github.com/RikVN/AMR\nSection Titles (if exists):  ['3 Speak the Graph Languages ( SGL )', '3.1 Graph Linearizations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: For ACMLM, we build our model based on the BERT implementation from HuggingFace. [URL_CITE] We initialize our decoder using the pre-trained \u2018Bert-base\u2019 model and set the max sequence length to 30. We train the model for 5 epochs using Adam with learning rate 2e \u22125 .\nFootnote or Reference text (if exists): 7 https://github.com/huggingface/pytorch-pretrained- BERT\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Baroni et al. develop the word embeddings using word2vec [URL_CITE] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013).\nFootnote or Reference text (if exists): 6 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['4 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiangfeng1124/acl15-clnndep\nCitation Context: We also incorporate word embedding results into a downstream task, dependency parsing, to eval-uate whether the truncated embedding results are still good features compared to the original fea-tures. We follow the setup of (Guo et al., 2015) in a monolingual setting [URL_CITE] . We train the parser with 5,000 iterations using different truncation set-tings for word2vec embedding.\nFootnote or Reference text (if exists): 3 https://github.com/jiangfeng1124/ acl15-clnndep\nSection Titles (if exists):  ['5 Experiments on Dependency Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.unicode.org/udhr/\nCitation Context: Following Scannell (2007), we collected small monolingual samples of 643 languages from four sources: the Universal Declaration of Human Rights [URL_CITE] , non-English Wikipedias , the Jehovah\u2019s Witnesses website , and the Rosetta project (Lands-bergen, 1989).\nFootnote or Reference text (if exists): 2 The Universal Declaration of Human Rights is a document created by the United Nations and translated into many lan-guages. As of February 2011 there were 365 versions available from http://www.unicode.org/udhr/\nSection Titles (if exists):  ['3 Task Definition', '3.4 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J07-4004\nCitation Context: In the CCG parsing literature, some work op-timizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007 [URL_CITE] ; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric.\nFootnote or Reference text (if exists): Stephen Clark and James R. Curran. 2007. Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Lin-guistics, Volume 33, Number 4, December 2007 http://aclweb.org/anthology/J07-4004.\nSection Titles (if exists):  ['7 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPWM-WHU/RACL\nCitation Context: We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [URL_CITE] , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models.\nFootnote or Reference text (if exists): 7 https://github.com/NLPWM-WHU/RACL.\nSection Titles (if exists):  ['4 Modeling', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/MuGNN\nCitation Context: Extensive experiments on five pub-licly available datasets demonstrate our su-perior performance (5% Hits@1 up on av-erage). Source code and data used in the experiments can be accessed at [URL_CITE] https://github.com/thunlp/MuGNN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OceanskySun/\nCitation Context: For GRAFT-Net (Sun et al., 2018), we use the implementation published by the author; [URL_CITE] how-ever, we retrieve data with a simpler process, as described in Section 4.4.\nFootnote or Reference text (if exists): 6 https://github.com/OceanskySun/\nSection Titles (if exists):  ['4 Experiments and Results', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) \u2013 a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [URL_CITE] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.\nFootnote or Reference text (if exists): 10 https://spacy.io\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method and Material)\nTYPE: Mixed (Tool and DataSource)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pstadler1990/nmt_paper21_appendix\nCitation Context: We created 10 sentences per phe-nomenon, resulting in a total of 585 sentences, ex-amining 49 phenomena organised in 13 categories. The raw test items, as well as the translations eval-uated can be found in our repository [URL_CITE] . The phe-nomena selected for this experiment are a subset of the ones of German\u2192English MT, as described in Macketanz et al.\nFootnote or Reference text (if exists): 1 https://github.com/pstadler1990/nmt_ paper21_appendix\nSection Titles (if exists):  ['4 Experiment setup', '4.1 Test suite setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/grmm/\nCitation Context: In this work, we use the G RMM package (Sutton, 2006) [URL_CITE] for building both the linear-chain CRF (L-C RF ) and factorial CRF (F-C RF ). The tree-based reparameterization (TRP) schedule for belief propa-gation (Wainwright et al., 2001) is used for approxi-mate inference.\nFootnote or Reference text (if exists): C. Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.\nSection Titles (if exists):  ['5 Factorial Conditional Random Fields']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cfmrp/mtool\nCitation Context: (2017). For UCCA parsing, we employ the official evaluation metric [URL_CITE] of the shared task, conceptually similar to the Smatch score.\nFootnote or Reference text (if exists): 15 https://github.com/cfmrp/mtool\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/\nCitation Context: Social media platforms, particularly microblog-ging services such as Twitter, have become in-creasingly popular (Statista, 2019) [URL_CITE] as a means to express thoughts and opinions. Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they reflect a user\u2019s person-ality, brand and interests. This observation mo-tivates the question we consider here, of how to quantify the degree to which tweets are character-istic of their author?\nFootnote or Reference text (if exists): Statista. 2019. Twitter: Num-ber 2010-2018.of active users (https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). [URL_CITE] http://hierplane.allenai.org/explain/ with http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://facebook.github.io/react\nCitation Context: The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [URL_CITE] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:\nFootnote or Reference text (if exists): 8 https://facebook.github.io/react\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/\nCitation Context: We used the \u201cModApte\u201d split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [URL_CITE] . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ).\nFootnote or Reference text (if exists): Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/\nSection Titles (if exists):  ['5 Results', '5.1 Reuters-21578']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://textinspector.com/help/lexical-diversity/\nCitation Context: Here, we describe in detail the linguistic complex-ity measures we used, which span lexical diversity ( ), syntactic simplicity ( ), readability ( ), and prototypicality ( ). samples of different length, is merely a com-plex approximation (R \u201c 0.971) of a hypergeo-metric distribution, which they use in an index called HD-D. [URL_CITE] HD-D measures the mean con-tribution that each type makes to the TTR of all possible combinations of a samples of size 35-\nFootnote or Reference text (if exists): 1 See https://textinspector.com/help/lexical-diversity/ for McCarthy\u2019s recommendation on vocd-D vs HD-D.\nSection Titles (if exists):  ['-', '8.1 Definitions of Complexity Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rule.alibaba.com/rule/detail/2034.htm\nCitation Context: Tags are also nested with varying depths. Figure 1 illus-trates this problem: similar title and prose text seg-ments from four website privacy policies [URL_CITE] have al-together different HTML tag structures. The sec-ond problem is that it is not straightforward to dis-tinguish the information (encoded in HTML) that is necessary for title-prose detection from the rest of the HTML structure, including unrelated links, multiple tags with little or no content and page headers and footers.\nFootnote or Reference text (if exists): 2 All the policies were retrieved on 2018-01-20, from the below URLs: https://rule.alibaba.com/rule/detail/2034.htm https://www.apple.com/legal/privacy/en-ww/ https://www.cbsinteractive.com/legal/cbsi/privacy-policy https://help.bet365.com/en/privacy-policy\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/\nCitation Context: 1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM [URL_CITE] (version 3.20) (Chang and Lin, 2011). It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification.\nFootnote or Reference text (if exists): 1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.5281/zenodo.4432958\nCitation Context: TextEssence is an interactive tool for comparative analysis of word and concept embeddings. Our implementation and experimental code is avail-able at https://github.com/drgriffis/ text-essence, and the database derived from our CORD-19 analysis is available at [URL_CITE] https://doi.org/10.5281/zenodo.4432958. A screencast of TextEssence in action is available at https://youtu.be/1xEEfsMwL0k.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://inex.is.informatik.uni-duisburg.de/index.html\nCitation Context: A number of relevant works are also pub-lished in the proceedings of the INEX workshop. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://inex.is.informatik.uni-duisburg.de/index.html\nSection Titles (if exists):  ['2 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/docs/en/crawl-vectors.html\nCitation Context: We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [URL_CITE] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf.\nFootnote or Reference text (if exists): 7 https://fasttext.cc/docs/en/crawl-vectors.html\nSection Titles (if exists):  ['6 Language Modeling Analysis', '6.1 Predicting Insertion Locations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [URL_CITE] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis).\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['3 Defining Stability']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/\nCitation Context: We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The implementations are available at [URL_CITE] https://github.com/declare-lab/ identifiable-transformers .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Oneplus/Tweebank\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [URL_CITE] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 5 https://github.com/Oneplus/Tweebank\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/sentence-transformers\nCitation Context: For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [URL_CITE] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments.\nFootnote or Reference text (if exists): 10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everytrail.com/best/\nCitation Context: In our setting, we take as input a natural language query and extract entities from a single web page. trails near Baltimore) and a web page (e.g., [URL_CITE] http://www.everytrail.com/best/ hiking-baltimore-maryland), the goal is to extract all entities corresponding to the query on that page (e.g., Avalon Super Loop, etc.). Figure 1 summarizes the task setup.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svn.nlpl.eu/mrp/2019/public/resources.txt\nCitation Context: Finally, we wish to point out that direct com-parability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT ) whose usage was forbidden to the shared task par-ticipants. [URL_CITE] Nevertheless, we believe that their re-sults are needed here to better contextualize the performances SGL obtains.\nFootnote or Reference text (if exists): 19 Allowed resources are specified at: http://svn.nlpl.eu/mrp/2019/public/resources.txt\nSection Titles (if exists):  ['5 Results', '5.3 UCCA Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/WSDTutorial.html\nCitation Context: Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [URL_CITE] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.\nFootnote or Reference text (if exists): Ted Pedersen and Rada. F. Mihalcea, \u201cAdvances in Word Sense Disambiguation,\u201d tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.\nSection Titles (if exists):  ['2 The proposal']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.cornell.edu/people/pabo/movie-review-data/\nCitation Context: \u2022 MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://www.cs.cornell.edu/people/pabo/movie-review-data/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf\nCitation Context: Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data 3 . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [URL_CITE] . Thus, human as-sessments are in a 2-10 scale.\nFootnote or Reference text (if exists): LDC. 2005. Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations. Revision 1.5. Tech-nical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://technologies.kmi.open.ac.uk/aqualog/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [URL_CITE] , NLBean etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 8 http://technologies.kmi.open.ac.uk/aqualog/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.google.com/research/NaturalQuestions/leaderboard\nCitation Context: To our best knowledge, it is the first single model that out-performs the single human performance. Fur-thermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard [URL_CITE] .\nFootnote or Reference text (if exists): 1 Till our submission time, 29 Nov. 2019. We re-fer readers to https://ai.google.com/research/NaturalQuestions/leaderboard for the latest re-sults.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cc-cedict.org/wiki/\nCitation Context: Note that experiments using them are done in August, 2017. Another baseline method for Chinese is CC-CEDICT [URL_CITE] (CC), an on-line public Chinese-English dictionary, which is constantly updated for popular slang terms.\nFootnote or Reference text (if exists): 10 https://cc-cedict.org/wiki/\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://corpus.korean.go.kr\nCitation Context: We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [URL_CITE] . We split these corpora into three, and each is for training, validation, and test.\nFootnote or Reference text (if exists): 1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yym6472/OVSlotTagging\nCitation Context: (3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/yym6472/OVSlotTagging\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fjoch.com/mkcls.html\nCitation Context: Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the sig-nificant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language mod-elling and bilingual word alignment, and an imple-mentation with improved inference called mkcls (Och, 1999) [URL_CITE] has become a standard part of statis-tical machine translation systems.\nFootnote or Reference text (if exists): 1 Available from http://fjoch.com/mkcls.html.\nSection Titles (if exists):  ['2 Background']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [URL_CITE] as our proposal distribution. As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity de-spite substantially worse performance than RN-NGs in number agreement with multiple attrac-tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success.\nFootnote or Reference text (if exists): 11 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.3 Further Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mgiza\nCitation Context: Postprocessing To perform unknown word re-placement (Luong et al., 2015a), we built a bilin-gual English-Japanese dictionary from all of the three million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.0 [URL_CITE] (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mgiza\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kilgarriff.co.uk/bnc-readme.html\nCitation Context: We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997). [URL_CITE] After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC). Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list.\nFootnote or Reference text (if exists): 9 https://www.kilgarriff.co.uk/bnc-readme.html\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1220175.1220321\nCitation Context: Association for Computational Linguistics, Sydney, Australia, pages 1161\u2013 1168. [URL_CITE] https://doi.org/10.3115/1220175.1220321.2016. Word alignment without null words.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/giza/GIZA++.html\nCitation Context: As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [URL_CITE] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.\nFootnote or Reference text (if exists): 12 http://www.statmt.org/moses/giza/GIZA++.html\nSection Titles (if exists):  ['8 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.twitter.com/company\nCitation Context: With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [URL_CITE] . In addi-tion to individual users, various organizations and public figures such as newspapers, government officials and entertainers have established them-selves on social media in order to disseminate in-formation or promote their products.\nFootnote or Reference text (if exists): 1 https://about.twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Efraim\nCitation Context: If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map \u201cEphraim Sneh\u201d to [URL_CITE] http://en.wikipedia.org/wiki/Efraim Sneh and to assign it the Israeli nationality, male gender, and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Wikipedia as Knowledge', 'Named Entities with Wikipedia Disambiguation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/row-column-intersection\nCitation Context: Experi-ments on recent benchmarks prove that the pro-posed methods can effectively locate cell val-ues on tables (up to \u223c98% Hit@1 accuracy on WikiSQL lookup questions). Also, the inter-action model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (T A P AS and T A B ERT ), achieving \u223c3.4% and \u223c18.86% additional pre-cision improvement on the standard WikiSQL benchmark [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code and the models we built are available at https://github.com/IBM/row-column-intersection.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yizhen20133868/Retriever-Dialogue\nCitation Context: To justify the gen-eralization of the proposed model, we also use an-other public CamRest dataset (Wen et al., 2017b) and partition the datasets into training, validation and testing set in the ratio 3:1:1. [URL_CITE] Especially, we hired some human experts to format the CamRest dataset by equipping the corresponding KB to ev-ery dialogues.\nFootnote or Reference text (if exists): 3 The dataset can be available at: https://github.com/yizhen20133868/Retriever-Dialogue\nSection Titles (if exists):  ['4 Training the KB-Retriever', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sweetalyssum/Seq2Seq-DU\nCitation Context: Experimental results on benchmark datasets show that Seq2Seq-DU [URL_CITE] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT).\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/sweetalyssum/Seq2Seq-DU.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shashiongithub/XSum\nCitation Context: We demonstrate exper-imentally that this architecture captures long-range dependencies in a document and recog-nizes pertinent content, outperforming an or-acle extractive system and state-of-the-art ab-stractive approaches when evaluated automat-ically and by humans. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.utexas.edu/ml/nldata/jobquery.html\nCitation Context: We directly use Prolog logical forms provided on the web-site. [URL_CITE] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance.\nFootnote or Reference text (if exists): 9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"\nSection Titles (if exists): [\\'-\\', \\'A.1 Details of Benchmark Construction\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://accent.gmu.edu\nCitation Context: The theoretical and practical value of studying human accented speech is of interest to language teachers, linguists, and computational linguists. It is also part of the research program behind the Speech Accent Archive ( [URL_CITE] http://accent.gmu.edu) housed at George Mason University. The Archive is a growing database of English speech varieties that contains more than 1,100 samples of native and non-native speakers reading from the same English paragraph.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.jhu.edu/~jason/tutorials/variational.html\nCitation Context: Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth-ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables which we refer to as the marginal energy. Follow-ing the approach described for undirected models by Eisner (2011) [URL_CITE] , we approximate these quantities and their gradients with respect to the model pa-rameters \u03b8 as we will now describe (thread-level embeddings are omitted in this section for clarity).\nFootnote or Reference text (if exists): Jason Eisner. 2011. High-level explanation of varia-tional inference. https://www.cs.jhu.edu/~jason/tutorials/variational.html. Accessed: 2018-02-22.\nSection Titles (if exists):  ['3 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/yetnVye_hg4\nCitation Context: Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset with such strat-egy is also published to promote further re-search. An online demo of NAMER is pro-vided to visualize our framework and supply extra information for users, a video illustra-tion [URL_CITE] of NAMER is also available.\nFootnote or Reference text (if exists): 3 https://youtu.be/yetnVye_hg4\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/WeimingWen/CCRV\nCitation Context: We published our code and dataset on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/WeimingWen/CCRV\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/software/sutime.shtml\nCitation Context: Temporal expressions are automatically identified with SU-Time (Chang and Manning, 2012), an off-the-shelf temporal expression detector. [URL_CITE] We keep only the dialogs with more than 3 temporal expressions and at least one expression that contains numerals like \u201ctwo weeks\u201d (as opposed to non-numeric spans, like \u201csummer\u201d, \u201cright now\u201d, and \u201clater\u201d). In our initial experiment, we observe that language models can often correctly predict these non-numerical tempo-ral phrases.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/software/sutime.shtml\nSection Titles (if exists):  ['3 Dataset: T IME D IAL', '3.1 Data Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cl.aist-nara.ac.jp/~taku-ku/software/cabocha/\nCitation Context: In addition, we decided to use the output of ChaSen 2.2.9 and CaboCha 0.34 [URL_CITE] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus.\nFootnote or Reference text (if exists): 4 http://cl.aist-nara.ac.jp/\u02dctaku-ku/software/cabocha/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pilehvar.github.io/wic/\nCitation Context: To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno-tations curated by experts, for generic evalua-tion of context-sensitive representations. WiC is released in [URL_CITE] https://pilehvar.github.io/wic/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nCitation Context: This intuition is reflected in the bigram transition probabilities obtained from our corpus. [URL_CITE]\nFootnote or Reference text (if exists): 1 Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available at http://www.cs.mu.oz.au/\u223cedwardi/papers/datransitions.html\nSection Titles (if exists):  ['4 Training on Speech Acts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Tool, DataSource)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex- 2017) trained on the [URL_CITE] Fr-En corpus (Callison-Burch et al., 2009) (Appendix A). ploits the benefits of large-scale pretraining, in the form of GloVe embeddings and MT pretrain-ing, both absent in the CRF.\nFootnote or Reference text (if exists): 10 300-dimensional, uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/; 15,533 out-of-vocabulary words across all datasets were assigned a random embedding (uniformly from [\u2212 01 01]).. , . Embeddings remained fixed during training.\nSection Titles (if exists):  ['3 \u201cNeural-Davidsonian\u201d Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/GeoText/\nCitation Context: Geo-tagged Microblog Corpus As a second eval-uation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al. (2010). [URL_CITE] It contains 380,000 mes-sages from 9,500 users tweeting within the 48 states of the continental USA.\nFootnote or Reference text (if exists): 4 http://www.ark.cs.cmu.edu/GeoText/\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ml4nlp.de/code-and-data/treex2conll\nCitation Context: Their annotation in the CoNLL-ST is not identi-cal, but the guidelines for \u201ccore\u201d semantic roles are similar (Kingsbury et al., 2004), so we eval-uate only on core roles here. The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Hajic\u030c et al., 2012), which we converted to a format simi-lar to that of CoNLL-ST [URL_CITE] . The original annotation uses the tectogrammatical representation (Hajic\u030c, 2002) and an inventory of semantic roles (or func-tors), most of which are interpretable across vari-ous predicates.\nFootnote or Reference text (if exists): 1 see http://www.ml4nlp.de/code-and-data/treex2conll\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/CRF-NER.shtml\nCitation Context: To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [URL_CITE] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types. We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to .\nFootnote or Reference text (if exists): 5 We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it.\nSection Titles (if exists):  ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908\nCitation Context: The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated an-swers (ROUGE-L against reference answers) and retrieved documents (R-precision against human-annotated relevant documents). In this paper, we build a state-of-the-art system [URL_CITE] for ELI5 by using a sparse Transformer variant (Roy et al., 2020) to condition over Wikipedia paragraphs returned by a REALM-style retriever (Guu et al., 2020).\nFootnote or Reference text (if exists): 2 State-of-the-art as of April 3, 2021 \u2014 the \u201cGoogle Research & UMass Amherst\u201d team entry on https: //evalai.cloudcv.org/web/challenges/ challenge-page/689/leaderboard/1908\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation\nCitation Context: Full results are available on the WAT \u201916 Website. [URL_CITE]\nFootnote or Reference text (if exists): 10 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/srparser.shtml\nCitation Context: Our parser architecture follows previous work, particularly Zhu et al. (2013) and Bauer (2014) [URL_CITE] .\nFootnote or Reference text (if exists): John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/srparser.shtml.\nSection Titles (if exists):  ['2 Discontinuous Shift-Reduce Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016), Kasai et al. (2017) [URL_CITE] , and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://x-factr.github.io\nCitation Context: We run both the independent and confidence-based decoding methods with 3 M-LMs, and when avail-able 8 monolingual LMs, across 23 languages, [URL_CITE] with results shown in Fig. 3.\nFootnote or Reference text (if exists): 6 Check https://x-factr.github.io for latest results.\nSection Titles (if exists):  ['5 X-FACTR Benchmark Performance', '5.1 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A34739.html\nCitation Context: The Pearson\u2019s correlation coefficient [URL_CITE] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.\nFootnote or Reference text (if exists): 5 For a quick overview of the Pearson\u2019s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://framenet.icsi.berkeley.edu/\nCitation Context: We use FrameNet-style (Fillmore, 1976) se-mantic frames as our main source of weak supervision. [URL_CITE] We also explore named entity recognition (NER).\nFootnote or Reference text (if exists): 1 See http://framenet.icsi.berkeley.edu/\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.fb.com/fasttext/\nCitation Context: 1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016) [URL_CITE] (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768).\nFootnote or Reference text (if exists): Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).\nSection Titles (if exists):  ['4 Approaches to authorship verification', '4.4 Approach 4: Document embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/S10-1009\nCitation Context: Finally, we apply our new parsers to the down-stream tasks of Parsing Evaluation using Tex-tual Entailements (PETE, Yuret et al. (2010) [URL_CITE] ) and Unbounded Dependency Recovery (Rimell et al., 2009). We demonstrate that our end-to-end parser outperforms the best results in both tasks.\nFootnote or Reference text (if exists): Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51\u2013 56. http://www.aclweb.org/anthology/S10-1009.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py\nCitation Context: Text pre-processing For all the RoBERTa-based models, we used the RoBERTa roberta-base\u2019s tokenizer provided in the transformers library. [URL_CITE] We did not perform any additional pre-processing in our experiments.\nFootnote or Reference text (if exists): 13 https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://continual-vista.github.io/\nCitation Context: Evidence from child language acquisi-tion indicates that children learn Wh-questions before polar (Yes/No) questions (Moradlou and Ginzburg, 2016; Moradlou et al., 2018). Mo-tivated by this finding, we design a set of linguistically-informed experiments: i) to inves-tigate whether the order in which children ac-quire question types facilitates continual learning for computational models and, accordingly, the impact of task order on catastrophic forgetting; ii) to measure how far two well-known CL ap-proaches help to overcome the problem (Robins, 1995; Kirkpatrick et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Code and data are available at the link http://continual-vista.github.io/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://open.weibo.com/wiki/Trends/hourly\nCitation Context: Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags [URL_CITE] on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.\nFootnote or Reference text (if exists): 6 http://open.weibo.com/wiki/Trends/ hourly\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/segmenter.shtml\nCitation Context: The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002) [URL_CITE] ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.\nFootnote or Reference text (if exists): 7 http://nlp.stanford.edu/software/segmenter.shtml\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions\nCitation Context: Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, \"AG\") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017 [URL_CITE] ) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) for ancient literary texts.\nFootnote or Reference text (if exists): Laura Gawlinski. 2017. Review: Packard hu-manities institute\u2019s searchable greek inscrip-tions. https://classicalstudies.org/scs-blog/laura-gawlinski/review-packard-humanities-institutes-searchable-greek-inscriptions. Accessed on 2019-08-26.\nSection Titles (if exists): [\\'3 Generating PHI-ML\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service [URL_CITE] (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 11 https://aws.amazon.com/translate\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Preprocessing Each dataset is tokenized using the spaCy [URL_CITE] tokenizer, converted to lowercase, and non-\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['4 Model Training', '4.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.inevent-project.eu\nCitation Context: The work described in this article was sup-ported by the European Union through the inEvent project FP7-ICT n. 287872 (see [URL_CITE] http://www.inevent-project.eu).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/machelreid/afromt\nCitation Context: All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code, pretrained models, and data can be found at https://github.com/machelreid/afromt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/iirc\nCitation Context: We fol-low recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at [URL_CITE] https://allennlp.org/iirc.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/\nCitation Context: Wei Lu is an Assistant Professor at the Sin-gapore University of Technology and Design (SUTD), directing the StatNLP research group ( [URL_CITE] http://statnlp.org/). He received his Ph.D. from the National University of Singapore (NUS) in 2009.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['About the Instructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ucrel.lancs.ac.uk/claws7tags.html\nCitation Context: USAS semantic categories [URL_CITE] ): advance JJ N4 advance NN1 A9- M1 A5.1+/A2.1 advance VV0 M1 A9- Q2.2 A5.1+/A2.1 advance VVI M1 S8+ A9- A5.1+/A2.1 Q2.1\nFootnote or Reference text (if exists): 1 For definitions of the POS and semantic tags, see websites http://ucrel.lancs.ac.uk/claws7tags.html and http://ucrel.lancs.ac.uk/usas/USASSemanticTagset.pdf\nSection Titles (if exists):  ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EducationalTestingService/skll\nCitation Context: Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [URL_CITE]\nFootnote or Reference text (if exists): 2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.\nSection Titles (if exists):  ['3 Models for Short Answer Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://idebate.org/debatabase\nCitation Context: We start by observing that framing the related-ness question within a pre-specified context may simplify the task for humans and machines alike, in particular since the correct sense of ambigu-ous terms can be identified. Correspondingly, we focus on 47 topics selected from Debatabase [URL_CITE] . For each topic, 5 human annotators searched Wikipedia for relevant articles as done in (Aharoni et al., 2014).\nFootnote or Reference text (if exists): 2 http://idebate.org/debatabase\nSection Titles (if exists):  ['3 Dataset generation methodology', '3.1 Defining topics and articles of interest']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://files.pushshift.io/reddit/\nCitation Context: We consider publicly-available Reddit submissions and comments [URL_CITE] from 2006 to 2018 as users\u2019 ut-terances. Given a Reddit user having a set of ut-terances U = u 0 ..u N , we aim to label the user with a set of profession and hobby values, based on explicit personal assertions (e.g., \u201cI work as a doctor\u201d) found in the user\u2019s posts.\nFootnote or Reference text (if exists): 4 https://files.pushshift.io/reddit/\nSection Titles (if exists):  ['4 Dataset', '4.1 Users\u2019 utterances']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://keras.io/\nCitation Context: Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras [URL_CITE] . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 2 .\nFootnote or Reference text (if exists): 1 https://keras.io/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html\nCitation Context: In the second experiment, we use the reverse order of the two corpora and denote it by \u201cB-to-WSJ\u201d. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [URL_CITE] with the stan-dard head rules (Yamada and Matsumoto, 2003).\nFootnote or Reference text (if exists): 3 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/subhadarship/learning-to-unjumble\nCitation Context: Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/subhadarship/learning-to-unjumble.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/sw2v\nCitation Context: For each sense, it extracts from the resource the set of semantically related words, called sense biasing words, which are in turn used to compute the sense embedding. SW2V [URL_CITE] (Mancini et al., 2017) is an extension of Word2Vec (Mikolov et al., 2013a) for jointly learning word and sense embeddings, pro-ducing a shared vector space of words and senses as a result. For these three methods we follow the disambiguation strategy suggested by Pelev-ina et al.\nFootnote or Reference text (if exists): 13 http://lcl.uniroma1.it/sw2v\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cog.brown.edu/\u223cmj/software.htm\nCitation Context: There are 0.8M fea-tures in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea-tures in the updated version. [URL_CITE] However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Ta-bles 3 and 4). This result confirms that our feature set design is appropriate, and the averaged percep-tron learner is a reasonable candidate for reranking.\nFootnote or Reference text (if exists): 5 http://www.cog.brown.edu/\u223cmj/software.htm. We follow this version as it corrects some bugs from their 2005 paper which leads to a 0.4% increase in performance (see Table 4).\nSection Titles (if exists):  ['packed forest for each sentence. 3', '5.1 Data Preparation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ipsc.jrc.ec.europa.eu/index.php?id=198\nCitation Context: Our EN\u2013FR dataset is from the pub-licly available JRC-Acquis corpus. [URL_CITE] Word align-ment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and.\nFootnote or Reference text (if exists): 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198\nSection Titles (if exists):  ['3 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: We first pre-train an agreement classifier on a stance detection dataset provided by the Fake News Challenge [URL_CITE] . This dataset provides pairs of English sentences with their agreement annota-tions in \u201cagree\u201d, \u201cdisagree\u201d, \u201cdiscuss\u201d and \u201cunre-lated\u201d.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['5 Cross-lingual Cross-platform Features', '5.2 Agreement Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GuyAllard/markov_clustering\nCitation Context: Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [URL_CITE] .\nFootnote or Reference text (if exists): 4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_ clustering\nSection Titles (if exists):  ['3 Strategies for Timeline Summarization', 'Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.py\nCitation Context: We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [URL_CITE] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.\nFootnote or Reference text (if exists): 1 https://github.com/saffsd/langid.py\nSection Titles (if exists):  ['6 Data and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [URL_CITE] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 5 http://pytorch.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.bbn.com/ears\nCitation Context: All systems were evaluated using rteval (Rich Tran-scription Evaluation) version 2.3 (Kubala and Srivastava, 2003) [URL_CITE] . Rteval aligns the system output to the annotated reference transcripts in such a way as to minimize the lex-eme error rate.\nFootnote or Reference text (if exists): Francis Kubala and Amit Srivastava. 2003. A Framework for Evaluating Rich Transcription Technology. BBN Ears Web-site. http://www.speech.bbn.com/ears.\nSection Titles (if exists):  ['4 Results', '4.2 Top Rules Learned']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iesl.cs.umass.edu/data/bibtex\nCitation Context: For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries. [URL_CITE] We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset labeled for coref-erence, in order to assess accuracy.\nFootnote or Reference text (if exists): 3 http://www.iesl.cs.umass.edu/data/bibtex\nSection Titles (if exists):  ['4 Experiments: Author Coreference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sites.google.com/site/trecliveqa2015/\nCitation Context: Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time. [URL_CITE] The Yahoo! Answers man-ner question dataset prepared by Jansen et al.\nFootnote or Reference text (if exists): 5 https://sites.google.com/site/trecliveqa2015/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hznlp/giza-kn\nCitation Context: The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) [URL_CITE] . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem.\nFootnote or Reference text (if exists): 12 https://github.com/hznlp/giza-kn\nSection Titles (if exists):  ['4 Experiments', '4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, DataSource, Code)\nFUNCTION: Introduce, Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1231\nCitation Context: Andor et al. (2016) [URL_CITE] developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective ad-dresses the label bias problem and makes global training effective in the transition-based parsing setting.\nFootnote or Reference text (if exists): Daniel Andor, Chris Alberti, David Weiss, Aliak-sei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Glob-ally normalized transition-based neural networks. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 2442\u20132452. http://www.aclweb.org/anthology/P16-1231.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html\nCitation Context: There are 35 different frames in to-tal. [URL_CITE] We manually assigned a set of clause types to each frame (e.g., SVO to frame \u201csomebody verb something\u201d). Table 1 shows an example frame for each of the seven clause types.\nFootnote or Reference text (if exists): 3 See http://wordnet.princeton.edu/wordnet/man/wninput.5WN.html.\nSection Titles (if exists):  ['4 Syntactic Pruning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/\nCitation Context: The third set of features that we use are stylis-tic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK [URL_CITE] (Loper and Bird, 2002).\nFootnote or Reference text (if exists): 7 http://www.nltk.org/\nSection Titles (if exists):  ['4 Features', '4.1 Text-based Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset + Code)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/MUSE\nCitation Context: (2019), because the order in which the languages are added is an additional hyperparameter that would explode the experimental space. [URL_CITE] We also do not com-pare to UMH, as we consider it conceptually similar to MAT+MPSR and no code is publicly available. For BWE experiments, we use MUSEs 14 (MUSE, semisupervised) and VecMap 15 systems, and we additionally compare them to MAT+MPSR for completeness.\nFootnote or Reference text (if exists): 13 We refer the reader to Table 2 from Heyman et al. (2019) which compares to MAT+MPSR, and to Table 7 of their appendix 14 https://github.com/facebookresearch/MUSE which shows the dramatic influence of language order. 15 https://github.com/artetxem/vecmap\nSection Titles (if exists):  ['4 Lexicon Induction Experiments', '4.1 Methods and Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ipsc.jrc.ec.europa.eu/index.php?id=198\nCitation Context: Our EN\u2013FR dataset is from the pub-licly available JRC-Acquis corpus. [URL_CITE] Word align-ment is performed by GIZA++ (Och and Ney, 2003) with heuristic function grow-diag-final-and.\nFootnote or Reference text (if exists): 4 http://ipsc.jrc.ec.europa.eu/index. php?id=198\nSection Titles (if exists):  ['3 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://paracrawl.eu/\nCitation Context: In the study, conducted on the impact of various types of noise on MT quality, untranslated and misaligned segments had the most detrimental effect. Misaligned segments were by far the most prevalent type of noise in the ParaCrawl [URL_CITE] parallel corpus they used, twice as common as accepted segments. However, misalign-ments vary; a segment can have one extraneous word, it can have twice the content its counterpart has, or anything in between.\nFootnote or Reference text (if exists): 1 https://paracrawl.eu/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-transformers\nCitation Context: Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [URL_CITE] See Appendix C for training details. We present L2 results on PDTB 2.0 in Table 1 and results on PDTB 3.0 in Table 2 (see Appendix D for L1 re-sults).\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-transformers\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol', '3.1 Baseline results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: The Dela was manually developed in the 90\u2019s by a team of linguists. We used the distribution freely available in the platform Unitex (Paumier, 2011) [URL_CITE] . It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['4 Resources', '4.2 Lexical resources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/webis-de/acl20-editorials-style-persuasive-effect\nCitation Context: (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [URL_CITE]\nFootnote or Reference text (if exists): 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Material and Method)\nTYPE: Mixed (Dataset and Code)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/XLM\nCitation Context: We experimented with Masked Language Model (MLM) pretraining of the translation mod-els, p s\u2192t (q|q 0 ) and p t\u2192s (q 0 |q). We use the XLM implementation ( [URL_CITE] https://github.com/facebookresearch/XLM) and use de-fault hyperparameters for both MLM pretraining and and unsupervised NMT fine-tuning. The UNMT encoder is initialized with the MLM model\u2019s parameters, and the decoder is randomly initialized.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A.6 Language Model Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/mecab/\nCitation Context: For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [URL_CITE] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING).\nFootnote or Reference text (if exists): 10 http://taku910.github.io/mecab/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Conditions and Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mecab.sourceforge.net/\nCitation Context: Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [URL_CITE] is used to obtain morphemes.\nFootnote or Reference text (if exists): 3 http://mecab.sourceforge.net/\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cocodataset.org/\nCitation Context: We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [URL_CITE] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al.\nFootnote or Reference text (if exists): 8 http://cocodataset.org/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://syntagnet.org/\nCitation Context: In this paper we presented and described the architecture of SyntagRank, our state-of-the-art knowledge-based system for multilingual Word Sense Disambiguation using syntagmatic informa-tion. We also provided details concerning the use of SyntagRank\u2019s Web interface and RESTful API, accessible at [URL_CITE] http://syntagnet.org/ and http://api.syntagnet.org , respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: Progress on the aforementioned tasks has bene-fited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently con-ducted Fake News Challenge (Pomerleau and Rao, 2017) [URL_CITE] with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 as-sociated news articles which is orders of magni-tude smaller than those used for TE and QA.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cnts.uia.ac.be/conll2000/\nCitation Context: Syntactic-chunk label: The value of this feature is a B-I-O style label indicating what kind of syntactic chunk the word is contained in, e.g. noun phrase, verb phrase, or prepositional phrase. These are assigned using a word-chunking SVM-based system trained on the CoNLL-2000 data [URL_CITE] (which uses the lowest nodes of the Penn TreeBank syntactic trees to break sentences into base phrases).\nFootnote or Reference text (if exists): 2 http://cnts.uia.ac.be/conll2000/\nSection Titles (if exists):  ['5 Classifier Features', '5.4 Word class features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). Kasai et al.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OpenNMT/OpenNMT-py\nCitation Context: Cross For Cross, we leverage the implementa-tion available in OpenNMT-py [URL_CITE] (Klein et al., 2017) and define a Transformer model closely following Vaswani et al. (2017), except for the embedding modification we described.\nFootnote or Reference text (if exists): 20 https://github.com/OpenNMT/OpenNMT-py\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://osf.io/rtzv4\nCitation Context: We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were \u201cabout the same\u201d (see Appendix A for details) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.\nSection Titles (if exists):  ['3 The DAIS dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chopaal.org\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service [URL_CITE] . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012).\nFootnote or Reference text (if exists): 9 http://chopaal.org\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: We used equal numbers of sentences from three different genres [URL_CITE] :\nFootnote or Reference text (if exists): 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['3 Linguistic acceptability study', '3.1 Dataset creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2002T07\nCitation Context: RST is a descriptive framework that has been widely used in the analysis of discourse organiza-tion of written texts (Taboada and Mann, 2006b), and has been applied to various natural lan-guage processing tasks, including language gen-eration, text summarization, and machine trans-lation (Taboada and Mann, 2006a). In particu-lar, the availability of RST annotations on a se-lection of 385 Wall Street Journal articles from the Penn Treebank [URL_CITE] (Carlson et al., 2001) has facilitated RST-based discourse analysis of writ-ten texts, since it provides a standard benchmark for comparing the performance of different tech-niques for document-level discourse parsing (Joty et al., 2013; Feng and Hirst, 2014).\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/ LDC2002T07\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.efset.org/research/~/media/centralefcom/efset/pdf/\nCitation Context: First, as mentioned previously, in our dataset the MET and reported TOEFL scores have a Pearson\u2019s r correlation of 0.74. We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [URL_CITE] . Using 384 participants who took both tests, the study found a Pearson\u2019s r of 0.63 for the reading comprehension and 0.69 for the listen-ing comprehension sections of these tests.\nFootnote or Reference text (if exists): Richard M Luecht. 2015. EFSET PLUS - TOEFL iBT correlation study report. https://www.efset.org/research/\u02dc/media/centralefcom/efset/pdf/ EFSET_TOEFL_correlational_report_ Sep_v1.pdf.\nSection Titles (if exists):  ['4 English Proficiency Scoring Based on Eye Movements in Reading', '4.1 Correlation with MET and TOEFL']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GChrysostomou/tasc.git\nCitation Context: Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/GChrysostomou/tasc.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://members.unine.ch/jacques.savoy/clef/englishST.txt\nCitation Context: (2013) (Section 2). Meanwhile, we create a dictionary of NEs from a title list of English Wikipedia articles, excepting stop words, provided by UniNE [URL_CITE] . Regarding parsing features, we use baseline features and rich non-local features pro-posed by Zhang and Nivre (2011).\nFootnote or Reference text (if exists): 9 http://members.unine.ch/jacques.savoy/clef/englishST.txt\nSection Titles (if exists):  ['3 Models for MWE identification and MWE-aware dependency parsing', '3.1 Pipeline Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nCitation Context: In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [URL_CITE] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007).\nFootnote or Reference text (if exists): 4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 1: Stability of word2vec as a property of fre-quency in the PTB. Stability is measured across ten randomized embedding spaces trained on the training portion of the PTB (determined using language model-ing splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (x-axis), and each column (fre-quency bucket) is normalized.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [URL_CITE] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains.\nFootnote or Reference text (if exists): 3 https://github.com/SimonHFL/CWEB\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [URL_CITE] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 4 http://www.natcorp.ox.ac.uk/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec\nCitation Context: Next we consider the context-predicting vectors (DS Mikolov ) available as part of the word2vec [URL_CITE] project (Mikolov et al., 2013a). We use the publicly avail-able vectors which were trained on a Google News dataset of circa 100 billion tokens.\nFootnote or Reference text (if exists): 6 https://code.google.com/p/word2vec\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/tools/lextool.html\nCitation Context: If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [URL_CITE] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited.\nFootnote or Reference text (if exists): 1 http://www.speech.cs.cmu.edu/tools/lextool.html\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/elmo\nCitation Context: We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [URL_CITE] . We compare ELMo\u2019s contextual embeddings as-is, and also after con-catenating them with each of Tgt, SrcTune, and SrcSel embeddings in Table 8.\nFootnote or Reference text (if exists): 11 https://allennlp.org/elmo\nSection Titles (if exists):  ['Topics and tasks', '4.4 Contextual embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/I08-3008\nCitation Context: POS tags). The rationale behind this is that related languages show common syntactic structure that can be transferred to the original language, with delex-icalized parsing (Zeman and Resnik, 2008 [URL_CITE] ; Mc-Donald et al., 2011, inter alia) being one popular approach. 3\nFootnote or Reference text (if exists): Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation be-tween related languages. In IJCNLP. http://www.aclweb.org/anthology/I08-3008.\nSection Titles (if exists):  ['3 Our Approach \u2013 B I S PARSE -D EP', '3.2 Dependency Contexts without a Treebank']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lang.ee.washington.edu/MAWPS\nCitation Context: We report the important characteristics of the current available data as well as the perfor-mance of some state-of-the-art systems on various subsets of the data. M AWPS is located at [URL_CITE] http: //lang.ee.washington.edu/MAWPS.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [URL_CITE] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 1 https://slack.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/liblinear/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/v2.8.0/examples/run_glue.py\nCitation Context: The rest of the hyperparameters are set to default values. [URL_CITE]\nFootnote or Reference text (if exists): 4 The default hyperparameters are as in https: //github.com/huggingface/transformers/ blob/v2.8.0/examples/run_glue.py.\nSection Titles (if exists):  ['4 Experiments', '4.4 Downstream Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dadangewp/ACL19-SRW\nCitation Context: Last, we take advantage of the availability of multilingual word embeddings to build a joint-learning approach in the cross-lingual setting. All code and resources are available at [URL_CITE] https://github.com/dadangewp/ACL19-SRW.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Dataset)\nFUNCTION: Introduce, Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: These player ut-terances are the \u201clinguistic\u201d turns. In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, \u201cnon-linguistic\u201d turns (Hunter et al., 2018) [URL_CITE] . All turns are seg-mented into DUs, and these units are then con-nected by semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/bcEysXmR09c\nCitation Context: Ecco is available at https: //www.eccox.io/ . [URL_CITE]\nFootnote or Reference text (if exists): 2 Video demo available at https://youtu.be/ bcEysXmR09c\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.uexpress.com/dearabby/archives\nCitation Context: We use a situation to denote the one-sentence prompt given to a worker as the basis for writ-ing RoTs. We gather a total of 104k real life situations from four domains: scraped titles of posts in the subreddits r/confessions (32k) and r/amitheasshole (r/AITA, 30k), which largely focus on moral quandaries and interper-sonal conflicts; 30k sentences from the ROCSto-ries corpus (rocstories, Mostafazadeh et al., 2016); and scraped titles from the Dear Abby ad-vice column archives [URL_CITE] (dearabby, 12k).\nFootnote or Reference text (if exists): 3 https://www.uexpress.com/dearabby/archives\nSection Titles (if exists):  ['3 S OCIAL -C HEM -101 Dataset', '3.1 Situations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: We roughly follow the method presented in Xu et al. (2017) [URL_CITE] with extensions. Under the TAG analysis, VP coordination involves a VP-recursive auxiliary tree headed by the coordinator that in-cludes a VP substitution node (for the second con-junct) with label 1.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['A.2 Coordination']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kakaoenterprise/KorAdvMRSTestData\nCitation Context: Statistics and examples are described in Table 1. We release this data set at [URL_CITE] https: //github.com/kakaoenterprise/ KorAdvMRSTestData.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Adversarial Test Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nCitation Context: only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de-pendency parsing (INDP) [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code is available at http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nSection Titles (if exists):  ['1 Introduction and Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.google.com/research/atNaturalQuestions/visualization\nCitation Context: Recently, a new benchmark MRC dataset called Natural Questions [URL_CITE] (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater chal-lenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0.\nFootnote or Reference text (if exists): 2 NQ provides some visual examples of the data https://ai.google.com/research/atNaturalQuestions/visualization.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org\nCitation Context: By contrast, a large number of example inflections are freely available in a semi-structured format on the Web. The English Wik-tionary [URL_CITE] is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages. We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology.\nFootnote or Reference text (if exists): 1 http://en.wiktionary.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fjoch.com/YASMET.html\nCitation Context: We use the published package YASMET [URL_CITE] to conduct parameters training and classification. YASMET requires supervised learning for the training of maximum entropy model.\nFootnote or Reference text (if exists): 1 http://www.fjoch.com/YASMET.html\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: Online news platforms such as Google News and MSN [URL_CITE] News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 2 https://www.msn.com/en-us/news SearchQueries whether\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://chenhaot.com/papers/changemyview.html\nCitation Context: We consider a number of hand-designed features related to the textual content of posts in-spired by Tan et al. (2016) [URL_CITE] . TFIDF.\nFootnote or Reference text (if exists): Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee. 2016. Win-ning arguments: Interaction dynamics and per-suasion strategies in good-faith online discus-sions. In WWW, pages 613\u2013624. Paper home-page at https://chenhaot.com/papers/changemyview.html.\nSection Titles (if exists):  ['4 Early Prediction of Controversy', '4.1 Comparing Text Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/LASER\nCitation Context: To detect parallel sentences we use the LASER toolkit, [URL_CITE] which achieves state-of-the-art perfor-mance in mining parallel sentences (Artetxe and Schwenk, 2019). LASER uses multilingual sen-tence embeddings and a distance or margin cri-terion in the embeddings space to detect parallel sentences.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ LASER\nSection Titles (if exists):  ['2 The MLQA corpus', '2.1 Parallel Sentence Mining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\nCitation Context: 3. Dependency Weights: We use pre-trained vectors [URL_CITE] weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015.\nFootnote or Reference text (if exists): 7 https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\nSection Titles (if exists):  ['5 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: KaggleDBQA We introduce KaggleDBQA, a new dataset and evaluation setting for text-to-SQL parsers to bridge the gap between SOTA DBQA research and its real-life deployment. [URL_CITE] It systemati-cally addresses three aforementioned challenges:\nFootnote or Reference text (if exists): 1 Available at https://aka.ms/KaggleDBQA.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.ds.cam.ac.uk/iv250/hyperlex.html\nCitation Context: As shown by the high inter-annotator agreement on HyperLex (0.85), humans are able to consis-tently reason about graded LE . [URL_CITE] However, current state-of-the-art representation architectures are far from this ceiling. For instance, Vulic\u0301 et al.\nFootnote or Reference text (if exists): 10 For further details concerning HyperLex, we refer the reader to the resource paper (Vulic\u0301 et al., 2017). The dataset is available at: http://people.ds.cam.ac.uk/iv250/hyperlex.html\nSection Titles (if exists):  ['4 Results and Discussion', '4.2 Graded Lexical Entailment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/lyme-disease.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/genesis\nCitation Context: Along with an extensive analysis of G ENE S IS results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets and the code to repro-duce the experiments at [URL_CITE] https://github.com/SapienzaNLP/genesis.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.microsoft.com/en-us/cortana/skills/mva32-building-conversations\nCitation Context: Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018; Microsoft, 2018 [URL_CITE] ).\nFootnote or Reference text (if exists): Microsoft. 2018. Building conversations. https://docs.microsoft.com/en-us/cortana/skills/mva32-building-conversations.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: After incorporating all the constraints mentioned above, the final optimization problem (an IQP) is shown in Figure 2. We used Gurobi toolkit (Url6, 2013) [URL_CITE] to solve such IQPs. In our case, it solves 76 IQPs per second on a quad-core server with In-tel Xeon X5650 @ 2.67 GHz processors and 50 GB RAM.\nFootnote or Reference text (if exists): Url6. 2013. Gurobi optimization toolkit (http://www.gurobi.com/) (accessed july 1, 2013).\nSection Titles (if exists):  ['3 Modeling Global Inference', '3.2 Final Optimization Problem - An IQP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alpage.inria.fr/~sagot/lefff-en.html\nCitation Context: The continuous word representations are used to train a logis-tic regression classifier 2 for each morphological feature: gender and number for noun and adjec-tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [URL_CITE] . To ensure a fair comparison be-tween context-independent and context-dependent embedding classification, words that are ambigu-ous with respect to a given feature are excluded from the respective classifier\u2019s training and test data.\nFootnote or Reference text (if exists): 3 Lexique des Formes Fle\u0301chies du Franc\u0327ais: http://alpage.inria.fr/\u02dcsagot/lefff-en.html\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: All datasets used in the experiments are available at [URL_CITE] https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at [URL_CITE] https://aka.ms/KaggleDBQA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion & Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/r/singapore/comments/8gfewd/\nCitation Context: A reading of the Level 7 texts including Singlish terms confirm that this is indeed the case. For ex-ample, one post is written in very eloquent standard English [URL_CITE] , but includes Singlish quotes as well as specific, appropriate Singlish terms (with English explanations in brackets).\nFootnote or Reference text (if exists): 5 https://www.reddit.com/r/singapore/comments/8gfewd/ the singaporean male version of metoo an exguards/\nSection Titles (if exists):  ['6 Qualitative evaluation and analysis', '6.3 Lexical closeness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [URL_CITE] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/jalammar/ecco\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/tuvuumass/task-transferability\nCitation Context: We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [URL_CITE]\nFootnote or Reference text (if exists): 2 Library and code available at http://github.com/tuvuumass/task-transferability.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [URL_CITE] ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rivercold/BERT-unsupervised-OOD\nCitation Context: Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Source"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: Even on this task we report promising results. Code and data used in this paper can be downloaded from [URL_CITE] http: //sarathchandar.in/bridge-corrnet.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [URL_CITE] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data.\nFootnote or Reference text (if exists): 14 https://spacy.io/\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://liblinear.bwaldvogel.de/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: 2 We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.3 Joint Modeling']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/joccfqy\nCitation Context: This provides a reasonable upper bound on the performance of any extractor trained on this data. Implementation Details We use CPLEX4 [URL_CITE] to solve the ILP described in Sec. 2.\nFootnote or Reference text (if exists): 11 http://tinyurl.com/joccfqy\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.academia.edu/6888756/\nCitation Context: The features are own text length, next text length, number of punctuation symbols, number of sentences, num-ber of stop words (stop words were derived from Weka (Hall et al., 2009)), number of discourse cues, number of named entity slots and number of words with capitalized initial letter. We cal-culated the number of discourse cues as the sum of explicit discourse markers provided by Denver (2018) [URL_CITE] and the number of coreference chains. We used Stanford CoreNLP (Manning et al., 2014) for identifying coreference chains and named entities.\nFootnote or Reference text (if exists): Tanya Denver. 2018. Discourse Markers \u2013 Connec-tors a list of discourse markers with examples. https://www.academia.edu/6888756/ discourse_markers_connectors_a_ list_of_discourse_markers_with_ examples.\nSection Titles (if exists):  ['3 Approach', '3.1 Domain-Independent Approach (DI)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amzn.to/2qDjNcJ\nCitation Context: The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains [URL_CITE] . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and third-party applications.\nFootnote or Reference text (if exists): 1 https://amzn.to/2qDjNcJ\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Paper)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ownthink.com\nCitation Context: There are four types of personal information in a Chinese loan application form: \u201cSchool\u201d, \u201cCom-pany\u201d, \u201cResidence\u201d and \u201cBirthPlace\u201d. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [URL_CITE] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked.\nFootnote or Reference text (if exists): 3 https://www.ownthink.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [URL_CITE] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://blog.lawgeex.com/contractcosts/\nCitation Context: Cur-rently, legal review remains an arduous and expen-sive process. For instance, a procurement contract requires 5 hours of legal review on average, con-tributing to thousands of dollars in total cost (Cum-mins, 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Tim Cummins. 2017. Cost of processing a ba-sic contract soars to $6900 (accessed: 2021- 04-09). https://blog.lawgeex.com/contractcosts/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Webcite\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [URL_CITE] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/wmaier/rparse\nCitation Context: We run further experiments with rparse [URL_CITE] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.\nFootnote or Reference text (if exists): 5 http://github.com/wmaier/rparse\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mmoradi-iut/NLP-perturbation\nCitation Context: The perturbation methods were implemented in Python using the NLTK library. The source code is available at [URL_CITE] https://github.com/mmoradi-iut/NLP-perturbation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org\nCitation Context: By contrast, a large number of example inflections are freely available in a semi-structured format on the Web. The English Wik-tionary [URL_CITE] is a crowd-sourced lexical resource that in-cludes complete inflection tables for many lexical items in many languages. We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology.\nFootnote or Reference text (if exists): 1 http://en.wiktionary.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We initialized our word representations using publicly available 300-dimensional Glove vec-tors [URL_CITE] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.\nFootnote or Reference text (if exists): 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['5 Experiments', '5.3 Hyperparameters and Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nCitation Context: (2019), who publishes a manually labeled sentence-level test set. [URL_CITE] Besides, they also publish a test set for evaluating noise-filtering ability. Details of the adopted dataset are shown in Table 1.\nFootnote or Reference text (if exists): 4 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/Data/QA/QC/\nCitation Context: We call this QClassification. Next, we also use a question/answer classification [URL_CITE] from Li and Roth (2002). This classifies questions into dif-ferent semantic classes based on the possible se-mantic types of the answers sought.\nFootnote or Reference text (if exists): 7 http://cogcomp.cs.illinois.edu/Data/QA/QC/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://diamt.limsi.fr/eval.html\nCitation Context: We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [URL_CITE] For each test set, models are assessed on their ability to rank the cor-rect translation of an ambiguous sentence higher than the incorrect translation, using the disam-biguating context provided in the previous source and/or target sentence. All examples in the test sets are hand-crafted but inspired by real examples from OpenSubtitles2016 (Lison and Tiedemann, 2016) to ensure that they are credible and that vo-cabulary and syntactic structures are varied.\nFootnote or Reference text (if exists): 2 The test sets are freely available at https://diamt.limsi.fr/eval.html.\nSection Titles (if exists):  ['2 Evaluating contextual phenomena', '2.1 Our contrastive discursive test sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nCitation Context: : Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3\u2013 [URL_CITE] fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%.\nFootnote or Reference text (if exists): 5 https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/songhaoyu/KvPI\nCitation Context: The code, data, and trained model are available at [URL_CITE] https://github.com/songhaoyu/KvPI.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Task I: Reranking Retrieved Responses', '5.5 Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/skip-thoughts/\nCitation Context: The encoder combines the sentence em-beddings to produce 4800-dimensional combine-skip vectors with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model. This work uses the 4800-dimensional vectors as they have been found to be the best performing in experiments [URL_CITE] . For training of the STGAN, the Skip-Thought encoder produces sentence embed-ding vectors which are labelled as real samples for GAN discriminator.\nFootnote or Reference text (if exists): 1 https://github.com/ryankiros/skip-thoughts/\nSection Titles (if exists):  ['3 Skip-Thought Generative Adversarial Network', '3.3 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ualberta.ca/~lindek/minipar.htm\nCitation Context: These features attempt to include some simple dependency information from the surrounding words, using the dependency parses produced by Minipar [URL_CITE] . These features aim to identify events that are expressed as phrases or that require knowledge of the surrounding phrase to be iden-tified.\nFootnote or Reference text (if exists): 3 http://www.cs.ualberta.ca/~lindek/minipar.htm\nSection Titles (if exists):  ['5 Classifier Features', '5.5 Governing features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1161\nCitation Context: Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [URL_CITE] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.\nFootnote or Reference text (if exists): Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dykang/xslue\nCitation Context: The preprocessed datasets and code are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dykang/xslue\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/pkumod/gAnswer\nCitation Context: Baselines We compare our results with the top ranking team \"jchl\" (Luo et al., 2019) in the contest and a competitive KBQA system gAnswer [URL_CITE] (Hu et al., 2018) that reached first place in QALD-9 (Ngomo, 2018). Since the NE and RE module in gAnswer does not officially support Chinese, we replace them with those in our system. Hence, the gAnswer evaluated can be partly viewed as our system with a rule-based QG module and its comparison with us indicates the effectiveness of our generative QG module.\nFootnote or Reference text (if exists): 7 https://github.com/pkumod/gAnswer\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Experimental Setup\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sr-research.com/products/eyelink-portable-duo/\nCitation Context: Start-ups, like Cogisen , have started using gaze features in their applications, such as using gaze information to improve input to image processing systems. Recently, SR Research has come up with a portable eye-tracking system [URL_CITE] .\nFootnote or Reference text (if exists): 5 https://www.sr-research.com/products/eyelink-portable-duo/\nSection Titles (if exists):  ['2 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [URL_CITE] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://psd.museum.upenn.edu/\nCitation Context: The process begins with the human expert submitting an unlemmatized transliteration in a format called ATF (ASCII Transliteration For-mat). This format is the standard interchange for-mat for transliteration across many projects dealing in and exchanging Assyriological textual represen-tations (such as CDLI, BDTNS, the Pennsylvania Sumerian Dictionary (PSD, 2006) [URL_CITE] , and Digital Cor-pus of Cuneiform Lexical Texts (DCCLT, 2014)). Via the Emacs interface, the transliteration is sub-mitted to the linguistic annotatation system, which identifies an existing project-specific glossary based on directives provided by the human expert in the transliteration, and returns a preliminary lemmatiza-tion whose completeness and content depends on the referenced project glossary.\nFootnote or Reference text (if exists): PSD: The Pennsylvania Sumerian Dictionary. 2006. http://psd.museum.upenn.edu/\nSection Titles (if exists):  ['2 CDLI and the Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.darpa.muk/iao/EARS.htm\nCitation Context: One of the major goals of the DARPA program for Effective, Affordable, Reusable Speech-to-Text (EARS) (Wayne, 2003) [URL_CITE] is to provide a rich transcription of speech recognition output, including speaker identification, sen-tence boundary detection and the annotation of disfluen-cies in the transcript (This collection of additional fea-tures is also known as Metadata). One result of this pro-gram has been production of an annotation specification for disfluencies in speech transcripts and the transcription of sizable amounts of speech data, both from conversa-tional telephone speech and broadcast news, according to this specification (Strassel, 2003).\nFootnote or Reference text (if exists): Charles Wayne. 2003. Effective, Affordable, Reusable Speech-to-Text (EARS). Official web site for DARPA/EARS Pro-gram. http://www.darpa.muk/iao/EARS.htm.\nSection Titles (if exists):  ['2 EARS Disfluency Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_searches-show-id\nCitation Context: The Twitter dataset is constructed based on TREC2011 microblog track . To recover conver-sations, we used Tweet Search API [URL_CITE] to retrieve full information of a tweet with its \u201cin reply to status id\u201d included. Recursively, we searched the \u201cin re-ply to\u201d tweet till the entire conversation is recov-ered.\nFootnote or Reference text (if exists): 5 http://developer.twitter.com/en/docs/tweets/search/api-reference/get-saved_ searches-show-id\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/mate-tools/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [URL_CITE] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 1 http://code.google.com/p/mate-tools/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf\nCitation Context: Given unstructured transcribed CC, we extract en-tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [URL_CITE] :\nFootnote or Reference text (if exists): NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.\nSection Titles (if exists):  ['2 Approach', '2.1 Event Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleyaligner/\nCitation Context: We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [URL_CITE] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/berkeleyaligner/\nSection Titles (if exists):  ['6 Experiments', '6.3 Performance in Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.twitter.com/en/docs\nCitation Context: Training tweets are collected using the Twitter streaming API. [URL_CITE] The tweets are collected for the duration of September 2017 and October 2017. We consider day-wise trending topics during this period.\nFootnote or Reference text (if exists): 5 https://developer.twitter.com/en/docs\nSection Titles (if exists):  ['4 Data Sets', '4.1 Training Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md\nCitation Context: We use an annotated corpus of 1154 English Elec-tronic Health Records from cancer patients. Each note was annotated [URL_CITE] by two annotators who label clinical entities into several categories. These cate-gories can be broadly divided into two groups, Clin-ical Events and Attributes.\nFootnote or Reference text (if exists): 2 The annotation guidelines can be found at https://github.com/abhyudaynj/LSTM-CRF-models/blob/master/annotation.md\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Youmna-H/Coherence_AES\nCitation Context: The net-work is trained for 60 epochs and performance is monitored on the development sets \u2013 we select the model that yields the highest PRA value. [URL_CITE]\nFootnote or Reference text (if exists): 12 Our implementation is available at https: //github.com/Youmna-H/Coherence_AES\nSection Titles (if exists):  ['5 Model Parameters and Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/semi-supervised-qa\nCitation Context: We attain more than 50% F1 score on SQuAD and TriviaQA with less than a thou-sand labelled examples. We are also releasing a set of 3.2M cloze-style questions for practi-tioners to use while building QA systems [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://bit.ly/semi-supervised-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://christophm.github.io/interpretable-ml-book/\nCitation Context: Interpretable NLP aims to better understand pre-dictive models\u2019 internals for purposes such as de-bugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019) [URL_CITE] . These objectives require faithful rationales\u2014 explanations of the model\u2019s behavior that are accu-rate representations of its decision process (Melis and Jaakkola, 2018).\nFootnote or Reference text (if exists): Christoph Molnar. 2019. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html\nCitation Context: To improve recall, we take lowercase, uppercase, singular and plural forms of the head word into account, and replace proper names by fine-grained named entity types (using a gazetteer). All raw hit counts are converted into the Dunning Root Loglikelihood association measure, [URL_CITE] then nor-malized using Formula 2 within all antecedent can-didates of one anaphor.\nFootnote or Reference text (if exists): 6 http://tdunning.blogspot.de/2008/03/surprise-and-coincidence.html\nSection Titles (if exists):  ['5 Features', '5.1 Local features', '5.1.2 Other features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxjiao/UncSamp\nCitation Context: Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code is available at https://github.com/wxjiao/UncSamp\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/svcca\nCitation Context: Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [URL_CITE] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).\nFootnote or Reference text (if exists): 4 https://github.com/google/svcca\nSection Titles (if exists):  ['6 System Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~treebank/tokenization.html\nCitation Context: In much tagging and parsing work, PTB data has been used with gold-standard tokens, to a point where many re-searchers are unaware of the existence of the orig-inal \u2018raw\u2019 (untokenized) text. Accordingly, the for-mal definition of PTB tokenization [URL_CITE] has received lit-tle attention, but reproducing PTB tokenization au-tomatically actually is not a trivial task (see \u00a7 3).\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~treebank/tokenization.html for available \u2018documentation\u2019 and a sed script for PTB-style tokenization.\nSection Titles (if exists):  ['2 Common Conventions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/metricsmatr/\nCitation Context: Our choice of metrics was based on their popular-ity in the MT community, their performance in open competitions such as the NIST MetricsMATR chal-lenge (NIST, 2008) [URL_CITE] and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity.\nFootnote or Reference text (if exists): NIST. 2008. NIST MetricsMATR Challenge. Informa-tion Access Division. http://www.itl.nist.gov/iad/mig/tests/metricsmatr/.\nSection Titles (if exists):  ['3 Classifying with MT Metrics', '3.2 MT metrics used']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [URL_CITE] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/r/WritingPrompts/\nCitation Context: For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS [URL_CITE] and FUNNY from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox.\nFootnote or Reference text (if exists): 5 https://www.reddit.com/r/WritingPrompts/\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Tiiiger/bert_score\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [URL_CITE]\nFootnote or Reference text (if exists): 13 https://github.com/Tiiiger/bert_score\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [URL_CITE]\nFootnote or Reference text (if exists): 5 CRF experiments used the CRF++ package http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['4 From MaxEnt to CRFs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/chenyangh/Seq2Emo\nCitation Context: In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) ap-proaches in a fair setting. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/chenyangh/Seq2Emo\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Transformer-DyNet\nCitation Context: The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet [URL_CITE] implementation and extend it for our context-aware NMT model. The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively.\nFootnote or Reference text (if exists): 7 https://github.com/duyvuleo/Transformer-DyNet\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~jbg/static/downloads_and_media.html\nCitation Context: We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [URL_CITE]\nFootnote or Reference text (if exists): 6 We followed Pang and Lee\u2019s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Sentiment Prediction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [URL_CITE] ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/TomKenter/siamese-cbow\nCitation Context: We use Theano (Theano Development Team, 2016) to implement our network. [URL_CITE] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016).\nFootnote or Reference text (if exists): 4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.\nSection Titles (if exists):  ['3 Experimental Setup', '3.4 Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OnlpLab/morphodetection\nCitation Context: We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available on https://github.com/OnlpLab/morphodetection.\nSection Titles (if exists):  ['table, given a few forms of the same lexeme. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Languageisolate\nCitation Context: This method also has drawbacks. First, the tax-onomy built on language family does not cover all the languages in the world since some languages are isolate [URL_CITE] . Second, there are many language families (152 according to Paul et al.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/wiki/Language isolate\nSection Titles (if exists):  ['3 Multilingual NMT with Language Clustering', '3.3 Discussions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/decomp-sem/neural-sprl\nCitation Context: achieves state-of-the-art performance for SPRL. [URL_CITE] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art.\nFootnote or Reference text (if exists): 2 Implementation available at https://github.com/decomp-sem/neural-sprl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://visionandlanguage.net/VIST/dataset.html\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset [URL_CITE] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset (Wang and Wan, 2019; Mostafazadeh et al., 2016).\nFootnote or Reference text (if exists): 3 http://visionandlanguage.net/VIST/dataset.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2041\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [URL_CITE] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Wenduan Xu, Michael Auli, and Stephen Clark. 2015. CCG supertagging with a recurrent neural network. In ACL. Association for Computational Linguistics, Beijing, China, pages 250\u2013255. http://www.aclweb.org/anthology/P15-2041.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, Code)\nFUNCTION: Compare, Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://riejohnson.com/cnn_data.html\nCitation Context: We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015). [URL_CITE] We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are sig-nificantly different in size (see Appendix A), so we can see how our approach behaves with differ-ent amounts of training data.\nFootnote or Reference text (if exists): 7 http://riejohnson.com/cnn_data.html\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: As the Deterministic, Statistical, and Deep-RL model are included in the Stanford CoreNLP toolkit , we use their released model as baselines. For the End-to-end model, we also use their re-leased code [URL_CITE] .\nFootnote or Reference text (if exists): 7 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 The Experiment', '5.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.usf.edu/Freeassociation\nCitation Context: (1999). [URL_CITE] These were established by presenting a large num-ber of participants with a cue word (e.g., rice) and asking them to name an associate word in response (e.g., Chinese, wedding, food, white). For each word, the norms provide a set of associates and the fre-quencies with which they were named.\nFootnote or Reference text (if exists): 3 http://www.usf.edu/Freeassociation.\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rajarshd/GaussianLDA\nCitation Context: \u2022 GaussianLDA (Das et al., 2015). A new technique for topic modeling by treating the document as a collection of word embed-dings and topics itself as multivariate Gaus-sian distributions in the embedding space [URL_CITE] . We use default values for the parameters.\nFootnote or Reference text (if exists): 7 https://github.com/rajarshd/Gaussian LDA\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Microsoft/NeuronBlocks\nCitation Context: An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by sav-ing their learning cost and guiding them to find optimal solutions to their tasks. In this pa-per, we introduce NeuronBlocks [URL_CITE] , a toolkit encapsulating a suite of neural network mod-ules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple con-figuration of JSON files.\nFootnote or Reference text (if exists): 1 Code: https://github.com/Microsoft/NeuronBlocks\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ucasir/NPRF\nCitation Context: Unsupervised ranking models serve as baselines for comparisons. We use the open source Terrier platform\u2019s (Macdonald et al., 2012) implementa-tion of these ranking models: the DRMM and K-NRM models, and report re-sults for all six variants. Our implementation of the NPRF framework is available to enable fu-ture comparisons [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/ucasir/NPRF\nSection Titles (if exists):  ['3 Evaluation', '3.1 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983xxx\nCitation Context: We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [URL_CITE] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).\nFootnote or Reference text (if exists): Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.darpa.muk/iao/EARS.htm\nCitation Context: One of the major goals of the DARPA program for Effective, Affordable, Reusable Speech-to-Text (EARS) (Wayne, 2003) [URL_CITE] is to provide a rich transcription of speech recognition output, including speaker identification, sen-tence boundary detection and the annotation of disfluen-cies in the transcript (This collection of additional fea-tures is also known as Metadata). One result of this pro-gram has been production of an annotation specification for disfluencies in speech transcripts and the transcription of sizable amounts of speech data, both from conversa-tional telephone speech and broadcast news, according to this specification (Strassel, 2003).\nFootnote or Reference text (if exists): Charles Wayne. 2003. Effective, Affordable, Reusable Speech-to-Text (EARS). Official web site for DARPA/EARS Pro-gram. http://www.darpa.muk/iao/EARS.htm.\nSection Titles (if exists):  ['2 EARS Disfluency Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paracrawl.eu/download.html\nCitation Context: There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [URL_CITE] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a).\nFootnote or Reference text (if exists): 6 http://paracrawl.eu/download.html\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf\nCitation Context: Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) and semantic role la-beling (Chen and Rambow, 2003) [URL_CITE] . In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance. These re-sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.\nFootnote or Reference text (if exists): John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In EMNLP. pages 41\u201348. http://aclanthology.coli.uni-saarland.de/pdf/W/W03/W03-1006.pdf. Nasr, Owen Rambow, and Srinivas Bangalore. 2016. Revisiting supertagging and parsing: How to use supertags in transition-based parsing. In TAG+. pages 85\u201392. http://www.aclweb.org/anthology/W16-3309.\nSection Titles (if exists):  ['5 Downstream Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [URL_CITE] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bzhangXMU/transformer-aan\nCitation Context: We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/bzhangXMU/transformer-aan.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017 [URL_CITE] ) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html\nCitation Context: In Akamon, we support the usage of deep syn-tactic structures for obtaining fine-grained transla-tion rules as described in our former work (Wu et al., 2010) . Similarly, Enju [URL_CITE] , a state-of-the-art and freely available HPSG parser for English, can be used to generate packed parse forests for source sentences . Deep syntactic structures are included in the HPSG trees/forests, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence.\nFootnote or Reference text (if exists): 8 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html\nSection Titles (if exists):  ['4 Using Deep Syntactic Structures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/RSN\nCitation Context: Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at [URL_CITE] https://github.com/thunlp/RSN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tensorflow_constrained_optimization\nCitation Context: Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/google-research/ tensorflow_constrained_optimization\nSection Titles (if exists):  ['3 Proposed Approaches', '3.2 Bias-constrained Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~nbach/papers/\nCitation Context: Semantic parsing in general and AMR parsing specifically can be considered a subsumption of many semantic resolution sub-tasks, e.g. named entity recognition (Nadeau and Sekine, 2007), se-mantic role labeling (Gildea and Jurafsky, 2002), word sense disambiguation (Navigli, 2009) and re-lation finding (Bach and Badaskar, 2007) [URL_CITE] .\nFootnote or Reference text (if exists): Nguyen Bach and Sameer Badaskar. 2007. A Review of Relation Extraction. Unpublished. http: //www.cs.cmu.edu/\u02dcnbach/papers/ A-survey-on-Relation-Extraction. pdf.\nSection Titles (if exists):  ['9 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: The third dataset (wikipedia, henceforth) comes from the Wikipedia [URL_CITE] discussion section. When a topic on Wikipedia is disputed, the editors of that topic start a discussion about it.\nFootnote or Reference text (if exists): 4 http://www.wikipedia.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crowdflower.com\nCitation Context: While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [URL_CITE] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.\nFootnote or Reference text (if exists): 4 http://crowdflower.com\nSection Titles (if exists):  ['4 Crowd-Sourcing Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tesseract-ocr/\nCitation Context: These data were originally available in PDF version. We apply the Optical Character Recog-nition (OCR) via Tesseract OCR engine [URL_CITE] to ex-tract the Cherokee and English text. Then our co-author, a proficient second-language speaker of Cherokee, manually aligned the sentences and fixed the errors introduced by OCR.\nFootnote or Reference text (if exists): 6 https://github.com/tesseract-ocr/\nSection Titles (if exists):  ['3 Data Description', '3.1 Parallel Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/twitter-customer-care-document-prediction\nCitation Context: Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [URL_CITE] . We summarize the statistics of the three datasets in Table 2.\nFootnote or Reference text (if exists): 3 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: For all experiments, we use the Word2Vec imple-mentation in Gensim [URL_CITE] to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PhilipMay/stsb-multi-mt\nCitation Context: Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [URL_CITE] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 10 https://github.com/PhilipMay/ stsb-multi-mt\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: Figure 1 shows our biaffine parsing ar-chitecture. Following Dozat and Manning (2017) [URL_CITE] and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence. We add highway connections in the same fashion as our supertagging model.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.netlib.org/ode/vode.f\nCitation Context: The CSLS scheme with a neighbourhood size of 10 (CSLS-10) is adopted to build synthetic dictio-naries via the input CLWEs. A variable-coefficient ordinary differential equation (VODE) solver [URL_CITE] was implemented for the system described in Eq. (7).\nFootnote or Reference text (if exists): 3 http://www.netlib.org/ode/vode.f\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation Details of Algorithm 1']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html\nCitation Context: We extract 1,322,985 author mentions, each containing first, middle, last names, bags-of-words of paper titles, topics in paper titles (by running la-tent Dirichlet allocation (Blei et al., 2003)), and last names of co-authors. In addition we include 2,833 mentions from the REXA dataset [URL_CITE] labeled for coref-erence, in order to assess accuracy. We also include \u223c5 million mentions from DBLP.\nFootnote or Reference text (if exists): 4 http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html\nSection Titles (if exists):  ['4 Experiments: Author Coreference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: As Kernel Methods are the state-of-the-art su-pervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to per-form all our experiments, in particular Support Vector Machines [URL_CITE] . Taking into account the exter-nal knowledge provided by a MDM it is possible estimate the topic similarity among two texts ex-pressed in different languages, with the following kernel: where D is defined as in equation 1.\nFootnote or Reference text (if exists): 3 We adopted the efficient implementation freely available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['3 Exploiting Comparable Corpora', '3.1 Multilingual Domain Model', 'w il evaluated in the corpus T l .']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleylm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [URL_CITE] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 6 http://code.google.com/p/berkeleylm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/tensorboard\nCitation Context: Notable features of EGG include: (a) Prim-itives for implementing single-symbol or variable-length communication (with vanilla RNNs (Elman, 1990), GRUs (Cho et al., 2014), LSTMs (Hochreiter and Schmidhuber, 1997)); (b) Training with optimization of the com-munication channel through REINFORCE or Gumbel-Softmax relaxation via a common interface; (c) Simplified configuration of the general components, such as check-pointing, optimization, Tensorboard support, [URL_CITE] etc. ; (d) A simple CUDA-aware command-line tool for hyperparameter grid-search.\nFootnote or Reference text (if exists): 2 https://www.tensorflow.org/ tensorboard\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/qa.html\nCitation Context: We are now studying open-domain question an-swering systems [URL_CITE] , and we expect QA systems to benefit from anaphora resolution. Typical QA sys-tems try to answer a user\u2019s question by finding rel-evant phrases from large corpora.\nFootnote or Reference text (if exists): 1 http://trec.nist.gov/data/qa.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf\nCitation Context: Traditional studies on this problem generally re-sort to feature-engineering methods, which first ex-tracts various psychological categories via LIWC (Tausczik and Pennebaker, 2010) or statistical fea-tures by the bag-of-words model (Zhang et al., 2010). These features are then fed into a classi-fier such as SVM (Cui and Qi, 2017) [URL_CITE] and XGBoost (Tadesse et al., 2018) to predict the personality traits. Despite interpretable features that can be expected, feature engineering has such limitations as it relies heavily on manually designed features.\nFootnote or Reference text (if exists): Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).\nSection Titles (if exists):  ['2 Related Work', '2.1 Personality Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/mlqa\nCitation Context: We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [URL_CITE] MLQA contains QA instances in 7 languages, English, Ara-bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in-stances in English and 5K in each other lan-guage, with each instance parallel between 4 languages on average.\nFootnote or Reference text (if exists): 1 MLQA is publicly available at https://github.com/facebookresearch/mlqa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleyaligner/\nCitation Context: We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [URL_CITE] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/berkeleyaligner/\nSection Titles (if exists):  ['6 Experiments', '6.3 Performance in Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ukp.tu-darmstadt.de/fnwkde/\nCitation Context: The approach is in theory applicable to any language, but there are some obstacles: first, it relies on the availability of the target sense in the target language Wiktionary. For many of the top 30 languages in FNWKxx, the Wiktionary edi-tions seem sufficiently large to provide targets for translation disambiguation, [URL_CITE] and they are contin-uously extended. Second, our approach requires access to the target language Wiktionary, but the data format across Wiktionary language editions is not standardized.\nFootnote or Reference text (if exists): 4 see overview table at http://www.ukp.tu-darmstadt.de/fnwkde/.\nSection Titles (if exists):  ['9 Discussion: a Multilingual FrameNet']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/sgl\nCitation Context: Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at [URL_CITE] https: //github.com/SapienzaNLP/sgl.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://panlex.org/\nCitation Context: For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex [URL_CITE] . The dictionaries are of vary-ing qualities and sizes. The total number of entries in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis) are 4682, 76634 and 6628, respectively.\nFootnote or Reference text (if exists): 6 http://panlex.org/\nSection Titles (if exists):  ['3 Experiments', '3.1 Publicly available Wordnets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fchollet/keras\nCitation Context: All models are implemented with Keras (Chol-let et al., 2015) [URL_CITE] using Tensorflow as backend, and are trained on servers with multiple CPU cores. In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant.\nFootnote or Reference text (if exists): Franc\u0327ois Chollet et al. 2015. Keras. https://github.com/fchollet/keras.\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: We employ both BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) as our PLMs to evaluate Meta-DTL [URL_CITE] . Three sets of NLP tasks are used for evaluation, with the statistics of all the seven public datasets reported in Table 1:\nFootnote or Reference text (if exists): 8 We use Google\u2019s official base models. See: https://github.com/google-research/bert and https://github.com/google-research/albert.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LIAAD/yake\nCitation Context: To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation [URL_CITE] (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) within each dataset.\nFootnote or Reference text (if exists): 10 https://github.com/LIAAD/yake\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleyaligner/\nCitation Context: We use the entire IWSLT09 BTEC training set for train-ing the translation system. The state-of-the-art un-supervised Berkeley aligner [URL_CITE] (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexi-calized reordering model enabled.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/berkeleyaligner/\nSection Titles (if exists):  ['6 Experiments', '6.3 Performance in Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/uclanlp/gn_glove\nCitation Context: We experiment with de-bias methods of both categories, Hard-Debias refers to the method in (Bolukbasi et al., 2016) and Gn-Glove refers to the method in (Zhao et al., 2018). For each method, we compare it with an unde-biased ver-sion. For Hard-Debias, we compare with em-beddings before these procedures and for Gn-Glove, we compare with the original Glove (Pen-nington et al., 2014) offered in its project home-page [URL_CITE] .\nFootnote or Reference text (if exists): 13 https://github.com/uclanlp/gn_glove\nSection Titles (if exists):  ['5 Empirical Evaluation and Application', '5.5 Case Study: Do De-bias Methods Really Remove Gender Bias?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [URL_CITE] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification.\nFootnote or Reference text (if exists): 1 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['1 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://res.qyzhou.me/redistribute.zip\nCitation Context: The questions are written by crowd-workers and the answers are spans of tokens in the arti-cles. We employ two different data splits by fol-lowing (Zhou et al., 2017) [URL_CITE] and (Du et al., 2017) . In (Zhou et al., 2017), the original SQuAD de-velopment set is evenly divided into dev and test sets, while (Du et al., 2017) treats SQuAD devel-opment set as its test set and splits SQuAD training set into a training set (90%) and a development set (10%).\nFootnote or Reference text (if exists): 2 https://res.qyzhou.me/redistribute.zip\nSection Titles (if exists):  ['3 Experimental Setting', '3.1 Dataset & Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.artint.2008.12.002\nCitation Context: The task of generating descriptions from time-series or structured data has been tackled in vari-ous domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009 [URL_CITE] ; Banaee et al., 2013b), and sports (Liang et al., 2009). Traditionally, many studies used hand-crafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005).\nFootnote or Reference text (if exists): Fran\u00e7ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter, Somayajulu Sripada, Yvonne Freer, and Cindy Sykes. 2009. Automatic generation of textual sum-maries from neonatal intensive care data. Artificial Intelligence 173(7-8):789\u2013816. https://doi.org/10.1016/j.artint.2008.12.002.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nCitation Context: The expectation and variance of Dirichlet(\u03b8|v) are given as follows. [URL_CITE] where v 0 = i v i . Therefore the variance of a scaled Dirichlet is:\nFootnote or Reference text (if exists): 3 http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nSection Titles (if exists):  ['2 Bayesian Model of Summaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/jazzydicts\nCitation Context: For spell-checking, we use British and US-American English Jazzy dic-tionaries. [URL_CITE] Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011).\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/ jazzydicts\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/pretrained_models.html\nCitation Context: LMs examined in this paper share similar archi-tecture and pre-training setting as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), but are trained on different corpora. We provide the shortcut name of each LM in the Hugging-Face\u2019s Transformer library ( [URL_CITE] https://huggingface.co/transformers/pretrained_models.html) and their training corpora in Tab. 8, from which you can find more information.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['D Details of Pre-trained LMs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.sri.com/projects/srilm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [URL_CITE] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 5 http://www.speech.sri.com/projects/srilm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.utexas.edu/ml/nldata/jobquery.html\nCitation Context: We directly use Prolog logical forms provided on the web-site. [URL_CITE] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance.\nFootnote or Reference text (if exists): 9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"\nSection Titles (if exists): [\\'-\\', \\'A.1 Details of Benchmark Construction\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/machelreid/afromt\nCitation Context: All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code, pretrained models, and data can be found at https://github.com/machelreid/afromt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html\nCitation Context: Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018 [URL_CITE] ; Microsoft, 2018).\nFootnote or Reference text (if exists): Amazon. 2018. Create the interaction model for your skill. https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EdoFrank/emnlp2019_morio_egawa\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Mixed (Dataset, DataSource, Code)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/utils.html\nCitation Context: 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [URL_CITE] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation.\nFootnote or Reference text (if exists): 8 https://radimrehurek.com/gensim/utils.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UFAL-DSG/tgen\nCitation Context: The generator source code, along with config-uration files for experiments on the BAGEL data set, is available for download on Github. [URL_CITE]\nFootnote or Reference text (if exists): 11 https://github.com/UFAL-DSG/tgen\nSection Titles (if exists):  ['8 Conclusions and Further Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://idir.uta.edu/claimbuster/\nCitation Context: In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [URL_CITE] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates.\nFootnote or Reference text (if exists): 6 https://idir.uta.edu/claimbuster/\nSection Titles (if exists):  ['2 System Architecture and Components', '2.3 Claim Spotter']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method and Material)\nTYPE: Mixed (Tool and Knowledge)\nFUNCTION: Use, Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://metaoptimize.com/projects/wordreprs\nCitation Context: We used the extra-trees ensemble classifier im-plemented in (Pedregosa et al., 2011), with hyper-parameters optimized on the validation data: for-est of 500 trees and the maximum number of features\u221ato consider when looking for the best split is number features. We present a 3-fold evaluation: first, we compare our system with state-of-the-art systems on the same task, then we compare our domain-relevant vector representa-tions (DRVR-50) to more generic word embed-dings (C&W50, HLBL-50) [URL_CITE] and finally to another word representation construction on the domain-specific data (W2V-50) 4 .\nFootnote or Reference text (if exists): 3 C&W-50 are described in (Collobert and Weston, 2008), HLBL-50 are the Hierarchical log-bilinear embed-dings (Mnih and Hinton, 2007), provided by (Turian et al., 2010), available at http://metaoptimize.com/projects/wordreprs induced from the Reuters-RCV1\nSection Titles (if exists):  ['3 Experiments and Results', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pahendra.blogspot.com/2005/06/june-14th.html\nCitation Context: (http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) Definition 3 Association for Computational Linguistics http://www.aclweb.org/ It will be published in the upcoming leading ACL conference. ( [URL_CITE] http://pahendra.blogspot.com/2005/06/june-14th.html) Table 1 Acronym \u201cACL\u201d without its definition in three different meanings found in blogs\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jungokasai/graph_parser\nCitation Context: We implement all of our models in TensorFlow (Abadi et al., 2016). [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.mpi-inf.mpg.de/~corrogg/\nCitation Context: The candidate senses of each entry are obtained from WordNet. For instance, in the sentence \u201cHe takes a deep and long breath\u201d, the set of potential entries includes take (verb, 44 candidate senses), take a breath (verb, [URL_CITE] candidate sense), and breath (noun, 5 candidate senses). Note that in contrast to Werdy, most existing word-sense disambiguation methods assume that entries have already been (correctly) identified.\nFootnote or Reference text (if exists): 1 The VOS repository, Werdy\u2019s source code, and results of our experimental study are available at http://people.mpi-inf.mpg.de/\u02dccorrogg/.\nSection Titles (if exists):  ['2 Overview of Werdy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, [URL_CITE] and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 5 http://www.snopes.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: Since all the benchmark approaches use BERT for encodings and C HASE is constructed for Chinese, we replace BERT with Chinese-BERT. [URL_CITE] During the adaptations of EditSQL and IGSQL, we iden-tified and fixed 3 bugs in their pre-process and post-process procedures. The string-match based schema linking method in RAT-SQL utilizes the Stanford CoreNLP Toolkit (Manning et al., 2014) to tokenize a question, and the method performs string matches between the resulting words and schema items.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['A Appendix', 'A.3.1 Adaptation to Chinese Inputs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://babelnovel.com/\nCitation Context: Data collection. We construct our corpus from a professional Chinese novel reading site named Babel Novel [URL_CITE] . Unlike news, the novel dataset cov-ers a mixture of literary style including historical novels, and martial arts novels in the genre of fan-tasy, mystery, romance, military, etc.\nFootnote or Reference text (if exists): 9 https://babelnovel.com/\nSection Titles (if exists):  ['B Details of the Datasets', 'B.2 Novel Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [URL_CITE] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/\nCitation Context: The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [URL_CITE] . The statistics are shown in Table 1.\nFootnote or Reference text (if exists): 3 https://dev.twitter.com/\nSection Titles (if exists):  ['3 Experiments and Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://u.cs.biu.ac.il/~nlp/resources/downloads/\nCitation Context: We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://u.cs.biu.ac.il/~nlp/resources/downloads/\nSection Titles (if exists):  ['2 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/neural-storyteller\nCitation Context: Finally, it discusses how the automated corpus-based eval-uations correlate with human judgements. In fu-ture, this work aims to be applied for synthesizing images from text, exploring complementary archi-tectures to projects like neural-storyteller [URL_CITE] where skip-thought embeddings are already used to per-form image captioning with story-style transfer.\nFootnote or Reference text (if exists): 3 https://github.com/ryankiros/ neural-storyteller\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Code, Paper)\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nCitation Context: To identify the replaceable entities, we apply named entity recognition (NER) and POS tag-ging using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [URL_CITE] . This improves the likelihood that the terms are familiar to both headline editors and humor judges.\nFootnote or Reference text (if exists): 1 http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nSection Titles (if exists):  ['2 The Humor Dataset', '2.1 Task Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: Bousmalis et al. (2016) [URL_CITE] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pkubase.gstore.cn/\nCitation Context: Dataset We utilize the dataset published in CCKS Chinese KBQA Contest for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE [URL_CITE] , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.\nFootnote or Reference text (if exists): 5 A KB endpoint: http://pkubase.gstore.cn/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.lth.se/lth_srl\nCitation Context: We have described a dependency-based system [URL_CITE] for semantic role labeling of English in the PropBank framework. Our evaluations show that the perfor-mance of our system is close to the state of the art.\nFootnote or Reference text (if exists): 1 Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nCitation Context: (2015b). [URL_CITE]\nFootnote or Reference text (if exists): 7 http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nSection Titles (if exists):  ['4 Results and Discussion', '4.1 LE Directionality and Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenQA\nCitation Context: Experimen-tal results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines. The source code and data of this paper can be obtained from [URL_CITE] https: //github.com/thunlp/OpenQA\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jacobeisenstein/gt-css-class\nCitation Context: Jacob\u2019s Georgia Tech course on Computational Social Science covers some of the same themes as this tutorial, and includes some additional material. [URL_CITE] He recently completed an in-troductory textbook on natural language process-ing.\nFootnote or Reference text (if exists): 2 https://github.com/jacobeisenstein/ gt-css-class\nSection Titles (if exists):  ['3 Presenter']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kamalkraj/BERT-NER\nCitation Context: Model setup As the encoder f(x,X) in Equa-tion 2 in Section 2.1, we use BERT [URL_CITE] (Devlin et al., 2019), which is a state-of-the-art language model. As the baseline model, we use the general label embedding matrix without considering label com-ponents, i.e., each label embedding W[y] in Equa-tion 2 is randomly initialized and independently learned.\nFootnote or Reference text (if exists): 5 We use the open-source NER model utilizing BERT: https://github.com/kamalkraj/BERT-NER.\nSection Titles (if exists):  ['3 Experiments', '3.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/S10-1009\nCitation Context: Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010) [URL_CITE] . The task was intended to evaluate syn-tactic parsers across different formalisms, focus-ing on entailments that could be determined en-tirely on the basis of the syntactic representa-tions of the sentences that are involved, with-out recourse to lexical semantics, logical reason-ing, or world knowledge.\nFootnote or Reference text (if exists): Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. SemEval-2010 task 12: Parser evaluation using tex-tual entailments. In SemEval. Association for Com-putational Linguistics, Uppsala, Sweden, pages 51\u2013 56. http://www.aclweb.org/anthology/S10-1009.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/KB2E\nCitation Context: In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [URL_CITE] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub.\nFootnote or Reference text (if exists): 4 https://github.com/thunlp/KB2E\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/KodairaTomonori/EvaluationDataset\nCitation Context: \u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/KodairaTomonori/EvaluationDataset\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/universal-tagger/\nCitation Context: We made our code are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://code.google.com/p/universal-tagger/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nCitation Context: A detailed derivation is provided in the supplementary file. [URL_CITE]\nFootnote or Reference text (if exists): 8 The supplementary file is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012.\nSection Titles (if exists):  ['5 Parameter Estimation', '5.1 Optimizing the Mapping A']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tartarus.org/~martin/PorterStemmer\nCitation Context: To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [URL_CITE] . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy.\nFootnote or Reference text (if exists): 1 http://www.tartarus.org/\u02dcmartin/PorterStemmer\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sighan.org/bakeoff2005/\nCitation Context: We collect all the MSUs from the benchmark datasets provided by the second International Chi-nese Word Segmentation Bakeoff [URL_CITE] . We choose the Peking University (PKU) data because it is more fine-grained than all other corpora. Suppose we represent the segmented data as L (In our case L is the PKU word segmentation data), the MSU se-lecting algorithm is shown in T ABLE 3.\nFootnote or Reference text (if exists): 2 http://www.sighan.org/bakeoff2005/\nSection Titles (if exists):  ['2 System Architecture', '2.2 MSU Based Tagging', '2.2.1 Minimum Semantic Unit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2016-supersense-embeddings\nCitation Context: The outcomes of this work are available to the research community. [URL_CITE] . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the train-ing data beyond WordNet vocabulary.\nFootnote or Reference text (if exists): 11 https://github.com/UKPLab/ acl2016-supersense-embeddings\nSection Titles (if exists):  ['8 Conclusions and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://extremereader.megagon.info/\nCitation Context: These examples have redundant opinions and incorrect extractions in the input, but O PINION D IGEST is able to convert the input opinions into natural summaries. Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [URL_CITE] that allows users to customize the generated summary by specifying search terms.\nFootnote or Reference text (if exists): 5 http://extremereader.megagon.info/\nSection Titles (if exists):  ['3 Evaluation', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Document, Website)\nFUNCTION: Introduce, Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/IJCNN.2015.7280517\nCitation Context: Existing word em-bedding techniques are not suited to learn relation-ships between words appearing in different docu-ments and contexts (Le and Mikolov, 2014). Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015 [URL_CITE] ). However, relationships between events affecting stock prices can be quite com-plex, and their mentions can be spread across mul-tiple documents.\nFootnote or Reference text (if exists): Y. Shynkevich, T. M. McGinnity, S. Coleman, and A. Belatreche. 2015. Stock price prediction based on stock-specific and sub-industry-specific news ar-ticles. In 2015 International Joint Conference on Neural Networks (IJCNN). 1\u20138. https://doi.org/10.1109/IJCNN.2015.7280517\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.everyzing.com/\nCitation Context: We\u2019ve taken the rather nar-row view that systems might only expose the list of scores they assigned to retrieved documents, a plau-sible scenario considering the many systems now emerging which are effectively doing this already. Some examples include EveryZing, [URL_CITE] the MIT Lec-ture Browser, 2 and Comcast\u2019s video search. 3 This trend is likely to continue as the underlying repre-sentations of the content are themselves becoming increasingly complex (e.g., word and subword level lattices or confusion networks).\nFootnote or Reference text (if exists): 1 http://www.everyzing.com/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf\nCitation Context: 2004. LIB-SVM: a Library for Support Vector Machines, [URL_CITE] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/gale/data/Catalog.html\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [URL_CITE] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.\nFootnote or Reference text (if exists): 2 http://projects.ldc.upenn.edu/gale/data/Catalog.html\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/\nCitation Context: An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [URL_CITE] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time.\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/row-column-intersection\nCitation Context: As men-tioned in Section 1, our primary focus in this paper is on lookup questions that require selection and projection operations over tables (i.e., identifying the row and column of a table with very high pre-cision for a given natural language question). We are releasing the processing and evaluation code for the datasets to support reproducibility [URL_CITE] . Table 2 gives a summary of these datasets.\nFootnote or Reference text (if exists): 3 https://github.com/IBM/row-column-intersection\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.lighttag.io/\nCitation Context: Its main purpose is to classify scientific documents in a biomedical context, which is lim-ited in scope and configuration options and not suit-able as a general-purpose tool. The tools tagtog (Cejuela et al., 2014), prodigy 3 , and LightTag [URL_CITE] are all feature-rich and support some form of machine learning integration, but are commercial with either no free version or a free version with limited func-tionality. This makes these tools less accessible for projects with limited monetary resources.\nFootnote or Reference text (if exists): 4 https://www.lighttag.io/ Demonstrations, pages 99\u2013105\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nCitation Context: We resolve spelling errors with a public spell checker [URL_CITE] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vzhong/e3\nCitation Context: In addition to out-performing previous methods, E 3 is explainable in the sense that one can visualize what rules the model extracted and how previous interactions and inquiries ground to the extracted rules. We re-lease source code for E 3 and the BERTQA model at [URL_CITE] https://github.com/vzhong/e3.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: ELI5 (Fan et al., 2019b) [URL_CITE] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4).\nFootnote or Reference text (if exists): 8 https://yjernite.github.io/lfqa.html\nSection Titles (if exists):  ['3 Tasks', '3.4 Open Domain Question Answering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scopesys.com/\nCitation Context: This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [URL_CITE] . Gazetters have been widely used in NER systems to improve performance.\nFootnote or Reference text (if exists): 5 http://www.scopesys.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.1 Internal Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/\nCitation Context: Reproducibility. All code, data, and ex-periments for this paper are available on the CodaLab platform at [URL_CITE] https://worksheets.codalab.org/worksheets/0xe3eb416773ed4883bb737662b31b4948/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Related Work and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ssli.ee.washington.edu/data/puns\nCitation Context: We collected 75 puns from various joke websites such as Tumblr, Reddit, and Twitter and soliciting examples from friends and colleagues. [URL_CITE] These were collected without reference to the sources used by Sobkowiak to assemble the puns used in building the phonetic edit model. This data was used for test data only and is completely separate from the training data used by the language model and the phonetic edit model.\nFootnote or Reference text (if exists): 2 Data available at http://ssli.ee.washington.edu/data/puns.\nSection Titles (if exists):  ['4 Experiments', '4.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.rochester.edu/~gildea/Verbs/\nCitation Context: Slot structures are similar to verb frames, which are manually created for the PropBank annota-tion (Palmer et al., 2005). [URL_CITE] An example of the PropBank frame for the verb to kill is:\nFootnote or Reference text (if exists): 6 http://www.cs.rochester.edu/\u02dcgildea/Verbs/\nSection Titles (if exists):  ['5 Creating Templates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-metrics\nCitation Context: 1. We calculate n-gram metric scores using the E2E-metrics module [URL_CITE] https://github.com/tuetschek/e2e-metrics\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) [URL_CITE] and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [URL_CITE] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 8 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/3dxyZ1i\nCitation Context: We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset [URL_CITE] , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels).\nFootnote or Reference text (if exists): 3 https://bit.ly/3dxyZ1i\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Indexing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org/\nCitation Context: This research is part of the Tanbih project, [URL_CITE] which aims to limit the effect of \u2018fake news,\u201d disinforma-tion, propaganda, and media bias by making users aware of what they are reading.\nFootnote or Reference text (if exists): 14 http://tanbih.qcri.org/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Project)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.leffingwell.com/top_10.htm\nCitation Context: The lack of vocabulary for smells and flavors contrasts starkly with the interest people in the West have for flavors and fragrances, and what they are willing to spend on such products. The flavor and fragrance industry is estimated to be worth over $20 billion in 2015 [URL_CITE] . In this context, experts\u2019 recommendations are used by the pub-lic in order to help them make decisions about purchases.\nFootnote or Reference text (if exists): 1 http://www.leffingwell.com/top_10.htm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://chywang.github.io/data/emnlp17.zip\nCitation Context: In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://chywang.github.io/data/emnlp17.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.barna.com/research/hate-speech-increased/\nCitation Context: The occurrence of hatespeech has been increas-ing (Barna, 2019) [URL_CITE] . It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappro-priate behaviors such as hatespeech, and potential damage to social systems.\nFootnote or Reference text (if exists): Barna. 2019. U.S. adults believe hate speech has increased \u2014 mainly online. research releases in culture & media. https://www.barna.com/research/hate-speech-increased/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: 5. CorrNet + MT: Here, we train an En-Image Cor-rNet using Z 1 and an Fr/De-En MT system [URL_CITE] using Z 2 . For the task of retrieving images given a French\nFootnote or Reference text (if exists): 4 http://www.statmt.org/moses/\nSection Titles (if exists):  ['6 Experiment 2: Cross modal access using a pivot language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.icsi.berkeley.edu/~framenet/\nCitation Context: This section presents the semantic role paradigm and the role-annotated corpus on which the present study is based. FrameNet [URL_CITE] is a lexical resource based on Fillmore\u2019s Frame Semantics (Fillmore, 1985). It de-scribes frames, representations of prototypical situa-tions.\nFootnote or Reference text (if exists): 1 http://www.icsi.berkeley.edu/~framenet/\nSection Titles (if exists):  ['2 FrameNet']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/\nCitation Context: Here is an screen cast of the relevant anno-tation interface interface: [URL_CITE] https://youtu.be/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Franck-Dernoncourt/pubmed-rct\nCitation Context: PubMed: Medical abstracts of randomized con-trolled trials. Source: [URL_CITE] https://github.com/Franck-Dernoncourt/pubmed-rct.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com\nCitation Context: We take advantage of the annotation platform in Figure Eight. [URL_CITE] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator\u2019s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.\nFootnote or Reference text (if exists): 5 https://www.figure-eight.com\nSection Titles (if exists):  ['4 Dataset', '4.1 Annotation using Crowd Workers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/J11-2003\nCitation Context: Our architecture is similar to the pipelined struc-tures of earlier work in question answering that rely on a retrieval step followed by a more expen-sive supervised ranking approach (Surdeanu et al., 2011 [URL_CITE] ; Higashinaka and Isozaki, 2008). We primar-ily chose this architecture because it allows the user to directly relate the results of the system to the an-swer selection model.\nFootnote or Reference text (if exists): Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank an-swers to non-factoid questions from web collections. Computational Linguistics 37(2). http://aclweb.org/anthology/J11-2003.\nSection Titles (if exists):  ['2 System Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sshaar/That-is-a-Known-Lie\nCitation Context: \u2022 We create a specialized dataset, which we release to the research community. [URL_CITE] Un-like previous work in fact-checking, which used normalized claims from fact-checking datasets, we work with naturally occurring claims, e.g., in debates or in social media.\nFootnote or Reference text (if exists): 8 Data and code are available at the following URL: https://github.com/sshaar/ That-is-a-Known-Lie\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ow.ly/iNxF\u201d\nCitation Context: (1) \u201cSarah and Todd Palin to divorce, according to local Alaska paper. [URL_CITE] http://ow.ly/iNxF\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data', '4.1 Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Implementation details: The spaCy tool [URL_CITE] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing.\nFootnote or Reference text (if exists): 2 https://spacy.io\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.uni-weimar.de/medien/webis/corpora\nCitation Context: It will be made freely avail-able to other researchers. [URL_CITE] Second, we obtain a ro-bust classifier for argumentativeness, providing ev-idence that distant supervision does not only save money and time, but also benefits the effectiveness of cross-domain and cross-register argumentation mining. Third, we evaluate\u2014for the first time\u2014 the robustness of several features in classifying ar-gumentativeness across domains and registers.\nFootnote or Reference text (if exists): 1 http://www.uni-weimar.de/medien/webis/corpora\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nCitation Context: (2011), as implemented in the Eda toolkit. [URL_CITE] In order to convert dependencies into phrase-structure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing.\nFootnote or Reference text (if exists): 5 http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.galaxyforums.net/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [URL_CITE] . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 5 http://www.galaxyforums.net/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other (The URL citation is used to provide an example of a discussion forum, but it is not used or produced by the research)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/cabocha/\nCitation Context: For this reason, we evaluate the parser outputs by converting them to bunsetsu dependencies, the syntactic representation ordi-nary used in Japanese NLP (Kudo and Matsumoto, 2002). Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha [URL_CITE] and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b. For ex-ample, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai).\nFootnote or Reference text (if exists): 11 http://taku910.github.io/cabocha/\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.twitter.com/company\nCitation Context: With the rise in popularity of social media, mes-sage broadcasting sites such as Twitter and other microblogging services have become an important means of communication, with an estimated 500 million tweets being written every day [URL_CITE] . In addi-tion to individual users, various organizations and public figures such as newspapers, government officials and entertainers have established them-selves on social media in order to disseminate in-formation or promote their products.\nFootnote or Reference text (if exists): 1 https://about.twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo\nCitation Context: The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [URL_CITE] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures.\nFootnote or Reference text (if exists): 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.berro.com/Jokes\nCitation Context: The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, [URL_CITE] http://www.berro.com/Jokes or http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset, Webcite)\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/\nCitation Context: Due to availability of digitised epigraphic corpora, P YTHIA has been trained on ancient Greek (hence-forth, \"AG\") inscriptions, written in the ancient Greek language between 7 th century BCE and 5 th century CE. We chose AG epigraphy as a case study for two reasons: a) the variability of contents and context of the AG epigraphic record makes it an excellent challenge for NLP; b) several digital AG textual corpora have been recently created, the largest ones being PHI (The Packard Humanities Institute, 2005; Gawlinski, 2017) for epigraphy; Perseus (Smith et al., 2000) and First1KGreek (Crane et al., 2014) [URL_CITE] for ancient literary texts.\nFootnote or Reference text (if exists): Gregory Ralph Crane, Monica Berti, Annette Ge\u00dfner, Matthew Munson, and Tabea Selle. 2014. Open greek and latin project. http://www.dh.uni-leipzig.de/wo/projects/open-greek-and-latin-project/. Ac-cessed on 2019-04-24.\nSection Titles (if exists): [\\'3 Generating PHI-ML\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: For each component related to RoBERTa and SRoBERTa, we solely follow the two libraries, transformers and sentence-transformers, for the sake of easy reproduction of our experiments. [URL_CITE] The example code to train the NLI-style models is also available. We use the roberta-base configuration for all the RoBERTa/SRoBERTa-based models in our experiments.\nFootnote or Reference text (if exists): 10 https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://acube.di.unipi.it/tmn-dataset/\nCitation Context: (2012). [URL_CITE] This dataset con-tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an-notated with one from seven labels, e.g., sci-tech.\nFootnote or Reference text (if exists): 5 http://acube.di.unipi.it/tmn-dataset/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.texar.io.1\nCitation Context: The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [URL_CITE] https: //www.texar.io. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cogsci.princeton.edu/~wn\nCitation Context: WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998) [URL_CITE] . In Word-Net, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, ad-jectives and adverbs).\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].\nSection Titles (if exists):  ['2 Background']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.commonsensemedia.org\nCitation Context: To evaluate the performance of our model, and directly compare it to previous work, we manu-ally align a subset of 989 movie scripts from our dataset to the content ratings found in (Martinez et al., 2019). These ratings come from Common Sense Media (CSM), a non-profit organization that promotes safe technology and media for children [URL_CITE] . CSM experts rate movies from 0 (lowest) to 5 (high-est) with each rating manually checked by the ex-ecutive editor to ensure consistency across raters.\nFootnote or Reference text (if exists): 6 http://www.commonsensemedia.org\nSection Titles (if exists):  ['4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennlp.apache.org\nCitation Context: POS tagging was carried out using the OpenNLP POS tagger. [URL_CITE] The vandalism word list contains a hand-crafted set of around 100 vandalism and spam words from various places in the web.\nFootnote or Reference text (if exists): 5 Maxent model for English, http://opennlp.apache.org\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lexvo.org/uwn/\nCitation Context: the number of senses existing as entries in a WordNet for a given target word t i or a source word s i . We employ the Universal WordNet, [URL_CITE] which provides access to WordNets of various languages. Pseudo-reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by an-other MT system.\nFootnote or Reference text (if exists): 5 http://www.lexvo.org/uwn/\nSection Titles (if exists):  ['3 Features', '3.1 Word level']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gutenberg.org/\nCitation Context: For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg [URL_CITE] as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations.\nFootnote or Reference text (if exists): 7 http://gutenberg.org/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clg.ox.ac.uk/tedcorpus\nCitation Context: To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [URL_CITE] http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [URL_CITE] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data.\nFootnote or Reference text (if exists): 14 https://spacy.io/\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmdrr/testSignificanceNLP.git\nCitation Context: We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [URL_CITE] .\nFootnote or Reference text (if exists): 1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Websail-NU/CODAH\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jflanigan/jamr/tree/Generator\nCitation Context: The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen [URL_CITE] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).\nFootnote or Reference text (if exists): 5 https://github.com/jflanigan/jamr/tree/Generator\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/emorynlp/nlp4j\nCitation Context: To break down the episode-level summaries into scene-level, they are segmented into sentences by the tokenizer in NLP4J. [URL_CITE] Each sentence in the plot summaries is then queried to Elasticsearch that has indexed the selected scenes, and the scene with the highest relevance is retrieved. Finally, the retrieved scene along with the queried sentence are sent to a crowd worker who is asked to determine whether or not they are relevant, and perform anaphora resolution to replace all pronouns in the sentence with the corresponding character names.\nFootnote or Reference text (if exists): 2 https://github.com/emorynlp/nlp4j\nSection Titles (if exists):  ['3 Corpus', '3.1 Passage Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007) [URL_CITE] , SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!)). Such predicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over all c\u2019s) and are prevalent in NLP applications.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['5 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://barbar.cs.lth.se:8081\nCitation Context: For this reason, the output of SRL systems has been com-monly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Ad-ditionally, a considerable amount of training data is available for training SRL models (Table [URL_CITE] in Sec. 3), which made neural SRL models success-ful (Zhou and Xu, 2015; Yang and Mitchell, 2017).\nFootnote or Reference text (if exists): 2 http://barbar.cs.lth.se:8081\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www-i6.informatik.rwth-aachen.de/goldAlignment/\nCitation Context: The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments [URL_CITE] pro-vided by Vilar et al. (2006).\nFootnote or Reference text (if exists): 4 https://www-i6.informatik.rwth-aachen.de/goldAlignment/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://datasets.maluuba.com/Frames\nCitation Context: At the end of each dialogue, the agent receives a positive reward of 2\u21e4max turn (max turn = 60 in our experiments) for success, or a negative re-ward of max turn for failure. Furthermore, at each turn, the agent receives a reward of [URL_CITE] so that shorter dialogue sessions are encouraged.\nFootnote or Reference text (if exists): 1 https://datasets.maluuba.com/Frames\nSection Titles (if exists):  ['4 Experiments and Results', '4.3 User Simulator']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.idpf.org/epub\nCitation Context: We use the EPUB format [URL_CITE] to create what we call a \u201cRead Aloud eBook\" used by MTTR. To link the text in the book to the synchronized audio, we use SMIL (Synchronized Multimedia Integration Lan-guage), as defined in the EPUB Media Overlays specification.\nFootnote or Reference text (if exists): 5 http://www.idpf.org/epub\nSection Titles (if exists): [\\'3 My Turn To Read App\\', \\'3.1 Read Aloud eBooks\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/WSDTutorial.html\nCitation Context: Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [URL_CITE] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.\nFootnote or Reference text (if exists): Ted Pedersen and Rada. F. Mihalcea, \u201cAdvances in Word Sense Disambiguation,\u201d tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.\nSection Titles (if exists):  ['2 The proposal']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: As a result, 65 French documents were ex-tracted (about 257,000 words). The English part has been extracted from the medical website PubMed [URL_CITE] using the keywords: dia-betes, nutrition and feeding. We only kept the free fulltext available documents.\nFootnote or Reference text (if exists): 3 http://www.ncbi.nlm.nih.gov/pubmed/\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [URL_CITE] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cqa.iyas.qcri.org/cQA-Arabic-Demo\nCitation Context: A second example is a cQA demo [URL_CITE] in Arabic, which retrieves data from multiple medical forums from middle-east. In this case physicians answer to patients\u2019 questions: the left panel shows a ques-tion from a user and the right panel similar ques-tions with the answers from the expert. In general there is only one (good) answer from the doctor, so this is mostly a question re-ranking task.\nFootnote or Reference text (if exists): 4 http://cqa.iyas.qcri.org/ cQA-Arabic-Demo\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/coder352/HAHSum\nCitation Context: The contributions of this paper are as below: 1) We propose a hierarchical attentive heteroge-neous graph based model(HAHSum) to guide the redundancy information propagating between sen-tences and learn redundancy-aware sentence rep-resentation; 2) Our architecture is able to extract flexible quantity of sentences with a threshold, in-stead of top-k strategy; 3) We evaluate HAHSum on three popular benchmarks (CNN/DM, NYT, NEWSROOM) and experimental results show that HAHSum outperforms the existing state-of-the-art approaches. Our source code will be available on Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://github.com/coder352/HAHSum\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PyTorchLightning/pytorch-lightning,3\nCitation Context: Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019) [URL_CITE] and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP.\nFootnote or Reference text (if exists): et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,3.\nSection Titles (if exists):  ['C Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/2623330.2623709\nCitation Context: (Amodeo et al., 2011) proposed a hybrid model consisting of time-series analysis, to predict future events using the New York Times corpus. FBLG (Cheng et al., 2014) [URL_CITE] focused on discovering temporal dependency from time series data and applied it to a Twitter dataset mention-ing the Haiti earthquake. Similar work by Luo et al.\nFootnote or Reference text (if exists): Dehua Cheng, Mohammad Taha Bahadori, and Yan Liu. 2014. FBLG: A Simple and Effective Ap-proach for Temporal Dependence Discovery from Time Series Data (KDD \u201914). 382\u2013391. https: //doi.org/10.1145/2623330.2623709\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/BLINK\nCitation Context: We don\u2019t evaluate multi-hop specific baselines on KILT but the current best F1-score for HotpotQA is 75.43 according to the official lead-earboard , that is quite far from what achieved by our general solutions. BLINK results are in line with what reported in the GitHub repository [URL_CITE] for all three entity linking datasets. The Tranformer MemNet of Dinan et al.\nFootnote or Reference text (if exists): 20 https://github.com/facebookresearch/ BLINK\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bing.com/translator/api/Dictionary/Lookup?from=en&to=zh-CHS&text=\nCitation Context: Bilingual Lexicon Our bilingual lexicon is collected from Microsoft Translator [URL_CITE] , which trans-lates English words to multiple Chinese words with confidence scores. Note that all named en-tities and slang terms used in the following exper-iments are excluded from this bilingual lexicon.\nFootnote or Reference text (if exists): 6 http://www.bing.com/translator/api/Dictionary/ Lookup?from=en&to=zh-CHS&text=\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cia.gov/cia/publications/factbook\nCitation Context: We used BBN\u2019s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [URL_CITE] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset.\nFootnote or Reference text (if exists): 1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.\nSection Titles (if exists):  ['2 References to people', '2.3 Automatic semantic tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: The three-minute video demonstrating the interac-tion capabilities of the system can be viewed at [URL_CITE] https://youtu.be/-hGd2399dnA. (Same video as linked in the paper footnote).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors. The library is re-leased under the Apache 2.0 license and is available on GitHub [URL_CITE] . Detailed documentation and tutorials are available on Hugging Face\u2019s website .\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebp/scikit-survival\nCitation Context: Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models\u2019 performances dealing with different time-to-events [URL_CITE] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 \u00b1 0.025 0.504 \u00b1 0.032 0.503 \u00b1 0.037 CPH + Feature Set 2 0.621 \u00b1 0.014 0.632 \u00b1 0.033 0.642 \u00b1 0.030 CPH + Hidden Features 0.674 \u00b1 0.023 0.696 \u00b1 0.022 0.710 \u00b1 0.022 MLP + Feature Set 1 0.502 \u00b1 0.023 0.509 \u00b1 0.026 0.509 \u00b1 0.030 MLP + Feature Set 2 0.658 \u00b1 0.010 0.671 \u00b1 0.025 0.685 \u00b1 0.023 MLP + Hidden Features 0.704 \u00b1 0.017 0.726 \u00b1 0.020 0.744 \u00b1 0.019 LSTM + Sequential HF 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 0.501 \u00b1 0.028 0.642 \u00b1 0.030 0.697 \u00b1 0.026 0.501 \u00b1 0.032 0.683 \u00b1 0.008 0.734 \u00b1 0.018 0.742 \u00b1 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 \u00b1 0.115 0.611 \u00b1 0.123 0.618 \u00b1 0.134 BioBert (Lee et al., 2020) 0.701 \u00b1 0.021 0.714 \u00b1 0.027 0.734 \u00b1 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 \u00b1 0.019 0.705 \u00b1 0.023 0.723 \u00b1 0.025 BlueBert (Peng et al., 2019) 0.713 \u00b1 0.019 0.735 \u00b1 0.024 0.755 \u00b1 0.024 CheXbert (Smit et al., 2020) 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 AUC@365 0.620 \u00b1 0.136 0.739 \u00b1 0.028 0.727 \u00b1 0.029 0.756 \u00b1 0.021 0.742 \u00b1 0.023\nFootnote or Reference text (if exists): 1 https://github.com/sebp/ scikit-survival\nSection Titles (if exists):  ['4 Experiments', '4.2 Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: In this work, we only focus on the masked token prediction. We use the PyTorch implementation of B ERT [URL_CITE] and the pre-trained weights for B ERT -large released by Devlin et al. (2018).\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Model', '4.1 B ERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Languageisolate\nCitation Context: This method also has drawbacks. First, the tax-onomy built on language family does not cover all the languages in the world since some languages are isolate [URL_CITE] . Second, there are many language families (152 according to Paul et al.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/wiki/Language isolate\nSection Titles (if exists):  ['3 Multilingual NMT with Language Clustering', '3.3 Discussions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 4 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.unt.edu/~rada/wa\nCitation Context: The earliest is the Blinker Project (Melamed, 1998); more re-cent systems have been released to support more lan-guages and visualization features (Ahrenberg et al., 2003; Lambert and Castell, 2004). [URL_CITE] Our interface does share some similarities with these systems, but it is designed with additional features to support our experimental goals of guideline development, active learning and resource projection. Following the ex-perimental design proposed by Och and Ney (2000), we instruct the annotators to indicate their level of confidence by choosing sure or unsure for each align-ment they made.\nFootnote or Reference text (if exists): 2 Rada Mihalcea maintains an alignment resource repository (http://www.cs.unt.edu/~rada/wa) that contains other downloadable interface packages that do not have companion papers.\nSection Titles (if exists):  ['2 Annotation Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Babylonpartners/corrsim\nCitation Context: Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pear-son correlation for STS13 reported in our exper-iments has been re-calculated accordingly. The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Babylonpartners/ corrsim\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://biotext.berkeley.edu/lql/\nCitation Context: Below we illustrate our Layered Query Language (LQL) using examples from bio-science NLP. [URL_CITE]\nFootnote or Reference text (if exists): 5 See http://biotext.berkeley.edu/lql/ for a formal description of the language and additional examples.\nSection Titles (if exists):  ['3 The Layered Query Language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://hadi763.wordpress.com/\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry [URL_CITE] , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service .\nFootnote or Reference text (if exists): 6 https://hadi763.wordpress.com/\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doccano/doccano\nCitation Context: Another annotation tool with limited sup-port for document-level annotations is Doccano [URL_CITE] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.\nFootnote or Reference text (if exists): 5 https://github.com/doccano/doccano\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/tweets\nCitation Context: This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. [URL_CITE] Follow-ing previous settings (Yan et al., 2013; Li et al., 2016a), hashtags, i.e., user-annotated topic la-bels in each tweet such as \u201c#Trump\u201d and \u201c#Su-perBowl\u201d, serve as our ground-truth class labels.\nFootnote or Reference text (if exists): 6 http://trec.nist.gov/data/tweets\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html\nCitation Context: Conversational interfaces are a prominent fea-ture of many consumer technology products to-day. Popular voice assistants, such as Alexa, Siri, Google Assistant, and Cortana, have been built us-ing a methodical approach of domain-intent-entity classification and dialogue management that is be-coming an industry standard (Dialogflow, 2018; Wit.ai, 2018; Amazon, 2018 [URL_CITE] ; Microsoft, 2018).\nFootnote or Reference text (if exists): Amazon. 2018. Create the interaction model for your skill. https://developer.amazon.com/docs/custom-skills/create-the-interaction-model-for-your-skill.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cosyne.h-its.org/vex\nCitation Context: VEX is available as free, open-source soft-ware for download at http://github.com/noutenki/vex and as a web service at [URL_CITE] http://cosyne.h-its.org/vex.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf\nCitation Context: (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of B LEU developed by NIST (2002) [URL_CITE] has been used in two recent large-scale machine translation evalua-tions.\nFootnote or Reference text (if exists): NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics. AAAAAAAAAAA http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nCitation Context: Relevance (R) We use the Lucene implementa-tion [URL_CITE] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.\nFootnote or Reference text (if exists): 4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nSection Titles (if exists):  ['3 Approach', '3.1 Ranking of Candidate Justification Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/mlm-scoring\nCitation Context: (2020) for computing pseudo-likelihood. [URL_CITE] PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects.\nFootnote or Reference text (if exists): 8 https://github.com/awslabs/ mlm-scoring\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mjpost/sacrebleu\nCitation Context: PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [URL_CITE] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b).\nFootnote or Reference text (if exists): 3 https://github.com/mjpost/sacrebleu\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: There are other important roles in Were-wolf, such as seer, vigilante, etc, the goals and avail-able actions of which are not the focus of this paper. (Barnwell, 2012) [URL_CITE] provides a broader description of the game and roles.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paraphrase.org/#/download\nCitation Context: Each paraphrase rule in the database has an automatically-assigned quality score between 1 and 5 (Pavlick et al., 2015). In this work, we use the PPDB-TLDR [URL_CITE] dataset, which contains 14 mil-lion high-scoring lexical and phrasal paraphrases, and is intended to give a generally good tradeoff between precision and recall. To preprocess the data, we lemmatize all of the phrases, and remove rules which differ only by morphology, punctu-ation, or stop words, or which involve phrases longer than 3 words.\nFootnote or Reference text (if exists): 2 http://paraphrase.org/#/download\nSection Titles (if exists):  ['2 Identifying Simplification Rules', '2.1 Paraphrase Rules']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/newspaper3k/\nCitation Context: 2. We download these web pages, remove excess markup and detect publication dates with the newspaper3k framework [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://pypi.org/project/newspaper3k/(cf. Pustejovsky et al. (2003)) and mentions using\nSection Titles (if exists):  ['from Common Crawl 2 and apply it to create the']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lil.nlp.cornell.edu/nlvr/\nCitation Context: This paper includes four main contributions: (1) a procedure for collecting visually rich im-ages paired with semantically-diverse language descriptions; (2) NLVR2, which contains 107,292 examples of captions and image pairs, includ-ing 29,680 unique sentences and 127,502 im-ages; (3) a qualitative linguistically-driven data analysis showing that our process achieves a broader representation of linguistic phenomena compared to other resources; and (4) an evalu-ation with several baselines and state-of-the-art visual reasoning methods on NLVR2. The rel-atively low performance we observe shows that NLVR2 presents a significant challenge, even for methods that perform well on existing vi-sual reasoning tasks. NLVR2 is available at [URL_CITE] http://lil.nlp.cornell.edu/nlvr/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sjmielke.com/papers/syncretism\nCitation Context: We empirically evaluated our method on 5 languages under two evaluation metrics. The code is availabile at [URL_CITE] https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Code, DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.edikt.org.uk\nCitation Context: We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, Andre\u0301 Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative ( [URL_CITE] http://www.edikt.org.uk).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sr-research.com/products/eyelink-portable-duo/\nCitation Context: Start-ups, like Cogisen , have started using gaze features in their applications, such as using gaze information to improve input to image processing systems. Recently, SR Research has come up with a portable eye-tracking system [URL_CITE] .\nFootnote or Reference text (if exists): 5 https://www.sr-research.com/products/eyelink-portable-duo/\nSection Titles (if exists):  ['2 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws\nCitation Context: We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS and Stan-ford Chinese word segmenter [URL_CITE] , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/projects/chinese-nlp.shtml\\\\#cws\nSection Titles (if exists): ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2009/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nCitation Context: Specifically, For each test or target domain, we only use 10% of the training examples to sim-ulate data scarcity. In the following experiments, we report the slot F-measure, using the standard CoNLL evaluation script [URL_CITE]\nFootnote or Reference text (if exists): 3 http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nSection Titles (if exists):  ['6 Experiments', '6.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser\nCitation Context: Using the dimen-sions discussed in the next section, we required between 8 and 12 hours to reach convergence on a held-out dev set. [URL_CITE]\nFootnote or Reference text (if exists): 6 Software for replicating the experiments is available from https://github.com/clab/lstm-parser.\nSection Titles (if exists):  ['4 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sec.gov/edgar.shtml\nCitation Context: These reports are available to the public and pub-lished on the SEC\u2019s web site. [URL_CITE] The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the period 1996\u20132006 from 10,492 different companies.\nFootnote or Reference text (if exists): 3 http://www.sec.gov/edgar.shtml\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/wordprobs/\nCitation Context: This paper studies the task of learning to automatically solve such problems given only the natural language. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data for this work are available at http://groups.csail.mit.edu/rbg/code/wordprobs/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://tac.nist.gov/2019/SM-KBP/\nCitation Context: 5.2.2) and report the results in Table 6. [URL_CITE] We observe dramatic drops in performance when re- Han et al., 2018; Mesquita et al., 2019) or dis-moving either intra- or cross-document supervision. tant supervision (Riedel et al., 2010; Zhang et al., This shows that cross-doc RE requires deep text 2017; Elsahar et al., 2018).\nFootnote or Reference text (if exists): 5 The results do not include the pipeline model, since both 6 https://tac.nist.gov/2019/SM-KBP/ supervisions are necessary for the model to infer the relation. index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/SearchQA\nCitation Context: SearchQA [URL_CITE] (Dunn et al., 2017) is a large-scale open domain question answering dataset, which consists of question-answer pairs crawled from J! Archive, and the paragraphs are obtained by retrieving 50 webpages for each question from Google Search API.\nFootnote or Reference text (if exists): 2 https://github.com/nyu-dl/SearchQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huashiyiqike/TMBP/tree/master/DocNADE\nCitation Context: \u2022 DocNADE (Larochelle and Lauly, 2012b). An unsupervised neural network topic model of documents and has shown that it is a com-petitive model both as a generative model and as a document representation learning algo-rithm [URL_CITE] . In DocNADE, the hidden size is 50, the learning rate is 0.0004 , the bath size is 64 and the max training number is 50000.\nFootnote or Reference text (if exists): 6 https://github.com/huashiyiqike/TMBP/tree/master/DocN ADE\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stackexchange.com/sites\nCitation Context: MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [URL_CITE] to 10 news articles share a single summary. We consider every pair of articles that share a summary to be a similar document pair. Table 2 shows summary statistics of the two document ranking datasets.\nFootnote or Reference text (if exists): 2 https://stackexchange.com/sites\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mecab.sourceforge.net/\nCitation Context: Features for CRFs and SVMs The features used in the classification with CRFs and SVMs are shown in Table 2. Japanese morphological analyzer MeCab [URL_CITE] is used to obtain morphemes.\nFootnote or Reference text (if exists): 3 http://mecab.sourceforge.net/\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [URL_CITE] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia.\nFootnote or Reference text (if exists): 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)', '3.1 Inducing Crosslingually Constrained Word Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pushshift.io/\nCitation Context: Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io [URL_CITE] to mine comments. Through this process we collect 87,843 from Reddit.\nFootnote or Reference text (if exists): 8 https://pushshift.io/\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.aist-nara.ac.jp/\nCitation Context: In addition, we decided to use the output of ChaSen 2.2.9 [URL_CITE] and CaboCha 0.34 instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose. Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha\u2019s depen-dency output is very similar to that of the Corpus.\nFootnote or Reference text (if exists): 3 http://chasen.aist-nara.ac.jp/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/futurulus/colors-in-context\nCitation Context: Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public with our code and trained model parameters. [URL_CITE] setting up the data collection platform, and mem-bers of the Stanford NLP group\u2014particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon\u2014for valuable feedback on earlier drafts. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant Nos.\nFootnote or Reference text (if exists): 8 https://github.com/futurulus/ colors-in-context\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trainomatic.org\nCitation Context: We believe that the ability of T-O-M to over-come the current paucity of annotated data for WSD, coupled with video games with a pur-pose for validation purposes (Jurgens and Nav-igli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at [URL_CITE] http://trainomatic.org.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/TomKenter/siamese-cbow\nCitation Context: We use Theano (Theano Development Team, 2016) to implement our network. [URL_CITE] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016).\nFootnote or Reference text (if exists): 4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.\nSection Titles (if exists):  ['3 Experimental Setup', '3.4 Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.rulequest.com/see5-info.html\nCitation Context: To evaluate our model, we have conducted ex-periments with other frequently used machine learning models, on the same dataset, using the same features. Table 6 shows a comparison between the results obtained with the Semantic Scattering algorithm and the decision trees (C5.0, [URL_CITE] http://www.rulequest.com/see5-info.html), the naive Bayes model (jBNC, Bayesian Network Classifier Toolbox, http://jbnc.sourceforge.net), and Support Vector Machine (libSVM, Chih-Chung Chang and Chih-Jen Lin. 2004.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com/\nCitation Context: To leverage this align-ment flexibility, we apply MASSAlign to English articles and Spanish articles machine translated into English by Google translate. [URL_CITE] An important property of Google translated articles is that they are aligned 1-1 at the sentence level. This lets us deterministically find the Spanish replacement for the aligned Google translated English version re-turned by MASSAlign.\nFootnote or Reference text (if exists): 2 https://translate.google.com/\nSection Titles (if exists):  ['4 The Newsela Cross-Lingual Simplification Dataset', '4.1 Cross-Lingual Segment Alignment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rwth-i6/sisyphus\nCitation Context: The large collection of tools for Python can be used with-out modification for editing, debugging, and docu-menting the workflow, since it is written in Python. It is freely available online [URL_CITE] under the Mozilla License v2.0 to encourage the adoption by other groups.\nFootnote or Reference text (if exists): 4 https://github.com/rwth-i6/sisyphus\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://worrydream.com/ExplorableExplanations/\nCitation Context: Ecco provides tools and interactive explorable explanations [URL_CITE] aiding the examination and intuition of:\nFootnote or Reference text (if exists): 3 http://worrydream.com/ExplorableExplanations/ Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fenchri/dsre-vae\nCitation Context: Experimental results on two datasets created via distant supervision indi-cate that multi-task learning results in perfor-mance benefits. Additional exploration of em-ploying Knowledge Base priors into the VAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results [URL_CITE] .\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/fenchri/dsre-vae\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website, Tool)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/newspaper\nCitation Context: We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [URL_CITE] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/newspaper\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/WadeYin9712/SentiBERT\nCitation Context: Furthermore, we conduct com-prehensive quantitative and qualitative analyses to evaluate the effectiveness of SentiBERT under various situations and to demonstrate the seman-tic compositionality captured by the model. The source code is available at [URL_CITE] https://github.com/WadeYin9712/SentiBERT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/yHWeJp\nCitation Context: In practice, exact values of percent-upvoted are not directly available; the site adds \u201cvote fuzzing\u201d to fight vote manipulation. [URL_CITE] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.\nFootnote or Reference text (if exists): 6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can \u201cactually be trusted;\u201d https://goo.gl/yHWeJp\nSection Titles (if exists):  ['2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Document, DataSource)\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/KodairaTomonori/EvaluationDataset\nCitation Context: \u2022 The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/KodairaTomonori/EvaluationDataset\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/webis-de/acl20-editorials-style-persuasive-effect\nCitation Context: (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [URL_CITE]\nFootnote or Reference text (if exists): 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/asap-aes/\nCitation Context: At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neu-ral AES approaches with respect to adversarial in-put related to local aspects of coherence. For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [URL_CITE] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4).\nFootnote or Reference text (if exists): 3 https://www.kaggle.com/c/asap-aes/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: VecOff. We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [URL_CITE] and represented word pairs as described in Section 2.1.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Evaluation', '4.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://inklab.usc.edu/XCSR/\nCitation Context: To improve the performance be-yond English, we propose a simple yet effec-tive method \u2014 multilingual contrastive pre-training (MCP). It significantly enhances sen-tence representations, yielding a large perfor-mance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R L ) [URL_CITE] .\nFootnote or Reference text (if exists): 1 We release our code and data at the project website: https://inklab.usc.edu/XCSR/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/lexicons/subjlexicon/\nCitation Context: In this paper, both Chinese and English [URL_CITE] sen-timental lexicons are employed to identify can-didate opinion expressions by searching the oc-currences of negative and positive expressions in text, and predict the polarity of both Chinese and English texts through the word-counting approach (Turney, 2002).\nFootnote or Reference text (if exists): 6 English sentiment lexicon is uti-lized from MPQA Subjectivity Lexicon: http://mpqa.cs.pitt.edu/lexicons/subjlexicon/\nSection Titles (if exists):  ['4 Emotion Detection via Bilingual and Sentiment Information', '4.2 Sentimental Information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation\nCitation Context: Full results are available on the WAT \u201916 Website. [URL_CITE]\nFootnote or Reference text (if exists): 10 http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax/blob/master/flax/nn\nCitation Context: Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation [URL_CITE] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison.\nFootnote or Reference text (if exists): 6 https://github.com/google/flax/blob/master/flax/nn\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract [URL_CITE] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus.\nFootnote or Reference text (if exists): 1 https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://plato.stanford.edu/archives/fall2009/entries/davidson/\nCitation Context: Later on, a different theory was proposed by Quine (1985) who considered that each event refers to a physical object (which is well defined in space and time), and therefore, two events are identical if they have the same spatiotemporal location. In (Davidson, 1985), Davidson abandoned his sug-gestion to embrace the Quinean theory on event identity (Malpas, 2009) [URL_CITE] .\nFootnote or Reference text (if exists): Jeff Malpas. 2009. Donald Davidson. In The Stanford Encyclopedia of Philosophy (Fall 2009 Edition), Edward N. Zalta (ed.), http://plato.stanford.edu/archives/fall2009/entries/davidson/.\nSection Titles (if exists):  ['2 Event Coreference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alpage.inria.fr/statgram/frdep/fr\nCitation Context: We first tested a standalone MWE recognizer based on CRF. We then combined MWE pregrouping based on this recognizer and the Berkeley parser [URL_CITE] (Petrov et al., 2006) trained on the FTB where the com-pounds were concatenated (BKYc). Finally, we combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker.\nFootnote or Reference text (if exists): 5 We used the version adapted to French in the software Bonsai (Candito and Crabbe\u0301, 2009): http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html. The original version is available at: http://code.google.com/p/berkeleyparser/. We trained the parser as follows: right binarization, no parent annotation, six split-merge cycles and default random seed initialisation (8).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf\nCitation Context: However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009 [URL_CITE] ; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains.\nFootnote or Reference text (if exists): Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh. 2009. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems 21, Curran Associates, Inc., pages 1041\u20131048. http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [URL_CITE] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 7 https://www.reddit.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ucrel.lancs.ac.uk/usas/\nCitation Context: In this paper, we have investigated the feasibility of rapidly bootstrapping semantic annotation tools for new target languages [URL_CITE] by mapping an existing semantic lexicon and software architecture. In par-ticular, we tested the possibility of automatically translating existing English semantic lexicons into other languages, Italian, Chinese and Brazilian Portuguese in this particular case.\nFootnote or Reference text (if exists): 2 The results are available at http://ucrel.lancs.ac.uk/usas/\nSection Titles (if exists):  ['6 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: The PPCMBE is a phrase-structure corpus, and so we parse with the Berkeley parser (Petrov et al., 2008) and score using the standard evalb program (Sekine and Collins, 2008) [URL_CITE] . We used the Train and Val sections for training, with the parser using the Val section for fine-tuning parameters (Petrov et al., 2006).\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['5 Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [URL_CITE] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.\nFootnote or Reference text (if exists): 6 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/\nCitation Context: We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The dataset and word vectors can be downloaded at http://ai.stanford.edu/ \u223c ehhuang/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Website and DataSource)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trainomatic.org\nCitation Context: Train-O-Matic achieves consis-tently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the bur-den of manual annotation. All the training data is available for research purposes at [URL_CITE] http://trainomatic.org.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/\nCitation Context: Before describing our methods, we describe text representation which we used in this study. We to-kenize a document to words, downcase them and then remove stopwords, where the stopword list of freeWAIS-sf [URL_CITE] is used. Stemming is not performed.\nFootnote or Reference text (if exists): 2 Available at http://ls6-www.informatik.uni-dortmund.de/ir/projects/freeWAIS-sf/\nSection Titles (if exists):  ['4 Virtual Examples for Text Classification', 'We assume on text classification the following:']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/qa.html\nCitation Context: We are now studying open-domain question an-swering systems [URL_CITE] , and we expect QA systems to benefit from anaphora resolution. Typical QA sys-tems try to answer a user\u2019s question by finding rel-evant phrases from large corpora.\nFootnote or Reference text (if exists): 1 http://trec.nist.gov/data/qa.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov\nCitation Context: \u201cmetabolic diseases(C18.452)\u201d node is root of the subtree, and the subtree consists of 436 disease names which are target terms of specificity measuring. A set of journal abstracts was extracted from MEDLINE [URL_CITE] database using the disease names as quires. Therefore, all the abstracts are related to some of the disease names.\nFootnote or Reference text (if exists): 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine, USA. (http://www.nlm.nih.gov)\nSection Titles (if exists):  ['4. Experiment and Evaluation', '4.1. Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: The Amazon dataset contains 2000 samples for each of the four domains: book, DVD, electron-ics, and kitchen, with binary labels (positive, neg-ative). Following Wu and Huang (2015) [URL_CITE] , we con-duct 5-way cross validation. Three out of the five folds are treated as the training set, one serves as the validation set, while the remaining is the test set.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multi-Domain Text Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: Figure 1 shows our biaffine parsing ar-chitecture. Following Dozat and Manning (2017) [URL_CITE] and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence. We add highway connections in the same fashion as our supertagging model.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Sections 0 and 1 of the PDTB were used for de-velopment of the features described in the previous section. Here we report results using a maximum entropy classifier [URL_CITE] using ten-fold cross-validation over sections 2-22.\nFootnote or Reference text (if exists): 2 http://mallet.cs.umass.edu\nSection Titles (if exists):  ['3 Discourse vs. non-discourse usage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/iesl/diora\nCitation Context: In our experiments, we use the default settings for CCL. DIORA In addition to CCL, we also experiment with the more recent neural unsupervised parser DIORA [URL_CITE] . As the name suggests, DIORA is built on the application of recursive auto-encoders.\nFootnote or Reference text (if exists): 3 https://github.com/iesl/diora\nSection Titles (if exists):  ['3 Method', '3.3.1 Constituency structure induction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flask.pocoo.org\nCitation Context: This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [URL_CITE] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters.\nFootnote or Reference text (if exists): 4 http://flask.pocoo.org\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/electra\nCitation Context: Experiment settings The ELECTRA-small model we implemented follow all official settings [URL_CITE] except that it is fully-trained on one GTX 1080Ti GPU for 6 days. The ELECTRA-small model has 12 layers with 4 heads in every layer\u2019s attention.\nFootnote or Reference text (if exists): 11 https://github.com/google-research/electra\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [URL_CITE] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word.\nFootnote or Reference text (if exists): 7 We use Jieba for segmentation: https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['3 Embeddings for Chinese Text']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.utexas.edu/ml/nldata/jobquery.html\nCitation Context: We directly use Prolog logical forms provided on the web-site. [URL_CITE] To provide annotations for FunQL, we semi-copying a source word to a logical form. In ATIS, following (Jia and Liang, 2016), we leverage an ex-ternal lexicon to identify potential copy candidates, e.g., slc:ap can be identified as a potential entity for description \u201csalt lake city airport\u201d in utterance.\nFootnote or Reference text (if exists): 9 http://www.cs.utexas.edu/ml/nldata/jobquery.html river_name := \"ohio\" | \"colorado\u201d | \u2026 | \"red\" state_abbrev := \"dc\" | \"sd\" | \u2026 | \"me\" number := \"0\" | \"1.0\"\nSection Titles (if exists): [\\'-\\', \\'A.1 Details of Benchmark Construction\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org\nCitation Context: At QCRI, he leads the Tanbih project, [URL_CITE] devel-oped in collaboration with MIT, which aims to limit the effect of \u201cfake news\u201d, propaganda and media bias by making users aware of what they are reading. The project was featured by over 100 news outlets, including Forbes, Boston Globe, Al-jazeera, MIT Technology Review, Science Daily, Popular Science, Fast Company, The Register, WIRED, and Engadget, among others.\nFootnote or Reference text (if exists): 2 Tanbih project: http://tanbih.qcri.org\nSection Titles (if exists):  ['6 Lecturers', '6.1 Preslav Nakov']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-eval\nCitation Context: System outputs from the E2E NLG Challenge participants can be found at [URL_CITE] https://github.com/tuetschek/e2e-eval\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hznlp/giza-kn\nCitation Context: The proposed leave-one-word word align-ment method was empirically compared to l 0 -normalized GIZA++ (Vaswani et al., 2012) and Kneser-Ney smoothed GIZA++ (Zhang and Chiang, 2014) [URL_CITE] . l 0 -normalization and Kneser-Ney smoothing methods are established methods to overcome the sparse problem.\nFootnote or Reference text (if exists): 12 https://github.com/hznlp/giza-kn\nSection Titles (if exists):  ['4 Experiments', '4.5 Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nCitation Context: For our system, we used the Le Zhang\u2019s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [URL_CITE] We set the iteration limit to 100. The accuracy on the training data is 90.18%.\nFootnote or Reference text (if exists): 4 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [URL_CITE] for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 2 https://www.reddit.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Yupei-Du/bias-in-wat\nCitation Context: More importantly, compar-ing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/Yupei-Du/bias-in-wat.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://finance.yahoo.com\nCitation Context: The news dataset we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices [URL_CITE] from 2010 to 2012 for training and 2013 as test data for prediction. The prediction is done with varying step sizes (1,3,5), which indicates the time lag between the news data and the day of the predicted stock price in days.\nFootnote or Reference text (if exists): 2 https://finance.yahoo.com\nSection Titles (if exists):  ['5 Results', '5.1 Data and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JetRunner/beyond-preserved-accuracy\nCitation Context: By com-bining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/JetRunner/beyond-preserved-accuracy.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://conceptnet.io/\nCitation Context: 2. Retrieval (RTRVL): We also experiment with a retrieval approach where we retrieve a VEHICLE from ConceptNet (Speer et al., 2017) having the highest HasProperty rela-tion w.r.t our input (i.e., an adjective or adverb at the end of literal sentence) [URL_CITE] . For the in-put The city was beautiful we query Concept-Net with beautiful, which returns sunset as the VEHICLE having the highest weight for HasProperty beautiful.\nFootnote or Reference text (if exists): 10 ConceptNet is a weighted graph with multiple relations as can be viewed here http://conceptnet.io/. We use \u2018has property\u201d for our work. There are multiple edges for objects with their properties. We choose the edge with the highest weight with the title Sunset and Car Accident. We replace the literal sentences with generated similes from SCOPE.\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Baseline Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=BJJsrmfCZ\nCitation Context: More details of us-ing CNN for text classification can be found in the original paper (Kim, 2014). MAN is implemented using PyTorch (Paszke et al., 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Adam Paszke, Sam Gross, Soumith Chintala, Gre-gory Chanan, Edward Yang, Zachary DeVito, Zem-ing Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in py-torch. NIPS 2017 Autodiff Workshop https://openreview.net/pdf?id=BJJsrmfCZ.\nSection Titles (if exists):  ['5 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPIR-team/NLPIR\nCitation Context: If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [URL_CITE] messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).\nFootnote or Reference text (if exists): 10 https://github.com/NLPIR-team/NLPIR\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/thunlp/OpenKE\nCitation Context: In this section, we mainly present the implemen-tations of acceleration modules and special sam-pling algorithm in OpenKE. OpenKE has been available to the public on GitHub [URL_CITE] and is open-source under the MIT license.\nFootnote or Reference text (if exists): 1 http://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['4 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/ace/2005\nCitation Context: NER systems, how-ever, do not identify nominal NP instances (e.g., \u201ca software manufacturer\u201d or \u201cthe beach\u201d), or han-dle semantic classes that are not associated with proper named entities (e.g., symptoms). ACE mention detection systems (e.g., see (ACE, 2005 [URL_CITE] ; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with super-vised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different lan-guages (e.g., (Zitouni and Florian, 2009)).\nFootnote or Reference text (if exists): ACE. 2005. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2005.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://clic.cimec.unitn.it/composes\nCitation Context: The data set contains the following 14 preposi-tions: after, against, at, before, between, by, for, from, in, on, per, under, with, without. [URL_CITE]\nFootnote or Reference text (if exists): 8 This dataset is available at http://clic.cimec.unitn.it/composes\nSection Titles (if exists):  ['5 Noun phrase generation', '5.2 Recursive decomposition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/webis-de/acl20-editorials-style-persuasive-effect\nCitation Context: (2018) with ideology-specific effect annotations (Section 4), we compare style-oriented with content-oriented clas-sifiers for persuasive effect (Section 5). [URL_CITE]\nFootnote or Reference text (if exists): 1 For reproducibility, the code of our experiments can be found here: https://github.com/webis-de/ acl20-editorials-style-persuasive-effect\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/jax\nCitation Context: We have implemented the proposed hierarchical attention using Jax, an open source library [URL_CITE] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax.\nFootnote or Reference text (if exists): 5 https://github.com/google/jax\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/antonisa/at\nCitation Context: In Following this simple triangulation approach, we cre-the case of feature mismatch (for instance, Greek uses 2 ate 4,704 new dictionaries over pairs between the 50 numbers, 4 cases and 3 genders while Italian has 2 num-languages of the MUSE dictionaries. [URL_CITE] For consistency, bers, 2 genders, and no cases) or if we only find a partial we keep the same train and test splits as with MUSE, so tag match over a feature subset, we filter out transla-that the source-side types are equal across all dictionar-tions with disagreeing tags. We ignore the grammatical ies with the same source language.\nFootnote or Reference text (if exists): 9 Available unable to filter dictionaries in the following 11 languages: aze,https://github.com/antonisa/atembeddings. bel, ben, bos, lit, mkd, msa, sqi, tam, tha, tel.\nSection Titles (if exists):  ['3 New LI Evaluation Dictionaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://oracc.museum.upenn.edu/doc/about/aboutoracc/index.html\nCitation Context: When the Sumerologists transliterate the tablets, they use metacharacters such as \u201c[...]\u201d and \u201c#\u201d to indicate damage to the text, and \u201c!\u201d, \u201c?\u201d, \u201c*\u201d, and \u201c\u201d to represent correction, querying or colla-tion (Tinney and Robson, 2014) [URL_CITE] . For \u201c[...]\u201d and \u201c\u201d cases, the Sumerologists put their \u201cbest guess\u201d within the brackets.\nFootnote or Reference text (if exists): Steve Tinney and Eleanor Robson. 2014. Oracc: The Open Richly Annotated Cuneiform Corpus. http://oracc.museum.upenn.edu/doc/about/aboutoracc/index.html\nSection Titles (if exists):  ['3 Sumerian Personal Name Recognition', '3.2 Our System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [URL_CITE] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger\nCitation Context: Since then, the system, resp. its reimple-mentation by Heilman [URL_CITE] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013).\nFootnote or Reference text (if exists): 2 https://github.com/kutschkem/SmithHeilmann_fork/tree/master/ MIRATagger\nSection Titles (if exists):  ['2 Related Work', '2.2 Supersense Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Tool, Paper)\nFUNCTION: Use, Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/uhh-lt/Taxonomy_Refinement_Embeddings\nCitation Context: We compare two types of dense vector em-beddings: the standard word2vec CBOW model (Mikolov et al., 2013a,b), that embeds terms in Euclidean space based on distributional similarity, and the more recent Poincare\u0301 embeddings (Nickel and Kiela, 2017), which capture similarity as well as hierarchical relationships in a hyperbolic space. The source code has been published [URL_CITE] to recreate the employed embedding, to refine taxonomies as well as to enable further research of Poincare\u0301 em-beddings for other semantic tasks.\nFootnote or Reference text (if exists): 1 https://github.com/uhh-lt/Taxonomy_ Refinement_Embeddings\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chdw.de\nCitation Context: Dictionaries A dictionary can be viewed as a many to many mapping, where each entry e i maps one or more words in one language s i to one or more words t i in another language. Entries were taken from an English-German dictionary (Richter, 2008) a Chinese-English dictionary (Denisowski, 1997), and a Chinese-German dictionary (Hefti, 2005) [URL_CITE] . As with WordNet, the words in entries for English and German were stemmed to improve coverage.\nFootnote or Reference text (if exists): Jan Hefti. 2005. HanDeDict. http://chdw.de.\nSection Titles (if exists):  ['3 Bridges Across Languages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pilehvar.github.io/deconf/\nCitation Context: The tech-nique is built upon earlier contextual representa-tions, including ELMo, but differs in the fact that, unlike those models which are mainly unidirec-tional, BERT is bidirectional, i.e., it considers con-texts on both sides of the target word during repre-sentation. We experimented with two pre-trained BERT models: base (768 dimensions, [URL_CITE] layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models.\nFootnote or Reference text (if exists): 12 https://pilehvar.github.io/deconf/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [URL_CITE] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['6 Experiments', '6.1 System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://babelnet.org\nCitation Context: To cope with these issues, we exploit Babel-Net, [URL_CITE] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word-Net (such as hyperlinks in a Wikipedia article con-necting it to other articles). The greatest contribu-tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar-ticles (e.g., the English Wikipedia Bus article is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).\nFootnote or Reference text (if exists): 2 http://babelnet.org\nSection Titles (if exists):  ['3 Creating a Denser and Multilingual Semantic Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: The Fake News challenge (Pomerleau and Rao, 2017) [URL_CITE] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wiktionary.org\nCitation Context: Regarding MWE annotations, Shigeto et al. (2013) first constructed an MWE dictionary by extract-ing functional MWEs from the English-language Wiktionary [URL_CITE] , and classified their occurrences in Ontonotes into either MWE or literal usage. Kato et al.\nFootnote or Reference text (if exists): 6 https://en.wiktionary.org\nSection Titles (if exists):  ['2 MWE-aware Dependency Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://creativecommons.org/licenses/by-nc-sa/4.0/\nCitation Context: Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [URL_CITE] .\nFootnote or Reference text (if exists): 10 https://creativecommons.org/licenses/by-nc-sa/4.0/\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mediabiasfactcheck.com\nCitation Context: (2019), a large corpus of political headlines from 2018 paired with news source rat-ings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable). [URL_CITE]\nFootnote or Reference text (if exists): 10 We use the MediaBias/FactCheck ratings: https://mediabiasfactcheck.com. Shoumen, BULGARIA. know\u201d \u2013 Follows: \u201cIt\u2019s devastating to be excluded from a wedding you were invited to\u201d\nSection Titles (if exists):  ['6 Morality & Political Bias']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nCitation Context: We perform 5-shot KG completion task for all the methods. Our implementation for KG embedding baselines is based on OpenKE [URL_CITE] (Han et al., 2018) with their best hyperparameters reported in the orig-inal literature. During training, all triples in back-ground KG G 0 and training set, as well as few-shot reference triples of validation and testing set are used to train models.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/OpenKE/tree/OpenKE-PyTorch\nSection Titles (if exists):  ['5 Experiments', '5.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jstatsoft.org/v18/i02/\nCitation Context: The pls package: Principal component and partial least squares regression in R. Journal of Statistical Soft-ware, 18(2). Published online: [URL_CITE] http://www.jstatsoft.org/v18/i02/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: 1. The EVALB tool [URL_CITE] to calculate the labeled bracketing F1-score.\nFootnote or Reference text (if exists): 3 http://nlp.cs.nyu.edu/evalb/\nSection Titles (if exists):  ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/\nCitation Context: To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005) [URL_CITE] .\nFootnote or Reference text (if exists): Amazon. 2005. MTurk. (https://www.mturk.com/).\nSection Titles (if exists):  ['5 User study']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openke.thunlp.org/\nCitation Context: Besides the toolkit, we also provide the pre-trained embeddings of several well-known large-scale KGs, which can be used directly for other relevant works without repeatedly spending much time for embedding KGs. In this paper, we mainly present the architecture design and implementa-tion of OpenKE, as well as the benchmark eval-uation results of some typical KE models im-plemented with OpenKE. Other related resources and details can be found on [URL_CITE] http://openke.thunlp.org/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: The Dela was manually developed in the 90\u2019s by a team of linguists. We used the distribution freely available in the platform Unitex [URL_CITE] (Paumier, 2011). It is composed of 840,813 lexical entries in-cluding 104,350 multiword ones (91,030 multiword nouns).\nFootnote or Reference text (if exists): 3 http://igm.univ-mlv.fr/\u02dcunitex\nSection Titles (if exists):  ['4 Resources', '4.2 Lexical resources']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BorealisAI/de-simple\nCitation Context: For TA-Distmult, the vocabulary of tempo-ral tokens consists of year, month, and day for all the datasets. We use the released code to imple-ment DE-SimplE [URL_CITE] , TNTComplEx , and CyGNet . All the baselines are trained with Adam Optimizer (Kingma and Ba, 2017), and the batch size is set to 512.\nFootnote or Reference text (if exists): 5 https://github.com/BorealisAI/de-simple\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We have created a Rel-grams database with 1.1M entries and a set of over 2K event schemas from a corpus of 1.8M New York Times articles. Both are freely available to the research community [URL_CITE] and may prove useful for a wide range of NLP applications.\nFootnote or Reference text (if exists): 9 available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['7 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://reporterslab.org/fact-checking/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. [URL_CITE] While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 2 http://reporterslab.org/fact-checking/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/PubMed/\nCitation Context: Our biomedi-cal data comes from the GENIA treebank (Tateisi et al., 2005), a corpus of abstracts from the Med-line database. [URL_CITE] We downloaded additional sentences from Medline for our self-trained MEDLINE corpus. Unlike the other two self-trained corpora, we include two versions of MEDLINE .\nFootnote or Reference text (if exists): 9 http://www.ncbi.nlm.nih.gov/PubMed/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paraphrase.org\nCitation Context: For instance, the word foolishness is simpler than its meaning-preserving substitution folly even though foolish-ness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006). In fact, we found that 21% of the 2272 meaning-equivalent word pairs randomly sampled from PPDB [URL_CITE] (Ganitkevitch et al., 2013) had the simpler word longer than the complex word, while 14% had the simpler word less frequent.\nFootnote or Reference text (if exists): 2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http://paraphrase.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mosaickg.apps.allenai.org/comet_conceptnet\nCitation Context: COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models. Our work only leverages the HasProp-erty relation from COMET [URL_CITE] .\nFootnote or Reference text (if exists): 9 https://mosaickg.apps.allenai.org/ comet_conceptnet\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com\nCitation Context: Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [URL_CITE] https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P11-1048\nCitation Context: Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011) [URL_CITE] . Nguyen et al.\nFootnote or Reference text (if exists): Michael Auli and Adam Lopez. 2011. A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing. In ACL. Association for Computational Linguistics, pages 470\u2013480. http://www.aclweb.org/anthology/P11-1048.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/iterative-search-semparse\nCitation Context: Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at [URL_CITE] https://github.com/allenai/iterative-search-semparse.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Experiments', '6.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/\nCitation Context: We compare the above-proposed curricula (CWA, Skip\u2013fact) against a baseline curriculum (Original) where we initialize the verification models with standard pretrained BERT weights (bert-base-cased). We use huggingface transform-ers (Wolf et al., 2019) in all of our experiments. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://huggingface.co/transformers/\nSection Titles (if exists):  ['2 Constrained Verification', '2.3 Methodology', 'Taker datasets following Clark et al. (2020).']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.thestocksonar.com/\nCitation Context: The Stock Sonar [URL_CITE] has a timeline chart as the main interface. In this timeline, both positive and negative sentiments are displayed throughout time.\nFootnote or Reference text (if exists): 3 http://www.thestocksonar.com/\nSection Titles (if exists):  ['2 Analyzing Sentiment in Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://quora.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora [URL_CITE] and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 2 http://quora.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-30586-638\nCitation Context: Dimension of human ratings Study Sentence Planning this paper weak positive (\u03c1 = 0.33, WPS) Surface Realisation Domain weak negative (\u03c1 = 0. \u2212 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson\u2019s r = 0.96, NIST ) NLG, weather forecast (Stent et al., 2005) [URL_CITE] weak positive (\u03c1 = 0.47, LSA ) (Liu et al., 2016) weak positive (\u03c1 = 0.35, BLEU -4 ) (Elliott and Keller, 2014) positive (\u03c1 = 0.53, METEOR ) (Kilickaya et al., 2017) positive (\u03c1 = 0.64, SPICE ) (Cahill, 2009) N/A (Espinosa et al., 2010) weak positive (\u03c1 = 0.43, TER ) negative (\u03c1 = \u22120.56, NIST ) paraphrasing of news N/A dialogue/Twitter pairs N/A image caption N/A image caption negative (\u03c1 = \u22120.64, ROUGE ) NLG, German news texts positive (\u03c1 = 0.62, BLEU -4 ) NLG, news texts\nFootnote or Reference text (if exists): Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341\u2013351. https://doi.org/10.1007/978-3-540-30586-638.\nSection Titles (if exists):  ['7 Relation of Human and Automatic Metrics', '8.3 Example-based Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.acm.org/citation.cfm?id=1557769.1557821\nCitation Context: Associa-tion for Computational Linguistics, Strouds-burg, PA, USA, ACL \u201907, pages 177\u2013 180. [URL_CITE] http://dl.acm.org/citation.cfm?id=1557769.1557821.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.politifact.com\nCitation Context: We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [URL_CITE] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it.\nFootnote or Reference text (if exists): 7 https://www.politifact.com\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.informatik.uni-mannheim.de/AnyBURL/\nCitation Context: 3.2 Rule Mining Since developing advanced rule mining models is not the focus of this paper and several mature tools are available online, such as AMIE+ (Gala\u0301rraga et al., 2015) and AnyBURL (Meilicke et al., 2019). We utilize AnyBURL [URL_CITE] in experiments due to its ticularly, we follow the suggested configuration of AnyBURL. We run it for 500 seconds to ensure that all triples can be traversed at least once and obtain 251,317 rules, where 168,996 out of them whose confidence meets \u03bb p > 0.1 have been selected as the rule set to guide dataset construction.\nFootnote or Reference text (if exists): 2 http://web.informatik.uni-mannheim.de/AnyBURL/\nSection Titles (if exists):  ['3 Dataset Design', '3.1 Data Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk\nCitation Context: \u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [URL_CITE]\nFootnote or Reference text (if exists): 9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nCitation Context: Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [URL_CITE] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq. ), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-\nFootnote or Reference text (if exists): 20 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nSection Titles (if exists):  ['References Christopher Clark, Kenton Lee, Ming-Wei Chang,']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/grmm/\nCitation Context: In this work, we use the G RMM package (Sutton, 2006) [URL_CITE] for building both the linear-chain CRF (L-C RF ) and factorial CRF (F-C RF ). The tree-based reparameterization (TRP) schedule for belief propa-gation (Wainwright et al., 2001) is used for approxi-mate inference.\nFootnote or Reference text (if exists): C. Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.\nSection Titles (if exists):  ['5 Factorial Conditional Random Fields']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://news.google.com/\nCitation Context: Online news platforms such as Google News [URL_CITE] and MSN News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 1 https://news.google.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: Posterior word alignment probabilities were ob-tained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007) [URL_CITE] , trained on approxi-mately 1.7 million sentence pairs. For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the pos-teriors using DeNero and Klein\u2019s (2007) soft union method.\nFootnote or Reference text (if exists): 6 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Makwen1995/LDGNMLTC\nCitation Context: We select the best parameter configuration based on performance on the valida-tion set and evaluate the configuration on the test set. Our code is available on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Makwen1995/LDGN MLTC\nSection Titles (if exists):  ['3 Experiment', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk\nCitation Context: \u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [URL_CITE]\nFootnote or Reference text (if exists): 9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/lsl_acl20\nCitation Context: Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [URL_CITE] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/licensed-sw/RSTTool/index.html\nCitation Context: This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [URL_CITE] that was appended to the relation label.\nFootnote or Reference text (if exists): 2 Downloaded from http://www.isi.edu/licensed-sw/RSTTool/index.html\nSection Titles (if exists):  ['3 Annotation', '3.1 Guidelines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://facebook.github.io/react\nCitation Context: The graphical user interface of our system is im-plemented as a single page Web application using the React framework. [URL_CITE] The application performs disambiguation of a text entered by a user. In par-ticular, the Web application features two modes:\nFootnote or Reference text (if exists): 8 https://facebook.github.io/react\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://tizirinagh.github.io/acl2019/\nCitation Context: This allows LFs to exploit information about previously predicted attachments and dialogue history in new predictions. Our complete rule set, along with de-scriptions of each of the relation types, is available here: [URL_CITE] https://tizirinagh.github.io/acl2019/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data Programming Experiments', '4.1 Candidates and Labeling Functions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thu-coai/OpenMEVA\nCitation Context: We also provide an open-source toolkit which implements various metrics, and therefore supports the comparison and analysis of metrics. In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [URL_CITE] .\nFootnote or Reference text (if exists): 1 All the tools, data, and evaluation scripts are available at https://github.com/thu-coai/OpenMEVA\nSection Titles (if exists):  ['\u2217 Corresponding author']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/successar/FRESH\nCitation Context: In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to \u2018end-to-end\u2019 ap-proaches, while being more general and easier to train. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/successar/FRESH\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/dbikel/software.html\nCitation Context: The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel\u2019s randomized parsing evaluation com-parator [URL_CITE] and found that both the precision and recall gains were significant at p \u2264 0.01.\nFootnote or Reference text (if exists): 6 Available at http://www.cis.upenn.edu/dbikel/software.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml\nCitation Context: Since the constraint type is determined by syntactic and dependency parse features, this component does not need to be retrained for new domains. In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013) [URL_CITE] , and used this model for all experiments.\nFootnote or Reference text (if exists): M. Lichman. 2013. UCI machine learning repository. http://archive.ics.uci.edu/ml.\nSection Titles (if exists):  ['3 Learning Classifiers from Language', '3.1 Mapping language to constraints', '3.1.1 Semantic Parser components']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://seekingalpha.com/\nCitation Context: Our data [URL_CITE] consists of transcripts of 12,285 earn-ings calls held between January 1, 2010 and De-cember 31, 2017. In order to control for analyst coverage effects (larger companies with a greater market share will typically be covered by more an-alysts), we include only calls from S&P 500 com-panies. We split the data by year into training, val-idation and testing sets (see Table 1).\nFootnote or Reference text (if exists): 2 In Appendix A in supplemental material we provide the stock tickers for the calls in our data; the corpus can be re-assembled from multiple sources, such as https: //seekingalpha.com/.\nSection Titles (if exists):  ['3 Data and pre-processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academic.research.microsoft.com\nCitation Context: Our second keyphrase dataset comes from the Mi-crosoft Academic (MSRA) search corpus (Mi-crosoft, 2015) [URL_CITE] . While particularly focused on\nFootnote or Reference text (if exists): Microsoft. 2015. Microsoft academic search. http://academic.research.microsoft.com. Accessed: 2015-2-26.\nSection Titles (if exists):  ['4 Overlap with Keyphrase Corpora', '4.2 Microsoft Academic (MSRAC ORPUS )']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [URL_CITE] for infer-ence.\nFootnote or Reference text (if exists): 3 http://www.gurobi.com/\nSection Titles (if exists):  ['6 Empirical Evaluation', '6.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jdepp/\nCitation Context: Special end-of-chunk tokens were inserted at the end of the chunks. Our word-level decoders described in \u00a7 [URL_CITE] will stop generating words after each end-of-chunk token. For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website.\nFootnote or Reference text (if exists): 3 http://www.tkl.iis.u-tokyo.ac.jp/\u02dcynaga/jdepp/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/mate-tools/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [URL_CITE] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 1 http://code.google.com/p/mate-tools/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/pubs/\nCitation Context: For lack of space, some proofs are only sketched, or entirely omitted when relatively intuitive: Melamed et al. (2004) [URL_CITE] provide more details. where ( 0( represents some grouping into strings of all and only the variables appearing in the left-hand side, possibly with some additional termi-nal symbols.\nFootnote or Reference text (if exists): I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.\nSection Titles (if exists):  ['4 Generative Capacity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ml4nlp.de/code-and-data/treex2conll\nCitation Context: Their annotation in the CoNLL-ST is not identi-cal, but the guidelines for \u201ccore\u201d semantic roles are similar (Kingsbury et al., 2004), so we eval-uate only on core roles here. The data for the second language pair is drawn from the Prague Czech-English Dependency Treebank 2.0 (Hajic\u030c et al., 2012), which we converted to a format simi-lar to that of CoNLL-ST [URL_CITE] . The original annotation uses the tectogrammatical representation (Hajic\u030c, 2002) and an inventory of semantic roles (or func-tors), most of which are interpretable across vari-ous predicates.\nFootnote or Reference text (if exists): 1 see http://www.ml4nlp.de/code-and-data/treex2conll\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [URL_CITE] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Paper, Website)\nFUNCTION: Use, Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. [URL_CITE] Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128.\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com/talks\nCitation Context: The dataset consists of transcripts directly col-lected from [URL_CITE] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don\u2019t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tylin/coco-caption\nCitation Context: We report the paragraph generation (upper part of Table 1) and one sentence generation (lower part of Table 1) results using the standard image cap-tioning evaluation tool [URL_CITE] which provides evalua-tion on the following metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), ROUGE (Lin, 2004), and CIDER (Vedan-tam et al., 2015).\nFootnote or Reference text (if exists): 4 https://github.com/tylin/coco-caption\nSection Titles (if exists):  ['4 Experiments', '4.4 Quantitative Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: For all other hyper-parameters, we use the default values of previous implementations. [URL_CITE] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.\nFootnote or Reference text (if exists): 6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://snips.ai/\nCitation Context: We also compare our proposed model C APSULE -NLU with existing commercial natural language understanding services, includ-ing api.ai (Now called DialogFlow) , Waston Assistant , Luis , wit.ai , snips.ai [URL_CITE] , recast.ai , and Amazon Lex .\nFootnote or Reference text (if exists): 6 https://snips.ai/\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: A CRF models the entire label sequence y as: where F(y, x) is a global feature vector for input sequence x and label sequence y and Z(x) is a nor-malization term. [URL_CITE]\nFootnote or Reference text (if exists): 5 CRF experiments used the CRF++ package http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['4 From MaxEnt to CRFs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://open.weibo.com/wiki\nCitation Context: We evaluate our method using the data from weibo.com, which is the biggest micro-blog ser-vice in China. We use the API provided by weibo.com [URL_CITE] to crawl 500,000 micro-blog texts of weibo.com, which contains 24,243,772 charac-ters. To keep the experiment tractable, we first ran-domly choose 50,000 of all the texts as unlabeled data, which contain 2,420,037 characters.\nFootnote or Reference text (if exists): 1 http://open.weibo.com/wiki\nSection Titles (if exists):  ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.summarization.com\nCitation Context: MEAD [URL_CITE] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods.\nFootnote or Reference text (if exists): 2 http://www.summarization.com\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.2 MEAD summarization toolkit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/snowblink14/smatch\nCitation Context: We evaluate AMR and cross-lingual AMR parsing by using the Smatch score [URL_CITE] (Cai and Knight, 2013), a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems\u2019 performances, we also re-port the fine-grained scores as computed by the evaluation toolkit 14 of Damonte et al.\nFootnote or Reference text (if exists): 13 https://github.com/snowblink14/smatch\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clg.ox.ac.uk/tedcorpus\nCitation Context: To assess the ability of our framework to han-dle multiple target languages, we ran a second set of experiments on the TED corpus (Cettolo et al., 2012), using the training and test parti-tions created by Hermann and Blunsom (2014), downloaded from [URL_CITE] http://www.clg.ox.ac.uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985 parallel sentences).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 TED Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [URL_CITE] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).\nFootnote or Reference text (if exists): 2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).\nSection Titles (if exists):  ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincare\u0301 Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/streaming/\nCitation Context: The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [URL_CITE] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/\nCitation Context: We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [URL_CITE] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings.\nFootnote or Reference text (if exists): 9 https://github.com/google-research/bert/blob/master/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.verify-sy.com\nCitation Context: Claim Extraction We consider two websites as the source of our claims. V ERIFY [URL_CITE] is a project that was established to expose false claims made about the war in Syria and other related Middle Eastern issues. It is an independent platform that debunks claims made by all parties to the conflict.\nFootnote or Reference text (if exists): 4 http://www.verify-sy.com\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sequencecomparison.github.io\nCitation Context: In this test, words drawn from a random-ized sample are repeatedly aligned with each other in order to create a distribution of sound transi-tions for unrelated languages. This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b) [URL_CITE] . SCA is very fast in computation, but Lex-Stat has a much higher accuracy.\nFootnote or Reference text (if exists): Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.\nSection Titles (if exists):  ['3 Methods', '3.1 Sequence Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Method & Material)\nTYPE: Mixed (Code & Document)\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Here we focus on four meth-ods: Tgt, SrcTune, RegFreq, and RegSense. First note that Tgt continues to improve on both per-plexity and AUC metrics beyond five epochs (the default in word2vec code [URL_CITE] and left unchanged in RegFreq (Yang et al., 2017)). In contrast, Src-Tune, RegSense, and RegFreq are much better than Tgt at five epochs, saturating quickly.\nFootnote or Reference text (if exists): 7 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['Topics and tasks', '4.2 Epochs vs. regularization results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/projects/morpho/\nCitation Context: Turkish is another a highly-inflected and agglutina-tive language with relatively free word order. The same Morfessor tool (Creutz and Lagus, 2005) [URL_CITE] as in Finnish and Estonian was applied to Turkish texts as well. Using the 360k most common words from the training corpus, 34k morph units were obtained.\nFootnote or Reference text (if exists): Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.4 Turkish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1002/9781444395037.ch5\nCitation Context: Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [URL_CITE] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport.\nFootnote or Reference text (if exists): Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181\u2013224. https://doi.org/10.1002/9781444395037.ch5.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.collocations.de/temp/kappa_example.zip\nCitation Context: (1969)), observed agreement 0.96. [URL_CITE] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs.\nFootnote or Reference text (if exists): 7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.\nSection Titles (if exists):  ['p = Bv (5)', '3.3 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bitbucket.org/tclup/alto\nCitation Context: Furthermore, chart constraints can be combined effectively with coarse-to-fine parsing for PCFGs (for an overall speedup of 70x) and supertagging for TAG (overall speedup of 124x), all while improving the accuracy over those of the baseline parsers. Our code is part of the Alto parser (Gontrum et al., 2017), available at [URL_CITE] http://bitbucket.org/tclup/alto.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://labelstud.io/\nCitation Context: IDiag For guaranteeing the diversity of the ex-perimental data, we use Label Studio [URL_CITE] to create a new medical NER dataset. We collect 12127 health record images from the hospital, which are converted into text paragraphs by optical character recognition (OCR).\nFootnote or Reference text (if exists): 2 https://labelstud.io/\nSection Titles (if exists):  ['3 Experiments', '3.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gunviolencearchive.org\nCitation Context: Several projects collect data about gun violence via newspaper teams (Boyle, 2013; Swaine et al., 2015) or volunteer crowds (Burghart, 2014; Wagner, 2014; Kirk and Kois, 2013). Perhaps the largest such ef-fort is the Gun Violence Archive [URL_CITE] . However, none are aimed at the eventual automation of the process.\nFootnote or Reference text (if exists): 5 http://www.gunviolencearchive.org\nSection Titles (if exists):  ['4 Related Efforts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/raylin1000/dropbert\nCitation Context: Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koc\u030cisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 2.0 (Rajpurkar et al., 2018). We use the NABERT [URL_CITE] (Numerically Augmented BERT) model with an additional reasoning type to allow \u201cNo Answer\u201d as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 \u201cNo Answer\u201d questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch.\nFootnote or Reference text (if exists): 3 https://github.com/raylin1000/drop bert\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum\nCitation Context: WikiSum consist of Wikipedia articles each of which are associated with a set of reference docu-ments. [URL_CITE] We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al.\nFootnote or Reference text (if exists): 3 We take the processed Wikipedia articles from https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum released on April 25th 2018.\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.answers.com\nCitation Context: For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [URL_CITE] (Fader et al., 2014). The Yahoo!\nFootnote or Reference text (if exists): 5 http://wiki.answers.com\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhaozj89/TensorEmbeddingNLP\nCitation Context: The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [URL_CITE] https://github.com/zhaozj89/TensorEmbeddingNLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.politifact.com\nCitation Context: We created new frames after conducting a sur-vey of existing fact-checks from PolitiFact [URL_CITE] and followed it by grouping together semantically and syntactically similar factual claims from these fact-checks. If a group of claims did not share a common existing frame, we created a new frame for it.\nFootnote or Reference text (if exists): 7 https://www.politifact.com\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: After Dai and Le (2015) [URL_CITE] proposed the framework of semi-supervised sequence learning, there have been several attempts to extend sequence learn-ing models for different tasks to semi-supervised settings. Cheng et al.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.chokkan.org/software/crfsuite/\nCitation Context: Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007) [URL_CITE] . Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009).\nFootnote or Reference text (if exists): Naoaki Okazaki. 2007. CRFsuite: a fast im-plementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [URL_CITE] https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/vilbert-multi-task\nCitation Context: Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [URL_CITE] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\".\nFootnote or Reference text (if exists): 5 https://github.com/facebookresearch/ vilbert-multi-task\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nCitation Context: Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [URL_CITE] (Isozaki et al., 2010). Follow-ing Cho et al.\nFootnote or Reference text (if exists): 7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/downloads.html\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [URL_CITE]\nFootnote or Reference text (if exists): 5 http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hunch.net/~vw/\nCitation Context: Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba-sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [URL_CITE] with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.\nFootnote or Reference text (if exists): 4 http://hunch.net/\u02dcvw/\nSection Titles (if exists):  ['3 High Water Mark for Tandem Anchors', '3.2 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.quest.dcs.shef.ac.uk/\nCitation Context: While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [URL_CITE] (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).\nFootnote or Reference text (if exists): 2 http://www.quest.dcs.shef.ac.uk/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://banner.sourceforge.net\nCitation Context: We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [URL_CITE] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }.\nFootnote or Reference text (if exists): 3 http://banner.sourceforge.net\nSection Titles (if exists):  ['3 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE\nCitation Context: Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [URL_CITE] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.\nFootnote or Reference text (if exists): 5 https://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://jsoup.org/\nCitation Context: Text Collection: Using jsoup (Hedley, 2017) [URL_CITE] , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath.\nFootnote or Reference text (if exists): Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.\nSection Titles (if exists):  ['3 Approach', '3.1 Domain-Independent Approach (DI)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clinc/oos-eval\nCitation Context: We also thank Jin Qu, Tian Xie, Xinyi Yang, and Yingbo Zhou for their support in the deployment of DNNC into the internal system. Appendix A Training Details Dataset preparation To use the CLINC150 dataset (Larson et al., 2019) [URL_CITE] in our ways, espe-cially for the single-domain experiments, we pro-vide preprocessing scrips accompanied with our code.\nFootnote or Reference text (if exists): 9 https://github.com/clinc/oos-eval.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/dataset-factoid-curated/tree/master/trec\nCitation Context: CuratedTREC [URL_CITE] (Voorhees et al., 1999) is based on the benchmark from the TREC QA tasks, which contains 2, 180 questions extracted from the datasets from TREC1999, 2000, 2001 and 2002.\nFootnote or Reference text (if exists): 4 https://github.com/brmson/dataset-factoid-curated/tree/master/trec\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: After Dai and Le (2015) [URL_CITE] proposed the framework of semi-supervised sequence learning, there have been several attempts to extend sequence learn-ing models for different tasks to semi-supervised settings. Cheng et al.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/parag1604/A2L\nCitation Context: : where, y max = arg max y s(y). The strategy se-lects examples for which M margin \u2264 \u03c4 [URL_CITE] , where \u03c4 1 is a hyper-parameter. We use Viterbi\u2019s algorithm (Ryan and Nudd, 1993) to compute the scores s(y).\nFootnote or Reference text (if exists): 1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.\nSection Titles (if exists):  ['4 Experiments', '4.1 Active Learning Strategies for Sequence Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.cs.washington.edu/triviaqa\nCitation Context: The authors would like to greatly thank the team be-hind Natural Questions for the held out data, that defines our NQ test set; FEVER 15 , HotpotQA 16 and TriviaQA [URL_CITE] teams for sharing official test data for the KILT leaderboard; Luke Zettlemoyer and Scott Wen-tau Yih for helpful discussions; Rishabh Jain for the help in setting up the EvalAI challenge. A Appendix Wikipedia Representation We represent the KILT knowledge source as a collection of JSON records, one per Wikipedia page.\nFootnote or Reference text (if exists): 17 https://nlp.cs.washington.edu/ triviaqa\nSection Titles (if exists):  ['11 Acknowledgment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Other systems also have this issue, possibly due to source-reference divergence and train-validation overlap: We note that this issue is not unique to our system \u2014 other systems on the KILT leaderboard like BART + DPR and RAG actually perform worse than their no-retrieval counterpart (BART) in generation quality, as shown in Table 1. Qualitatively, we found no evidence of retrieval usage in a publicly hosted ELI5 model demo by Jernite (2020) [URL_CITE] . 10 A possible explanation for this issue is high source-reference divergence, a common problem in table-to-text generation (Wiseman et al., 2017; Tian et al., 2019).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['3 Analysis', '3.1 Are generations grounded in retrieval?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nCitation Context: The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [URL_CITE] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:\nFootnote or Reference text (if exists): 1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cogsci.princeton.edu/~wn\nCitation Context: Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely. Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) [URL_CITE] and HowNet (Dong, 2000) (among others) have inspired the work of many re-searchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998; Voorhees, 1993).\nFootnote or Reference text (if exists): Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press. http://www.cogsci.princeton.edu/\u02dcwn [2000, Septem-ber 7].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: GPE. Geo-political entities are single units that contain a population, government, physical loca-tion, and political boundaries (LDC, 2005) [URL_CITE] . In literary data, this includes not only cities that have known geographical locations within the real world (London, New York), or nations (England, the United States), but also both named and com-mon imagined entities as well (the town, the vil-lage).\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['3 Annotation', '3.1 Entity types']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/TinySVM/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mappings.dbpedia.org/server/ontology/classes/\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dcferreira/multilingual-joint-embeddings\nCitation Context: We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://languagetool.org/\nCitation Context: We use (1) attack success rate \u2013 the ratio of the suc-cessful attacks to total number of attacks, (2) query count \u2013 the number of queries, (3) perturbation rate \u2013 the percentage of words substituted in an in-put and (4) grammatical correctness \u2013 the average grammatical error increase rate (calculated using Language-Tool [URL_CITE] ) to verify the quality of generated adversarial examples. For all the metrics except attack success rate, lower the value better the re-sult.\nFootnote or Reference text (if exists): 2 https://languagetool.org/\nSection Titles (if exists):  ['4 Experiments', '4.4 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wing.comp.nus.edu.sg/~linzihen/parser/\nCitation Context: We use the data from AESOP 2009 and 2010 as the training data, and test our metrics on AESOP 2011 data. To obtain the discourse relations of a summary, we use the discourse parser [URL_CITE] developed in Lin et al. (2010).\nFootnote or Reference text (if exists): 2 http://wing.comp.nus.edu.sg/\u02dclinzihen/parser/\nSection Titles (if exists):  ['4 DICOMER: Evaluating Summary Readability', '4.3 Predicting Readability Scores']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmaww/SENT\nCitation Context: We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/rtmaww/SENT\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2016-supersense-embeddings\nCitation Context: The outcomes of this work are available to the research community. [URL_CITE] . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the train-ing data beyond WordNet vocabulary.\nFootnote or Reference text (if exists): 11 https://github.com/UKPLab/ acl2016-supersense-embeddings\nSection Titles (if exists):  ['8 Conclusions and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/research/sp\nCitation Context: Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our system and code available at http://statnlp.org/research/sp.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser/tree/easy-to-use\nCitation Context: DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [URL_CITE] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02\u201321; development: 22).\nFootnote or Reference text (if exists): 2 https://github.com/clab/lstm-parser/tree/easy-to-use.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bitbucket.org/yoavgo/word2vecf\nCitation Context: SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014). [URL_CITE]\nFootnote or Reference text (if exists): 3 http://bitbucket.org/yoavgo/word2vecf\nSection Titles (if exists):  ['2 Experiment Setup', '2.1 Word Representations', '2.1.2 Representation Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/robertostling/eflomal/\nCitation Context: Dictionary Extraction As our data augmenta-tion technique requires a dictionary, we propose to extract the dictionary from parallel corpora using a statistical word aligner, eflomal [URL_CITE] (\u00d6stling and Tiedemann, 2016). Once we produce word align-ments between tokens in our parallel corpora, we simply take word alignments that appear over 20 times to produce our bilingual dictionary.\nFootnote or Reference text (if exists): 7 https://github.com/robertostling/eflomal/\nSection Titles (if exists):  ['3 AfroBART', '3.2 Dictionary Augmentation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html\nCitation Context: It takes a long time to construct high-quality an-notated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki\u2019s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus [URL_CITE] 2.0. These data are divided into two groups: gen-eral and editorial.\nFootnote or Reference text (if exists): 2 http://pine.kuee.kyoto-u.ac.jp/nl-resource/courpus-e.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://help.twitter.com/en/rules-and-policies/twitter-limits\nCitation Context: User tweets are simply all the tweets from that user\u2019s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [URL_CITE] https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day (https://business.twitter.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.purl.com/net/lexicons\nCitation Context: The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu\u2019s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014). [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.purl.com/net/lexicons\nSection Titles (if exists):  ['6 English Sentiment Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed (Material and Supplement)\nTYPE: Mixed (Knowledge and Document)\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nCitation Context: We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [URL_CITE] . Model and Hyper-parameters.\nFootnote or Reference text (if exists): 6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://132.70.6.148:at:8080/exploration\nCitation Context: The web application of our system is available [URL_CITE] http://132.70.6.148:at:8080/exploration\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Application to the Health-care Domain']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/treex\nCitation Context: Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and \u017dabokrtsk\u00fd, 2010). [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://ufal.mff.cuni.cz/treex. Domain-independent deep syntax analysis for several languages is included in this framework; the English pipeline used here involves a statistical part-of-speech tagger (Spoustov\u00e1 et al., 2007) and a dependency parser (McDonald et al., 2005), fol-lowed by a rule-based conversion to deep syntax trees.\nSection Titles (if exists):  ['2 Generator Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://reporterslab.org/fact-checking/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. [URL_CITE] While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 2 http://reporterslab.org/fact-checking/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz\nCitation Context: The remaining baseline results are computed using the metric scores as submitted to (Ma et al., 2019) [URL_CITE]\nFootnote or Reference text (if exists): 14 http://data.statmt.org/wmt19/translation-task/wmt19-submitted-data-v3.tgz\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: We compare our agent with the baseline agent (Tan et al., 2019) [URL_CITE] on the R2R test leaderboard. Our syntax-aware agent achieves 47.8% in success rate and 0.45 in SPL, improving the baseline model by 2.1% in success rate and 2% in SPL.\nFootnote or Reference text (if exists): 3 The baseline is the re-implementation of their model without back translation based on code: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['5 Results and Analysis', '5.1 Room-to-Room Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/faiss\nCitation Context: As for the evalu-ation network R, we use a multilayer perceptron with two hidden layers of size 512. For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [URL_CITE] . We calculate the similarity of sentences in each episode instead of each batch for computa-tional efficiency.\nFootnote or Reference text (if exists): 2 https://github.com/facebookresearch/ faiss\nSection Titles (if exists):  ['4 Experiments', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lil.nlp.cornell.edu/nlvr/\nCitation Context: This paper includes four main contributions: (1) a procedure for collecting visually rich im-ages paired with semantically-diverse language descriptions; (2) NLVR2, which contains 107,292 examples of captions and image pairs, includ-ing 29,680 unique sentences and 127,502 im-ages; (3) a qualitative linguistically-driven data analysis showing that our process achieves a broader representation of linguistic phenomena compared to other resources; and (4) an evalu-ation with several baselines and state-of-the-art visual reasoning methods on NLVR2. The rel-atively low performance we observe shows that NLVR2 presents a significant challenge, even for methods that perform well on existing vi-sual reasoning tasks. NLVR2 is available at [URL_CITE] http://lil.nlp.cornell.edu/nlvr/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [URL_CITE] Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nmrksic/attract-repel\nCitation Context: cialisation framework (Mrk\u0161ic\u0301 et al., 2017). [URL_CITE] The key idea of LEAR , illustrated by Figure 1, is to pull desirable ( ATTRACT ) examples described by the constraints closer together, while at the same time pushing undesirable ( REPEL ) word pairs away from each other. Concurrently, LEAR (re-)arranges vector norms so that norm values in the Euclidean space reflect the hierarchical organisation of con-cepts according to the given LE constraints: put simply, higher-level concepts are assigned larger norms.\nFootnote or Reference text (if exists): 2 https://github.com/nmrksic/attract-repel\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20\nCitation Context: Zhang et al. (2018) [URL_CITE] approach retweet prediction as a multi-class classification problem, and present a feature-weighted model, where weights are com-puted using information gain.\nFootnote or Reference text (if exists): Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model. http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20.\nSection Titles (if exists):  ['3 Related work', '3.2 Predicting tweet popularity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/\nCitation Context: Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters\u2019 Lab lists 237 active fact-checking orga-nizations plus another 92 inactive. While some organizations debunked just a couple of hundred claims, others such as Politifact, FactCheck.org, 4 Snopes, [URL_CITE] and Full Fact have fact-checked thou-sands or even tens of thousands of claims.\nFootnote or Reference text (if exists): 5 http://www.snopes.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.davidsilver.uk/teaching/\nCitation Context: The agent reacts to its envi-ronment by accounting for past predictions and getting rewarded along the way, while its state is defined by the partial sequence generated so far. This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015 [URL_CITE] ). More precisely, a sequence gen-eration task can be recast as a Markov Decision Process (MDP) where the agent behavior follows a policy \u03c0(a t |s t ).\nFootnote or Reference text (if exists): David Silver. 2015. Lectures on reinforcement learn-ing. URL : https://www.davidsilver.uk/teaching/.\nSection Titles (if exists):  ['3 Models', '3.1 Reinforcement Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/wmaier/rparse\nCitation Context: We run further experiments with rparse [URL_CITE] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.\nFootnote or Reference text (if exists): 5 http://github.com/wmaier/rparse\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msci.com/gics\nCitation Context: Breakdown of results by industry. We ana-lyze errors on the validation data by segmenting earnings calls by each company\u2019s Global Industry Classification Standard (GICS) sector [URL_CITE] . See Fig-ure 2 for the breakdown results.\nFootnote or Reference text (if exists): 21 See https://www.msci.com/gics. There are 11 broad industry sectors.\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.4 Results.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.utexas.edu/users/ml/nldata/geoquery\nCitation Context: To test the coverage and precision of Locutus, I have customized it to answer questions from the G EOQUERY 250 corpus (Mooney, 1996) [URL_CITE] , which consists of a database of geographical information paired with 250 English sentences requesting in-formation from that database. 25 of these sentences are held out for the purposes of another study, and I have not examined the behavior of Locutus with respect to these sentences.\nFootnote or Reference text (if exists): Mooney, Raymond. Geoquery 1996.Data. http://www.cs.utexas.edu/users/ml/nldata/geoquery. html (accessed February 13, 2010).\nSection Titles (if exists):  ['4 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nCitation Context: Our re-sults demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code and data for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/treelstm\nCitation Context: Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks. Implementations of our mod-els and experiments are available at [URL_CITE] https://github.com/stanfordnlp/treelstm.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://textessence.github.io\nCitation Context: A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at [URL_CITE] https: //textessence.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/WeiboTreebank.zip\nCitation Context: \u2022 We present a freely available Chinese Weibo dependency treebank [URL_CITE] , manually annotated with more than 18,000 tokens;\nFootnote or Reference text (if exists): 3 http://www.cs.cmu.edu/\u02dcyww/data/WeiboTreebank.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://worksheets.codalab.org/worksheets/0x593676a278fc4e5abe2d8bac1e3df486/\nCitation Context: Reproducibility. All code, data, and experiments for this paper are avail-able on the CodaLab platform at [URL_CITE] https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/ .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Discussion and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [URL_CITE] children of Body Regions. The longer the ID of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous\nFootnote or Reference text (if exists): 1 http://www.nlm.nih.gov/mesh/meshhome.html; the work reported in this paper uses MeSH 2001.\nSection Titles (if exists):  ['3 The Lexical Hierarchy: MeSH']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/mt-para\nCitation Context: We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they at-tempted to accurately identify all error categories for most sentence pairs, it is possible that the errors in some sentence pairs were not comprehensively iden-tified. [URL_CITE]\nFootnote or Reference text (if exists): 4 The data is available at http://bit.ly/mt-para.\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf\nCitation Context: \u2022 Lang, SeqAE: Pretrained with the language model and autoencoder model proposed in (Dai and Le, 2015) [URL_CITE] . The language model is the de-coder part of the encoder-decoder model using a zero vector as the initial hidden layer value, and the autoencoder model is the same structure of the encoder-decoder model, where input and output are the same.\nFootnote or Reference text (if exists): Andrew M Dai and Quoc V Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems 28 (NIPS 2015), Curran Associates, Inc., pages 3079\u20133087. http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf.\nSection Titles (if exists):  ['3 Experiments', '3.3 Compared Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/viczong/measuring_forecasting_skill_from_text\nCitation Context: This could potentially be useful for identifying ac-curate predictions or potentially skilled fore-casters earlier. [URL_CITE]\nFootnote or Reference text (if exists): 1 We provide our code and dataset descriptions at: https://github.com/viczong/measuring_forecasting_skill_from_text.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf\nCitation Context: Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [URL_CITE] but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al.\nFootnote or Reference text (if exists): Oriol Vinyals, \u0141 ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-mar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2773\u2013 2781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.1 Soft attention']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LinyangLee/BERT-Attack\nCitation Context: We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack [URL_CITE] are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set.\nFootnote or Reference text (if exists): 12 https://github.com/LinyangLee/ BERT-Attack\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cmusphinx.org\nCitation Context: Though tools for quickly training recognizers for new lan-guages exist (e.g. CMUSphinx [URL_CITE] ), they typically require many hours of training audio to produce effective models, data which is by definition not available for LRLs. In efforts to overcome this data scarcity problem, recent years have seen the development of techniques for rapidly adapt-ing multilingual or language-independent acoustic and language models to new languages from rela-tively small amounts of data (Schultz and Waibel, 2001; Kim and Khudanpur, 2003), methods for building resources such as pronunciation dictio-naries from web-crawled data (Schlippe et al., 2014), and even a web-based interface, the Rapid Language Adaptation Toolkit 4 (RLAT), which al-lows non-expert users to exploit these techniques to create speech recognition and synthesis tools for new languages (Vu et al., 2010).\nFootnote or Reference text (if exists): 3 http://www.cmusphinx.org\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN\nCitation Context: We used JUMAN (Kurohashi and Kawahara, 2009a) [URL_CITE] for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b).\nFootnote or Reference text (if exists): Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.\nSection Titles (if exists):  ['5 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/msgs\nCitation Context: Second, since models learn useful features early, there is hope that fu-ture advances could accelerate by reducing the amount of data needed to learn which features mat-ter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: [URL_CITE] https://github.com/nyu-mll/msgs.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: Simple PPDB, along with the hu-man judgements collected as part of its creation, is freely available with the publication of this paper. [URL_CITE] rial is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.\nFootnote or Reference text (if exists): 6 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/v2.8.0/examples/run_glue.py\nCitation Context: The rest of the hyperparameters are set to default values. [URL_CITE]\nFootnote or Reference text (if exists): 4 The default hyperparameters are as in https: //github.com/huggingface/transformers/ blob/v2.8.0/examples/run_glue.py.\nSection Titles (if exists):  ['4 Experiments', '4.4 Downstream Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.suda.edu.cn/index.php/SUCDT\nCitation Context: Experimental results show that 1) the two conversion approaches achieve nearly the same conversion accuracy, and 2) direct treebank conversion is superior to indirect multi-task learning in exploiting multiple treebanks in methodology simplicity and performance, yet with the cost of manual annotation. We release the annotation guideline and the newly annotated data in [URL_CITE] http://hlt.suda.edu.cn/index.php/SUCDT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed (Dataset, Document)\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/VinAIResearch/BERTweet\nCitation Context: We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: [URL_CITE] https://github.com/VinAIResearch/BERTweet.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jflanigan/jamr/tree/Generator\nCitation Context: The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMR-gen [URL_CITE] (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).\nFootnote or Reference text (if exists): 5 https://github.com/jflanigan/jamr/tree/Generator\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/CodRED\nCitation Context: We conduct comprehensive ex-periments to show that CodRED is challeng-ing to existing RE methods including strong BERT-based models. We make CodRED and the code for our baselines publicly available at [URL_CITE] https://github.com/thunlp/CodRED.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE\nCitation Context: \u2022 ERNIE (115M) [URL_CITE] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc.\nFootnote or Reference text (if exists): 3 https://github.com/PaddlePaddle/ERNIE\nSection Titles (if exists):  ['5 Experiments', '5.1 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/docs/master/index.html\nCitation Context: We use PyTorch [URL_CITE] to implement our models.\nFootnote or Reference text (if exists): 6 http://pytorch.org/docs/master/index.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/ACE/\nCitation Context: Our proposed graph-based method is evaluated on the ACE corpus [URL_CITE] , which contains 519 files from sources including broadcast, newswire, and news-paper. A break-down of the tagged data by different relation subtypes is given in Table 1.\nFootnote or Reference text (if exists): 1 http://www.ldc.upenn.edu/Projects/ACE/\nSection Titles (if exists):  ['3 Experiments and Results', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: The reason we have not included these aspects in our representation and conversion yet is that we are focused here first on the evaluation for comparison with previous work, and the basis for this previous work is the usual evalb program (Sekine and Collins, 2008) [URL_CITE] , which ignores function tags and empty cate-gories. We return to this issue in the conclusion.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['2 Overview and Example', '2.1 Creation of Dependency Representation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2QHE5O3\nCitation Context: Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [URL_CITE] and Germany (Burger, 2019).\nFootnote or Reference text (if exists): 5 China Electricity Council; trans. China Energy Portal: https://bit.ly/2QHE5O3\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/\nCitation Context: A Appendix: Supplemental Material In Section 2.3, we made use of a freely available CPM implementation available from [URL_CITE] http://www.cs.toronto.edu/ \u223c jenn/CPM/ in MAT-LAB, Version 8.5.0.197613 (R2015a).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: In prior work (Huenerfauth and Kacorri, 2015b), we investigated key methodological con-siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc-tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work (Kacorri et al., 2015) [URL_CITE] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation. Specifically, we screened for par-ticipants that identified themselves as \u201cdeaf/Deaf\u201d or \u201chard-of-hearing,\u201d who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/itl/iad/mig/openmt12.cfm\nCitation Context: We carried out experiments in two different settings, both involving data from NIST Open MT 2012. [URL_CITE] The first setting uses data from the Chinese to En-glish constrained track, comprising 283M English tokens. We manually identified 14 sub-corpora on the basis of genres and origins.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/itl/iad/mig/openmt12.cfm\nSection Titles (if exists):  ['4 Experiments', '4.1 Data setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stream.twitter.com\nCitation Context: We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter\u2019s streaming API [URL_CITE] . The tweets are order by their time-stamps.\nFootnote or Reference text (if exists): 1 http://stream.twitter.com\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/ace/2005\nCitation Context: NER systems, how-ever, do not identify nominal NP instances (e.g., \u201ca software manufacturer\u201d or \u201cthe beach\u201d), or han-dle semantic classes that are not associated with proper named entities (e.g., symptoms). ACE mention detection systems (e.g., see (ACE, 2005 [URL_CITE] ; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with super-vised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different lan-guages (e.g., (Zitouni and Florian, 2009)).\nFootnote or Reference text (if exists): ACE. 2005. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2005.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [URL_CITE] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set.\nFootnote or Reference text (if exists): 18 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Mallet (McCallum, 2002) [URL_CITE] is more oriented to NLP applications. It is entirely in Java and in-cludes feature extraction tools for converting text into vectors and statistical analysis tools for docu-ment classification, clustering, topic modeling, in-formation extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on Lib-SVM, and obviously inherit its limits.\nFootnote or Reference text (if exists): Andrew Kachites McCallum. 2002. Mal-let: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-scorer\nCitation Context: This task has three evaluations: (i) N O S CORE E V \u2013 accuracy of claim verifica-tion, neglecting the validity of evidence; (ii) S CORE E V \u2013 accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for S UPPORTED and R E - FUTED ; (iii) F 1 \u2013 between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/fever-scorer\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/scalanlp/breeze\nCitation Context: Besides, for each sense, the database stores an aggregated context word rep-resentation in the form of a serialized object con-taining a sparse vector in the Breeze format. [URL_CITE] Dur-ing the disambiguation phrase, the input context is represented in the same sparse feature space and the classification is reduced to the computation of the cosine similarity between the context vector and the vectors of the candidate senses retrieved from the database. This back-end is implemented as a RESTful API using the Play framework.\nFootnote or Reference text (if exists): 6 https://github.com/scalanlp/breeze\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.2 WSD API']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.efset.org/research/~/media/centralefcom/efset/pdf/\nCitation Context: First, as mentioned previously, in our dataset the MET and reported TOEFL scores have a Pearson\u2019s r correlation of 0.74. We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [URL_CITE] . Using 384 participants who took both tests, the study found a Pearson\u2019s r of 0.63 for the reading comprehension and 0.69 for the listen-ing comprehension sections of these tests.\nFootnote or Reference text (if exists): Richard M Luecht. 2015. EFSET PLUS - TOEFL iBT correlation study report. https://www.efset.org/research/\u02dc/media/centralefcom/efset/pdf/ EFSET_TOEFL_correlational_report_ Sep_v1.pdf.\nSection Titles (if exists):  ['4 English Proficiency Scoring Based on Eye Movements in Reading', '4.1 Correlation with MET and TOEFL']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-dent answers (A s ,A t ) such that grade(A i ,A s ) > grade(A i ,A t ), where A i is the corresponding in-structor answer. We use the SVMLight [URL_CITE] implemen-tation of SVMRank with tuned parameters.\nFootnote or Reference text (if exists): 8 http://svmlight.joachims.org/\nSection Titles (if exists):  ['6 http://Infomap-nlp.sourceforge.net/']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.grammarbot.io/\nCitation Context: (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, [URL_CITE] , and Grammarly to help us create a set of rules.\nFootnote or Reference text (if exists): 2 https://www.grammarbot.io/\nSection Titles (if exists):  ['4 Experimental setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: API\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/1OJGL9Q\nCitation Context: We compare UrduPhone with Soundex and its variants. [URL_CITE] These algorithms are used to group words based on their encoding and then evalu-ated against the gold standard. Table 2 shows the results on Web dataset.\nFootnote or Reference text (if exists): 10 We use NLTK-Trainer\u2019s phonetic library http://bit.ly/1OJGL9Q\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.2 UrduPhone Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.rd.nacsis.ac.jp/\"ntcadm/,Sep\nCitation Context: 5 Experiments 2+, E1 tf,where,ef fit-E .354 .363 2+, E2 tf,where,ef fit-E .350 .359 2+, E4 tf,where,ef fit-E .333 .341 Two measures of performance are reported: (1) 11 2+ tf,where,ef fit-E .332 .366 point average precision and (2) R, precision after .360 .351NA NA JCB1 retrieving Nrd documents, where Nrd is the num-ber of relevant documents. We used the \"short query\" condition of the NACSIS NTCIR-1 Test Table 10: The best filters (Ek) improve the per- Collection (Kando et al., 1999) [URL_CITE] which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1. 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements the K filter is slightly unhelpful.\nFootnote or Reference text (if exists): Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, Koji Eguchi, and Hiroyuki Katoand Souichiro Hi-daka. 1999. Overview of ir tasks at the first nt-cir workshop. In NTCIR Workshop 1, pages 11-44, http://www.rd.nacsis.ac.jp/\"ntcadm/,Sep.\nSection Titles (if exists): [\\'3 Burstiness\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://duc.nist.gov\nCitation Context: We used data from the DUC 2004 Multilingual summarization task. The Document Understanding Conference ( [URL_CITE] http://duc.nist.gov) has been run annu-ally since 2001 and is the biggest summarization evaluation effort, with participants from all over the world. In 2004, for the first time, there was a multi-lingual multi-document summarization task.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 References to people', '2.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/P96-1011\nCitation Context: For our models, we adopt the pruning strate-gies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.00001, and utilize a tag dictio-nary, which maps frequent words to the possible supertags 7 . Unless otherwise stated, we only al-low normal form parses (Eisner, 1996 [URL_CITE] ; Hocken-maier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).\nFootnote or Reference text (if exists): Jason Eisner. 1996. Efficient Normal-Form Parsing for Combinatory Categorial Grammar. In 34th Annual Meeting of the Association for Computational Lin-guistics. http://aclweb.org/anthology/P96-1011.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nCitation Context: NeuronBlocks provides convenient user interface [URL_CITE] for users to build, train, and test DNN models. The details are described in the following.\nFootnote or Reference text (if exists): 4 https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nSection Titles (if exists):  ['3 Design', '3.3 User Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\nCitation Context: This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by Garbe (2012) [URL_CITE] , which we denote as symDelete(\u00b7,\u00b7,\u00b7). This al-gorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical af-ter k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair moderno-modern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.\nFootnote or Reference text (if exists): Wolf Garbe. 2012. 1000x faster spelling correc-tion algorithm. http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/. Accessed: 2018-02-12.\nSection Titles (if exists):  ['4 Orthographic Similarity Adjustment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [URL_CITE] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tombosc/cpae\nCitation Context: For the evaluation of definition embeddings, we used the modified Word Embedding Benchmarks projects [URL_CITE] , following Bosc and Vincent (2018). These benchmarks include SimLex999 (SL999), SimLex333 (SL333) (Hill et al., 2015), SimVerb (SV) (Gerz et al., 2016), MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012).\nFootnote or Reference text (if exists): 3 https://github.com/tombosc/cpae\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://psy.takelab.fer.hr/datasets/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora [URL_CITE] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 6 https://psy.takelab.fer.hr/datasets/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iep.utm.edu/ethics/\nCitation Context: Our work bridges this gap by illustrating how a philosophical theory of ethics can be applied to NLP research. Ethics (or ethical theory), is a the-oretical and applied branch of philosophy which studies what is good and right, especially as it per-tains to how humans ought to behave in the most general sense (Fieser, 1995) [URL_CITE] . As NLP research qual-ifies as a human activity, it is within the purview of ethics.\nFootnote or Reference text (if exists): James Fieser. 1995. Ethics. https://iep.utm.edu/ethics/ (accessed: 11-03-2020).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QAML/S3QACoreFramework\nCitation Context: Our cQA pipeline is available for download [URL_CITE] and is distributed under the terms of the Apache 2.0 Li-cense. By taking advantage of the Apache Maven project management tool, most dependencies are automatically handled.\nFootnote or Reference text (if exists): 1 https://github.com/QAML/ S3QACoreFramework\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ictclas.org/\nCitation Context: We compare our method with three baseline methods. The first two are both famous Chinese word segmentation tools: ICTCLAS [URL_CITE] and Stan-ford Chinese word segmenter , which are widely used in NLP related to word segmentation. Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard is chosen as the PKU standard, which is the same to ours.\nFootnote or Reference text (if exists): 3 http://ictclas.org/\nSection Titles (if exists):  ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.gsk.or.jp/en/catalog//\nCitation Context: 6. We have released a part of it to the public on the web [URL_CITE] .\nFootnote or Reference text (if exists): 4 The corpus data are available at https://www.gsk.or.jp/en/catalog//.\nSection Titles (if exists):  ['4 Data Development']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MASS\nCitation Context: BART 3 (P:0.83, R: 1.0) [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mturk.com\nCitation Context: To look at more perceptual measures of dialogue quality, we used Amazon Mechanical Turk [URL_CITE] to an-notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants. Details on the annotation pro-cess can be found in (Gravano et al., 2011).\nFootnote or Reference text (if exists): 2 http://www.mturk.com\nSection Titles (if exists):  ['5 Entrainment and dialogue quality', '5.1 Entrainment correlates of dialogue charac-teristics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: Our BERT reader is based on Google\u2019s refer-ence implementation [URL_CITE] (TensorFlow 1.12.0). For training, we begin with the BERT-Base model (un-cased, 12-layer, 768-hidden, 12-heads, 110M pa-rameters) and then fine tune the model on the train-ing set of SQuAD (v1.1). All inputs to the reader are padded to 384 tokens; the learning rate is set to 3 \u00d7 10 \u22125 and all other defaults settings are used.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 System Architecture', '3.2 BERT Reader']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wpdev.uservoice.com/forums/110705-universal-windows-platform\nCitation Context: 1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app , and Windows App studio [URL_CITE] . We crawled the suggestion for improvement posts for these products, and labeled only a subset of them due to the annotation costs.\nFootnote or Reference text (if exists): 3 https://wpdev.uservoice.com/forums/110705-universal- windows-platform\nSection Titles (if exists):  ['3 Research Methodology', '3.2 RQ2: Suggestion Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org/wiki/Wiktionary:Statistics\nCitation Context: The English language edition contains over 500,000 word senses. [URL_CITE]\nFootnote or Reference text (if exists): 2 as of May 2013, see http://en.wiktionary.org/wiki/Wiktionary:Statistics.\nSection Titles (if exists):  ['2 Resource Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cadillacforums.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [URL_CITE] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 3 http://www.cadillacforums.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dmoz.org/\nCitation Context: We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project [URL_CITE] as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented.\nFootnote or Reference text (if exists): 3 http://www.dmoz.org/\nSection Titles (if exists):  ['4 Building Noun Similarity Lists', '4.1 Web Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sweaglesw.org/linguistics/ace/\nCitation Context: To be specific, for each input sentence S, we gener-ate K-best semantic graphs G 1 , G 2 , ..., G K with an ERG-based processor, i.e. ACE [URL_CITE] . The created grammar-licensed analyses contain both a deriva-tion tree recording the used grammar rules and lex-ical entries, and the associated semantic represen-tation constructed compositionally via this deriva-tion (Bender et al., 2015).\nFootnote or Reference text (if exists): 2 http://sweaglesw.org/linguistics/ace/\nSection Titles (if exists):  ['3 Literal versus Intended Meaning', '3.3.2 SemBanking with ERG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org\nCitation Context: There are custom solutions that can easily simplify the slot filling task. For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [URL_CITE] ) could be used to get all an-swers right. Nevertheless, we are interested in test-ing if a general model can extract attributes about specific entities from a large body of text.\nFootnote or Reference text (if exists): 13 https://www.wikidata.org\nSection Titles (if exists):  ['8 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf\nCitation Context: Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data 3 . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [URL_CITE] . Thus, human as-sessments are in a 2-10 scale.\nFootnote or Reference text (if exists): LDC. 2005. Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations. Revision 1.5. Tech-nical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fjoch.com/YASMET.html\nCitation Context: We use the published package YASMET [URL_CITE] to conduct parameters training and classification. YASMET requires supervised learning for the training of maximum entropy model.\nFootnote or Reference text (if exists): 1 http://www.fjoch.com/YASMET.html\nSection Titles (if exists):  ['3 Cascading Guidance Technique', 'SVM Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iep.utm.edu/ethics/\nCitation Context: Our work bridges this gap by illustrating how a philosophical theory of ethics can be applied to NLP research. Ethics (or ethical theory), is a the-oretical and applied branch of philosophy which studies what is good and right, especially as it per-tains to how humans ought to behave in the most general sense (Fieser, 1995) [URL_CITE] . As NLP research qual-ifies as a human activity, it is within the purview of ethics.\nFootnote or Reference text (if exists): James Fieser. 1995. Ethics. https://iep.utm.edu/ethics/ (accessed: 11-03-2020).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/wpt/index.html#resources\nCitation Context: The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task [URL_CITE] (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M.\nFootnote or Reference text (if exists): 3 http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/idirlab/factframe\nCitation Context: The corpus of the newly-defined frames along with their annotated exemplary sentences is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/idirlab/factframe\nSection Titles (if exists):  ['2 System Architecture and Components', '2.4 Detecting Claim Types', '2.4.1 Frame detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories\nPlease classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-30586-638\nCitation Context: In general, correlations reported by previous work range from weak to strong. The re-sults confirm that metrics can be reliable indica-tors at system-level (Reiter and Belz, 2009), while they perform less reliably at sentence-level (Stent et al., 2005) [URL_CITE] . Also, the results show that the met-rics capture realization better than sentence plan-ning.\nFootnote or Reference text (if exists): Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341\u2013351. https://doi.org/10.1007/978-3-540-30586-638.\nSection Titles (if exists):  ['9 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chopaal.org\nCitation Context: The first dataset, Web dataset, is scraped from Ro-man Urdu websites on news , poetry , SMS and blog . The second dataset, SMS dataset, is ob-tained from chopaal, an internet based group SMS service [URL_CITE] . For evaluation, we use a manually an-notated database of Roman Urdu variations (Khan and Karim, 2012).\nFootnote or Reference text (if exists): 9 http://chopaal.org\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.1 Dataset and Gold Standard']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Then we used the bilingual dictionary to compute the data uncer-tainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped them 38 into [URL_CITE] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual 36 data.\nFootnote or Reference text (if exists): 5 https://github.com/clab/fast_align\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ]
]
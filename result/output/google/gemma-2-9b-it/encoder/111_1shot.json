[
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sinantie/NeuralAmr\nCitation Context: We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evalua-tion metrics. [URL_CITE] We report results on the AMR dataset LDC2015E86 and LDC2017T10. All sys-tems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017).\nFootnote or Reference text (if exists): 1 We used the evaluation script available at https://github.com/sinantie/NeuralAmr.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salaniz/pycocoevalcap\nCitation Context: We use implementations of BLEU, METEOR, and ROUGE using Microsoft MS COCO evaluation scripts [URL_CITE] . We removed question marks, periods, and exclamation marks from references and candi-dates when evaluating with BLEU, METEOR, and ROUGE.\nFootnote or Reference text (if exists): 4 https://github.com/salaniz/pycocoevalcap\nSection Titles (if exists):  ['B Details on Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\nThe citation points to a GitHub repository (`https://github.com/salaniz/pycocoeval"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/syxu828/CSRL_dataset\nCitation Context: Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [URL_CITE] https: //github.com/syxu828/CSRL_dataset.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset\n* **Citation Reason:**  Code Availability \n\n**Explanation:**\n\nThe URL points to a GitHub repository (`https://github.com/syxu828"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/schmid\nCitation Context: Besides the slash features, we used other fea-tures in order to improve the parsing accuracy of the PCFG, inspired by the work of Klein and Man-ning (2003). The most important ones of these features [URL_CITE] will now be described in detail. Sec-tion 4.3 shows the impact of these features on labeled bracketing accuracy and empty category prediction.\nFootnote or Reference text (if exists): 1 The complete annotation program is available from the author\u2019s home page at http://www.ims.uni-stuttgart.de/ schmid\nSection Titles (if exists):  ['2 Feature Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://godel.iis.sinica.edu.tw/ROCLING\nCitation Context: Xu et al. (2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): 2 See http://godel.iis.sinica.edu.tw/ROCLING.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:** Data Source\n\n**Explanation:**\n\n* The URL points to a website (http://godel.iis.sinica.edu."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.sfu.ca/~mtaboada\nCitation Context: This corpus is composed of Wall Street Journal articles, it has been annotated over the Penn Treebank. We also report perfor-mance on the SFU review corpus [URL_CITE] (En-SFU-DT) containing product reviews, and on the instruc-tional corpus (En-Instr-DT) (Subba and Di Euge-nio, 2009) built on instruction manuals. 7\nFootnote or Reference text (if exists): 6 https://www.sfu.ca/\u02dcmtaboada\nSection Titles (if exists):  ['5 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/data.php\nCitation Context: For our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manu-ally annotated them with comma information [URL_CITE] . This annotated corpus served as both training and test datasets (using cross-validation).\nFootnote or Reference text (if exists): 3 The guidelines and annotations are available at http://L2R.cs.uiuc.edu/\u02dccogcomp/data.php. 19: Remove all examples from p that are covered by r 20: end for 21: end for\nSection Titles (if exists):  ['4 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Material\n**TYPE:** Dataset\n**FUNCTION:**  Training/Testing \n\n**Citation Reason:**\n\nThe citation is used to provide the source of the original Penn Treebank data"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mrlyk423/relation_extraction\nCitation Context: Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from [URL_CITE] https://github.com/mrlyk423/relation_extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tsafavi/NegatER\nCitation Context: Most notably, training KB completion models with highly-ranked negative examples from N EGAT ER results in statistically significant accuracy improvements of up to 1.90 % . Code and data are available at [URL_CITE] https://github.com/tsafavi/NegatER.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**Artifact Type:** Code\n\n**Citation Reason:**  Data and Code Availability\n\n**Explanation:**\n\nThe citation explicitly states that \"Code and data are available at [URL_CITE]\"."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [URL_CITE] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Artifact Type: Dataset\n\nCitation Reason:  Data Source\n\nThe URL points to a GitHub repository containing the \"skytrax-reviews-dataset,\" which is cited as the source of the Airline review data used in the experiments described in the text."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.naist.jp/hiki/ChaSen/\nCitation Context: We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [URL_CITE] 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization.\nFootnote or Reference text (if exists): 1 http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/ttc-project\nCitation Context: The French and English documents were then normalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation. These steps were car-ried out using the TTC TermSuite [URL_CITE] that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/ttc-project\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Method\n**TYPE:** Software\n**FUNCTION:** Linguistic Pre-processing \n\n\n**Reason for Citation:**\n\nThe citation is used to explicitly state the software tool (TTC"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kytea/\nCitation Context: To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the [URL_CITE] million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data). Table 1 shows the details of the extracted data.\nFootnote or Reference text (if exists): 2 http://www.phontron.com/kytea/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  The URL points to the website for the KyTea chunker software.  While the citation doesn't explicitly state it, the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cnts.uia.ac.be/conll2000/\nCitation Context: Syntactic-chunk label: The value of this feature is a B-I-O style label indicating what kind of syntactic chunk the word is contained in, e.g. noun phrase, verb phrase, or prepositional phrase. These are assigned using a word-chunking SVM-based system trained on the CoNLL-2000 data [URL_CITE] (which uses the lowest nodes of the Penn TreeBank syntactic trees to break sentences into base phrases).\nFootnote or Reference text (if exists): 2 http://cnts.uia.ac.be/conll2000/\nSection Titles (if exists):  ['5 Classifier Features', '5.4 Word class features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: For the description of features used in our adap-tation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009) [URL_CITE] . In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-of-speech) take specific values at the neighbor of an arc. One example of a feature assigned to a diag-onal arc at row i and column j of the alignment graph is where POS [i] denotes the part-of-speech of the ith word in a sentence.\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['2 Alignment-based coordinate structure analysis', '2.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Technical Report\n\n**Citation Reason:**\n\n* **Descriptive:** The citation primarily serves to describe the source of information about specific features used in a particular model adaptation ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1160\nCitation Context: Association for Compu-tational Linguistics, San Diego, California, pages 1346\u20131356. [URL_CITE] http://www.aclweb.org/anthology/N16-1160.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P14-1006\nCitation Context: Association for Compu-tational Linguistics, Baltimore, Maryland, pages 58\u201368. [URL_CITE] http://www.aclweb.org/anthology/P14-1006.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Paper\n\n**Citation Reason:**  To provide a reference to a specific paper presented at a conference. \n\n\nLet me know if you have any other citations you'd like me to analyze! \n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/petsc/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html\nCitation Context: Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we\u2019ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [URL_CITE]\nFootnote or Reference text (if exists): 6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.\nSection Titles (if exists):  ['3 Corpus, Features, and Data Creation', '3.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  \n\n* **Availability:** The citation points to the URL where the software (TreeTagger) can be downloaded for academic use."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.engappai.2020.103641\nCitation Context: Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [URL_CITE] ), character-level perturbations are not expected to change the text\u2019s meaning in most cases. Therefore, they can be automatically produced and used for testing the robustness of NLP systems.\nFootnote or Reference text (if exists): Jincheng Xu, and Qingfeng Du. 2020. TextTricker: Loss-based and gradient-based adversarial attacks on text classification models. Engineering Applications of Artificial Intelligence, 92: 103641. https://doi.org/10.1016/j.engappai.2020.103641.\nSection Titles (if exists):  ['5 Experimental results', '5.1 Performance on perturbed inputs']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Paper\n* **Citation Reason:**  Supporting Argument\n\n**Explanation:**\n\nThe citation is used to support the claim that character-level perturbations are unlikely to"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [URL_CITE] http: //sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/h-shahidi/\nCitation Context: Experimen-tal results demonstrate that a basic attention-based seq2seq model trained with the expo-nential moving average technique achieves the state of the art in both tasks. Code is avail-able at [URL_CITE] https://github.com/h-shahidi/ 2birds-gen .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Code\n* **FUNCTION:**  Implementation of the attention-based seq2seq model with exponential moving average training.\n\n**Citation Reason:**\n\nThe"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.google.com/group/hoo-nlp/\nCitation Context: We experimentally test our M 2 method in the con-text of the HOO shared task. The HOO test data [URL_CITE] consists of text fragments from NLP papers to-gether with manually-created gold-standard correc-tions (see (Dale and Kilgarriff, 2011) for details). We test our method by re-scoring the best runs of the participating teams in the HOO shared task with our M 2 scorer and comparing the scores with the of-ficial HOO scorer, which simply uses GNU wdiff to extract system edits.\nFootnote or Reference text (if exists): 2 Available at http://groups.google.com/group/hoo-nlp/ after registration.\nSection Titles (if exists):  ['3 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.comp.nus.edu.sg/software/\nCitation Context: Experi-ments on the HOO data show that our method overcomes deficiencies in the current evaluation method. The M 2 scorer is available for download at [URL_CITE] http://nlp.comp.nus.edu.sg/software/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Software\nTYPE: Tool\nFUNCTION: Implementation \n\n\nLet me explain:\n\n* **ROLE: Software** - The URL points to a software download page, indicating the artifact is a piece of software.\n* **TYPE: Tool**"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2015/task3/\nCitation Context: The corpus contains data from the Qatar Living forum, and is publicly available on the task\u2019s website. [URL_CITE] The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads. Each question, and each answer, consist of a short title and a more de-tailed description.\nFootnote or Reference text (if exists): 2 http://alt.qcri.org/semeval2015/task3/\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bitterlemons.org\nCitation Context: Our corpus consists of articles published on the bitterlemons website [URL_CITE] . The website is set up to \u201ccontribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas\u201d.\nFootnote or Reference text (if exists): 1 http://www.bitterlemons.org\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Website\n\n**Citation Reason:** Source of Data\n\n**Explanation:**\n\n* The citation clearly states that the corpus (a collection of articles) is derived from the \""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iwslt.org/\nCitation Context: Dataset statistics are summarized in Table 1. For the low-resource experiments, we use [URL_CITE] : English-Vietnamese from IWSLT\u201915, German-English from IWSLT\u201914, and French-English from IWSLT\u201916. For the rich-resource experiments, we use the English-German dataset from WMT\u201916, which contains about 4.5 million training samples.\nFootnote or Reference text (if exists): 4 https://iwslt.org/\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bilbo.cs.illinois.edu/~ctsai12/xlwikifier-wikidata.zip\nCitation Context: Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at [URL_CITE] http://bilbo.cs.illinois.edu/\u02dcctsai12/xlwikifier-wikidata.zip.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1 Wikipedia Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Providing access to the data used in the experiments. \n\nThe citation sentence explicitly states that the dataset used in the experiments is publicly"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/conll/software.html\nCitation Context: The labeled F 1 score [F1] 9 , de-fined by the standard protocol called PARSEVAL (Black et al., 1991), takes into account the brack-eting and labeling of nodes. The unlabeled attache-ment score [UAS] evaluates the quality of unlabeled dependencies between words of the sentence [URL_CITE] . And finally, the Leaf-Ancestor score [LA] (Sampson, 2003) computes the similarity between all paths (se-quence of nodes) from each terminal node to the root node of the tree.\nFootnote or Reference text (if exists): 10 This score is computed by using the tool available at http://ilk.uvt.nl/conll/software.html. The constituent trees are automatically converted into dependency trees with the tool Bonsai.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/conll/software.html\nCitation Context: Parsing Quality The parsing quality was evalu-ated using the CONLL-X (Buchholz and Marsi, 2006) standard. Dependencies were extracted from the output HPSG trees, and evaluated using the official script [URL_CITE] . Due to this conversion, the accuracy on the relation labels is less important.\nFootnote or Reference text (if exists): 6 http://ilk.uvt.nl/conll/software.html\nSection Titles (if exists):  ['6 Evaluation', '6.3 Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Evaluation Metric\n* **TYPE:**  Software/Script\n* **FUNCTION:**  Dependency Extraction and Evaluation \n\n**Reasoning:**\n\nThe citation refers to a script available"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bplank/bleaching-text\nCitation Context: This results in an embedding space with a vocabulary size of 16M word types. All code is available at [URL_CITE] https://github.com/bplank/bleaching-text.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cogcomp.cs.illinois.edu/page/publication_view/790\nCitation Context: Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re-quired by a factor of 16. The truncated word embeddings are available from the papers web page at [URL_CITE] https://cogcomp.cs.illinois.edu/page/publication_view/790.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Data\n**TYPE:**  Model Weights/Embeddings\n**FUNCTION:**  Provide Access \n\n**Reasoning:**\n\n* **Data:** The citation refers to \""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/docs/master/index.html\nCitation Context: We use PyTorch [URL_CITE] to implement our models.\nFootnote or Reference text (if exists): 6 http://pytorch.org/docs/master/index.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ontotext.com/owlim/\nCitation Context: We use SwiftOwlim [URL_CITE] for storing and querying the data. SwiftOwlim is a \u201ctriple store\u201d, a kind of database which is specifically built for storing and querying RDF data.\nFootnote or Reference text (if exists): 3 http://www.ontotext.com/owlim/\nSection Titles (if exists):  ['3 Knowledge Representation and Semantic Inference']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Tool\n* **TYPE:** Software\n* **FUNCTION:** Data Storage and Querying \n\n**Reason for Citation:**\n\nThe citation explicitly states that SwiftOwlim is used for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rikdz/GraphWriter\nCitation Context: We use the standard split of AGENDA dataset in our experiments: 38,720 for training, 1000 for validation, and 1000 for testing. Experimental settings We follow the official settings [URL_CITE] in Koncel-Kedziorski et al. (2019) with the encoder containing 6 layers and 4-head graph attention in every layer.\nFootnote or Reference text (if exists): 12 https://github.com/rikdz/GraphWriter\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Code\n**TYPE:**  Library/Tool\n**FUNCTION:** Implementation Details \n\n**Reasoning:**\n\n* **Code Repository:** The URL points to a GitHub repository"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.irit.fr/STAC/\nCitation Context: In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [URL_CITE] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set.\nFootnote or Reference text (if exists): 1 https://www.irit.fr/STAC/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset\nCitation Context: The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [URL_CITE] .\nFootnote or Reference text (if exists): 1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to a GitHub repository containing a dataset specifically designed for Named Entity"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pushshift/api\nCitation Context: Data was scraped from the two subreddits by querying the Pushshift API. [URL_CITE] 3 years\u2019 worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters.\nFootnote or Reference text (if exists): 4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.)\nSection Titles (if exists):  ['4 Data', '4.1 Data collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zxshamson/dy-conv-rec\nCitation Context: For experiments [URL_CITE] , we collect Reddit conversa-tions from three subreddits \u2014 \u201ctechnology\u201d, \u201cto-dayilearned\u201d, and \u201cfunny\u201d, each exhibiting differ-ent data statistics, discussion topics, and language styles. An absolute date is used to separate training data (before the date) from test and validation data (after the date).\nFootnote or Reference text (if exists): 1 The datasets and codes are available at: https://github.com/zxshamson/dy-conv-rec\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Data Access & Reproducibility\n\n**Explanation:**\n\nThe citation points to a GitHub repository (`https://github.com/zxshamson"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/semi-supervised-qa\nCitation Context: We attain more than 50% F1 score on SQuAD and TriviaQA with less than a thou-sand labelled examples. We are also releasing a set of 3.2M cloze-style questions for practi-tioners to use while building QA systems [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://bit.ly/semi-supervised-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/natural-language/docs/analyzing-entities\nCitation Context: We analyze four QA tasks: NQ , [URL_CITE] SQuAD (Rajpurkar et al., 2016), QB (Boyd-Graber et al., 2012) and Trivia QA (Joshi et al., 2017). Google CLOUD - NL 2 finds and links entity mentions in QA examples.\nFootnote or Reference text (if exists): 1 For NQ , we only consider questions with short answers. 2https://cloud.google.com/natural-language/docs/ analyzing-entities\nSection Titles (if exists):  ['2 Mapping Questions to Entities']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  API Documentation \n* **Citation Reason:**  Supporting Information/Background\n\n**Explanation:**\n\nThe URL points to Google Cloud's Natural Language API"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/e-bug/nmt-difficulty\nCitation Context: We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at [URL_CITE] https://github.com/e-bug/nmt-difficulty.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Tutorial Materials \n* **Citation Reason:**  Supplementary Information/Reproducibility\n\n**Explanation:**\n\nThe citation points to a GitHub repository containing tutorial materials related"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/king-kong-1933.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lycos.com/info/zoloft.html\nCitation Context: Figure 1 shows a sample Retriever page for the topic \u201cMario Lemieux\u201d. [URL_CITE] The topic is indicated at the upper left. Below it is a category assigned to the topic, in this case Sports > Hockey > Ice Hockey > National Hockey League > Lemieux, Mario.\nFootnote or Reference text (if exists): 4 For other categories, see e.g. King Kong (1933): http://www.lycos.com/info/king-kong-1933.html, Zoloft: http://www.lycos.com/info/zoloft.html, Public-Key Cryptography: http://www.lycos.com/info/public-key-cryptography.html , Lyme Disease: http://www.lycos.com/info/lyme-disease.html, Reggaeton: http://www.lycos.com/info/reggaeton.html\nSection Titles (if exists):  ['3 Lycos Retriever pages']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Knowledge Base Entry\n\n**Citation Reason:**  Example\n\nThe URL points to a Lycos Retriever page for the topic \"Zoloft.\"  The citation in"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/jacobe/naacl09.html\nCitation Context: Source code is available at [URL_CITE] http://people.csail.mit.edu/jacobe/naacl09.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/noa/naacl2021\nCitation Context: Our code, data splits, and scripts to repro-duce our experiments are available at [URL_CITE] http://github.com/noa/naacl2021.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code & Data\n* **Citation Reason:** Reproducibility \n\n**Explanation:**\n\nThe citation explicitly states that the provided URL contains:\n\n* **Code:**  Enables"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supporting Evidence\nTYPE: Paper\nFUNCTION:  Provide evidence for a claim about the effectiveness of unlexicalized neural network TAG parsers. \n\n\nThe citation supports the claim that unlexicalized neural network TAG parsers can achieve"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/segtok/\nCitation Context: Before running keyword extrac-tion on each dataset, we apply standard text pre-processing operations. The documents are first tok-enized into words by segtok [URL_CITE] , a python library for tokenization and sentence splitting. Then, each word is stemmed to reduce it to its base form for comparison purpose by Porter Stemmer from NLTK (Bird et al., 2009), a widely used python li-brary for text processing.\nFootnote or Reference text (if exists): 8 https://pypi.org/project/segtok/\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/\nCitation Context: Text Span Extraction. Sentences are ex-tracted from the documents using the sentence tokenizer from the nltk Python package [URL_CITE] (Bird et al., 2009).\nFootnote or Reference text (if exists): 6 https://www.nltk.org/\nSection Titles (if exists):  ['C Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Library\n* **FUNCTION:**  Sentence Tokenization \n\n\n**Citation Reason:**\n\nThe citation is used to acknowledge the source of the sentence tokenizer used"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/docs/master/index.html\nCitation Context: We use PyTorch [URL_CITE] to implement our models.\nFootnote or Reference text (if exists): 6 http://pytorch.org/docs/master/index.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nCitation Context: \u2022 We use Comet\u2019s Bayes algorithm [URL_CITE] , which bal- Learning rate\nFootnote or Reference text (if exists): 6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Tool\n* **FUNCTION:**  Specific Algorithm \n\n**Reasoning:**\n\n* **Comet's Bayes algorithm** is clearly identified as the specific"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csc.fi/kielipankki/\nCitation Context: Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. The units of the morph lexicon for the experiments in this paper were learned from a joint corpus con-taining newspapers, books and newswire stories of totally about 150 million words (CSC, 2001) [URL_CITE] . We obtained a lexicon of 25k morphs by feeding the learning algorithm with the word list containing the 160k most common words.\nFootnote or Reference text (if exists): CSC Tieteellinen laskenta Oy. 2001. Finnish Lan-guage Text Bank: Corpora Books, Newspapers, Magazines and Other. http://www.csc.fi/kielipankki/.\nSection Titles (if exists):  ['4 Speech recognition experiments', '4.2 Finnish']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/projects/morpho/\nCitation Context: The approach presented in this paper relies on a data-driven algorithm called Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2005 [URL_CITE] ) which is a language independent unsupervised machine learn-ing method to find morpheme-like units (called sta-tistical morphs) from a large text corpus. This method has several advantages over the rule-based grammatical morphemes, e.g.\nFootnote or Reference text (if exists): Mathias Creutz and Krista Lagus. 2005. Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor. Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology. URL: http://www.cis.hut.fi/projects/morpho/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  \n\n* **Algorithm Description:** The citation is primarily to describe the \"Morfessor\" algorithm, which is the core of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/7KG2U\nCitation Context: For German, there are several resources with derivational information. We use version 1.3 of DE RIV B ASE (Zeller et al., 2013), [URL_CITE] a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 non-singleton derivational families. It has a precision of 84% and a recall of 71%.\nFootnote or Reference text (if exists): 1 Downloadable from: http://goo.gl/7KG2U\nSection Titles (if exists):  ['3 A Resource for German Derivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://files.pushshift.io/reddit/comments/\nCitation Context: (2017) use Wikipedia, we extract the dataset from Reddit. [URL_CITE] Since productively formed derivatives are not part of the language norm ini-tially (Bauer, 2001), social media is a particularly fertile ground for our study.\nFootnote or Reference text (if exists): 3 We draw upon the entire Baumgartner Reddit Corpus, a collection of all public Reddit posts available at https: //files.pushshift.io/reddit/comments/.\nSection Titles (if exists):  ['3 Dataset of Derivatives']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to a repository containing a large collection of Reddit comments,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/fnlp\nCitation Context: We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [URL_CITE] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times.\nFootnote or Reference text (if exists): 9 https://github.com/FudanNLP/fnlp\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use the third-party tool jieba [URL_CITE] for word seg-mentation and POS tagging; both steps are cus-tomized in order to achieve a better performance on domain- and task-specific data. Specifically, the dictionary provided by the tool is inter-sected with a user-specified dictionary.\nFootnote or Reference text (if exists): 3 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Processing steps', '5.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Tool\n\n**Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\n* **Tool:**  The URL points to a GitHub repository for the `jieba` library"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [URL_CITE] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Source Description\n\n**Explanation:**\n\n* **Dataset:** The URL points to a website describing the KFTT (Kyoto Free Translation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/LDNOOBW\nCitation Context: All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models\u2019 toxicity using an openly available blocklist [URL_CITE] and an offen-sive language classifier presented in Dinan et al. (2019b).\nFootnote or Reference text (if exists): 3 https://github.com/LDNOOBWdataset. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.\nSection Titles (if exists): ['6 Analysis of Safety and Gender Bias', '6.3 Safety']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\nCitation Context: Saleh et al. (2019) incor-porate a Toxicity Classifier trained with data from the Toxic Comment Classification Challenge [URL_CITE] as a reward in the training hierarchical RL dialog mod-els. We compute Toxicity reward scores using this classifier as Bot Toxicity (e.g.\nFootnote or Reference text (if exists): 3 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge\nSection Titles (if exists):  ['E Implicit Rewards Details', 'E.7 Toxicity']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Data Source:** The citation is used to identify the specific dataset (\"Toxic Comment Classification Challenge\") that was used to train a toxicity classifier"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017 [URL_CITE] ). Ganin et al.\nFootnote or Reference text (if exists): Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence genera-tive adversarial nets with policy gradient. In AAAI Conference on Artificial Intelligence. https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Paper\n\n* **Citation Reason:**  Foundational Work\n\n**Explanation:**\n\nThis citation introduces the original concept of Generative Adversarial Networks ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/Asiya\nCitation Context: As for evaluation measures, for MT we have used a rich set of 64 measures provided within the ASIYA Toolkit (Gime\u0301nez and Ma\u0300rquez, 2010) [URL_CITE] . This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic.\nFootnote or Reference text (if exists): 5 http://www.lsi.upc.edu/\u02dcnlp/Asiya\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.2 Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [URL_CITE] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation.\nFootnote or Reference text (if exists): 4 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 MT Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Tool\n**TYPE:** Software/Toolkit\n**FUNCTION:** Evaluation \n\n**Reason for Citation:**\n\nThe authors cite the ASIYA toolkit because they used it to extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1161\nCitation Context: Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [URL_CITE] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.\nFootnote or Reference text (if exists): Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/\nCitation Context: (2016), existing methods often use language and task-specific rules and strate-gies (Lei et al., 2013; Raghothaman et al., 2016). A similar line is to use NL queries for code re-trieval (Wei et al., 2015 [URL_CITE] ; Allamanis et al., 2015). The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).\nFootnote or Reference text (if exists): Yi Wei, Nirupama Chandrasekaran, Sumit Gul-wani, and Youssef Hamadi. 2015. Build-ing bing developer assistant. Techni-cal report. https://www.microsoft.com/en-us/research/publication/building-bing-developer-assistant/.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Technical Report\n* **Citation Reason:**  \n\n    The citation is used to provide context and background information on the related work in the field of code-"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [URL_CITE] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment.\nFootnote or Reference text (if exists): 2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.\nSection Titles (if exists):  ['4 Experiment', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\nCitation Context: Depending on the method we use D T , D S , or u S to train topic-specific embeddings and eval-uate them as-is on two tasks that train task-specific layers on top of these fixed embeddings. The first is an unsupervised language modeling task where we train a LSTM [URL_CITE] on the adapted embed-dings (which are pinned) and report perplexity on Df T . The second is a Duplicate question detec-tion task.\nFootnote or Reference text (if exists): 4 https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Method\n**TYPE:** Code\n**FUNCTION:**  Implementation of LSTM \n\n**Citation Reason:**\n\nThe code at the provided URL implements a Long Short-Term Memory (LSTM)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: For instance: [URL_CITE] http://hierplane.allenai.org/explain/stanford/ He%20ate%20pasta%20with% 20chopsticks\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Tool/Demo\n\n**Citation Reason:**  Example/Illustration\n\n**Explanation:**\n\n* **Tool/Demo:** The URL points to a web interface for the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koc\u030cisky\u0301 et al., 2018). Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [URL_CITE] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/document-qa\nSection Titles (if exists):  ['5 Experiments', '5.1 Data and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.washington.edu/triviaqa/\nCitation Context: TriviaQA [URL_CITE] (Joshi et al., 2017) includes 95, 000 question-answer pairs authored by trivia enthusi-asts and independently gathered evidence docu-ments, six per question on average, and utilizes Bing Web search API to collect 50 webpages re-lated to the questions.\nFootnote or Reference text (if exists): 3 http://nlp.cs.washington.edu/triviaqa/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset\n* **Citation Reason:** Description and Source \n\n**Explanation:**\n\nThe citation points to the TriviaQA dataset. The primary reason for citing it is to:\n\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016) [URL_CITE] , MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271\u2013 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ccsasuke/man\nCitation Context: Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [URL_CITE] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions.\nFootnote or Reference text (if exists): 1 The source code of MAN is available at https://github.com/ccsasuke/man.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Source Code\nTYPE: Software\nFUNCTION: Implementation \n\n\nLet me know if you have any other citations you'd like me to analyze!\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.baidu.com/broad/subordinate?dataset=duconv\nCitation Context: The DuConv dataset [URL_CITE] : a knowledge graph en-hanced conversation dataset in Chinese proposed by Wu et al. (2019).\nFootnote or Reference text (if exists): 2 https://ai.baidu.com/broad/subordinate?dataset=duconv\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/opendialkg\nCitation Context: The OpenDialKG dataset [URL_CITE] : a knowledge graph enhanced conversation dataset in English proposed by Moon et al. (2019).\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/opendialkg\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software/Code Repository\n\n**Citation Reason:**  \n\n* **Dataset Source:** The URL points to a GitHub repository that hosts the OpenDialKG dataset.  "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Our three models are implemented in Python using TensorFlow [URL_CITE] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing.\nFootnote or Reference text (if exists): 9 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [URL_CITE] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/data/glove.840B.300d.zip\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**Artifact Type:**  Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to a downloadable archive containing pre-trained GloVe word embeddings."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jvking/reddit-RL-simulator\nCitation Context: There are two main contributions in this paper. First, we propose a novel reinforcement learning task with both states and combinatorial actions de-fined by natural language, [URL_CITE] which is introduced in section 2. This task, which is based on comment popularity prediction using data from the Reddit dis-cussion forum, can serve as a benchmark in social media recommendation and trend spotting.\nFootnote or Reference text (if exists): 1 Simulator code and Reddit discussion identifiers are re-leased at https://github.com/jvking/reddit-RL-simulator\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Code\n\n**Citation Reason:**  Data and Code Availability\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, strongly suggesting the artifact"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlpado.de/~sebastian/data.shtml\nCitation Context: Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [URL_CITE]\nFootnote or Reference text (if exists): 4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.\nSection Titles (if exists):  ['2 A Parallel Corpus of Literary Texts', '2.3 T/V Gold Labels for English Utterances']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Material\n**TYPE:** Dataset\n**FUNCTION:**  Analysis/Evaluation \n\n**Reason for Citation:**\n\nThe authors cite the URL because they created and used a specific dataset for their"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 2005 [URL_CITE] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously over-looks the arguments-scattering challenge. In con-trast, EE on financial documents, such as ChFi-nAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event.\nFootnote or Reference text (if exists): 2 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cninfo.com.cn/new/index\nCitation Context: Data Collection with Event Labeling. We uti-lize ten years (2008-2018) ChFinAnn [URL_CITE] documents and human-summarized event knowledge bases to conduct the DS-based event labeling. We focus on five event types: Equity Freeze (EF), Equity Re-purchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value.\nFootnote or Reference text (if exists): 4 Crawling from http://www.cninfo.com.cn/new/index\nSection Titles (if exists):  ['6 Experiments', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Dataset/Data Source\n\n**Citation Reason:**  Source of Data\n\n**Explanation:**\n\n* The URL points to the website of the China Securities Regulatory Commission"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/cove\nCitation Context: Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [URL_CITE] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension.\nFootnote or Reference text (if exists): 1 https://github.com/salesforce/cove\nSection Titles (if exists):  ['2 Proposed model: SAN']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/vecmap\nCitation Context: (2013) to bigram-context and trigram-context pairs in addition to the usual word-context pairs. Having done that, we map the embeddings to a cross-lingual space us-ing VecMap [URL_CITE] with identical initialization (Artetxe et al., 2018a), which builds an initial solution by aligning identical words and iteratively im-proves it through self-learning. Finally, we extract translation candidates by taking the 100 nearest-neighbors of each source phrase, and score them by applying the softmax function over their cosine similarities: where the temperature \u03c4 is estimated using max-imum likelihood estimation over a dictionary in-duced in the reverse direction.\nFootnote or Reference text (if exists): 3 https://github.com/artetxem/vecmap\nSection Titles (if exists):  ['3 Principled unsupervised SMT', '3.1 Initial phrase-table']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Implementation Details\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository (`https://github.com/"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T08\nCitation Context: Therefore, we prepared an evaluation data set that consists of a list of sen-tences containing at least one of the five temporal connectives since, betwen, from, before and after. To do this, we extract all sentences that satisfy the condition from 183 articles in the TimeBank 1.2 corpus [URL_CITE] . This results in a total of 486 sentences.\nFootnote or Reference text (if exists): 3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp? catalogId=LDC2006T08\nSection Titles (if exists):  ['3 Experimental Study', '3.1 Data Preparation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://onlinebooks.library.upenn.edu\nCitation Context: Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). [URL_CITE] The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed sep-arately on each of the twelve parts, with the task of correctly identifying the chapter and section bound-aries.\nFootnote or Reference text (if exists): 2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. Malioutov, and Masao Utiyama for making their topic segmentation systems publicly available, and to the anonymous reviewers for useful feedback. This research is supported by the Beckman Postdoctoral Fellowship.\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:**  Material\n**TYPE:**  Dataset\n**FUNCTION:**  Source \n\n**Explanation:**\n\n* **Material:** The artifact is a tangible resource \u2013 a medical textbook"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/freesunshine0316/neural-graph-to-seq-mp\nCitation Context: We release our code and models at [URL_CITE] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NP-NET-research/Recursive-Semi-Markov-Model\nCitation Context: Our code is released at [URL_CITE] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:**  Providing access to the source code for reproducibility and further development. \n\n**Explanation:**\n\nThe citation explicitly states \""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset/Benchmark\n\n**Citation Reason:**  Comparison\n\n**Explanation:**\n\n* **Dataset/Benchmark:** The URL points to the WMT17 Matrix,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.is.cs.cmu.edu/mie\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.im2.ch\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Project\n\n**Citation Reason:**  \n\n* **Background/Context:** The citation is used to provide examples of existing research projects focused on multimedia meeting data.  \n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ifttt.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Tool\n\n**Citation Reason:** Comparative Example\n\n**Explanation:**\n\n* **Tool:** IFTTT is presented as a visual programming tool used for automation. \n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/nlp/dpo3/\nCitation Context: 3. A free distribution of our implementation. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://groups.csail.mit.edu/nlp/dpo3/\nSection Titles (if exists):  ['3. A free distribution of our implementation. 2']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:**  Software/Code\n* **Citation Reason:**  Availability \n\n**Explanation:**\n\n* The URL points to a web page hosted by MIT's CSAIL NLP group"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/easonnie/combine-FEVER-NSMN\nCitation Context: For fact checking, we consider NSMN (Nie et al., 2019), the highest scoring system from the FEVER shared task (Thorne et al., 2018b). We use the public model [URL_CITE] pre-trained on FEVER, and con-sider not enough information predictions as false. Moreover, we develop a fact checking baseline that combines a BERT-base classifier with passages re-turned from DPR where the claim and retrieved passage are input.\nFootnote or Reference text (if exists): 22 available at https://github.com/easonnie/ combine-FEVER-NSMN\nSection Titles (if exists):  ['7:453\u2013466.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fever.ai/\nCitation Context: A rapid increase in the spread of misinformation on the Internet has necessitated automated solu-tions to determine the validity of a given piece of information. To this end, the Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018a) [URL_CITE] introduced a dataset for evidence-based fact verification. Given a claim, the task in-volves extracting relevant evidence sentences from a given Wikipedia dump and assigning a label to the claim by reasoning over the extracted evidence (S UPPORTS / R EFUTES / N OT E NOUGH I NFO ).\nFootnote or Reference text (if exists): 1 https://fever.ai/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset\n* **Citation Reason:**  The citation introduces the FEVER dataset, which is fundamental to the work described in the text. The dataset is used for training and evaluating"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [URL_CITE] from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [URL_CITE] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Paper\n\n**Citation Reason:**  Benchmarking and Comparison\n\nThe citation of Jernite (2020) serves as a **baseline** for comparison"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2016-supersense-embeddings\nCitation Context: The outcomes of this work are available to the research community. [URL_CITE] . In follow-up work, we aim to apply our embedding method on smaller, yet gold-standard corpora such as SemCor (Miller et al., 1994) and STREUSLE (Schneider and Smith, 2015) to examine the impact of the corpus choice in detail and extend the train-ing data beyond WordNet vocabulary.\nFootnote or Reference text (if exists): 11 https://github.com/UKPLab/ acl2016-supersense-embeddings\nSection Titles (if exists):  ['8 Conclusions and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/sentence-transformers\nCitation Context: Additional, we have tried different aggregation methods like taking the result with the maximum confidence and we did not find a meaningful improvement. For selective demonstra-tions, we take roberta-large-nli-stsb mean-tokens [URL_CITE] from Reimers and Gurevych (2019) as our sentence embedding model.\nFootnote or Reference text (if exists): 14 https://github.com/UKPLab/ sentence-transformers\nSection Titles (if exists):  ['C Experimental Details', '-', 'C.3 Fine-tuning with demonstrations']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software\n\n**Citation Reason:** Implementation Details\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository, which is a common platform for hosting"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aclweb.org/anthology/D16-1264\nCitation Context: Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) [URL_CITE] . This contains about 23K passages and 100K questions.\nFootnote or Reference text (if exists): Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.facebook.com/research/-babi/\nCitation Context: Bordes et al. (2015) released the SimpleQuestions data set [URL_CITE] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?)\nFootnote or Reference text (if exists): 11 https://research.facebook.com/research/-babi/\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **ROLE:** Material\n* **TYPE:** Dataset\n* **FUNCTION:**  Introduce/Provide Background \n\n**Explanation:**\n\nThe citation introduces the SimpleQuestions dataset, which is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com\nCitation Context: We created an Amazon Mechanical Turk [URL_CITE] project to investigate the possible scalability of our anno-tation using non-trained annotators.\nFootnote or Reference text (if exists): 7 https://www.mturk.com\nSection Titles (if exists):  ['4 Annotation Experiment', '4.2 Crowdsourcing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hub.microsofttranslator.com\nCitation Context: To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [URL_CITE] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.\nFootnote or Reference text (if exists): 2 http://hub.microsofttranslator.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Website\n* **Citation Reason:**  **Source of a Machine Translation Service** \n\nThe citation points to Microsoft Translator's hub, indicating that"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/downloads.html\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses as well as the sense-annotated SemCor corpus (Landes et al., 1998). [URL_CITE]\nFootnote or Reference text (if exists): 5 http://web.eecs.umich.edu/\u02dcmihalcea/downloads.html\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: We use three different methods to construct the repository. In particular, we harness the sense-annotated WordNet glosses [URL_CITE] as well as the sense-annotated SemCor corpus (Landes et al., 1998).\nFootnote or Reference text (if exists): 4 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['6 Verb-Object Sense Repository']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Data Source\n* **TYPE:** Lexical Database (specifically, WordNet)\n* **FUNCTION:**  Providing sense-annotated glosses for WordNet. \n\n\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bethard.info/data/fables-100-temporal-dependency.xml\nCitation Context: Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [URL_CITE] could be used to train a temporal dependency parsing model.\nFootnote or Reference text (if exists): 2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nCitation Context: The corpus of stories for children was drawn from the fables collection of (McIntyre and Lapata, 2009) [URL_CITE] and annotated as described in (Bethard et al., 2012). In this section we illustrate the main annotation princi-ples for coherent temporal annotation. As an example story, consider:\nFootnote or Reference text (if exists): 1 Data available at http://homepages.inf.ed.ac.uk/s0233364/McIntyreLapata09/\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Corpus\n\n**Citation Reason:** Source Material\n\n**Explanation:**\n\n* **Corpus:** The URL points to a collection of children's fables, which is a"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experimental Framework', '3.3 Tools and Models']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  \n\n    * **Descriptive:** The citation is used to identify and describe the specific tool (SpaCy) used for English language processing."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/mecab/\nCitation Context: For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [URL_CITE] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING).\nFootnote or Reference text (if exists): 10 http://taku910.github.io/mecab/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Conditions and Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/text/_modules/torchtext/data/utils.html\nCitation Context: We normalize the text by lower casing, removing special characters, etc. [URL_CITE] For each task, we construct separate 1-Gram vo-cabulary (U) and initialize a trainable randomly sampled token embedding (U \u00d7 d e ) from N (0, 1). Similarly, we randomly initialize a (d s-max \u00d7 d e ) positional embedding.\nFootnote or Reference text (if exists): 9 https://pytorch.org/text/_modules/torchtext/data/utils.html\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:**  Methodological Detail\n**TYPE:** Library/Framework\n**FUNCTION:** Text Preprocessing Techniques\n\n**Reason for Citation:**\n\nThe citation points to the PyTorch Text"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nCitation Context: We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [URL_CITE] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison.\nFootnote or Reference text (if exists): 10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://visagetechnologies.com/products-and-services/visagesdk/facetrack\nCitation Context: To extract face and head movement information from the video, a face-tracker (Visage, 2016) [URL_CITE] was used to produce a set of MPEG4 facial animation parameters for each frame of video: These values represent face-landmark or head movements of the human appearing in the video, including 14 fea-tures used in this study: head x, head y, head z, head pitch, head yaw, head roll, raise l i brow, raise r i brow, raise l m brow, raise r m brow, raise l o brow, raise r o brow, squeeze l brow, squeeze r brow. The first six values represent head location and orientation.\nFootnote or Reference text (if exists): Technologies Visage. 2016. Face tracking. https://visagetechnologies.com/products-and-services/visagesdk/facetrack. Accessed: 2016-03- 10.\nSection Titles (if exists):  ['2 Method', '2.1 Dataset and Feature Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software/Tool\n\n**Citation Reason:**  The citation is used to acknowledge the specific software used for face tracking in the study.  \n\n**Explanation:**\n\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/demos.php\nCitation Context: For the Named Entity information, we used a publicly available NE Recognizer capable of recognizing a range of cat-egories including Person, Location and Organiza-tion. On the CoNLL-03 shared task, its f-score is about 90% [URL_CITE] . We evaluate our system from different points of view, as described below.\nFootnote or Reference text (if exists): 4 A web demo of the NER is at http://L2R.cs.uiuc.edu/\u02dccogcomp/demos.php.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geohive.com/\nCitation Context: This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive [URL_CITE] , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems . Gazetters have been widely used in NER systems to improve performance.\nFootnote or Reference text (if exists): 3 http://www.geohive.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.1 Internal Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Data Source\n**TYPE:** Website\n**FUNCTION:**  Source of Gazetteer Data \n\n**Explanation:**\n\nThe citation points to GeoHive as one of the sources for a list of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://computing.open.ac.uk/coda/\nCitation Context: We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University\u2019s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [URL_CITE] http://computing.open.ac.uk/coda/) which was funded by the UK\u2019s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Website\n* **Citation Reason:** Funding Source \n\n\n**Explanation:**\n\nThe citation points to Rxhui Inc.'s website, indicating that the research received financial support from this"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) [URL_CITE] in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them.\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/litbank\nCitation Context: In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [URL_CITE] https://github.com/dbamman/litbank.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software/Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Software/Dataset:** The URL points to a GitHub repository, which is a"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gluebenchmark.com/\nCitation Context: Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark [URL_CITE] . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA).\nFootnote or Reference text (if exists): 10 https://gluebenchmark.com/\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://skylion007.github.io/OpenWebTextCorpus/\nCitation Context: Dataset Following the official code of Clark et al. (2020), ELECTRA models are pretrained on the OpenWebTextCorpus [URL_CITE] dataset, an open source ef-fort to reproduce OpenAI\u2019s WebText dataset. Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents.\nFootnote or Reference text (if exists): 9 https://skylion007.github.io/OpenWebTextCorpus/\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset\n* **Citation Reason:**  The citation points to the source of the training data used for the ELECTRA models.  The sentence explicitly states that ELECTRA models are"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/un33k/python-slugify\nCitation Context: Figure [URL_CITE] shows the average number of citations per paper, analogous to Figure 7, but for a larger number of citation ages. citations of 40 Number citations of 40 Number\nFootnote or Reference text (if exists): 10 We achieve this by using https://github.com/un33k/python-slugify.\nSection Titles (if exists):  ['A Fuzzy paper matching']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seatgeek/fuzzywuzzy\nCitation Context: Figure 7: Average number of citations per paper that are 15 years or older, by venue of publication. tance ratio [URL_CITE] is \u2264 95%.\nFootnote or Reference text (if exists): 12 As implemented by https://github.com/seatgeek/fuzzywuzzy.\nSection Titles (if exists):  ['A Fuzzy paper matching']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Method\n**TYPE:** Tool\n**FUNCTION:** Implementation Detail \n\n**Reasoning:**\n\n* **Method:** The citation refers to a tool used in a specific method"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LayneIns/Unsupervised_dialo_disentanglement\nCitation Context: Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system. [URL_CITE] In summary, our main contributions are three-fold:\nFootnote or Reference text (if exists): 3 Code will be publicly available at https: //github.com/LayneIns/Unsupervised_ dialo_disentanglement\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Implementation\n* **TYPE:** Code\n* **FUNCTION:**  Demonstrate\n\n**Citation Reason:**\n\nThe code is cited to demonstrate the practical application of the proposed method for unsupervised"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: CRF++ , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve [URL_CITE] , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 8 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: CRF++ [URL_CITE] , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve , which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data.\nFootnote or Reference text (if exists): 7 http://crfpp.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Evaluation Metric']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Tool\n* **FUNCTION:**  Implementation Detail \n\n**Reasoning:**\n\n* **CRF++ is explicitly stated as the tool used for sequence labeling"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [URL_CITE] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps.\nFootnote or Reference text (if exists): 3 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Implementation\n\n**Explanation:**\n\nThe citation points to the `https://github.com/nlpyang/PreSumm` repository"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: For better comparison, we implement three methods as baselines, which have been proved ef-fective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with the publicly available tool word2vec [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['5 Experiments', '5.1 Evaluation as a ranking problem']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2011T13\nCitation Context: We also compare a gazetteer based method L ATTICE (Zhang and Yang, 2018) and L ATTICE (R E E NT ), which replaces the word gazetteer of L ATTICE with our entity dictionary for fair comparison. We use the same embeddings as (Zhang and Yang, 2018), which are pre-trained on Giga-Word [URL_CITE] us-ing Word2vec (Mikolov et al., 2013). The entity embeddings are randomly initialized and fine-tuned during training.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2011T13\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  The URL points to the LDC2011T13 dataset, which is used as the training corpus for the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/cabocha/\nCitation Context: A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha [URL_CITE] (Kudo et al., 2002) and KNP (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications.\nFootnote or Reference text (if exists): 1 http://taku910.github.io/cabocha/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP\nCitation Context: A bunsetsu is a Japanese grammatical and phono-logical unit that consists of one or more con-tent words such as a noun, verb, or adverb fol-lowed by a sequence of zero or more function words such as auxiliary verbs, postpositional par-ticles, or sentence-final particles. Most publicly available Japanese parsers, including CaboCha (Kudo et al., 2002) and KNP [URL_CITE] (Kawahara et al., 2006), return bunsetsu-based dependency as syn-tactic structure. Such parsers are generally highly accurate and have been widely used in various NLP applications.\nFootnote or Reference text (if exists): 2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Tool\n\n**Citation Reason:**  Method\n\n**Explanation:**\n\n* **Tool:** The URL points to the website for KNP, a Japanese natural language processing ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://readium.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [URL_CITE] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 4 https://readium.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cordova.apache.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova [URL_CITE] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 3 https://cordova.apache.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Tool\n\n**Citation Reason:**  Implementation\n\n**Explanation:**\n\n* **Tool:** Apache Cordova is a software development framework, making it a tool used in the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/software/srparser.html\nCitation Context: We chunk the source and target sentences using constituent parsing (Bauer, 2014) [URL_CITE] . We consider all nodes with phrase-level tags (XP) to be con-stituents.\nFootnote or Reference text (if exists): John Bauer. 2014. Shift-reduce constituency parser. https://nlp.stanford.edu/software/srparser.html. Accessed: 2018-11-30.\nSection Titles (if exists):  ['3 Models', '3.1 Chunkings and Chunk Embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Tool \n* **Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\nThe citation refers to a specific tool used in the research: the Stanford SR"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/about\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. [URL_CITE] On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 1 http://stackexchange.com/about\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:User_pages\nCitation Context: Requests in online communities We base our analysis on two online communities where re-quests have an important role: the Wikipedia community of editors and the Stack Exchange question-answer community. On Wikipedia, to coordinate on the creation and maintenance of the collaborative encyclopedia, editors can in-teract with each other on user talk-pages; [URL_CITE] re-quests posted on a user talk-page, although pub-lic, are generally directed to the owner of the talk-page. On Stack Exchange, users often comment on existing posts requesting further information or proposing edits; these requests are generally di-rected to the authors of the original posts.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/ Wikipedia:User_pages\nSection Titles (if exists):  ['2 Politeness data']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Website\n\n**Citation Reason:**  **Explanation/Definition**\n\nThe citation is used to provide more context and detail about the concept of \"user talk-pages\""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: In all the experiments, we use PyTorch [URL_CITE] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 2 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [URL_CITE] . All evaluation code is from the PyTorch Transformers as well.\nFootnote or Reference text (if exists): 11 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Implementation\n**TYPE:**  Code Library\n**FUNCTION:**  Code Reuse\n\n**Reason for Citation:**\n\nThe authors are citing the PyTorch Transformers library because they directly used its"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) [URL_CITE] is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.\nFootnote or Reference text (if exists): 4 http://mallet.cs.umass.edu/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/5.5/\nCitation Context: We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) is employed to train individual component classifiers and lp_solver [URL_CITE] is used to construct our global argument inference model.\nFootnote or Reference text (if exists): 5 http://lpsolve.sourceforge.net/5.5/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Tool\n* **FUNCTION:**  Software Library \n\n**Reason for Citation:**\n\nThe citation is used to acknowledge the use of the `lp_"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['7 Experiments', '7.1 Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Reproducibility \n\n**Explanation:**\n\nThe citation explicitly states that the code is being provided to help others replicate the results reported in the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: Bousmalis et al. (2016) [URL_CITE] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICCV.2017.304\nCitation Context: For D, we consider two variants of MAN. The first one is to use the NLL loss same as C which suits the classification task; while an-other option is to use the Least-Square (L2) loss that was shown to be able to alleviate the gradient vanishing problem when using the NLL loss in the adversarial setting (Mao et al., 2017) [URL_CITE] : where d is the domain index of some sample and d\u02c6is the prediction. Without loss of generality, we\nFootnote or Reference text (if exists): Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, and Stephen Paul Smolley. 2017. Least squares generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV). pages 2813\u20132821. https://doi.org/10.1109/ICCV.2017.304.\nSection Titles (if exists):  ['2 Model', '2.2 Training']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Paper\n\n**Citation Reason:**\n\n* **Supporting a specific technique:** The citation supports the use of Least-Square (L2) loss in the adversarial setting to alleviate the gradient vanishing problem. The authors refer to Mao et al"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepmipt/ner\nCitation Context: We model the slot tagging task as sequence tag-ging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of re-spective tags (Lample et al., 2016). [URL_CITE] The output layer of the tagger network gives softmax proba-bility distributions over possible tags. To further increase recall, we add an inference-time rule \u2013 if slot candidate is split \u2013 it is just ranked for relevance multiple times (with respect to multiple contexts).\nFootnote or Reference text (if exists): 7 https://github.com/deepmipt/ner\nSection Titles (if exists):  ['3 Method', '3.3 Slot Tagger Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Code Repository\n* **Citation Reason:** Implementation Details\n\n**Explanation:**\n\nThe citation points to a GitHub repository (`https://github.com/deepmi"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nCitation Context: Keras demo [URL_CITE] , into which we incorporate the su-persense information. Figure 3 displays our net-work architecture.\nFootnote or Reference text (if exists): 7 https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Justin1904/Low-rank-Multimodal-Fusion\nCitation Context: In order to compare our fusion method with previ-ous work, we adopt a simple and straightforward model architecture [URL_CITE] for extracting unimodal rep-resentations. Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as f a , f v , f l , to extract unimodal representations z a , z v , z l from uni-modal input features x a , x v , x l .\nFootnote or Reference text (if exists): 2 The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion\nSection Titles (if exists):  ['4 Experimental Methodology', '4.3 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Code\n\n**Citation Reason:**  Implementation Details/Comparison\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository containing source code.\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets?search=kilt\nCitation Context: KILT data and code are available at https://github.com/facebookresearch/KILT. [URL_CITE]\nFootnote or Reference text (if exists): 1 and at https://huggingface.co/datasets? search=kilt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/KILT\nCitation Context: We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, out-performing more tailor-made approaches for fact checking, open-domain question answer-ing and dialogue, and yielding competitive re-sults on entity linking and slot filling, by gen-erating disambiguated text. KILT data and code are available at [URL_CITE] https://github.com/facebookresearch/KILT. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact type and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:**  Data and Code Availability\n\n**Explanation:**\n\n* The URL points to a GitHub repository (`https://github."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Reproducibility \n\n**Explanation:**\n\nThe citation context clearly states that the authors are making their code and predictions publicly available. This strongly"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.fbk.eu/en/irstlm\nCitation Context: For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar-get side of the training corpus using the IRSTLM Toolkit [URL_CITE] with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003).\nFootnote or Reference text (if exists): 5 http://hlt.fbk.eu/en/irstlm\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Tool\n**TYPE:** Software Library\n**FUNCTION:** Language Modeling\n\n**Citation Reason:**\n\nThe citation of  `http://hlt.fbk.eu/en/irstlm"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.org/details/twitterstream\nCitation Context: \u2022 We first download the general Twitter Stream grabbed by the Archive Team, [URL_CITE] containing 4TB of Tweet data streamed from 01/2012 to 08/2019 on Twitter. To identify English Tweets, we employ the language identification compo-nent of fastText (Joulin et al., 2017).\nFootnote or Reference text (if exists): 1 https://archive.org/details/ twitterstream\nSection Titles (if exists):  ['2 BERTweet', 'Pre-training data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/docs/streaming-api/methods\nCitation Context: All tweets were col-lected from September 2010 to January 2011 via the Twitter API. [URL_CITE] From the raw data we extract English tweets using a language identification tool (Lui and Baldwin, 2011), and then apply a simpli-fied Twitter tokeniser (adapted from O\u2019Connor et al. (2010)).\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/docs/streaming-api/methods\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:**  Documentation\n**TYPE:** API Specification\n**FUNCTION:**  Methodological\n\n**Reasoning:**\n\n* **Documentation:** The URL points to Twitter's developer documentation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/multilingual\nCitation Context: We use the open-sourced models from wav2vec 2.0 and mBART50 pretrained with multilingual par-allel text data. These models can be down-loaded from https://github.com/pytorch/fairseq/tree/master/examples/wav2vec and [URL_CITE] https://github.com/pytorch/fairseq/tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Alex-Fabbri/\nCitation Context: Datasets can be obtained by running the data scripts at [URL_CITE] https://github.com/Alex-Fabbri/ lang2logic-PyTorch , which re-implements Dong and Lapata (2016) in PyTorch. We use Dong and Lapata (2016)\u2019s training, development (for ATIS), and test sets. ATIS, JOBS, and GEO contain 5410, 640, and 880 examples respectively.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.2 Semantic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:**  Implementation\n\n**Explanation:**\n\n* The URL points to a GitHub repository (`https://github.com/Alex-Fabbri/"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scalanlp.org\nCitation Context: First, the text is processed with a part-of-speech and a named entity taggers. [URL_CITE] Next, each detected noun or entity is disambiguated in the same way as in the single word disambiguation mode described above, yet the disambiguation re-sults are represented as annotations of a running text. The best matching sense is represented by a hypernym and an image as depicted in Figure 3.\nFootnote or Reference text (if exists): 10 http://www.scalanlp.org\nSection Titles (if exists):  ['3 Unsupervised Knowledge-Free Interpretable WSD', '3.3 User Interface for Interpretable WSD']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Website\n\n**Citation Reason:**  Source of Information/Methodology\n\n**Explanation:**\n\n* **Website:** The URL points to the website of Scalanlp,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010 [URL_CITE] ; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Awais Athar. 2010. Dependensee: A dependency parse visualization tool. http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [URL_CITE] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Software/Tool\n* **Citation Reason:**  Example/Comparison\n\n**Explanation:**\n\nThe URL points to a web demo of the \"Displa"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [URL_CITE] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test.\nFootnote or Reference text (if exists): 3 Scripts publicly available at https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2012T21\nCitation Context: To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [URL_CITE] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2012T21\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Material\n**TYPE:** Dataset\n**FUNCTION:**  SourceData \n\n**Explanation:**\n\n* **ROLE: Material:**  The artifact is a tangible resource (in this case,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Code\nTYPE: Implementation\nFUNCTION: Provides a framework for training and evaluating machine translation models. \n\n\n**Citation Reason:** The authors cite the URL to their GitHub repository as the location where their implementation of the machine translation system is available for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt17/translation-task/preprocessed/\nCitation Context: All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://data.statmt.org/wmt17/translation-task/preprocessed/\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Material\n* **TYPE:** Dataset\n* **FUNCTION:** Use \n\n**Reasoning:**\n\n* **Dataset:** The URL points to a directory containing preprocessed datasets for machine"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isw3.naist.jp/IS/TechReport/report-list.html#2009\nCitation Context: A coordination is indicated by a specific relation of type \u201cand\u201d in the semantic frame. The scope of conjuncts (where a conjunct may be a word, or a series of words) can be obtained by combining this information with that of the syntactic tree. The detail of this procedure can be found in (Okuma et al., 2009) [URL_CITE] .\nFootnote or Reference text (if exists): H. Okuma, M. Shimbo, K. Hara, and Y. Matsumoto. 2009. Bypassed alignment graph for learning coor-dination in Japanese sentences: supplementary ma-terials. Tech. report, Grad. School of Information Science, Nara Inst. Science and Technology. http://isw3.naist.jp/IS/TechReport/report-list.html#2009.\nSection Titles (if exists):  ['4 Experimental setup', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.github.io\nCitation Context: \u2022 We convert the representation of coordination structures into a format inspired by Universal Dependencies. [URL_CITE] In the original Treex anno-tation style, the conjunction heads both con-juncts, whereas in our modification, the first conjunct is at the top, heading the coordina-tion and the second conjunct (see Figure 4).\nFootnote or Reference text (if exists): 8 http://universaldependencies.github.io\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Framework/Standard\n\n**Citation Reason:**  Inspiration/Methodology\n\n**Explanation:**\n\n* **Framework/Standard:** Universal Dependencies (UD) is a widely recognized"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Table 6 shows the results. Our joint parser outperforms the other parsers, including the neu-ral network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] . Our data-driven parsers yield relatively low performance in the ObQ and RNR constructions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Paper\n\n**Citation Reason:**  Benchmarking/Comparison\n\n**Explanation:**\n\nThe citation is used to compare the performance of the authors' joint parser against a previously"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). [URL_CITE] http://hierplane.allenai.org/explain/ with http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). http://hierplane.allenai.org/explain/ with [URL_CITE] http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Example \n\n\n**Reasoning:**\n\n* **Supplement:** The URL points to a specific visualization example within the broader HierPlane system. It's not a core theoretical concept or methodology but rather an"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [URL_CITE] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Multiple general machine learning frame-works, such as SCIKIT - LEARN (Pedregosa et al., 2011), P Y T ORCH , T HEANO (Theano Develop-ment Team, 2016) and T ENSOR F LOW (Abadi et al., 2015) [URL_CITE] , among others, enable quick proto-typing and deployment of ML models. However, unlike J ACK , they do not offer a simple framework for defining and evaluating MR models.\nFootnote or Reference text (if exists): Mart\u00edn Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software Library\n\n**Citation Reason:**  \n\n* **Contextual Introduction:** The citation introduces TensorFlow as one of several general-purpose machine learning frameworks. \n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/docs/datasets/master/#\nCitation Context: We train each model on a NVIDIA RTX 8000 GPU (48GB memory) for maximum 200 epochs with a batch size of 64 and a learning rate linearly decaying from 5e-5. Training ends if the validation set loss has not decreased for [URL_CITE] epochs. Early stopping occurs within 15 epochs for most mod-els.\nFootnote or Reference text (if exists): 10 https://huggingface.co/docs/datasets/master/#\nSection Titles (if exists):  ['A Additional Information', 'A.4 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/havakv/pycox\nCitation Context: The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [URL_CITE] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.\nFootnote or Reference text (if exists): 2 https://github.com/havakv/pycox\nSection Titles (if exists):  ['4 Experiments', '4.3 Training']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Implementation Details\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository (`https://github.com/hav"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/and\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "This URL points to the main website of the Carnegie Mellon University Computer Science department. \n\n**Artifact Type:**  Website\n\n**Citation Reason:**  The URL is likely cited as the general location where the SATED dataset can be found.  The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mynlp/jigg\nCitation Context: We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg [URL_CITE] (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.\nFootnote or Reference text (if exists): 9 https://github.com/mynlp/jigg\nSection Titles (if exists):  ['6 Experiments', '6.2 Japanese Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Software \n* **Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\nThe citation points to the GitHub repository of the \"Jigg\" software,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html\nCitation Context: On the TED corpus, we obtained general pur-pose multilingual embeddings for 11 target lan-guages, by considering the (auxiliary) task of reconstructing pre-trained English word vectors. The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest re-ported scores on 10 out of these 11 languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 We provide the trained embeddings at http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nCitation Context: We use hierarchical clustering (Rokach and Maimon, 2005) [URL_CITE] method to group the languages based on language embeddings. We use the el-bow method (Thorndike, 1953) to automatically decide the optimal number of clusters K. Note that we have tried to extract the language embed-dings from multiple model checkpoints randomly chosen in the later training process, and found that the clustering results based on these language em-beddings are stable.\nFootnote or Reference text (if exists): 10 https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.cluster.hierarchy.linkage.html\nSection Titles (if exists):  ['5 Results', '5.1 Results of Language Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Code/Library\n\n**Citation Reason:**  Documentation\n\n**Explanation:**\n\n* The URL points to the documentation for the `linkage` function within the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.answers.com\nCitation Context: For h W2W , GIZA++ is used to train word alignments on 11.6M \u2018question-related question\u2019 pairs (Fader et al., 2013) crawled from WikiAnswers. [URL_CITE] . For h W2V , Word2Vec (Mikolov et al., 2013) is used to train word embedding on sentences from Wikipedia in English.\nFootnote or Reference text (if exists): 7 http://wiki.answers.com\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2006/\nCitation Context: The Chinese word \u201cHE ZHANG,\u201d denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen-tence pairs from the NIST Open Machine Trans-lation (OpenMT) 2006 evaluation [URL_CITE] . The resulting alignment is shown in Figure 1(b).\nFootnote or Reference text (if exists): 2 http://www.itl.nist.gov/iad/mig/tests/mt/2006/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**Artifact Type:**  Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to the NIST Open Machine Translation (OpenMT) 200"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~ark/TweetNLP/\nCitation Context: We preprocessed Twitter dataset with Twitter NLP tool [URL_CITE] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation.\nFootnote or Reference text (if exists): 9 http://www.cs.cmu.edu/\u02dcark/TweetNLP/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn/tree/master/data\nCitation Context: (2019). [URL_CITE] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets.\nFootnote or Reference text (if exists): 10 https://github.com/sameenmaruf/selective-attn/tree/master/data\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Data\n**TYPE:** Dataset\n**FUNCTION:**  Description of Data Preprocessing \n\n\n**Explanation:**\n\n* **Data:** The URL points to a GitHub repository directory"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [URL_CITE] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia.\nFootnote or Reference text (if exists): 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)', '3.1 Inducing Crosslingually Constrained Word Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opus.nlpl.eu/\nCitation Context: For our benchmark, we leverage existing parallel data for each of our language pairs. This data is derived from two main sources: (1) open-source repository of parallel corpora, OPUS [URL_CITE] (Tiedemann, 2012) and (2) ParaCrawl (Espl\u00e0 et al., 2019). From OPUS, we use the JW300 corpus (Agic\u0301 and Vulic\u0301, 2019), OpenSubtitles (Lison and Tiedemann, 2016), XhosaNavy, Memat, and QED (Abdelali et al., 2014).\nFootnote or Reference text (if exists): 3 http://opus.nlpl.eu/\nSection Titles (if exists):  ['2 A FRO MT benchmark', '2.2 Data Sources']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Material\n**TYPE:**  DataSource\n**FUNCTION:**  Data  \n\n**Reasoning:**\n\n* **Material:** The URL points to a repository (OPUS) that provides tangible"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuzhaopeng/NMT-Coverage\nCitation Context: Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/MiuLab/CLUSE\nCitation Context: The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and dataset are available at http://github.com/MiuLab/CLUSE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code and Dataset\n\n**Citation Reason:**  Reproducibility\n\n**Explanation:**\n\n* **Code and Dataset:** The footnote explicitly states that both the code and dataset are"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.org/Data/QA/QC/\nCitation Context: \u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [URL_CITE] .\nFootnote or Reference text (if exists): 5 http://cogcomp.org/Data/QA/QC/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: \u2022 Logistic regression classifier: [URL_CITE] http://scikit-learn.org/stable/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Software Library\n* **Citation Reason:**  Documentation/Reference\n\n**Explanation:**\n\nThe URL points to the official documentation for scikit-learn, a popular machine learning library"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/download/details.aspx?id=52419\nCitation Context: WikiQA [URL_CITE] is a retrieval-based question answer-ing dataset based on Wikipedia. We follow the data split of original paper.\nFootnote or Reference text (if exists): 6 https://www.microsoft.com/en-us/download/details.aspx?id=52419\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Material\n**TYPE:** Dataset\n**FUNCTION:** Introduce \n\n**Reasoning:**\n\n* **Dataset:** The citation context clearly states that TrecQA is a dataset designed for answer"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2011/03/numbers.html\nCitation Context: Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! [URL_CITE] Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in.\nFootnote or Reference text (if exists): 2 In fact, the peak record is 6,939 tweets per second, reported by http://blog.twitter.com/2011/03/numbers.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nCitation Context: Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. [URL_CITE] This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in. A hypothetical tweet recommendation system could alleviate this acute information overload, e.g., by limiting the stream of tweets to those of interest to the user, or by discovering intriguing content outside the user\u2019s following network.\nFootnote or Reference text (if exists): 3 See http://engineering.twitter.com/2011/05/engineering-behind-twitters-new-search.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Blog Post\n\n**Citation Reason:**  Supporting Evidence\n\n**Explanation:**\n\n* **Blog Post:** The URL points to a blog post on the Twitter Engineering blog"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/sls/downloads/\nCitation Context: Table 1 shows the distribution over the stance labels, [URL_CITE] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.\nFootnote or Reference text (if exists): 7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [URL_CITE] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.\nFootnote or Reference text (if exists): 3 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Background/Context:** The citation introduces the Fake News Challenge (FNC) dataset as a well-known resource for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com/toolkit/\nCitation Context: Research has shown that less qualified transla-tors are able to increase their productivity and qual-ity disproportionally when given automatic assis-tance (Koehn and Haddow, 2009). Assistance may be as limited as offering machine translation in a post-editing environment, as for instance provided by Google Translator Toolkit [URL_CITE] (Galvez and Bhansali, 2009) which provides a special function to translate Wikipedia articles.\nFootnote or Reference text (if exists): 8 http://translate.google.com/toolkit/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Thus our method some-times acts as an efficient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour-aged by the Wikipedia community, [URL_CITE] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.\nFootnote or Reference text (if exists): 8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations\nSection Titles (if exists):  ['6 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Policy Document\n\n**Citation Reason:**  \n\n* **Contextual Background:** The citation is used to provide context about the Wikipedia community's stance on machine translation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/cgi-bin/cmudict\nCitation Context: To extract this feature, we identify and count the number of alliteration/rhyme chains in each exam-ple in our data set. The chains are automatically ex-tracted using an index created on top of the CMU pronunciation dictionary [URL_CITE] .\nFootnote or Reference text (if exists): 2 Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict\nSection Titles (if exists):  ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wndomains.itc.it\nCitation Context: To form a lexicon required for the identification of this feature, we extract from W ORD N ET D OMAINS [URL_CITE] all the synsets labeled with the domain S EXUALITY . The list is further processed by removing all words with high polysemy (\u2265 4).\nFootnote or Reference text (if exists): 3 W ORD N ET D OMAINS assigns each synset in W ORD N ET with one or more \u201cdomain\u201d labels, such as S PORT , M EDICINE , E CONOMY . See http://wndomains.itc.it.\nSection Titles (if exists):  ['3 Automatic Humor Recognition', '3.1 Humor-Specific Stylistic Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to WORDNET DOMAINS, which is a resource"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://farm2.user.srcf.net/research/bagel/\nCitation Context: (2010), which fits our usage scenario in a spoken dialogue sys-tem and is freely available. [URL_CITE] It contains a to-tal of 404 sentences from a restaurant informa-tion domain (describing the restaurant location, food type, etc. ), which correspond to 202 dia-logue acts, i.e., each dialogue act has two para-phrases.\nFootnote or Reference text (if exists): 7 Available for download at: http://farm2.user.srcf.net/research/bagel/.\nSection Titles (if exists):  ['5 Experimental Setup', '5.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION:  Implementation Detail \n\n\n**Explanation:**\n\n* **ROLE: Material:** The artifact is a tangible resource (a dataset) used in the research.\n* **TYPE: Dataset:** The artifact is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by (Diaz et al., 2016). The implementation of Word2Vec [URL_CITE] from (Mikolov et al., 2013) is employed. In par-ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 \u22123 .\nFootnote or Reference text (if exists): 1 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Evaluation', '3.1 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Baroni et al. develop the word embeddings using word2vec [URL_CITE] from a cor-pus of about 2.8 billion tokens, using the Continuous Bag-of-Words ( CBOW ) model proposed by Mikolov et al. (2013).\nFootnote or Reference text (if exists): 6 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['4 Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Tool\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* **Tool:** The URL points to the source code repository for the word2vec tool, a"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ruidan/IMN-E2E-ABSA\nCitation Context: We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN [URL_CITE] , RACL , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models.\nFootnote or Reference text (if exists): 6 IMN code available at https://github.com/ruidan/IMN-E2E-ABSA.\nSection Titles (if exists):  ['4 Modeling', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPWM-WHU/RACL\nCitation Context: We compare our proposed graph prediction ap-proach with three state-of-the-art baselines for extracting targets and expressions and predicting the polarity: IMN , RACL [URL_CITE] , as well as RACL-BERT, which also incorporates contextualized em-beddings. Instead of using BERT Large , we use the cased BERT-multilingual-base in order to fairly compare with our own models.\nFootnote or Reference text (if exists): 7 https://github.com/NLPWM-WHU/RACL.\nSection Titles (if exists):  ['4 Modeling', '4.3 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Code\n* **FUNCTION:** Baseline Comparison \n\n\n**Explanation:**\n\nThe citation clearly states that RACL is being used as a baseline method for comparison"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://framenet.icsi.berkeley.edu/\nCitation Context: We use FrameNet-style (Fillmore, 1976) se-mantic frames as our main source of weak supervision. [URL_CITE] We also explore named entity recognition (NER).\nFootnote or Reference text (if exists): 1 See http://framenet.icsi.berkeley.edu/\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: As sources of weak supervision providing slot can-didates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) \u2013 a union of labels pro-vided by both parsers is used in all our setups. In ad-dition, to explore combined sources on the named-entity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy. [URL_CITE] To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.\nFootnote or Reference text (if exists): 10 https://spacy.io\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Software Library \n* **Citation Reason:**  The authors mention using SpaCy, a popular natural language processing library, for a specific task: providing"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://about.Twitter.com/company\nCitation Context: In recent years, short texts are increasingly preva-lent due to the explosive growth of online social media. For example, about 500 million tweets are published per day on Twitter [URL_CITE] , one of the most popular online social networking services. Proba-bilistic topic models (Blei et al., 2003) are broadly used to uncover the hidden topics of tweets, s-ince the low-dimensional semantic representation is crucial for many applications, such as prod-uct recommendation (Zhao et al., 2014), hashtag recommendation (Ma et al., 2014), user interest tracking (Sasaki et al., 2014), sentiment analysis (Si et al., 2013).\nFootnote or Reference text (if exists): 1 See https://about.Twitter.com/company\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/\nCitation Context: Social media platforms, particularly microblog-ging services such as Twitter, have become in-creasingly popular (Statista, 2019) [URL_CITE] as a means to express thoughts and opinions. Twitter users emit tweets about a wide variety of topics, which vary in the extent to which they reflect a user\u2019s person-ality, brand and interests. This observation mo-tivates the question we consider here, of how to quantify the degree to which tweets are character-istic of their author?\nFootnote or Reference text (if exists): Statista. 2019. Twitter: Num-ber 2010-2018.of active users (https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users/).\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Statistical Data\n\n**Citation Reason:**  Supporting the claim about the popularity of Twitter. \n\nThe citation uses Statista data to back up the statement that Twitter is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/\nCitation Context: We used the \u201cModApte\u201d split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [URL_CITE] . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ).\nFootnote or Reference text (if exists): Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/\nSection Titles (if exists):  ['5 Results', '5.1 Reuters-21578']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Direct Use:** The sentence explicitly states \"We used the \u201cModApte\u201d split of the Reuters-2"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm\nCitation Context: 1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM (version 3.20) (Chang and Lin, 2011) [URL_CITE] . It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification.\nFootnote or Reference text (if exists): Chang, C-C. and Lin, C-J. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intel-ligent Systems and Technology, 2:27:1--27:27, http://www.csie.ntu.edu.tw/~cjlin/libsvm\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/\nCitation Context: 1-vs-Rest multiclass SVM (1-vs-rest-SVM). This is the standard 1-vs-Rest multiclass SVM with Platt Probability Estimation (Platt, 2000), and it is implemented based on LIBSVM [URL_CITE] (version 3.20) (Chang and Lin, 2011). It works in the same way as the proposed cbsSVM (Section 3.4) except that it uses the document space classification.\nFootnote or Reference text (if exists): 1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software Library\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* **Software Library:** The URL points to the LIBSVM tools website, indicating a software library"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [URL_CITE] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Method\n**TYPE:** Tool\n**FUNCTION:**  Data Processing (Specifically, Part-of-Speech Tagging) \n\n**Citation Reason:**\n\nThe authors are citing this URL"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [URL_CITE] . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 3 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website\n* **Citation Reason:** Data Source \n\n**Explanation:**\n\nThe citation is used to identify the source from which the researchers obtained the 3,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/sgl\nCitation Context: Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at [URL_CITE] https: //github.com/SapienzaNLP/sgl.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svn.nlpl.eu/mrp/2019/public/resources.txt\nCitation Context: Finally, we wish to point out that direct com-parability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT ) whose usage was forbidden to the shared task par-ticipants. [URL_CITE] Nevertheless, we believe that their re-sults are needed here to better contextualize the performances SGL obtains.\nFootnote or Reference text (if exists): 19 Allowed resources are specified at: http://svn.nlpl.eu/mrp/2019/public/resources.txt\nSection Titles (if exists):  ['5 Results', '5.3 UCCA Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Resource\nTYPE: List\nFUNCTION: Contextualization \n\n\n**Explanation:**\n\n* **ROLE: Resource:** The URL points to a list of allowed resources for a specific shared task (likely a machine translation or parsing task). \n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.markwatson.com/opensource/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [URL_CITE] etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 9 http://www.markwatson.com/opensource/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://technologies.kmi.open.ac.uk/aqualog/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [URL_CITE] , NLBean etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 8 http://technologies.kmi.open.ac.uk/aqualog/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Example\n\n**Explanation:**\n\nThe URL points to a specific tool called Aqualog, which is a structured QA system."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yym6472/OVSlotTagging\nCitation Context: (3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/yym6472/OVSlotTagging\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Reproducibility\n\n**Explanation:**\n\n* The URL points to a GitHub repository, which typically houses source code.\n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nCitation Context: BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45). [URL_CITE] The basic overview of the VerbNet structure already suggests that measur-ing verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.\nFootnote or Reference text (if exists): 6 https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kilgarriff.co.uk/bnc-readme.html\nCitation Context: We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997). [URL_CITE] After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC). Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list.\nFootnote or Reference text (if exists): 9 https://www.kilgarriff.co.uk/bnc-readme.html\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset Documentation\n\n**Citation Reason:**  Source of Frequency Data\n\n**Explanation:**\n\n* The URL points to a README file for the British National Corpus (BN"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/software_view/Wikifier\nCitation Context: Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011) [URL_CITE] , a context-sensitive system for disam-biguation to Wikipedia. Using context-sensitive dis-ambiguation to Wikipedia as well as high-precision set of knowledge attributes allows us to inject the knowledge to more mention pairs when compared to (Rahman and Ng, 2011). Our exact heuristic for injecting knowledge attributes to mentions is as fol-lows:\nFootnote or Reference text (if exists): 7 Available at: http://cogcomp.cs.illinois.edu/page/software_view/Wikifier\nSection Titles (if exists):  ['3 Wikipedia as Knowledge', '3.2 Injecting Knowledge Attributes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Efraim\nCitation Context: If the mention head is an NE matched to a Wikipedia page p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map \u201cEphraim Sneh\u201d to [URL_CITE] http://en.wikipedia.org/wiki/Efraim Sneh and to assign it the Israeli nationality, male gender, and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Wikipedia as Knowledge', 'Named Entities with Wikipedia Disambiguation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Data\nTYPE: Wikipedia Page\nFUNCTION:  Example/Demonstration \n\n\n**Explanation:**\n\n* **ROLE: Data:** The citation points to a specific Wikipedia page as a source of information (knowledge attributes) about a named entity. "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cindyxinyiwang/TrDec_pytorch\nCitation Context: Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sweetalyssum/Seq2Seq-DU\nCitation Context: Experimental results on benchmark datasets show that Seq2Seq-DU [URL_CITE] performs much better than the baselines on SGD, MultiWOZ2.2, and Multi-WOZ2.1 in multi-turn dialogue with schema de-scriptions, is superior to BERT-DST on WOZ2.0, DSTC2, and M2M, in multi-turn dialogue with-out schema descriptions, and works equally well as Joint BERT on ATIS and SNIPS in single turn dialogue (in fact, it degenerates to Joint BERT).\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/sweetalyssum/Seq2Seq-DU.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Reproducibility\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which strongly suggests the artifact is code"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nCitation Context: It is a sparsity-enhanced non-probabilistic topic model. We use the code released by the authors [URL_CITE] . We set the regularization constants as \u03bb = 0.3, \u03c1 = 0.0001 and the maximum number of itera-tions of hierarchical sparse coding, dictionary learning as 100.\nFootnote or Reference text (if exists): 5 http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.jhu.edu/~jason/tutorials/variational.html\nCitation Context: Inference in this model class in intractable, so as has been done in previous work on topic modeling (Ng and Jordan, 2003) we rely on variational meth-ods to approximate the gradients needed during training as well as the posteriors over the topic bit vectors. Specifically, we will need the gradients of the normalizer and the sum of the energy function over the hidden variables which we refer to as the marginal energy. Follow-ing the approach described for undirected models by Eisner (2011) [URL_CITE] , we approximate these quantities and their gradients with respect to the model pa-rameters \u03b8 as we will now describe (thread-level embeddings are omitted in this section for clarity).\nFootnote or Reference text (if exists): Jason Eisner. 2011. High-level explanation of varia-tional inference. https://www.cs.jhu.edu/~jason/tutorials/variational.html. Accessed: 2018-02-22.\nSection Titles (if exists):  ['3 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Tutorial\n\n**Citation Reason:**  \n\n* **Explanatory:** The citation is used to explain the authors' approach to variational inference, specifically referencing Eisner'"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.5281/zenodo.4064408\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Challenge/Benchmark\n**TYPE:**  Code & Data\n**FUNCTION:**  Evaluation & Benchmarking \n\n\n**Reasoning:**\n\n* **Challenge Set:** The citation context explicitly"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/\nCitation Context: We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [URL_CITE] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.\nFootnote or Reference text (if exists): 5 http://www.itl.nist.gov/iad/mig/tests/mt/\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Software\nTYPE: Parser\nFUNCTION:  Parsing  \n\n\nLet me break down why:\n\n* **ROLE: Software** - The citation refers to \"English and Chinese parsers,\" indicating a software tool used for processing language.\n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/CRF-NER.shtml\nCitation Context: To assign types to arguments, we apply Stanford Named Entity Recognizer (Finkel et al., 2005) [URL_CITE] , and also look up the argument in WordNet 2.1 and record the first three senses if they map to our target se-mantic types. We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to .\nFootnote or Reference text (if exists): 5 We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and used the seven class CRF model distributed with it.\nSection Titles (if exists):  ['3 Modeling Relational Co-occurrence', '3.1 Relations Extraction and Representation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**Artifact Type:** Tool\n\n**Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\n* **Tool:** The Stanford Named Entity Recognizer (CRF-NER) is a software tool used for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017), and Fried-man et al. (2017) [URL_CITE] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016), Kasai et al. (2017) [URL_CITE] , and Fried-man et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Paper\n* **Citation Reason:**  Supporting Previous Work\n\n**Explanation:**\n\nThe citation of Kasai et al. (2017) is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\nCitation Context: \u2022 The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017): [URL_CITE] https: //github.com/facebookresearch/ fastText/blob/master/ pretrained-vectors.md\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.fb.com/fasttext/\nCitation Context: 1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016) [URL_CITE] (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768).\nFootnote or Reference text (if exists): Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).\nSection Titles (if exists):  ['4 Approaches to authorship verification', '4.4 Approach 4: Document embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Software/Library\n\n**Citation Reason:**  \n\n* **Acknowledgement of Source:** The citation acknowledges the origin of the FastText embeddings used in the research."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.open.weibo.com/wiki/2/search/topics\nCitation Context: Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags on Weibo and used the hashtag-search API [URL_CITE] to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.\nFootnote or Reference text (if exists): 7 http://www.open.weibo.com/wiki/2/search/topics\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://open.weibo.com/wiki/Trends/hourly\nCitation Context: Note that we do not consider retweet rela-tions, i.e., reposting behaviors on Twitter, because retweets provide limited extra textual information for the reason that Twitter did not allow users to add comments in retweets until 2015. To build the Weibo dataset, we tracked real-time trending hashtags [URL_CITE] on Weibo and used the hashtag-search API to crawl the posts matching the given hash-tag queries. In the end, a large-scale Weibo corpus is built containing Weibo messages posted during January 2nd to July 31st, 2014.\nFootnote or Reference text (if exists): 6 http://open.weibo.com/wiki/Trends/ hourly\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Data Source\n\n**Citation Reason:**  \n\n* **Describes the source of trending hashtags:** The citation points to the Weibo API documentation for accessing trending topics (\""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/explosion/spaCy\nCitation Context: Data Preprocessing For Reddit data (i.e. r/AskScience and r/AskHistorians), we lower-cased, removed redundant spaces/urls, and used the Spacy [URL_CITE] library to tokenize each sentence. For training GloVe embeddings, we substi-tuted \u201c\u201d symbols in WikiText-103 with \u201c\u201d symbols.\nFootnote or Reference text (if exists): 11 https://github.com/explosion/spaCy The word embeddings are trained with GloVe on r/AskScience. The word embeddings are trained with GloVe on WikiText-103. The word embeddings are trained with SGNS on r/AskHistorians. SGNS word embeddings trained with three iterations or 100 dimensions on r/AskHistorians. dings trained with three iterations or 100 dimensions on r/AskHistorians.\nSection Titles (if exists):  ['B Environmental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Preprocessing Each dataset is tokenized using the spaCy [URL_CITE] tokenizer, converted to lowercase, and non-\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['4 Model Training', '4.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Software/Library\n\n**Explanation:**\n\nThe URL points to the official website of spaCy, a popular open-source"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.linguist.is/icelandic_treebank\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [URL_CITE] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset/Corpus\n\n**Citation Reason:**  \n\n* **Source Identification:** The URL points to the website hosting the York-Toronto-Helsinki Parsed Corpus of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: An input pair (A i ,A s ) is then associated with a grade g(A i , A s ) = u T \u03c8(A i , A s ) computed as a lin-ear combination of features. The weight vector u is trained to optimize performance in two scenarios: Regression: An SVM model for regression (SVR) is trained using as target function the grades as-signed by the instructors. We use the libSVM [URL_CITE] im-plementation of SVR, with tuned parameters.\nFootnote or Reference text (if exists): 7 http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nSection Titles (if exists):  ['6 http://Infomap-nlp.sourceforge.net/']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EducationalTestingService/skll\nCitation Context: Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [URL_CITE]\nFootnote or Reference text (if exists): 2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.\nSection Titles (if exists):  ['3 Models for Short Answer Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:**  Software Library\n\n**Citation Reason:** Implementation Detail\n\n**Explanation:**\n\n* **Software Library:** The URL points to a GitHub repository for the \"skll"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html\nCitation Context: In the second experiment, we use the reverse order of the two corpora and denote it by \u201cB-to-WSJ\u201d. The phrase structures in the treebank are converted into dependencies using Penn2Malt tool [URL_CITE] with the stan-dard head rules (Yamada and Matsumoto, 2003).\nFootnote or Reference text (if exists): 3 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  \n\n* **Functionality:** The citation explicitly states that the Penn2Malt tool was used to convert phrase structures in the treebank"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~ravichan/YASMET.html\nCitation Context: Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smooth-ing and using incremental feature selection as de-scribed in (Berger et al., 1996) and implemented in the yasmetFS package. [URL_CITE] Note that restricting our-selves to only backward looking features decreases the performance significantly, as we can see in Ta-ble 2.\nFootnote or Reference text (if exists): 6 http://www.isi.edu/\u02dcravichan/YASMET.html\nSection Titles (if exists):  ['3 Adjacency Pairs', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cog.brown.edu/\u223cmj/software.htm\nCitation Context: There are 0.8M fea-tures in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M fea-tures in the updated version. [URL_CITE] However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Ta-bles 3 and 4). This result confirms that our feature set design is appropriate, and the averaged percep-tron learner is a reasonable candidate for reranking.\nFootnote or Reference text (if exists): 5 http://www.cog.brown.edu/\u223cmj/software.htm. We follow this version as it corrects some bugs from their 2005 paper which leads to a 0.4% increase in performance (see Table 4).\nSection Titles (if exists):  ['packed forest for each sentence. 3', '5.1 Data Preparation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Software\n\n**Citation Reason:**  Comparison\n\n**Explanation:**\n\n* The citation refers to a specific version of software (\"Charniak and Johnson\") and its"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xijiz/cfgen\nCitation Context: As a result, our method eliminates part of the spu-rious correlations between context representa-tion and output labels. The code is available at [URL_CITE] https://github.com/xijiz/cfgen.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GuyAllard/markov_clustering\nCitation Context: Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [URL_CITE] .\nFootnote or Reference text (if exists): 4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_ clustering\nSection Titles (if exists):  ['3 Strategies for Timeline Summarization', 'Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* The citation sentence explicitly states that the authors used a specific implementation of Markov Clustering (MCL) from the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/mteval.shtml\nCitation Context: M T +R TE R uses all M T R and R TE R features, combining matching and entailment evidence. [URL_CITE]\nFootnote or Reference text (if exists): 3 Software for R TE R and M T +R TE R is available from http://nlp.stanford.edu/software/mteval.shtml.\nSection Titles (if exists):  ['4 Experimental Evaluation', '4.4 Combination Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.bbn.com/ears\nCitation Context: All systems were evaluated using rteval (Rich Tran-scription Evaluation) version 2.3 (Kubala and Srivastava, 2003) [URL_CITE] . Rteval aligns the system output to the annotated reference transcripts in such a way as to minimize the lex-eme error rate.\nFootnote or Reference text (if exists): Francis Kubala and Amit Srivastava. 2003. A Framework for Evaluating Rich Transcription Technology. BBN Ears Web-site. http://www.speech.bbn.com/ears.\nSection Titles (if exists):  ['4 Results', '4.2 Top Rules Learned']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Evaluation Metric\n**TYPE:** Software\n**FUNCTION:**  Introduce\n\n**Citation Reason:**\n\nThe citation introduces the specific software tool (`rteval`) used to evaluate the performance of the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/Anna146/CHARM\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Based on the provided information, here's a classification of the artifact type and citation reason:\n\n**Artifact Type:**  **Repository or Website**\n\n**Citation Reason:**  **Background/Related Work**\n\n**Explanation:**\n\n* **Repository"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/universal-tagger/\nCitation Context: We made our code are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://code.google.com/p/universal-tagger/\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code \n* **Citation Reason:**  Data and Code Availability \n\n**Explanation:**\n\nThe citation context clearly states \"Our data and code are available online.\" This indicates"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [URL_CITE] (d = 768). We imple-mented the HAN following Yang et al.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-transformers\nCitation Context: Following our proposed protocol, we report base-line results from two strong sentence encoder mod-els: BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019), using a publicly available codebase. [URL_CITE] See Appendix C for training details. We present L2 results on PDTB 2.0 in Table 1 and results on PDTB 3.0 in Table 2 (see Appendix D for L1 re-sults).\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-transformers\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol', '3.1 Baseline results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\nThe citation points to the `pytorch-transformers` repository on GitHub. This repository provides pre-trained models"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca-demic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact type and citation reason:\n\n* **Artifact Type:**  Code and Data \n\n* **Citation Reason:**  Data and code availability. The authors are providing access to the dataset and code used in their research"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cocodataset.org/\nCitation Context: We train our contextual token-image matching model (in Sec. 3.1) on MS COCO image cap-tioning dataset [URL_CITE] for 20 epochs. The concatena-tion of the last 4 layers of BERT outputs (fol-lowing Devlin et al.\nFootnote or Reference text (if exists): 8 http://cocodataset.org/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* The URL points to the official website of the MS COCO dataset.\n* The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). Kasai et al.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). Kasai et al.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Paper \n* **Citation Reason:**  Supporting Evidence\n\n**Explanation:**\n\nThe citation is used to support the claim that data-driven transition-based"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Documentation\n\n**Citation Reason:**  Source of Information\n\n* **Explanation:** The URL points to the Google Developers documentation for Freebase. The citation sentence indicates"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuzhaopeng/NMT-Coverage\nCitation Context: Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GChrysostomou/tasc.git\nCitation Context: Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at: https://github.com/GChrysostomou/tasc.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Implementation/Reproducibility\n\n**Explanation:**\n\nThe citation context states that the authors are comparing their method (TaSc) to other interpre"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [URL_CITE] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains.\nFootnote or Reference text (if exists): 3 https://github.com/SimonHFL/CWEB\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Data Availability\n\nThe citation provides the URL to the CWEB dataset, which is a key contribution of the work.  The authors"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/orenmel/context2vec\nCitation Context: One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models [URL_CITE] . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context.\nFootnote or Reference text (if exists): 7 https://github.com/orenmel/context2vec\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/elmo\nCitation Context: We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [URL_CITE] . We compare ELMo\u2019s contextual embeddings as-is, and also after con-catenating them with each of Tgt, SrcTune, and SrcSel embeddings in Table 8.\nFootnote or Reference text (if exists): 11 https://allennlp.org/elmo\nSection Titles (if exists):  ['Topics and tasks', '4.4 Contextual embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software/Model\n\n**Citation Reason:**  \n\n* **Description:** The citation provides a brief description of ELMo as a \"contextualized word representation model\""
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [URL_CITE] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 1 https://slack.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Website\n\n**Citation Reason:**  \n\n* **Example/Illustration:** The citation is used to provide a concrete example of an online group chat channel. Slack is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/MUSE\nCitation Context: We also propose a joint-learning model with multilingual word embedding. We take advantage of the availability of mul-tilingual word embeddings [URL_CITE] to build a joint-learning model. Figure 1 summarize how the data is transformed and learned in this model.\nFootnote or Reference text (if exists): 7 https://github.com/facebookresearch/ MUSE\nSection Titles (if exists):  ['5 Cross-lingual Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dadangewp/ACL19-SRW\nCitation Context: Last, we take advantage of the availability of multilingual word embeddings to build a joint-learning approach in the cross-lingual setting. All code and resources are available at [URL_CITE] https://github.com/dadangewp/ACL19-SRW.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**Artifact Type:** Code\n\n**Citation Reason:**  Providing access to the implementation\n\nThe citation clearly states that the URL points to the code and resources used to implement the joint-learning approach described"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.uexpress.com/dearabby/archives\nCitation Context: We use a situation to denote the one-sentence prompt given to a worker as the basis for writ-ing RoTs. We gather a total of 104k real life situations from four domains: scraped titles of posts in the subreddits r/confessions (32k) and r/amitheasshole (r/AITA, 30k), which largely focus on moral quandaries and interper-sonal conflicts; 30k sentences from the ROCSto-ries corpus (rocstories, Mostafazadeh et al., 2016); and scraped titles from the Dear Abby ad-vice column archives [URL_CITE] (dearabby, 12k).\nFootnote or Reference text (if exists): 3 https://www.uexpress.com/dearabby/archives\nSection Titles (if exists):  ['3 S OCIAL -C HEM -101 Dataset', '3.1 Situations']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:**  Text Corpus\n\n**Citation Reason:** Data Source\n\nThe citation is used to acknowledge the Dear Abby advice column archives as a source of real-life situations used in the creation of a dataset. \n\n\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://math-qa.github.io/math-QA/\nCitation Context: The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at: [URL_CITE] https: //math-qa.github.io/math-QA/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ai.google.com/research/atNaturalQuestions/visualization\nCitation Context: Recently, a new benchmark MRC dataset called Natural Questions [URL_CITE] (NQ) (Kwiatkowski et al., 2019) has presented a substantially greater chal-lenge for the existing MRC models. Specifically, there are two main challenges in NQ compared to the previous MRC datasets like SQuAD 2.0.\nFootnote or Reference text (if exists): 2 NQ provides some visual examples of the data https://ai.google.com/research/atNaturalQuestions/visualization.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Visualization/Dataset Explorer\n* **Citation Reason:**  Supplementary Material\n\n**Explanation:**\n\nThe URL points to a visualization tool for the Natural Questions ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://news.google.com/\nCitation Context: Online news platforms such as Google News [URL_CITE] and MSN News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 1 https://news.google.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: Online news platforms such as Google News and MSN [URL_CITE] News have gained huge popularity for on-line digital news reading (Das et al., 2007). Tens of thousands of news articles are streamed from various sources every day, making it very difficult for users to read all news to find their interested content (Phelan et al., 2011). Thus, personalized news recommendation is critical for online news platforms to target user interests and alleviate in-formation overload (IJntema et al., 2010).\nFootnote or Reference text (if exists): 2 https://www.msn.com/en-us/news SearchQueries whether\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Website\n\n**Citation Reason:**  \n\n* **Supporting Example:** The citation is used to provide a concrete example of an \"online news platform\" as discussed in"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: We hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state of the art. The KaggleDBQA dataset is available at [URL_CITE] https://aka.ms/KaggleDBQA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion & Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aka.ms/KaggleDBQA\nCitation Context: KaggleDBQA We introduce KaggleDBQA, a new dataset and evaluation setting for text-to-SQL parsers to bridge the gap between SOTA DBQA research and its real-life deployment. [URL_CITE] It systemati-cally addresses three aforementioned challenges:\nFootnote or Reference text (if exists): 1 Available at https://aka.ms/KaggleDBQA.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Dataset \n* **Citation Reason:**  Introduction and Definition\n\n**Explanation:**\n\nThe citation introduces the KaggleDBQA dataset as a key component of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wangcunxiang/Sen-Making-and-Explanation\nCitation Context: Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [URL_CITE] https://github.com/wangcunxiang/Sen-Making-and-Explanation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/genesis\nCitation Context: Along with an extensive analysis of G ENE S IS results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets and the code to repro-duce the experiments at [URL_CITE] https://github.com/SapienzaNLP/genesis.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Code and Data\n\n**TYPE:**  Software/Model and Dataset\n\n**FUNCTION:**  Reproducibility \n\n\n**Explanation:**\n\n* **Code and Data:** The citation explicitly mentions"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://infotabs.github.io/\nCitation Context: The dataset, along with associated scripts, are avail-able at [URL_CITE] https://infotabs.github.io/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: All datasets used in the experiments are available at [URL_CITE] https://github.com/Anna146/CHARM. We pro-vide IDs and texts of the posts used as training and test data for CHARM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset \n* **Citation Reason:**  Data Availability\n\n**Explanation:**\n\nThe citation explicitly states that the URL points to the location where the datasets used in the experiments are"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/svcca\nCitation Context: Ecco is built on top of open source libraries including Scikit-Learn (Pedregosa et al., 2011), Matplotlib (Hunter, 2007), NumPy (Walt et al., 2011), PyTorch (Paszke et al., 2019) and Trans-formers (Wolf et al., 2020). Canonical Correla-tion Analysis is calculated using the code open-sourced [URL_CITE] by the authors (Raghu et al., 2017; Mor-cos et al., 2018; Kornblith et al., 2019).\nFootnote or Reference text (if exists): 4 https://github.com/google/svcca\nSection Titles (if exists):  ['6 System Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [URL_CITE] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/jalammar/ecco\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Source Code\n\n**Explanation:**\n\nThe citation points to the GitHub repository containing the source code for the \"Ecco\" library.  The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [URL_CITE] . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 3 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website\n* **Citation Reason:** Source of Data \n\n**Explanation:**\n\nThe citation is used to identify the website (BBC News) from which the dataset"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/liblinear/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://liblinear.bwaldvogel.de/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Source\n\n* **Explanation:** The URL points to a website hosting a Java port of the LIBLINEAR library. The citation is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amzn.to/2qDjNcJ\nCitation Context: We have shown how it can be used to expand the set of supported use-cases to complex and cross-domain utterances, while leveraging a single compositional seman-tics. The representation has been released at AWS Re:Invent 2016 [URL_CITE] . It is also being used as a rep-resentation for expanded support for complex ut-terances, such as those with sequential composi-tion.\nFootnote or Reference text (if exists): 2 https://amzn.to/2qDjNcJ\nSection Titles (if exists):  ['6 Conclusions and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amzn.to/2qDjNcJ\nCitation Context: The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains [URL_CITE] . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and third-party applications.\nFootnote or Reference text (if exists): 1 https://amzn.to/2qDjNcJ\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software/API\n\n**Citation Reason:**  \n\n* **Release Announcement:** The citation is primarily used to announce the public release of the AMRL (Alexa Multi"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf\nCitation Context: There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1\u20135 for accuracy and fluency [URL_CITE] . We used 4 human judges, each judging 25 sentence pairs.\nFootnote or Reference text (if exists): 3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.\nSection Titles (if exists):  ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://blog.lawgeex.com/contractcosts/\nCitation Context: Cur-rently, legal review remains an arduous and expen-sive process. For instance, a procurement contract requires 5 hours of legal review on average, con-tributing to thousands of dollars in total cost (Cum-mins, 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Tim Cummins. 2017. Cost of processing a ba-sic contract soars to $6900 (accessed: 2021- 04-09). https://blog.lawgeex.com/contractcosts/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Blog Post\n* **Citation Reason:**  Supporting statistic \n\n**Explanation:**\n\nThe citation is used to support the claim that legal review is expensive. The blog post provides"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cornell.edu/People/pabo/movie-review-data/\nCitation Context: And 20NG is a collection of approximately 20,000 20-category documents . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset [URL_CITE] (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews.\nFootnote or Reference text (if exists): 2 http://www.cs.cornell.edu/People/pabo/movie-review-data/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Background/Context:** The citation is used to introduce the 20NG dataset as a standard corpus for text"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/JfV6J\nCitation Context: (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocess-ing [URL_CITE] as in Shen et al. (2019) and Kim et al.\nFootnote or Reference text (if exists): 4 https://git.io/JfV6J.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nCitation Context: (2019), who publishes a manually labeled sentence-level test set. [URL_CITE] Besides, they also publish a test set for evaluating noise-filtering ability. Details of the adopted dataset are shown in Table 1.\nFootnote or Reference text (if exists): 4 https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2019-ARNOR\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code Repository\n* **Citation Reason:**  Data Source\n\n**Explanation:**\n\nThe citation points to a GitHub repository containing a manually labeled sentence-level test set and a test"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets?search=kilt\nCitation Context: KILT data and code are available at https://github.com/facebookresearch/KILT. [URL_CITE]\nFootnote or Reference text (if exists): 1 and at https://huggingface.co/datasets? search=kilt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/songhaoyu/KvPI\nCitation Context: The code, data, and trained model are available at [URL_CITE] https://github.com/songhaoyu/KvPI.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Task I: Reranking Retrieved Responses', '5.5 Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:**  Reproducibility\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which is a common"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dykang/xslue\nCitation Context: The preprocessed datasets and code are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dykang/xslue\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code and Dataset\n* **Citation Reason:**  Providing access to the preprocessed datasets and the code used in the research. \n\n\nLet me know if you have any other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Recently, some attention has also been given to SMT decoding with multiple lexical constraints. The Pick-Revise (PRIMT) (Cheng et al., 2016) [URL_CITE] framework for Interactive Post Editing introduces the concept of edit cycles. Translators specify con-straints by editing a part of the MT output that is incorrect, and then asking the system for a new hypothesis, which must contain the user-provided correction.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/N/N16/N16-1148.pdf\nCitation Context: Pick-Revise is an interaction cycle for MT Post-Editing proposed by Cheng et al. (2016) [URL_CITE] . Starting with the original translation hypothesis, a (sim-ulated) user first picks a part of the hypothesis which is incorrect, and then provides the correct translation for that portion of the output.\nFootnote or Reference text (if exists): Shanbo Cheng, Shujian Huang, Huadong Chen, Xinyu Dai, and Jiajun Chen. 2016. PRIMT: A pick-revise framework for interactive machine trans-lation. In NAACL HLT 2016, The 2016 Con-ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, San Diego Califor-nia, USA, June 12-17, 2016. pages 1240\u20131249. http://aclweb.org/anthology/N/N16/N16-1148.pdf.\nSection Titles (if exists):  ['4 Experiments', '4.1 Pick-Revise for Interactive Post Editing']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Paper\n\n**Citation Reason:**\n\n* **Introduce a concept:** The citation introduces the \"Pick-Revise\" framework for interactive machine translation, which is the core concept being discussed in the paragraph. \n* **Attribution:**"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ankara.lti.cs.cmu.edu/side/video.swf\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Access\n\nThe URL points to the website for downloading the SIDE software. The citation provides readers with a direct link to access and potentially"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/streaming/\nCitation Context: The English tweets containing one of the selected hashtags are crawled using Twit-ter streaming API. [URL_CITE] The tweet temporal orienta-tion classifier is validated on a manually annotated test set. Finally, we use this classifier to automat-ically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 1 https://dev.twitter.com/streaming/overview. Last accessed on 10-01-2018.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developer.twitter.com/en/docs\nCitation Context: Training tweets are collected using the Twitter streaming API. [URL_CITE] The tweets are collected for the duration of September 2017 and October 2017. We consider day-wise trending topics during this period.\nFootnote or Reference text (if exists): 5 https://developer.twitter.com/en/docs\nSection Titles (if exists):  ['4 Data Sets', '4.1 Training Set']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Documentation\n\n**Citation Reason:**  Reference\n\n**Explanation:**\n\n* **Documentation:** The URL points to the official Twitter developer documentation. This type of resource provides information"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://christophm.github.io/interpretable-ml-book/\nCitation Context: Interpretable NLP aims to better understand pre-dictive models\u2019 internals for purposes such as de-bugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019) [URL_CITE] . These objectives require faithful rationales\u2014 explanations of the model\u2019s behavior that are accu-rate representations of its decision process (Melis and Jaakkola, 2018).\nFootnote or Reference text (if exists): Christoph Molnar. 2019. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Book\n\n**Citation Reason:**  \n\n* **Foundational Work:** The citation is likely used to introduce the broader concept of interpretable machine learning, of which interpre"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~treebank/tokenization.html\nCitation Context: In much tagging and parsing work, PTB data has been used with gold-standard tokens, to a point where many re-searchers are unaware of the existence of the orig-inal \u2018raw\u2019 (untokenized) text. Accordingly, the for-mal definition of PTB tokenization [URL_CITE] has received lit-tle attention, but reproducing PTB tokenization au-tomatically actually is not a trivial task (see \u00a7 3).\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~treebank/tokenization.html for available \u2018documentation\u2019 and a sed script for PTB-style tokenization.\nSection Titles (if exists):  ['2 Common Conventions']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Documentation\n\n**Citation Reason:**  \n\n* **Explanation:** The citation points to the official documentation for Penn Treebank (PTB) tokenization. "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bleurt\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d [URL_CITE] We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors.\nFootnote or Reference text (if exists): 12 https://github.com/google-research/bleurt\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Tiiiger/bert_score\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [URL_CITE]\nFootnote or Reference text (if exists): 13 https://github.com/Tiiiger/bert_score\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:** Implementation\n\n**Explanation:**\n\nThe URL points to a GitHub repository containing the code for BERTscore. The citation sentence explicitly states that the comparison"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2\nCitation Context: For the experiments in this paper, we used a full dump of Wikipedia from September 4, 2010. [URL_CITE] In-cluded in this dump is a total of 10,355,226 articles, of which 1,019,490 have been geotagged. Excluding various types of special-purpose articles used pri-marily for maintaining the site (specifically, redirect articles and articles outside the main namespace), the dump includes 3,431,722 content-bearing arti-cles, of which 488,269 are geotagged.\nFootnote or Reference text (if exists): 2 http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION:  SourceOfData \n\n\n**Explanation:**\n\n* **ROLE: Material:**  The artifact is a concrete, tangible resource (in this case, a website).\n* **TYPE: Dataset:**"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lair.cse.msu.edu/lair/projects/actioneffect.html\nCitation Context: This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have cre-ated a dataset for this task, which is available to the community [URL_CITE] . Our bootstrapping approach can serve as a baseline for future work on this topic.\nFootnote or Reference text (if exists): 1 This dataset is available at http://lair.cse.msu.edu/lair/projects/actioneffect.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OnlpLab/morphodetection\nCitation Context: We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available on https://github.com/OnlpLab/morphodetection.\nSection Titles (if exists):  ['table, given a few forms of the same lexeme. 2']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Code Availability\n\n**Explanation:**\n\n* The URL points to a GitHub repository, which is a common platform for hosting software projects."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1027\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016 [URL_CITE] ; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Ashish Vaswani, Yonatan Bisk, Kenji Sagae, and Ryan Musa. 2016. Supertagging with LSTMs. In NAACL. Association for Computational Linguistics, San Diego, California, pages 232\u2013237. http://www.aclweb.org/anthology/N16-1027.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P15-2041\nCitation Context: Recent work has explored neural network mod-els for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015 [URL_CITE] ; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve perfor-mance beyond non-neural models. We extend pre-viously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Net-works (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nFootnote or Reference text (if exists): Wenduan Xu, Michael Auli, and Stephen Clark. 2015. CCG supertagging with a recurrent neural network. In ACL. Association for Computational Linguistics, Beijing, China, pages 250\u2013255. http://www.aclweb.org/anthology/P15-2041.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce \n\n\nThis citation introduces a paper that explores neural network models for supertagging in CCG (Constituency Tree Grammar).  The authors are building upon this prior work. \n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Table 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs over a set of random seeds.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments', '3.2 Efficient Pseudo-Log-Likelihood Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Hence, we train and evaluate two SCRIPT mod-els \u201csmall\u201d and \u201cbase\u201d with an encoder of the 14M and 110M parameters, respectively. For a direct comparison, the models are trained on the Open-WebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] with identical pre-processing and optimization proce-dures as in (Devlin et al., 2018) and (Clark et al., 2020). We refer to the Appendix for details.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:** Training Data\n\n**Explanation:**\n\nThe citation sentence explicitly states that the models (\"small\" and \"base\") were trained on the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stp.lingfil.uu.se/~nivre/research/Penn2Malt.html\nCitation Context: We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour-nal sections 2\u201321 for training, 22 for development testing and 23 for final testing. Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [URL_CITE] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011).\nFootnote or Reference text (if exists): 1 http://stp.lingfil.uu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983xxx\nCitation Context: We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [URL_CITE] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).\nFootnote or Reference text (if exists): Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  The URL points to the Universal Dependencies (UD) 2.2 dataset, which is explicitly mentioned as one of the datasets"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced. We will make it available via [URL_CITE] http://www.isi.edu.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [URL_CITE] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Data Availability\nTYPE: Website\nFUNCTION:  Provide access to dataset and relations \n\n\nLet me explain:\n\n* **ROLE: Data Availability** - The primary purpose of the citation is to inform readers where they can access the dataset and"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neukg/GRTE\nCitation Context: Ex-perimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: [URL_CITE] https://github.com/neukg/GRTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/RSN\nCitation Context: Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at [URL_CITE] https://github.com/thunlp/RSN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:** Reproducibility \n\n**Explanation:**\n\nThe citation explicitly states that the code is provided for reproducibility. Users can access the code at the given"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crowdflower.com\nCitation Context: While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [URL_CITE] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.\nFootnote or Reference text (if exists): 4 http://crowdflower.com\nSection Titles (if exists):  ['4 Crowd-Sourcing Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Tool\n**TYPE:** Crowdsourcing Platform\n**FUNCTION:** Data Collection \n\n\n**Explanation:**\n\n* **Tool:** CrowdFlower is a platform used to obtain human"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software Implementation\n\n**Citation Reason:**  Implementation Source\n\n**Explanation:**\n\n* **Software Implementation:** The URL points to a GitHub repository (`https://github."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [URL_CITE] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Software\n* **Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\nThe citation points to the Wapiti software, which is specifically mentioned as being used to"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleylm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [URL_CITE] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 6 http://code.google.com/p/berkeleylm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Implementation Detail \n\n**Explanation:**\n\nThe citation refers to the `berkeleylm` toolkit, which is a software library used for training"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values\nCitation Context: We introduce the following modifica-tions for fairer multilingual evaluation: Instead of stripping ASCII punctuation, we strip all unicode characters with a punctuation General Category. [URL_CITE] When a language has stand-alone articles (English, Spanish, German and Vietnamese) we strip them. We use whitespace tokenization for all MLQA lan-guages other than Chinese, where we use the mixed segmentation method from Cui et al.\nFootnote or Reference text (if exists): 6 http://www.unicode.org/reports/tr44/tr44-4.html#General_Category_Values\nSection Titles (if exists):  ['4 Cross-lingual QA Experiments', '4.1 Evaluation Metrics for Multilingual QA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/mlqa\nCitation Context: We present MLQA, a multi-way aligned ex-tractive QA evaluation benchmark intended to spur research in this area. [URL_CITE] MLQA contains QA instances in 7 languages, English, Ara-bic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K in-stances in English and 5K in each other lan-guage, with each instance parallel between 4 languages on average.\nFootnote or Reference text (if exists): 1 MLQA is publicly available at https://github.com/facebookresearch/mlqa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Introduction:** The citation introduces the MLQA dataset to the reader. \n* **Description:** It provides key details about"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.xobdo.org/\nCitation Context: For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo [URL_CITE] ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex . The dictionaries are of vary-ing qualities and sizes. The total number of entries in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis) are 4682, 76634 and 6628, respectively.\nFootnote or Reference text (if exists): 5 http://www.xobdo.org/\nSection Titles (if exists):  ['3 Experiments', '3.1 Publicly available Wordnets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://panlex.org/\nCitation Context: For languages not supported by MT, we use three additional bilingual dictionaries: two dictio-naries Dict(eng,ajz) and Dict(eng,dis) provided by Xobdo ; one Dict(eng,asm) created by integrat-ing two dictionaries Dict(eng,asm) provided by Xobdo and Panlex [URL_CITE] . The dictionaries are of vary-ing qualities and sizes. The total number of entries in Dict(eng,ajz), Dict(eng,asm) and Dict(eng,dis) are 4682, 76634 and 6628, respectively.\nFootnote or Reference text (if exists): 6 http://panlex.org/\nSection Titles (if exists):  ['3 Experiments', '3.1 Publicly available Wordnets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website\n* **Citation Reason:** Source of Data \n\n**Explanation:**\n\nThe citation points to `http://panlex.org/`, which is the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/utils.html\nCitation Context: 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [URL_CITE] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation.\nFootnote or Reference text (if exists): 8 https://radimrehurek.com/gensim/utils.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Method\n* **TYPE:** Tool\n* **FUNCTION:**  Word Segmentation \n\n**Reason for Citation:**\n\nThe citation is used to explicitly state the tool used for Chinese word"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mounicam/lexical_simplification\nCitation Context: Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by ap-plying our model to the Paraphrase Database (PPDB). [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data are publicly available on the au-thors\u2019 homepages and GitHub: https://github.com/mounicam/lexical_simplification.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~nlp/resources/simple-ppdb.tgz\nCitation Context: In this work, we apply state-of-the-art machine learned models for lexical simplification in order to iden-tify phrase pairs from the Paraphrase Database (PPDB) applicable to the task of text simplifica-tion. We introduce Simple PPDB, [URL_CITE] a subset of the Paraphrase Database containing 4.5 million sim-plifying paraphrase rules. The large scale of Sim-ple PPDB will support research into increasingly advanced methods for text simplification.\nFootnote or Reference text (if exists): 1 http://www.seas.upenn.edu/\u02dcnlp/resources/simple-ppdb.tgz\nSection Titles (if exists):  ['1 Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset:** The URL points to a compressed archive containing the Simple PPDB dataset."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://doi.org/10.1007/978-3-540-\nCitation Context: There are two equivalent definitions of the generalized Jensen-Shannon divergence: the original def-inition based on Shannon entropy (Lin, 1991), and a reshaped one expressed as the average Kullback-Leibler divergence of each P i to the centroid P (Aslam and Pavlu, 2007) [URL_CITE] . We adopt the latter one here: f\u21e0P i \" # f\u21e0P i 4log P i (f) 35 E f\u21e0P i \\uf8ff\nFootnote or Reference text (if exists): Javed A. Aslam and Virgil Pavlu. 2007. Query hardness estimation using jensen-shannon di-vergence among multiple scoring functions. In Advances in Information Retrieval. Springer Berlin Heidelberg, Berlin, Heidelberg, pages 198\u2013209. https://doi.org/10.1007/978-3-540- 71496-5 20.\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nCitation Context: The expectation and variance of Dirichlet(\u03b8|v) are given as follows. [URL_CITE] where v 0 = i v i . Therefore the variance of a scaled Dirichlet is:\nFootnote or Reference text (if exists): 3 http://www.cis.hut.fi/ahonkela/dippa/dippa.html\nSection Titles (if exists):  ['2 Bayesian Model of Summaries']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Web Page\n\n**Citation Reason:**  Provides a formula or mathematical definition.\n\n**Explanation:**\n\n* The URL points to a web page hosted at a"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.sri.com/projects/srilm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [URL_CITE] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 5 http://www.speech.sri.com/projects/srilm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Tool\n* **TYPE:** Software Library\n* **FUNCTION:**  Training n-gram language models \n\n\n**Citation Reason:**\n\nThe citation is used to acknowledge the software"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://katfuji.lab.tuat.ac.jp/nlp_datasets/\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EdoFrank/emnlp2019_morio_egawa\nCitation Context: The annotated dataset, annotation guideline, and implementation of the neural model are available in public. [URL_CITE]\nFootnote or Reference text (if exists): 1 Dataset and annotation guideline: http: //katfuji.lab.tuat.ac.jp/nlp_datasets/. Implementation: https://github.com/EdoFrank/ emnlp2019_morio_egawa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:**  Software/Code \n* **Citation Reason:**  Implementation Details\n\n**Explanation:**\n\nThe URL points to a GitHub repository, which strongly suggests the artifact"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sharethefacts.org/\nCitation Context: ClaimPortal leverages the claim matching func-tion in the ClaimBuster API. The fact-check repository is composed of the Share-the-facts [URL_CITE] fact checks as well as fact checks collected from several fact-checking organizations like PolitiFact, Snopes, factcheck.org, Washington Post, etc. The system measures the similarity between a claim and a fact-check based on the similarity of their tokens.\nFootnote or Reference text (if exists): 10 http://www.sharethefacts.org/\nSection Titles (if exists):  ['2 System Architecture and Components', '2.5 Claim Matcher']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://idir.uta.edu/claimbuster/\nCitation Context: In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [URL_CITE] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates.\nFootnote or Reference text (if exists): 6 https://idir.uta.edu/claimbuster/\nSection Titles (if exists):  ['2 System Architecture and Components', '2.3 Claim Spotter']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Implementation Detail \n\n**Explanation:**\n\nThe URL points to the ClaimBuster API, a tool developed by the research group. The citation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['7 Experiments', '7.1 Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jungokasai/graph_parser\nCitation Context: We implement all of our models in TensorFlow (Abadi et al., 2016). [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Reproducibility \n\n**Explanation:**\n\nThe citation explicitly states that the code is provided for \"easy replication of our results\". This strongly"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sogou.com/labs/dl/c.html\nCitation Context: Data preparation The data is composed of two parts: source documents and background knowl-edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [URL_CITE] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary. We then randomly but equally select 600 articles as the set of source documents from 9 top-ics without data bias.\nFootnote or Reference text (if exists): 2 http://www.sogou.com/labs/dl/c.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Evaluation as a ranking problem']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://babelnovel.com/\nCitation Context: Data collection. We construct our corpus from a professional Chinese novel reading site named Babel Novel [URL_CITE] . Unlike news, the novel dataset cov-ers a mixture of literary style including historical novels, and martial arts novels in the genre of fan-tasy, mystery, romance, military, etc.\nFootnote or Reference text (if exists): 9 https://babelnovel.com/\nSection Titles (if exists):  ['B Details of the Datasets', 'B.2 Novel Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION:  DataOrigin \n\n\nLet me explain:\n\n* **ROLE: Material** - This indicates the artifact is a tangible resource used in the research.\n* **TYPE: DataSource** -  The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2008T19\nCitation Context: In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [URL_CITE] . Furthermore, we random-ly initialized an embedding table for each entity tags.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2008T19\nSection Titles (if exists):  ['3 Methodology', '3.2 Word/Entity Embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ryankiros/neural-storyteller\nCitation Context: Finally, it discusses how the automated corpus-based eval-uations correlate with human judgements. In fu-ture, this work aims to be applied for synthesizing images from text, exploring complementary archi-tectures to projects like neural-storyteller [URL_CITE] where skip-thought embeddings are already used to per-form image captioning with story-style transfer.\nFootnote or Reference text (if exists): 3 https://github.com/ryankiros/ neural-storyteller\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Comparison/Related Work\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository, indicating a software project."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.biendata.xyz/competition/ccks_2019_6/data/\nCitation Context: Dataset We utilize the dataset published in CCKS Chinese KBQA Contest [URL_CITE] for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.\nFootnote or Reference text (if exists): 4 https://www.biendata.xyz/competition/ccks_2019_6/data/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pkubase.gstore.cn/\nCitation Context: Dataset We utilize the dataset published in CCKS Chinese KBQA Contest for evaluation. The dataset consists of various Chinese open-domain complex (multi-hop) questions that require deep comprehension of questions and strong gener-alization ability, its background KB is PKUBASE [URL_CITE] , a Chinese KB based on Baidu Baike. We follow the raw separation of 2.2k/0.76k/0.76k train/dev/test data, note that no information in dev or test set are used when training.\nFootnote or Reference text (if exists): 5 A KB endpoint: http://pkubase.gstore.cn/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge Base (KB)\nFUNCTION: Background Knowledge \n\n\nLet me explain:\n\n* **ROLE: Material:** This indicates the artifact is a foundational resource used in the research.\n* **TYPE: Knowledge Base ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum\nCitation Context: In this paper, we use the same probabilistic cur-riculum strategy and code base [URL_CITE] as Zhang et al. (2018).\nFootnote or Reference text (if exists): 1 https://github.com/kevinduh/sockeye-recipes/tree/master/egs/ curriculum\nSection Titles (if exists):  ['2 Curriculum Learning for Adaptation', '2.2 Curriculum Learning Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jacobeisenstein/gt-css-class\nCitation Context: Jacob\u2019s Georgia Tech course on Computational Social Science covers some of the same themes as this tutorial, and includes some additional material. [URL_CITE] He recently completed an in-troductory textbook on natural language process-ing.\nFootnote or Reference text (if exists): 2 https://github.com/jacobeisenstein/ gt-css-class\nSection Titles (if exists):  ['3 Presenter']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Course Materials \n* **Citation Reason:**  Supplementary Information/Related Work\n\n**Explanation:**\n\nThe URL points to a GitHub repository containing materials for a course"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TimDettmers/ConvE\nCitation Context: We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [URL_CITE] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data.\nFootnote or Reference text (if exists): 6 https://github.com/TimDettmers/ConvE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/KB2E\nCitation Context: In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [URL_CITE] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub.\nFootnote or Reference text (if exists): 4 https://github.com/thunlp/KB2E\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation Comparison\n\n**Explanation:**\n\nThe citation is used to highlight the use of KB2E as an alternative implementation for comparing the training"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://glaros.dtc.umn.edu/gkhome/views/cluto\nCitation Context: The Clusterer generates individual levels of the taxonomy by using text clustering. We used CLUTO package [URL_CITE] for doing text clustering. We experimented with all the available clustering functions in CLUTO but no one clustering al-gorithm consistently outperformed others.\nFootnote or Reference text (if exists): 2 http://glaros.dtc.umn.edu/gkhome/views/cluto\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tartarus.org/~martin/PorterStemmer\nCitation Context: To generate the topic taxonomy, these sets containing 5 to 100 clusters are passed through the Taxonomy Builder compo-nent. This component (1) removes clusters con-taining less than n documents (2) introduces di-rected edges from cluster v 1 to v 2 if v 1 and v 2 share at least one document between them, and where v 2 is one level finer than v [URL_CITE] . Now v 1 and v 2 become nodes in adjacent layers in the taxonomy.\nFootnote or Reference text (if exists): 1 http://www.tartarus.org/\u02dcmartin/PorterStemmer\nSection Titles (if exists):  ['4 Generation of Domain Model', '4.2 Taxonomy Generation']"
                },
                {
                    "role": "assistant",
                    "content": "The artifact type is **Tool**, and the citation reason is **Implementation Detail**. \n\nHere's why:\n\n* **Tool:** The URL points to the Porter Stemmer, a widely used tool for text preprocessing.\n* **Implementation Detail"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/megagonlabs/opiniondigest\nCitation Context: Automatic evaluation on Y ELP data shows that our framework outperforms com-petitive baselines. Human studies on two cor-pora verify that O PINION D IGEST produces informative summaries and shows promising customization capabilities [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/megagonlabs/opiniondigest.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://extremereader.megagon.info/\nCitation Context: These examples have redundant opinions and incorrect extractions in the input, but O PINION D IGEST is able to convert the input opinions into natural summaries. Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [URL_CITE] that allows users to customize the generated summary by specifying search terms.\nFootnote or Reference text (if exists): 5 http://extremereader.megagon.info/\nSection Titles (if exists):  ['3 Evaluation', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Software Demo/Application\n\n**Citation Reason:**  \n\n* **Demonstrating Functionality:** The citation showcases the practical application of the OPINION DIGEST framework. The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.translationautomation.com/\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.translationautomation.com/\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projects.ldc.upenn.edu/gale/data/Catalog.html\nCitation Context: We combine several English-to-foreign bitext cor-pora to extract PPDB:Eng: Europarl v7 (Koehn, 2005), consisting of bitexts for the 19 European lan-guages, the 10 9 French-English corpus (Callison-Burch et al., 2009), the Czech, German, Span-ish and French portions of the News Commen-tary data (Koehn and Schroeder, 2007), the United Nations French- and Spanish-English parallel cor-pora (Eisele and Chen, 2010), the JRC Acquis cor-pus (Steinberger et al., 2006), Chinese and Arabic newswire corpora used for the GALE machine trans-lation campaign, [URL_CITE] parallel Urdu-English data from the NIST translation task, the French portion of the OpenSubtitles corpus (Tiedemann, 2009), and a collection of Spanish-English translation memories provided by TAUS.\nFootnote or Reference text (if exists): 2 http://projects.ldc.upenn.edu/gale/data/Catalog.html\nSection Titles (if exists):  ['4 English Paraphrases \u2013 PPDB:Eng']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Data Source\nTYPE: Website\nFUNCTION:  Provides access to the GALE machine translation corpora used in the creation of PPDB:Eng. \n\n\nThe citation indicates that the GALE corpora were used as a source of data for building"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/najoungkim/pdtb3\nCitation Context: We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [URL_CITE] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration.\nFootnote or Reference text (if exists): 4 https://github.com/najoungkim/pdtb3\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nCitation Context: We resolve spelling errors with a public spell checker [URL_CITE] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setting']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Service\n\n**Citation Reason:**  Tool/Methodology\n\n**Explanation:**\n\n* **Service:** The URL points to Microsoft's Azure Cognitive Services Spell Check API"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/demos.php\nCitation Context: For the Named Entity information, we used a publicly available NE Recognizer capable of recognizing a range of cat-egories including Person, Location and Organiza-tion. On the CoNLL-03 shared task, its f-score is about 90% [URL_CITE] . We evaluate our system from different points of view, as described below.\nFootnote or Reference text (if exists): 4 A web demo of the NER is at http://L2R.cs.uiuc.edu/\u02dccogcomp/demos.php.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.scopesys.com/\nCitation Context: This sub-feature can be determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string. In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive , a list of 10,000 organization names from websites such as Yahoo 4 and a list of 10,000 famous people from websites such as Scope Systems [URL_CITE] . Gazetters have been widely used in NER systems to improve performance.\nFootnote or Reference text (if exists): 5 http://www.scopesys.com/\nSection Titles (if exists):  ['3 Determining Word Feature', '3.1 Internal Sub-Features']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Data Source\n**TYPE:** Website\n**FUNCTION:**  Source of Named Entity Data (specifically, a list of famous people) \n\n\n**Explanation:**\n\nThe citation points to Scope"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/andreasvc/discodop\nCitation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\nFootnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-metrics\nCitation Context: 1. We calculate n-gram metric scores using the E2E-metrics module [URL_CITE] https://github.com/tuetschek/e2e-metrics\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Implementation Detail \n\n**Explanation:**\n\nThe citation points to a GitHub repository containing the \"E2E-metrics\" module. The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mponza/SalIE\nCitation Context: The source code and the processed datasets are publicly available [URL_CITE] to encourage further develop-ments of the fact salience task.\nFootnote or Reference text (if exists): 1 https://github.com/mponza/SalIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org/\nCitation Context: This research is part of the Tanbih project, [URL_CITE] which aims to limit the effect of \u2018fake news,\u201d disinforma-tion, propaganda, and media bias by making users aware of what they are reading.\nFootnote or Reference text (if exists): 14 http://tanbih.qcri.org/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Project Website\n* **Citation Reason:**  Background/Context \n\n**Explanation:**\n\n* **Project Website:** The URL points to a website that likely provides information about the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://research.microsoft.com/mct\nCitation Context: Datasets: We use two datasets for our evaluation. (1) First is the MCTest-500 dataset [URL_CITE] , a freely available set of 500 stories (split into 300 train, 50 dev and 150 test) and associated questions (Richardson et al., 2013). The stories are fictional so the answers can be found only in the story it-self.\nFootnote or Reference text (if exists): 1 http://research.microsoft.com/mct\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Material\n* **TYPE:** Dataset\n* **FUNCTION:**  Data Source \n\n\n**Explanation:**\n\nThe citation clearly states that the URL points to the VIST-Edit dataset"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Franck-Dernoncourt/pubmed-rct\nCitation Context: PubMed: Medical abstracts of randomized con-trolled trials. Source: [URL_CITE] https://github.com/Franck-Dernoncourt/pubmed-rct.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data Description']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset \n* **Citation Reason:** Source\n\n**Explanation:**\n\nThe URL points to a GitHub repository containing a dataset of PubMed abstracts specifically focused on randomized controlled trials (RCT"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/15StNc\u201d\nCitation Context: (11) \u201cTodd and Sarah Palin to divorce [URL_CITE] http://bit.ly/15StNc\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data', '4.1 Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ow.ly/iNxF\u201d\nCitation Context: (1) \u201cSarah and Todd Palin to divorce, according to local Alaska paper. [URL_CITE] http://ow.ly/iNxF\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data', '4.1 Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** News Article \n* **Citation Reason:**  Source of the claim about Sarah and Todd Palin's divorce. \n\n**Explanation:**\n\nThe citation sentence states that the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cadillacforums.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [URL_CITE] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 3 http://www.cadillacforums.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.galaxyforums.net/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [URL_CITE] . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 5 http://www.galaxyforums.net/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website\n* **Citation Reason:**  Example\n\n**Explanation:**\n\nThe citation is used to provide a concrete example of a discussion forum for a specific product ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mutedfaith.com/funny/life.htm\nCitation Context: The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, http://www.berro.com/Jokes or [URL_CITE] http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.berro.com/Jokes\nCitation Context: The first constraint is implemented using a set of keywords of which at least one has to appear in the URL of a retrieved webpage, thus poten-tially limiting the content of the webpage to a theme related to that keyword. The set of key-words used in the current implementation consists of six words that explicitly indicate humor-related content: oneliner, one-liner, humor, humour, joke, funny. For example, [URL_CITE] http://www.berro.com/Jokes or http://www.mutedfaith.com/funny/life.htm are the URLs of two webpages that satisfy this constraint.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Humorous and Non-humorous Data Sets', '2.1 Humorous Data']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Website\n\n**Citation Reason:**  Example\n\nThe citation provides the URL as an example of a webpage that satisfies a specific constraint: containing keywords related to humor in its URL. \n\n\nLet me know if you have any other URLs"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://arxiv.org/abs/1809.00794\nCitation Context: Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. [URL_CITE]\nFootnote or Reference text (if exists): 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.texar.io.1\nCitation Context: The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [URL_CITE] https: //www.texar.io. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website \n* **Citation Reason:**  Source of Information\n\n**Explanation:**\n\nThe citation points to the official website of Texar, a text generation toolkit"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task.html\nCitation Context: 4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://www.statmt.org/wmt14/translation-task.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lexvo.org/uwn/\nCitation Context: the number of senses existing as entries in a WordNet for a given target word t i or a source word s i . We employ the Universal WordNet, [URL_CITE] which provides access to WordNets of various languages. Pseudo-reference This binary feature explores the similarity between the target sentence and a translation for the source sentence produced by an-other MT system.\nFootnote or Reference text (if exists): 5 http://www.lexvo.org/uwn/\nSection Titles (if exists):  ['3 Features', '3.1 Word level']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Resource\n**TYPE:** Lexical Database\n**FUNCTION:**  Data Source \n\n**Reasoning:**\n\n* **Resource:**  Universal WordNet (UWN) is a collection"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nCitation Context: For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmdrr/testSignificanceNLP.git\nCitation Context: We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [URL_CITE] .\nFootnote or Reference text (if exists): 1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Supporting Material\n* **TYPE:** Code\n* **FUNCTION:** Reproducibility \n\n**Reason for Citation:**\n\nThe code repository is cited to provide readers with the means to reproduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq [URL_CITE] (Ott et al., 2019) to imple-ment our algorithms. We adopt the Transformer-base (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT\u201914 De-En.\nFootnote or Reference text (if exists): 5 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses. In the existing implementation in Alchemy (Kok et al., 2007) [URL_CITE] , SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m,c!)). Such predicates can be viewed as multi-valued predi-cates (e.g., Head(m) with value ranging over all c\u2019s) and are prevalent in NLP applications.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['5 Learning and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software/System\n\n**Citation Reason:**  \n\n* **Implementation Details:** The citation points to the Alchemy system's website because the authors are discussing a specific"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/aerkalov/ebooklib\nCitation Context: 3. We use ebooklib [URL_CITE] to generate a new EPUB file with sentences linked to time segments in the relevant MP3 file using SMIL.\nFootnote or Reference text (if exists): 7 https://github.com/aerkalov/ebooklib\nSection Titles (if exists):  ['3 My Turn To Read App', '3.1 Read Aloud eBooks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.idpf.org/epub\nCitation Context: We use the EPUB format [URL_CITE] to create what we call a \u201cRead Aloud eBook\" used by MTTR. To link the text in the book to the synchronized audio, we use SMIL (Synchronized Multimedia Integration Lan-guage), as defined in the EPUB Media Overlays specification.\nFootnote or Reference text (if exists): 5 http://www.idpf.org/epub\nSection Titles (if exists): [\\'3 My Turn To Read App\\', \\'3.1 Read Aloud eBooks\\']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:**  Standard/Specification\n* **Citation Reason:**  Justification/Background\n\n**Explanation:**\n\n* **Standard/Specification:** The URL points to the official website of the EP"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://opennlp.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [URL_CITE] or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 2 https://opennlp.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [URL_CITE] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Website\n\n**Citation Reason:**  \n\n* **Contextual Background:** The citation sentence lists Lucene as one of the Java NLP/IR tools that K E LP aims to be compatible with. \n* **Supporting Evidence:**"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/higher\nCitation Context: In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [URL_CITE] (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ higher\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PyTorchLightning/pytorch-lightning,3\nCitation Context: Training details All of our experiments were done on a single Nvidia GeForceRTX 2080 Ti. We base our implementation on PyTorch (Paszke et al., 2017) and also use PyTorch Lightning (Fal-con, 2019) [URL_CITE] and Huggingface (Wolf et al., 2019). The gates and the experts in our MoE model were a single layer MLP.\nFootnote or Reference text (if exists): et al. Falcon, WA. 2019. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning,3.\nSection Titles (if exists):  ['C Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**Artifact Type:** Tool\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* **Tool:** PyTorch Lightning is a framework designed to simplify and streamline the training process of deep learning models in"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [URL_CITE] (d = 768). We imple-mented the HAN following Yang et al.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cia.gov/cia/publications/factbook\nCitation Context: We used BBN\u2019s I DENTI F INDER (Bikel et al., 1999) to mark up person names, organizations and lo-cations. We marked up countries and (American) states using a list obtained from the CIA factsheet [URL_CITE] . To mark up roles, we used a list derived from Word-Net (Miller et al., 1993) hyponyms of the person synset.\nFootnote or Reference text (if exists): 1 http://www.cia.gov/cia/publications/factbook provides a list of countries and states, abbreviations and adjectival forms, for example United Kingdom/U.K./British/Briton and Califor-nia/Ca./Californian.\nSection Titles (if exists):  ['2 References to people', '2.3 Automatic semantic tagging']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset\n\n**Citation Reason:**  Source Data\n\n**Explanation:**\n\n* **Dataset:** The CIA World Factbook provides a structured list of countries, states, abbreviations"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebp/scikit-survival\nCitation Context: Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models\u2019 performances dealing with different time-to-events [URL_CITE] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 \u00b1 0.025 0.504 \u00b1 0.032 0.503 \u00b1 0.037 CPH + Feature Set 2 0.621 \u00b1 0.014 0.632 \u00b1 0.033 0.642 \u00b1 0.030 CPH + Hidden Features 0.674 \u00b1 0.023 0.696 \u00b1 0.022 0.710 \u00b1 0.022 MLP + Feature Set 1 0.502 \u00b1 0.023 0.509 \u00b1 0.026 0.509 \u00b1 0.030 MLP + Feature Set 2 0.658 \u00b1 0.010 0.671 \u00b1 0.025 0.685 \u00b1 0.023 MLP + Hidden Features 0.704 \u00b1 0.017 0.726 \u00b1 0.020 0.744 \u00b1 0.019 LSTM + Sequential HF 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 0.501 \u00b1 0.028 0.642 \u00b1 0.030 0.697 \u00b1 0.026 0.501 \u00b1 0.032 0.683 \u00b1 0.008 0.734 \u00b1 0.018 0.742 \u00b1 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 \u00b1 0.115 0.611 \u00b1 0.123 0.618 \u00b1 0.134 BioBert (Lee et al., 2020) 0.701 \u00b1 0.021 0.714 \u00b1 0.027 0.734 \u00b1 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 \u00b1 0.019 0.705 \u00b1 0.023 0.723 \u00b1 0.025 BlueBert (Peng et al., 2019) 0.713 \u00b1 0.019 0.735 \u00b1 0.024 0.755 \u00b1 0.024 CheXbert (Smit et al., 2020) 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 AUC@365 0.620 \u00b1 0.136 0.739 \u00b1 0.028 0.727 \u00b1 0.029 0.756 \u00b1 0.021 0.742 \u00b1 0.023\nFootnote or Reference text (if exists): 1 https://github.com/sebp/ scikit-survival\nSection Titles (if exists):  ['4 Experiments', '4.2 Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "## Artifact Classification and Citation Reason\n\n**Artifact Type:** Software Library\n\n**Citation Reason:**  The citation is used to acknowledge the use of the `scikit-survival` library for survival analysis in the context of the research. \n\n\nThe authors"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com\nCitation Context: We take advantage of the annotation platform in Figure Eight. [URL_CITE] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator\u2019s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.\nFootnote or Reference text (if exists): 5 https://www.figure-eight.com\nSection Titles (if exists):  ['4 Dataset', '4.1 Annotation using Crowd Workers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.unt.edu/~rada/wa\nCitation Context: The earliest is the Blinker Project (Melamed, 1998); more re-cent systems have been released to support more lan-guages and visualization features (Ahrenberg et al., 2003; Lambert and Castell, 2004). [URL_CITE] Our interface does share some similarities with these systems, but it is designed with additional features to support our experimental goals of guideline development, active learning and resource projection. Following the ex-perimental design proposed by Och and Ney (2000), we instruct the annotators to indicate their level of confidence by choosing sure or unsure for each align-ment they made.\nFootnote or Reference text (if exists): 2 Rada Mihalcea maintains an alignment resource repository (http://www.cs.unt.edu/~rada/wa) that contains other downloadable interface packages that do not have companion papers.\nSection Titles (if exists):  ['2 Annotation Interface']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact type and citation reason:\n\n* **Artifact Type:**  Repository/Resource\n* **Citation Reason:**  Supplementary Information/Context\n\n**Explanation:**\n\nThe URL points to a repository of alignment interface packages maintained"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doccano/doccano\nCitation Context: Another annotation tool with limited sup-port for document-level annotations is Doccano (Nakayama et al., 2018) [URL_CITE] , though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.\nFootnote or Reference text (if exists): Hiroki Nakayama, Takahiro Kubo, Junya Kamura, Ya-sufumi Taniguchi, and Xu Liang. 2018. doccano: Text annotation tool for human. Software available from https://github.com/doccano/doccano.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doccano/doccano\nCitation Context: Another annotation tool with limited sup-port for document-level annotations is Doccano [URL_CITE] (Nakayama et al., 2018), though it is not mentioned in the evaluation of Neves and \u0160eva. The open-source tool currently supports three distinct anno-tation tasks: text classification, sequence labeling, and sequence to sequence tasks.\nFootnote or Reference text (if exists): 5 https://github.com/doccano/doccano\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Software/Tool \n* **Citation Reason:**  Comparison\n\n**Explanation:**\n\n* **Software/Tool:** The URL points to a GitHub repository for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nCitation Context: We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [URL_CITE] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1).\nFootnote or Reference text (if exists): 4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nSection Titles (if exists):  ['4 Experiments', '4.4 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf\nCitation Context: (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of B LEU developed by NIST (2002) [URL_CITE] has been used in two recent large-scale machine translation evalua-tions.\nFootnote or Reference text (if exists): NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics. AAAAAAAAAAA http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Research Report\n\n**Citation Reason:**  \n\n* **Background/Historical Context:** The citation introduces the work of NIST (2002) as a"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/serenayj/DeSSE\nCitation Context: DeSSE is also used for connec-tive prediction tasks, as in (Gao et al., 2021). [URL_CITE]\nFootnote or Reference text (if exists): 2 DeSSE and MinWiki are available at https://github.com/serenayj/DeSSE.\nSection Titles (if exists):  ['3 Datasets', '3.1 DeSSE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: There are other important roles in Were-wolf, such as seer, vigilante, etc, the goals and avail-able actions of which are not the focus of this paper. (Barnwell, 2012) [URL_CITE] provides a broader description of the game and roles.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Website\n\n**Citation Reason:**  Supplementary Information\n\n**Explanation:**\n\n* **Website:** The URL points to a personal website dedicated to the Werewolf game.\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/andreasvc/discodop\nCitation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\nFootnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sjmielke.com/papers/syncretism\nCitation Context: We empirically evaluated our method on 5 languages under two evaluation metrics. The code is availabile at [URL_CITE] https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code and Data\n\n**Citation Reason:**  \n\n* **Code:** The citation provides access to the code implementing the method being evaluated.\n* **Data:** The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2009/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce \n\n\nThis URL, like the previous one, points to a dataset used in the research.  It's cited to provide readers with access to the specific dataset used for the multi-document"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/liushulinle/event\nCitation Context: We publish our code for further study by the NLP community. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/liushulinle/event detection without triggers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/wordprobs/\nCitation Context: This paper studies the task of learning to automatically solve such problems given only the natural language. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data for this work are available at http://groups.csail.mit.edu/rbg/code/wordprobs/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Data and Code Availability\n\n**Explanation:**\n\nThe citation explicitly states that the code and data for the work are available at the provided URL"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stackexchange.com/sites\nCitation Context: MultiNews (Fabbri et al., 2019) is a multi-document summarization dataset where [URL_CITE] to 10 news articles share a single summary. We consider every pair of articles that share a summary to be a similar document pair. Table 2 shows summary statistics of the two document ranking datasets.\nFootnote or Reference text (if exists): 2 https://stackexchange.com/sites\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "The provided context doesn't actually use the given URL.  \n\nHere's a breakdown:\n\n* **Artifact Type:** Website\n* **Citation Reason:**  The URL points to a list of Stack Exchange sites, but the citation sentence refers"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/r/funny/\nCitation Context: For our work, we hypothesize that similes are used frequently in cre-ative writing or humorous content on social media (Veale, 2013). Hence, we obtain training data by scraping the subreddits WRITINGPROMPTS and FUNNY [URL_CITE] from social media site Reddit for com-ments containing the phrase like a. Similes can be both Open and Closed. For example the Closed Simile, \u201cThe boy was as strong as an ox\u201d gives strong as the PROPERTY shared by the boy and ox.\nFootnote or Reference text (if exists): 6 https://www.reddit.com/r/funny/\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pushshift.io/\nCitation Context: Due to their implicit nature, generating open similes is often more challenging and hence we resort to only using like a as a comparator instead of as...as. We use the API provided by pushshift.io [URL_CITE] to mine comments. Through this process we collect 87,843 from Reddit.\nFootnote or Reference text (if exists): 8 https://pushshift.io/\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:** Tool\n**TYPE:** Data Access\n**FUNCTION:** Data Collection\n\n**Reason for Citation:**\n\nThe citation points to `pushshift.io` because the authors used its API to"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [URL_CITE] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trainomatic.org\nCitation Context: We believe that the ability of T-O-M to over-come the current paucity of annotated data for WSD, coupled with video games with a pur-pose for validation purposes (Jurgens and Nav-igli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD. All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at [URL_CITE] http://trainomatic.org.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Dataset/Repository\n* **Citation Reason:**  Providing access to the training data used in the research. \n\nThe citation points to the website where the \"approximately one million"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['7 Experiments', '7.1 Hyperparameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rwth-i6/sisyphus\nCitation Context: The large collection of tools for Python can be used with-out modification for editing, debugging, and docu-menting the workflow, since it is written in Python. It is freely available online [URL_CITE] under the Mozilla License v2.0 to encourage the adoption by other groups.\nFootnote or Reference text (if exists): 4 https://github.com/rwth-i6/sisyphus\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:** Availability and Licensing \n\n**Explanation:**\n\nThe citation points to the GitHub repository containing the \"sisyphus\" project, which is written"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://incubator.apache.org/opennlp\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software\nCitation Context: For instance, the BioNLP shared task organizers have provided output from a num-ber of parsers , alleviating the need for participat-ing systems to download and run unfamiliar tools. On the other hand, many community members provide downloads of NLP tools [URL_CITE] to increase ac-cessibility and replicability of core components.\nFootnote or Reference text (if exists): 2 See, for example, http://nlp.stanford.edu/software; http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; http://incubator.apache.org/opennlp\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Example\n\n**Explanation:**\n\nThe URL points to the Stanford NLP Group's software page, which hosts various NLP tools.  The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pushshift.io/\nCitation Context: We use a subset of Reddit comments from 2006- 2018 scraped from [URL_CITE] https://pushshift.io/. We con-struct a dictionary containing the 10,000 most popular words and preprocess the dataset by re-moving deleted posts, out-of-vocabulary tokens, profanity, comments with less than 10 upvotes, and comments with over 400 tokens.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A.4 Reddit Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://goo.gl/yHWeJp\nCitation Context: In practice, exact values of percent-upvoted are not directly available; the site adds \u201cvote fuzzing\u201d to fight vote manipulation. [URL_CITE] To begin with, we first dis-card posts with fewer than 30 comments. Then, we query for the noisy percent-upvoted from each post ten times using the Reddit API, and take a mean to produce a final estimate.\nFootnote or Reference text (if exists): 6 Prior to Dec. 2016, vote information was fuzzed accord-ing to a different algorithm; however, vote statistics for all posts were recomputed according to a new algorithm that, according to a reddit moderator, can \u201cactually be trusted;\u201d https://goo.gl/yHWeJp\nSection Titles (if exists):  ['2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Supporting Information\n**TYPE:**  Technical Detail\n**FUNCTION:**  Explanation\n\n**Reasoning:**\n\n* **Supporting Information:** The URL provides additional context and explanation for a specific"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We initialized our word representations using publicly available 300-dimensional Glove vec-tors [URL_CITE] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.\nFootnote or Reference text (if exists): 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['5 Experiments', '5.3 Hyperparameters and Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: VecOff. We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [URL_CITE] and represented word pairs as described in Section 2.1.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Evaluation', '4.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset/Resource\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset/Resource:** GloVe (Global Vectors for Word Representation) is"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax/blob/master/flax/nn\nCitation Context: Unless specified other-wise, the model parameters are: number of lay-ers is 6, number of heads is 8, word embedding size is 512 and the feed-forward module (FFN) size is 2048. We follow the API for the standard multihead scaled dot-product attention implemen-tation [URL_CITE] so that we can perform a simple drop-in re-placement of the standard multihead attention with our hierarchical attention implementation. This al-lows for an easy and fair comparison.\nFootnote or Reference text (if exists): 6 https://github.com/google/flax/blob/master/flax/nn\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Implementation Detail\n**TYPE:** Code\n**FUNCTION:**  Reference Implementation\n\n**Explanation:**\n\nThe citation points to the Flax library's implementation of multi-head scaled"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: Bousmalis et al. (2016) [URL_CITE] utilized adversar-ial training in a shared-private model for domain adaptation to learn domain-invariant features, but still focused on the SS,ST setting. Finally, the idea of using adversarial nets to discriminate over mul-tiple distributions was empirically explored by a very recent work (Liu et al., 2017) under the multi-task learning setting, and can be considered as a special case of our MAN framework with the NLL domain loss.\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf\nCitation Context: However, it cannot fully take advantage the training data if it comes from multiple source domains. MS,ST adaptation (Mansour et al., 2009 [URL_CITE] ; Zhao et al., 2017) can deal with multiple source domains but only transfers to a single target domain. Therefore, when multiple target domains exist, they need to treat them as independent prob-lems, which is more expensive and cannot utilize the additional unlabeled data in these domains.\nFootnote or Reference text (if exists): Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh. 2009. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems 21, Curran Associates, Inc., pages 1041\u20131048. http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Paper\n\n**Citation Reason:**\n\n* **Background/Context:** The citation is used to introduce the concept of Multiple Source, Single Target (MS,ST)"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Knowledge Base\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* The URL points to a specific version of the Wikidata knowledge base (Wikidata"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/\nCitation Context: We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The dataset and word vectors can be downloaded at http://ai.stanford.edu/ \u223c ehhuang/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Dataset & Resource \n* **Citation Reason:**  The URL points to a location where the dataset and word vectors used in the research are accessible to the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh\nCitation Context: Node Number Terms C18.452.297 diabetes mellitus insulin-dependent diabetes C18.452.297.267 mellitus C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH [URL_CITE] tree. Node numbers represent hierarchical structure of terms\nFootnote or Reference text (if exists): 1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research.\nSection Titles (if exists):  ['1. Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov\nCitation Context: \u201cmetabolic diseases(C18.452)\u201d node is root of the subtree, and the subtree consists of 436 disease names which are target terms of specificity measuring. A set of journal abstracts was extracted from MEDLINE [URL_CITE] database using the disease names as quires. Therefore, all the abstracts are related to some of the disease names.\nFootnote or Reference text (if exists): 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine, USA. (http://www.nlm.nih.gov)\nSection Titles (if exists):  ['4. Experiment and Evaluation', '4.1. Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Database\n\n**Citation Reason:**  Source of Data\n\n**Explanation:**\n\n* The citation points to the National Library of Medicine (NLM) website, which hosts"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) [URL_CITE] . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 3 https://www.statmt.org/wmt14/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Script\n\n**Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\n* **Script:** The URL points to a shell script (`prepare-iwslt14"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.elastic.co/products/elasticsearch\nCitation Context: ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [URL_CITE] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers.\nFootnote or Reference text (if exists): 3 https://www.elastic.co/products/ elasticsearch\nSection Titles (if exists):  ['2 System Architecture and Components', '2.1 System Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flask.pocoo.org\nCitation Context: This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask [URL_CITE] micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters.\nFootnote or Reference text (if exists): 4 http://flask.pocoo.org\nSection Titles (if exists):  ['2 System Architecture and Components', '2.2 Monitoring, Processing, and Storing Tweets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Framework\n\n**Citation Reason:**  \n\n* **Technical Implementation:** The citation explicitly states that the ClaimPortal API is \"designed using Python and the Flask micro-"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tanbih.qcri.org\nCitation Context: At QCRI, he leads the Tanbih project, [URL_CITE] devel-oped in collaboration with MIT, which aims to limit the effect of \u201cfake news\u201d, propaganda and media bias by making users aware of what they are reading. The project was featured by over 100 news outlets, including Forbes, Boston Globe, Al-jazeera, MIT Technology Review, Science Daily, Popular Science, Fast Company, The Register, WIRED, and Engadget, among others.\nFootnote or Reference text (if exists): 2 Tanbih project: http://tanbih.qcri.org\nSection Titles (if exists):  ['6 Lecturers', '6.1 Preslav Nakov']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Project\n\n**Citation Reason:**  \n\n* **Project Description:** The URL points to the website for the Tanbih project, which is the primary subject of the citation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack [URL_CITE] for work com-munication and Reddit for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 1 https://slack.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [URL_CITE] for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 2 https://www.reddit.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Website\n* **Citation Reason:**  **Example** \n\nThe citation is used to provide a concrete example of a platform used for general online discussion.  "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zdou0830/DAFE\nCitation Context: In addition, we show that combining our method with back translation can further improve the performance of the model. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at: https://github.com/zdou0830/DAFE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JetRunner/beyond-preserved-accuracy\nCitation Context: By com-bining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/JetRunner/beyond-preserved-accuracy.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Reproducibility \n\n**Explanation:**\n\nThe citation sentence highlights the authors' contribution of a \"practical strategy\" involving multiple compression techniques."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openke.thunlp.org/\nCitation Context: Besides the toolkit, we also provide the pre-trained embeddings of several well-known large-scale KGs, which can be used directly for other relevant works without repeatedly spending much time for embedding KGs. In this paper, we mainly present the architecture design and implementa-tion of OpenKE, as well as the benchmark eval-uation results of some typical KE models im-plemented with OpenKE. Other related resources and details can be found on [URL_CITE] http://openke.thunlp.org/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/thunlp/OpenKE\nCitation Context: In this section, we mainly present the implemen-tations of acceleration modules and special sam-pling algorithm in OpenKE. OpenKE has been available to the public on GitHub [URL_CITE] and is open-source under the MIT license.\nFootnote or Reference text (if exists): 1 http://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['4 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Software \n* **Citation Reason:**  Source Code Availability\n\n**Explanation:**\n\nThe URL points to a GitHub repository, which is a common platform for hosting"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax\nCitation Context: We have used Flax, an open-source library [URL_CITE] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone.\nFootnote or Reference text (if exists): 8 https://github.com/google/flax\nSection Titles (if exists):  ['8 Experiments And Results', '8.2 Language Models Trained on One-Billion Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/jax\nCitation Context: We have implemented the proposed hierarchical attention using Jax, an open source library [URL_CITE] for automatic gradient computation and linear alge-bra operations on GPUs and TPUs. All numer-ical operations in our algorithm use the Numpy native linear algebra functions supported by Jax.\nFootnote or Reference text (if exists): 5 https://github.com/google/jax\nSection Titles (if exists):  ['8 Experiments And Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:**  Implementation Tool\n**TYPE:**  Library\n**FUNCTION:**  Numerical computation and automatic differentiation \n\n**Explanation:**\n\n* **Implementation Tool:** Jax is not the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P11-1048\nCitation Context: Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011) [URL_CITE] . Nguyen et al.\nFootnote or Reference text (if exists): Michael Auli and Adam Lopez. 2011. A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing. In ACL. Association for Computational Linguistics, pages 470\u2013480. http://www.aclweb.org/anthology/P11-1048.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kutschkem/SmithHeilmann_fork/tree/master/MIRATagger\nCitation Context: Since then, the system, resp. its reimple-mentation by Heilman [URL_CITE] , was widely used in applied tasks (Agirre et al., 2011; Surdeanu et al., 2011; Laparra and Rigau, 2013). Supersense taggers have then been built also for Italian (Picca et al., 2008), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2013).\nFootnote or Reference text (if exists): 2 https://github.com/kutschkem/SmithHeilmann_fork/tree/master/ MIRATagger\nSection Titles (if exists):  ['2 Related Work', '2.2 Supersense Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Implementation Details/Source Code\n\n**Explanation:**\n\n* The URL points to a GitHub repository, indicating software.\n* The citation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [URL_CITE] framework.\nFootnote or Reference text (if exists): 1 https://pytorch.org/\nSection Titles (if exists):  ['3 Methodology', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [URL_CITE] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['6 Experiments', '6.1 System']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Foundation/Base\n* **TYPE:**  Software System\n* **FUNCTION:**  Implementation Context \n\n**Reasoning:**\n\n* **Foundation/Base:** The citation states"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wanqiulong0923/TED-CDB\nCitation Context: The dataset and our Chinese annotation guidelines has been made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://creativecommons.org/licenses/by-nc-sa/4.0/\nCitation Context: Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [URL_CITE] .\nFootnote or Reference text (if exists): 10 https://creativecommons.org/licenses/by-nc-sa/4.0/\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** License\n* **Citation Reason:**  To explicitly state the license under which the dataset is released. \n\nThe citation sentence clearly indicates that the dataset is being"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/20717\nCitation Context: F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://competitions.codalab.org/competitions/20717\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: 1. The EVALB tool [URL_CITE] to calculate the labeled bracketing F1-score.\nFootnote or Reference text (if exists): 3 http://nlp.cs.nyu.edu/evalb/\nSection Titles (if exists):  ['6 Extract Syntactic Trees from Encoder', '6.2 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  \n\n* **Functionality:** The citation points to EVALB as the tool used to calculate a specific metric (labeled bracketing F1"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which typically houses source code.\n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We have created a Rel-grams database with 1.1M entries and a set of over 2K event schemas from a corpus of 1.8M New York Times articles. Both are freely available to the research community [URL_CITE] and may prove useful for a wide range of NLP applications.\nFootnote or Reference text (if exists): 9 available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['7 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Dataset\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* The citation sentence explicitly states that the authors created the Rel-grams database and event schemas, making it clear that the URL points to the source of this data"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atcbosselut/comet-commonsense\nCitation Context: For retrieving commonsense properties of the vehi-cle, we use the pre-trained COMET model [URL_CITE] and retrieve top 5 candidates for each input. Vehicle and Overall Quality.\nFootnote or Reference text (if exists): 11 https://github.com/atcbosselut/ comet-commonsense\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mosaickg.apps.allenai.org/comet_conceptnet\nCitation Context: COMET is an adaptation framework for constructing common-sense knowledge based on pre-trained language models. Our work only leverages the HasProp-erty relation from COMET [URL_CITE] .\nFootnote or Reference text (if exists): 9 https://mosaickg.apps.allenai.org/ comet_conceptnet\nSection Titles (if exists):  ['2 SCOPE: Style Transfer through COmmonsense PropErty', '2.1 Automatic Parallel Corpus Creation']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Dataset/Knowledge Base\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Dataset/Knowledge Base:** The URL points to a web interface for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/transformers/\nCitation Context: We compare the above-proposed curricula (CWA, Skip\u2013fact) against a baseline curriculum (Original) where we initialize the verification models with standard pretrained BERT weights (bert-base-cased). We use huggingface transform-ers (Wolf et al., 2019) in all of our experiments. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://huggingface.co/transformers/\nSection Titles (if exists):  ['2 Constrained Verification', '2.3 Methodology', 'Taker datasets following Clark et al. (2020).']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Library/Framework\n\n**Citation Reason:**  \n\n* **Acknowledgement:** The authors are citing the `transformers` library developed by Hugging Face as a key tool"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.acm.org/citation.cfm?id=1557769.1557821\nCitation Context: Associa-tion for Computational Linguistics, Strouds-burg, PA, USA, ACL \u201907, pages 177\u2013 180. [URL_CITE] http://dl.acm.org/citation.cfm?id=1557769.1557821.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Paper\n\n**Citation Reason:**  To provide a reference to the original publication of the work. \n\n**Explanation:**\n\n* The URL points to a Digital Library of Computing (DL ACM) citation page, which is a common"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.wikipedia.com\nCitation Context: Lan-guage models, and especially transformer-based language models, have shown to contain com-monsense and factual knowledge (Petroni et al., 2019; Jiang et al., 2019). We adopt this direc-tion in our model and build an encoder, pre-trained with Roberta (Liu et al., 2019c), which has been pre-trained on the huge language corpora (e.g., BooksCorpus (Zhu et al., 2015), Wikipedia (Remy, 2002) [URL_CITE] ) to capture implicit knowledge. We tokenize a description Q using WordPiece (Wu et al., 2016) as in BERT (Devlin et al., 2019a), giving us a se-quence of |Q| tokens and embed them with the pre-trained Roberta embeddings and append Roberta\u2019s positional encoding, giving us a sequence of d-dimensional token representation x Q1 , ..., x Q|Q| .\nFootnote or Reference text (if exists): M. Remy. 2002. Wikipedia: The free encyclope-dia200214wikipedia: The free encyclopedia. 2001 \u2013 updated daily. gratis http://www.wikipedia.com.\nSection Titles (if exists):  ['3 Methodology', '3.3 Pre-trained Knowledge Encoder']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nCitation Context: Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [URL_CITE] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq. ), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-\nFootnote or Reference text (if exists): 20 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nSection Titles (if exists):  ['References Christopher Clark, Kenton Lee, Ming-Wei Chang,']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Dataset\n\n**Citation Reason:**  The Quora Question Pairs (QQP) dataset is cited as a source of data for natural language inference (NLI) tasks. \n\n\nLet me know if you have any other citations you'"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.cornell.edu/people/pabo/movie-review-data/\nCitation Context: \u2022 MR: A movie reviews dataset based on sub-jective rating and sentiment polarity [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://www.cs.cornell.edu/people/pabo/movie-review-data/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Dataset\n\n**Citation Reason:**  \n\n* **Data Source:** The citation is used to identify the source of the \"unbalanced dataset for polarity detection of opinions\" referred to as MPQA. \n\n\nLet me know if you"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/licensed-sw/RSTTool/index.html\nCitation Context: This response includes three disfluencies (EDUs 3, 6, and 9), which are satellites of the corresponding repair nuclei. In addition, the re-sponse also includes an awkward Comment-Topic relation between EDU 2 and the node combin-ing EDUs 3-11, indicated by awkward-Comment-Topic-2; in this multinuclear relation, the annota-tor judged that the second branch of the relation was awkward, which is indicated by the [URL_CITE] that was appended to the relation label.\nFootnote or Reference text (if exists): 2 Downloaded from http://www.isi.edu/licensed-sw/RSTTool/index.html\nSection Titles (if exists):  ['3 Annotation', '3.1 Guidelines']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software Tool\n\n**Citation Reason:**  The citation points to the RSTTool, a software used for annotating discourse relations.  The specific reason for citing it"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/lipiji/SongNet\nCitation Context: Extensive experiments conducted on two collected corpora demonstrate that our pro-posed framework generates significantly better results in terms of both automatic metrics and the human evaluation. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code: http://github.com/lipiji/SongNet\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/successar/FRESH\nCitation Context: In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to \u2018end-to-end\u2019 ap-proaches, while being more general and easier to train. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/successar/FRESH\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:**  Implementation Details and Reproducibility\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which strongly suggests the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [URL_CITE] that uses gold answers for distant supervision. The key idea is to push the encoded vector for a ques-tion close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Paper\n\n**Citation Reason:**  Methodological Basis\n\n* **Explanation:** The citation is used to establish the origin of the \"distantly supervised objective for training"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:** Tool\n* **TYPE:** Software\n* **FUNCTION:** Search Engine \n\n**Reason for Citation:**\n\nThe citation is used to identify the specific search engine used to retrieve candidate"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [URL_CITE] from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [URL_CITE] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Paper\n\n**Citation Reason:**  Benchmark/Implementation Details\n\n**Explanation:**\n\n* The citation is directly tied to a specific experimental result (batch size and"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [URL_CITE] (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: For all other hyper-parameters, we use the default values of previous implementations. [URL_CITE] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.\nFootnote or Reference text (if exists): 6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\nThe citation points to the official GitHub repository of the GloVe word embedding model.  The authors are"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Corpus\n\n**Citation Reason:**  Data Source\n\n**Explanation:**\n\n* **Corpus:** The British National Corpus (BNC) is a large collection of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jtf.org.tw\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark (http://www.psychpark.org) (Bai, 2001) and John Tung Foundation ( [URL_CITE] http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [URL_CITE] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).\nFootnote or Reference text (if exists): 2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).\nSection Titles (if exists):  ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincare\u0301 Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Software/Tool\n* **Citation Reason:**  Implementation Detail\n\n**Explanation:**\n\nThe URL points to the website for the PattaMaika tool.  The citation context"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sequencecomparison.github.io\nCitation Context: In this test, words drawn from a random-ized sample are repeatedly aligned with each other in order to create a distribution of sound transi-tions for unrelated languages. This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b) [URL_CITE] . SCA is very fast in computation, but Lex-Stat has a much higher accuracy.\nFootnote or Reference text (if exists): Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.\nSection Titles (if exists):  ['3 Methods', '3.1 Sequence Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software/Tool\n\n**Citation Reason:**  \n\n* **Methodological Explanation:** The citation introduces the \"Sequence Comparison Algorithm (SCA)\" as a tool used in"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/iceboal/word-representations-bptf\nCitation Context: These pairs were provided by Zhang et al. (2014) [URL_CITE] . There were 52,760 entries (words), each of which had 11.7 synonyms on average, and 21,319 entries, each of which had 6.5 antonyms on average.\nFootnote or Reference text (if exists): 1 https://github.com/iceboal/ word-representations-bptf\nSection Titles (if exists):  ['3 Experiments', '3.1 Evaluation settings', '3.1.2 Resource for training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.collocations.de/temp/kappa_example.zip\nCitation Context: (1969)), observed agreement 0.96. [URL_CITE] There were too few instances of idioms (17) for a quantitative analysis of the sort presented here, so these are collapsed with the subsective class in what follows. The dataset as used here consists of 239 intersective and 130 subsective ANs.\nFootnote or Reference text (if exists): 7 Code for the computation of inter-annotator agreement by Stefan Evert, available at http://www.collocations.de/temp/kappa_example.zip.\nSection Titles (if exists):  ['p = Bv (5)', '3.3 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**Artifact Type:** Software/Code\n\n**Citation Reason:**  Supporting Methodology\n\n**Explanation:**\n\n* **Software/Code:** The URL points to a zip file containing code for calculating inter-annot"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/raylin1000/drop-bert\nCitation Context: We treat the second ref-erence as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT [URL_CITE] models for DROP.\nFootnote or Reference text (if exists): 2 https://github.com/raylin1000/drop-bert\nSection Titles (if exists):  ['2 A Description of MOCHA', '2.2 Collecting Candidates']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/raylin1000/dropbert\nCitation Context: Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koc\u030cisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 2.0 (Rajpurkar et al., 2018). We use the NABERT [URL_CITE] (Numerically Augmented BERT) model with an additional reasoning type to allow \u201cNo Answer\u201d as an answer to accommodate the SQuAD 2.0 dataset which has about 40,000 \u201cNo Answer\u201d questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch.\nFootnote or Reference text (if exists): 3 https://github.com/raylin1000/drop bert\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation Details\n\n**Explanation:**\n\n* The URL points to a GitHub repository (`https://github.com/raylin10"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhaozj89/TensorEmbeddingNLP\nCitation Context: The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [URL_CITE] https://github.com/zhaozj89/TensorEmbeddingNLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software/Code\n\n**Citation Reason:**  \n\n* **Availability:** The primary reason for citing this URL is to provide readers with access to the authors' implementation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: [URL_CITE] https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [URL_CITE] https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "**Artifact Type:** Software\n\n**Citation Reason:**  \n\nThe citation provides the URL to the open-source code implementation of the LOA (likely an acronym for a specific system or framework). This code is being cited because it allows others to reproduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yumeng5/LOTClass\nCitation Context: Our method (1) associates semanti-cally related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% ac-curacy on four benchmark datasets including topic and sentiment classification without us-ing any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name [URL_CITE] .\nFootnote or Reference text (if exists): 1 Source code can be found at https://github.com/yumeng5/LOTClass.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hunch.net/~vw/\nCitation Context: Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba-sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [URL_CITE] with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.\nFootnote or Reference text (if exists): 4 http://hunch.net/\u02dcvw/\nSection Titles (if exists):  ['3 High Water Mark for Tandem Anchors', '3.2 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:**  Implementation\n\n**Explanation:**\n\n* **Software:** The URL points to the website for Vowpal Wabbit (vw),"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/miso-belica/jusText\nCitation Context: Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [URL_CITE] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences.\nFootnote or Reference text (if exists): 5 https://github.com/miso-belica/ jusText\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://jsoup.org/\nCitation Context: Text Collection: Using jsoup (Hedley, 2017) [URL_CITE] , we parse the HTML file and for each non-empty tag encountered we extract a tuple consisting of the text and its XPath.\nFootnote or Reference text (if exists): Jonathan Hedley. 2017. jsoup (1.11.3). https://jsoup.org/.\nSection Titles (if exists):  ['3 Approach', '3.1 Domain-Independent Approach (DI)']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Tool\n* **Citation Reason:**  Implementation Detail \n\n**Explanation:**\n\n* **Tool:**  jsoup is explicitly identified as a tool used for parsing HTML.\n*"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: (2009) we use asymmetric \u03b1 and symmetric \u03b2; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [URL_CITE] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations).\nFootnote or Reference text (if exists): 2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.\nSection Titles (if exists):  ['3 Models', '3.1 Latent variable context models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/parag1604/A2L\nCitation Context: : where, y max = arg max y s(y). The strategy se-lects examples for which M margin \u2264 \u03c4 [URL_CITE] , where \u03c4 1 is a hyper-parameter. We use Viterbi\u2019s algorithm (Ryan and Nudd, 1993) to compute the scores s(y).\nFootnote or Reference text (if exists): 1 Codes for the experiments are available at the following github link: https://github.com/parag1604/A2L.\nSection Titles (if exists):  ['4 Experiments', '4.1 Active Learning Strategies for Sequence Tagging']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**Artifact Type:** Code\n\n**Citation Reason:**  Implementation Details\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which strongly suggests the presence of source code.\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nCitation Context: The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task [URL_CITE] requires systems to take set of documents and produce a comprehensive set of triples that encode relationships between and at-tributes of the named-entities that are mentioned in the corpus. Systems are evaluated based on the fi-delity of the constructed knowledge base. For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:\nFootnote or Reference text (if exists): 1 See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Benchmark/Evaluation\n\n**Citation Reason:**  \n\n* **Describes the Task:** The citation provides a link to the official guidelines for the NIST TAC KBP"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mappings.dbpedia.org/server/ontology/classes/\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Ontology\n\n**Citation Reason:**  \n\n* **Definition Source:** This URL points to the DBPedia ontology, specifically the classes section. The citation is using this"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JRC1995/Multilingual-BERT-Disaster\nCitation Context: Our code, dataset, and other resources are available on Github. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JRC1995/ Multilingual-BERT-Disaster\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmaww/SENT\nCitation Context: We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/rtmaww/SENT\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:**  Providing access to the implemented algorithm and training processes described in the text. \n\n\nLet me know if you have any"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fairseq-py\nCitation Context: For C ONV S2S [URL_CITE] and T-C ONV S2S, we used 512 dimensional hidden states and 512 dimensional word and position embeddings. We trained our convolutional models with Nesterov\u2019s accelerated gradient method (Sutskever et al., 2013) using a momentum value of 0.99 and renormalized gra-dients if their norm exceeded 0.1 (Pascanu et al., 2013).\nFootnote or Reference text (if exists): 3 We used the code available at https://github.com/facebookresearch/fairseq-py.\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bitbucket.org/yoavgo/word2vecf\nCitation Context: SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014). [URL_CITE]\nFootnote or Reference text (if exists): 3 http://bitbucket.org/yoavgo/word2vecf\nSection Titles (if exists):  ['2 Experiment Setup', '2.1 Word Representations', '2.1.2 Representation Models']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Implementation\n* **TYPE:**  Software \n* **FUNCTION:**  Training of Word Embeddings\n\n**Reason for Citation:**\n\nThe citation points to the specific software implementation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.purl.com/net/ArabicSentiment\nCitation Context: The resources created as part of this project (Arabic sentiment lex-icons, Arabic sentiment annotations of social me-dia posts, and English sentiment annotations of their translations) are made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://www.purl.com/net/ArabicSentiment\nSection Titles (if exists):  ['9 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.purl.com/net/lexicons\nCitation Context: The sentiment lex-icon features are derived from existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad and Turney, 2010; Moham-mad and Turney, 2013), Bing Liu\u2019s Lexicon (Hu and Liu, 2004), and MPQA Subjectivity Lexicon (Wilson et al., 2005), as well as automatically gen-erated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Kiritchenko et al., 2014). [URL_CITE]\nFootnote or Reference text (if exists): 4 http://www.purl.com/net/lexicons\nSection Titles (if exists):  ['6 English Sentiment Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Lexicon/Resource Collection\n\n**Citation Reason:**  Source/Inspiration\n\n**Explanation:**\n\n* **Lexicon/Resource Collection:** The URL points to a collection of"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/treex\nCitation Context: Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and \u017dabokrtsk\u00fd, 2010). [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://ufal.mff.cuni.cz/treex. Domain-independent deep syntax analysis for several languages is included in this framework; the English pipeline used here involves a statistical part-of-speech tagger (Spoustov\u00e1 et al., 2007) and a dependency parser (McDonald et al., 2005), fol-lowed by a rule-based conversion to deep syntax trees.\nSection Titles (if exists):  ['2 Generator Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact classification:\n\n**ROLE:** Method\n**TYPE:** Software Framework\n**FUNCTION:**  Introduce and Explain\n\n**Reasoning:**\n\n* **Software Framework:** The citation points to the Treex NLP framework,"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016) and Kasai et al. (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 512 units each.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/faiss\nCitation Context: As for the evalu-ation network R, we use a multilayer perceptron with two hidden layers of size 512. For efficient nearest neighbor search in the extracting step, we use the open-source Faiss library (Johnson et al., 2017) [URL_CITE] . We calculate the similarity of sentences in each episode instead of each batch for computa-tional efficiency.\nFootnote or Reference text (if exists): 2 https://github.com/facebookresearch/ faiss\nSection Titles (if exists):  ['4 Experiments', '4.2 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Library/Software\n\n* **Citation Reason:**  \n\n    * **Acknowledgement:** The authors are citing the Faiss library because they used it in their implementation"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: We train a multi-class logistic regression model [URL_CITE] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Identifying Simplification Rules', '2.2 Lexical Simplification Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20\nCitation Context: Zhang et al. (2018) [URL_CITE] approach retweet prediction as a multi-class classification problem, and present a feature-weighted model, where weights are com-puted using information gain.\nFootnote or Reference text (if exists): Yang Zhang, Zhiheng Xu, and Qing Yang. 2018. Predicting Popularity of Messages in Twitter using a Feature-weighted Model. http://ww.nlp.ia.ac.cn/2012papers/gjhy/gh154.pdf,20.\nSection Titles (if exists):  ['3 Related work', '3.2 Predicting tweet popularity']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Research Paper\n\n**Citation Reason:**  \n\n* **Supporting Prior Work:** The citation is used to introduce and reference a previous study by Zhang et al. ("
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://seekingalpha.com/\nCitation Context: Our data [URL_CITE] consists of transcripts of 12,285 earn-ings calls held between January 1, 2010 and De-cember 31, 2017. In order to control for analyst coverage effects (larger companies with a greater market share will typically be covered by more an-alysts), we include only calls from S&P 500 com-panies. We split the data by year into training, val-idation and testing sets (see Table 1).\nFootnote or Reference text (if exists): 2 In Appendix A in supplemental material we provide the stock tickers for the calls in our data; the corpus can be re-assembled from multiple sources, such as https: //seekingalpha.com/.\nSection Titles (if exists):  ['3 Data and pre-processing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msci.com/gics\nCitation Context: Breakdown of results by industry. We ana-lyze errors on the validation data by segmenting earnings calls by each company\u2019s Global Industry Classification Standard (GICS) sector [URL_CITE] . See Fig-ure 2 for the breakdown results.\nFootnote or Reference text (if exists): 21 See https://www.msci.com/gics. There are 11 broad industry sectors.\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.4 Results.']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n**ROLE:**  Classification System\n**TYPE:**  Industry Classification\n**FUNCTION:**  Categorization \n\n**Reason for Citation:**\n\nThe citation points to the Global Industry Classification Standard (GICS"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sle.sharp.co.uk/senseval2/\nCitation Context: We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [URL_CITE] We set the NB model parameters in several ways. We optimized JL (using the RFE s).\nFootnote or Reference text (if exists): 2 http://www.sle.sharp.co.uk/senseval2/\nSection Titles (if exists):  ['2 Objective Functions: Naive-Bayes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Code\n\n**Citation Reason:** Implementation\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which strongly suggests the artifact is code.\n"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh\nCitation Context: Node Number Terms C18.452.297 diabetes mellitus insulin-dependent diabetes C18.452.297.267 mellitus C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH [URL_CITE] tree. Node numbers represent hierarchical structure of terms\nFootnote or Reference text (if exists): 1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research.\nSection Titles (if exists):  ['1. Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [URL_CITE] children of Body Regions. The longer the ID of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous\nFootnote or Reference text (if exists): 1 http://www.nlm.nih.gov/mesh/meshhome.html; the work reported in this paper uses MeSH 2001.\nSection Titles (if exists):  ['3 The Lexical Hierarchy: MeSH']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Knowledge Base\n\n**Citation Reason:**  Explanation and Source\n\n**Explanation:**\n\n* **Knowledge Base:** MeSH (Medical Subject Headings) is a controlled"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf\nCitation Context: This corresponds to one spatial location in the image. Hard attention has previously been used in the context of object recognition (Mnih et al., 2014 [URL_CITE] ; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al.\nFootnote or Reference text (if exists): Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. 2014. Recurrent models of visual attention. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2204\u2013 2212. http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.2 Hard Stochastic attention']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf\nCitation Context: Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [URL_CITE] but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al.\nFootnote or Reference text (if exists): Oriol Vinyals, \u0141 ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-mar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2773\u2013 2781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.1 Soft attention']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Paper\n* **Citation Reason:**  \n\n    * **Foundational Work:** The citation introduces soft attention and its initial application in syntactic constituency parsing.  "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP\nCitation Context: We used JUMAN (Kurohashi and Kawahara, 2009a) for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b) [URL_CITE] . Since KNP internally has a flag that indicates either an \u201cobligatory case\u201d or an \u201cadjacent case\u201d, we regarded dependency relations flagged by KNP as obligatory in the sentence com-pression.\nFootnote or Reference text (if exists): Sadao Kurohashi and Daisuke Kawahara, 2009b. KN parser (Kurohashi-Nagao parser) 3.0 Users Man-ual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.\nSection Titles (if exists):  ['5 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN\nCitation Context: We used JUMAN (Kurohashi and Kawahara, 2009a) [URL_CITE] for word segmentation and part-of-speech tagging, and we calculated idf over Mainichi newspaper articles from 1991 to 2005. For the de-pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b).\nFootnote or Reference text (if exists): Sadao Kurohashi and Daisuke Kawahara, 2009a. Japanese Morphological Analysis System JUMAN 6.0 Users Manual. http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.\nSection Titles (if exists):  ['5 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:** Method\n**TYPE:** Tool\n**FUNCTION:**  Word Segmentation and Part-of-Speech Tagging \n\n\n**Citation Reason:**\n\nThe authors cite JUMAN"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hlt.suda.edu.cn/index.php/SUCDT\nCitation Context: Experimental results show that 1) the two conversion approaches achieve nearly the same conversion accuracy, and 2) direct treebank conversion is superior to indirect multi-task learning in exploiting multiple treebanks in methodology simplicity and performance, yet with the cost of manual annotation. We release the annotation guideline and the newly annotated data in [URL_CITE] http://hlt.suda.edu.cn/index.php/SUCDT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:** Data Availability\n\n**Explanation:**\n\n* The URL points to a website (http://hlt.suda.edu.cn/index"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE/tree/repro\nCitation Context: ERNIE baselines. E RNIE [URL_CITE] (Sun et al., 2019a,b) enhances B ERT through knowledge integration using a entity-level masked LM task and more raw text from the Web resources, which achieves the currently best results on Chinese NER. E RNIE +F UR +E NT is a stronger baseline, which uses the same entity dictionary as ours for entity-level masking and further pre-trains E RNIE on the same raw text as ours.\nFootnote or Reference text (if exists): 5 https://github.com/PaddlePaddle/ERNIE/tree/repro\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PaddlePaddle/ERNIE\nCitation Context: \u2022 ERNIE (115M) [URL_CITE] , a.k.a Enhanced Represen-tation through Knowledge Integration (Sun et al., 2019), which is trained with not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc.\nFootnote or Reference text (if exists): 3 https://github.com/PaddlePaddle/ERNIE\nSection Titles (if exists):  ['5 Experiments', '5.1 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Code\n\n**Citation Reason:**  Source Code\n\n**Explanation:**\n\n* **Code:** The URL points to a GitHub repository, which is a common platform for"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2JTbGnI\nCitation Context: Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [URL_CITE] and Germany (Burger, 2019).\nFootnote or Reference text (if exists): 5 U.S. Dept. of Energy: https://bit.ly/2JTbGnI\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2QHE5O3\nCitation Context: Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, 4 China [URL_CITE] and Germany (Burger, 2019).\nFootnote or Reference text (if exists): 5 China Electricity Council; trans. China Energy Portal: https://bit.ly/2QHE5O3\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Report or Statistical Data\n* **Citation Reason:**  Supporting Data\n\n**Explanation:**\n\nThe URL points to a source likely containing data about China's energy consumption or production"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/\nCitation Context: The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [URL_CITE] . The statistics are shown in Table 1.\nFootnote or Reference text (if exists): 3 https://dev.twitter.com/\nSection Titles (if exists):  ['3 Experiments and Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stream.twitter.com\nCitation Context: We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter\u2019s streaming API [URL_CITE] . The tweets are order by their time-stamps.\nFootnote or Reference text (if exists): 1 http://stream.twitter.com\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** API Documentation\n\n**Citation Reason:**  \n\n* **Source of Data:** The citation points to the Twitter Streaming API (http://stream.twitter.com) as"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Our objective is to insert a collision c into x d such that the rank of S(x d , c) among all sentences is high. We use the published models [URL_CITE] for evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-scorer\nCitation Context: This task has three evaluations: (i) N O S CORE E V \u2013 accuracy of claim verifica-tion, neglecting the validity of evidence; (ii) S CORE E V \u2013 accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for S UPPORTED and R E - FUTED ; (iii) F 1 \u2013 between the predicted evidence sentences and the ones chosen by annotators. We use the officially released evaluation scorer [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/fever-scorer\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:**  Software\n\n**Citation Reason:**  Evaluation Metric\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository, which typically hosts software projects."
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.grammarly.com/\nCitation Context: (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, , and Grammarly [URL_CITE] to help us create a set of rules.\nFootnote or Reference text (if exists): 3 https://www.grammarly.com/\nSection Titles (if exists):  ['4 Experimental setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.grammarbot.io/\nCitation Context: (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API, [URL_CITE] , and Grammarly to help us create a set of rules.\nFootnote or Reference text (if exists): 2 https://www.grammarbot.io/\nSection Titles (if exists):  ['4 Experimental setup', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Software/API\n* **Citation Reason:**  The citation is for the GrammarBot API, which was used as a tool to help create a set of rules for the rule"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:**  Repository \n* **Citation Reason:** Implementation Details\n\n**Explanation:**\n\nThe URL points to the main GitHub repository for HuggingFace.  The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/dynet\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [URL_CITE] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 5 https://github.com/clab/dynet\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **ROLE:**  Framework/Library\n* **TYPE:**  Code\n* **FUNCTION:**  Implementation \n\n**Reasoning:**\n\n* **Framework/Library:** Mantidae is presented as"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/\nCitation Context: In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [URL_CITE] We first compare levels of truncation of word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors based methods for training word2vec vectors.\nFootnote or Reference text (if exists): 1 https://dumps.wikimedia.org/\nSection Titles (if exists):  ['4 Experiments on Word/Phrase Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tombosc/cpae\nCitation Context: For the evaluation of definition embeddings, we used the modified Word Embedding Benchmarks projects [URL_CITE] , following Bosc and Vincent (2018). These benchmarks include SimLex999 (SL999), SimLex333 (SL333) (Hill et al., 2015), SimVerb (SV) (Gerz et al., 2016), MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012).\nFootnote or Reference text (if exists): 3 https://github.com/tombosc/cpae\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Definition Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:**  Software/Code Repository\n\n**Citation Reason:**  \n\n* **Implementation Details:** The URL points to a GitHub repository (`https://github.com/tomb"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n* **Artifact Type:** Code \n* **Citation Reason:** Implementation\n\n**Explanation:**\n\nThe URL points to a GitHub repository containing the Charniak-Johnson parser implementation. The"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/unilm\nCitation Context: BART 3 (P:0.83, R: 1.0) [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MASS\nCitation Context: BART 3 (P:0.83, R: 1.0) [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/microsoft/unilm 8 https://github.com/microsoft/MASS\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact:\n\n* **Artifact Type:** Code\n* **Citation Reason:**  Implementation Details\n\n**Explanation:**\n\nThe URL points to a GitHub repository for the MASS model.  Since the citation context mentions BART "
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: We used equal numbers of sentences from three different genres [URL_CITE] :\nFootnote or Reference text (if exists): 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. Distributed by Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['3 Linguistic acceptability study', '3.1 Dataset creation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wiktionary.org/wiki/Wiktionary:Statistics\nCitation Context: The English language edition contains over 500,000 word senses. [URL_CITE]\nFootnote or Reference text (if exists): 2 as of May 2013, see http://en.wiktionary.org/wiki/Wiktionary:Statistics.\nSection Titles (if exists):  ['2 Resource Overview']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**ROLE:**  Statistical Data Source\n\n**TYPE:**  Repository\n\n**FUNCTION:**  Support a factual claim\n\n\n**Explanation:**\n\n* **Statistical Data Source:** The URL points to"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2003T05\nCitation Context: We also experiment with the English Gigaword corpus, [URL_CITE] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2003T05\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org\nCitation Context: There are custom solutions that can easily simplify the slot filling task. For instance, subject entities can be used for lookups by title in Wikipedia to retrieve knowledge (this heuristic will always work for zsRE), and structured human-curated resources (such as Wikidata [URL_CITE] ) could be used to get all an-swers right. Nevertheless, we are interested in test-ing if a general model can extract attributes about specific entities from a large body of text.\nFootnote or Reference text (if exists): 13 https://www.wikidata.org\nSection Titles (if exists):  ['8 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Resource\nTYPE: Knowledge Base\nFUNCTION:  Example/Illustrative  \n\n\n**Explanation:**\n\n* **ROLE: Resource:** Wikidata is a structured repository of knowledge, not a model or algorithm.\n* **TYPE: Knowledge Base:**"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www-i6.informatik.rwth-aachen.de/goldAlignment/\nCitation Context: The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German\u2192English we use the Europarl v7 cor-pus as training data and the gold alignments [URL_CITE] pro-vided by Vilar et al. (2006).\nFootnote or Reference text (if exists): 4 https://www-i6.informatik.rwth-aachen.de/goldAlignment/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.eecs.umich.edu/~mihalcea/wpt/index.html#resources\nCitation Context: The au-thors provide pre-processing and scoring scripts 2 for three different datasets: Romanian\u2192English, English\u2192French and German\u2192English. Train-ing data and test data for Romanian\u2192English and English\u2192French are provided by the NAACL\u201903 Building and Using Parallel Texts word align-ment shared task [URL_CITE] (Mihalcea and Pedersen, 2003). The Romanian\u2192English training data are aug-mented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M.\nFootnote or Reference text (if exists): 3 http://web.eecs.umich.edu/\u02dcmihalcea/wpt/index.html#resources\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup', '5.1.1 Alignment Task']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a breakdown of the artifact type and citation reason:\n\n**Artifact Type:** Dataset\n\n**Citation Reason:**  The URL points to a webpage that hosts pre-processing and scoring scripts *along with* the training and test data for the"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://kheafield.com/code/kenlm/\nCitation Context: For DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For S RC LM, we trained an N -gram language model (Heafield, 2011) [URL_CITE] on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we se-lected 8M monolingual data with the lowest cross-entropy for self-training.\nFootnote or Reference text (if exists): 9 https://kheafield.com/code/kenlm/\nSection Titles (if exists):  ['A Appendix', 'A.3 Comparison with Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Then we used the bilingual dictionary to compute the data uncer-tainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped them 38 into [URL_CITE] equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual 36 data.\nFootnote or Reference text (if exists): 5 https://github.com/clab/fast_align\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']"
                },
                {
                    "role": "assistant",
                    "content": "Here's a classification of the artifact and citation reason:\n\n**Artifact Type:** Software\n\n**Citation Reason:** Implementation Detail\n\n**Explanation:**\n\n* **Software:** The URL points to a GitHub repository (`https://github.com/clab"
                }
            ]
        }
    ]
]
[
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH [URL_CITE] and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 5 http://pytorch.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/solr/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR [URL_CITE] for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/solr/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y [URL_CITE] for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) [URL_CITE] for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): Mart\u00edn Abadi et al. 2015. TensorFlow: Large-scale machine learning on heterogeneous sys-tems. Software available from tensorflow.org. https://www.tensorflow.org/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE [URL_CITE] and S OLR for Information Retrieval, and SCIKIT - LEARN , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 2 https://lucene.apache.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org\nCitation Context: There are many successful frameworks such as S TANFORD C ORE NLP (Manning et al., 2014), NLTK (Bird et al., 2009), and SPA C Y for NLP, L UCENE and S OLR for Information Retrieval, and SCIKIT - LEARN [URL_CITE] , P Y T ORCH and T ENSOR - F LOW (Abadi et al., 2015) for general Machine Learning (ML) with a special focus on Deep Learning (DL), among others. All of these frame-works touch upon several aspects of Machine Reading, but none of them offers dedicated sup-port for modern MR pipelines.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yanaiela/demog-text-removal\nCitation Context: We explore means for im-proving the effectiveness of the adversarial train-ing procedure (section 5.2). [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/UnsupervisedQA\nCitation Context: To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [URL_CITE]\nFootnote or Reference text (if exists): 1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/antonisa/inflection\nCitation Context: The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. [URL_CITE] Also, we identify the crucial factors for suc-cess with cross-lingual transfer for morpho-logical inflection: typological similarity and a common representation across languages.\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/antonisa/inflection.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxjiao/UncSamp\nCitation Context: Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code is available at https://github.com/wxjiao/UncSamp\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/subhadarship/learning-to-unjumble\nCitation Context: Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/subhadarship/learning-to-unjumble.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs.stanford.edu/\nCitation Context: We see many opportunities for improvement, considering the poor performance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to the oracle models. [URL_CITE] To this end, it would be instructive to understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality.\nFootnote or Reference text (if exists): 11 To facilitate future work, all of our models are publicly available at http://cs.stanford.edu/ \u223c valentin/.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://132.70.6.148:at:8080/exploration\nCitation Context: The web application of our system is available [URL_CITE] http://132.70.6.148:at:8080/exploration\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Application to the Health-care Domain']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/airsplay/R2R-EnvDrop\nCitation Context: Empirically, we show that our agent is substantially better at generalizabil-ity when fine-tuned with these triplets, outper-forming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code, data, and models publicly available at: https://github.com/airsplay/R2R-EnvDrop\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/research/sp\nCitation Context: Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our system and code available at http://statnlp.org/research/sp.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zdou0830/DAFE\nCitation Context: In addition, we show that combining our method with back translation can further improve the performance of the model. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at: https://github.com/zdou0830/DAFE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xuuuluuu/SynLSTM-for-NER\nCitation Context: Our fur-ther analysis demonstrates that our model can capture longer dependencies compared with strong baselines. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our code publicly available at https://github.com/xuuuluuu/SynLSTM-for-NER.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ElliottYan/Multi\nCitation Context: These results demonstrate the effectiveness of the MUTE, as well as its ef-ficiency in both the inference process and pa-rameter usage. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/ElliottYan/Multi Unit Transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jialuli-luka/SyntaxVLN\nCitation Context: We also show that our agent is better at aligning instructions with the cur-rent visual information via qualitative visual-izations. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and models: https://github.com/jialuli-luka/SyntaxVLN\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/challenge\nCitation Context: \u2022 Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [URL_CITE] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it).\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/challenge\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: Our best neural system in turn outperformed our baseline, achieving an average success rate of 34%. Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at [URL_CITE] https://github.com/lijuncen/Sentiment-and-Style-Transfer.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset\nCitation Context: We use one of the classic NLP tasks that commonly suffer profanity issues - text style transfer, to eval-uate the effectiveness of the proposed framework. Particularly, we conduct experiments on a subset of the widely used Yelp dataset [URL_CITE] . The dataset consists of product reviews aligned with sentiment ratings from 1 to 5.\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 4 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wuch15/NRHUB\nCitation Context: We conducted experiments on a real-world news recommendation dataset [URL_CITE] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pushshift/api\nCitation Context: Data was scraped from the two subreddits by querying the Pushshift API. [URL_CITE] 3 years\u2019 worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters.\nFootnote or Reference text (if exists): 4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.)\nSection Titles (if exists):  ['4 Data', '4.1 Data collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://msnews.github.io/index.html\nCitation Context: We conduct extensive experiments on two real-world datasets to evaluate the effectiveness of Hi-eRec. The first one is the public MIND dataset (Wu et al., 2020d) [URL_CITE] . It is constructed by user behavior data collected from Microsoft News from October 12 to November 22, 2019 (six weeks), where user data in the first four weeks was used to construct users\u2019 reading history, user data in the penultimate week was used for model training and user data in the last week was used for evaluation.\nFootnote or Reference text (if exists): 1 We use the small version of MIND for quick experiments. This dataset is at https://msnews.github.io/index.html\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental Datasets and Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/bigquery\nCitation Context: We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [URL_CITE] . Each of these posts is a headline from a news source.\nFootnote or Reference text (if exists): 2 https://cloud.google.com/bigquery\nSection Titles (if exists):  ['2 The Humor Dataset', '2.2 Collecting Headlines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs in one month (Dec. 13, 2018 to Jan. 12, 2019). The detailed statistics are shown in Ta-ble 1.\nFootnote or Reference text (if exists): 3 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stp.lingfil.uu.se/~nivre/research/Penn2Malt.html\nCitation Context: We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour-nal sections 2\u201321 for training, 22 for development testing and 23 for final testing. Gold-standard de-pendency trees are derived from bracketed sentences in the treebank using Penn2Malt [URL_CITE] , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011).\nFootnote or Reference text (if exists): 1 http://stp.lingfil.uu.se/\u02dcnivre/research/Penn2Malt.html\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [URL_CITE] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nCitation Context: Treebank (POS-PTB; Marcus et al., 1993) and B; Cer et al., 2017) and Quora Question the Universal Dependencies English Web Tree-Pairs [URL_CITE] (QQP); natural language inference (NLI) bank (POS-EWT; Silveira et al., 2014); syntactic with Multi-Genre NLI (MNLI; Williams et al., constituency ancestor tagging, i.e., predicting the 2018), SQuAD (Rajpurkar et al., 2016) con-constituent label of the parent (Parent), grandpar-verted into Question-answering NLI (QNLI; Wang ent (GParent), and great-grandparent (GGParent) et al., 2019b), Recognizing Textual Entailment of each word in the PTB phrase-structure tree; 1,2,3,5 (RTE; Dagan et al., 2005, et seq. ), and the semantic tagging task (ST; Bjerva et al., 2016; Winograd Schema Challenge (Levesque, 2011) re-\nFootnote or Reference text (if exists): 20 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\nSection Titles (if exists):  ['References Christopher Clark, Kenton Lee, Ming-Wei Chang,']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://universaldependencies.github.io/docs/\nCitation Context: We proposed a typed dependency scheme based on the well-known and widely used Stanford typed dependencies (SD), which originated in English and has since been extended to many languages, but not to Japanese. The Universal dependencies (UD) (McDonald et al., 2013; de Marneffe et al., 2014) has been developed based on SD in order to design the cross-linguistically consistent treebank annotation [URL_CITE] . The UD for Japanese has also been discussed, but no treebanks have been provided yet.\nFootnote or Reference text (if exists): 4 http://universaldependencies.github.io/docs/.\nSection Titles (if exists):  ['2 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hdl.handle.net/11234/1-1983xxx\nCitation Context: We evaluate the proposed framework on the Stan-ford Dependency (SD) conversion of the English Penn Treebank (PTB 3.0) and the Universal De-pendencies (UD 2.2) (Nivre et al., 2018) [URL_CITE] tree-banks used in CoNLL 2018 shared task(Zeman et al., 2018). For English, we use the standard train/dev/test splits of PTB (train=\u00a72-21, dev=\u00a722, test=\u00a723), POS tags were assigned using the Stan-ford tagger with 10-way jackknifing of the training corpus (accuracy \u2248 97.3%). For 12 languages se-lected from UD 2.2, we use CoNLL 2018 shared task\u2019s official train/dev/test splits, POS tags were assigned by the UDPipe (Straka et al., 2016).\nFootnote or Reference text (if exists): Joakim Nivre et al. 2018. Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Insti-tute of Formal and Applied Linguistics, Charles Uni-versity, Prague, http://hdl.handle.net/11234/1-1983xxx.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://arxiv.org/abs/1809.00794\nCitation Context: Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. [URL_CITE]\nFootnote or Reference text (if exists): 1 An expanded version of the tech report can be found at https://arxiv.org/abs/1809.00794\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow\nCitation Context: It is also worth noting that, building the VAE language model (including data reading, model construction, and optimization) on Texar uses only 70 lines of code (with the length of each line < 80 chars). As a (rough) reference, a popular public TensorFlow code (Li, 2017) [URL_CITE] of the same model has used around 400 lines of code for the same part (without line length limit).\nFootnote or Reference text (if exists): Zhong-Yi Li. 2017. https://github.com/Chung-I/Variational-Recurrent-Autoencoder-Tensorflow.\nSection Titles (if exists):  ['4 Case Study: Transformer on Different Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/fairseq-py\nCitation Context: We have performed all our experiments with the freely available Sequence-to-Sequence Py-Torch toolkit from Facebook AI Research, [URL_CITE] called fairseq-py. It implements a convolutional model which achieves very competitive results (Gehring et al., 2017).\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ fairseq-py\nSection Titles (if exists):  ['5 Experimental evaluation: improving WMT\u201914 En-De NMT systems', '5.1 Baseline NMT systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/timvdc/poetry\nCitation Context: In order to facilitate reproduction of the results and encourage further research, the poetry genera-tion system is made available as open source soft-ware. The current version can be downloaded at [URL_CITE] https://github.com/timvdc/poetry.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/language/tree/master/language/realm\nCitation Context: All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [URL_CITE] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search).\nFootnote or Reference text (if exists): 19 https://github.com/google-research/language/tree/master/language/realm\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.texar.io.1\nCitation Context: The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at [URL_CITE] https: //www.texar.io. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/lda\nCitation Context: A classical proba-bilistic topic model. We use the LDA pack-age [URL_CITE] for its implementation. We use the set-tings with iteration number n = 2000, the Dirichlet parameter for distribution over top-ics \u03b1 = 0.1 and the Dirichlet parameter for distribution over words \u03b7 = 0.01.\nFootnote or Reference text (if exists): 4 https://pypi.python.org/pypi/lda\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nCitation Context: For our system, we used the Le Zhang\u2019s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [URL_CITE] We set the iteration limit to 100. The accuracy on the training data is 90.18%.\nFootnote or Reference text (if exists): 4 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/efficient-java-matrix-library/\nCitation Context: We regularize our learning objec-tive using the L 2 -norm and a \u03bb value of 0.1. The set of mathematical relations supported by our im-plementation is {+, \u2212, \u00d7, /}.Our implementation uses the Gaussian Elimination function in the Effi-cient Java Matrix Library (EJML) (Abeles, 2014) [URL_CITE] to generate answers given a set of equations.\nFootnote or Reference text (if exists): Peter Abeles. 2014. Efficient java matrix library. https://code.google.com/p/efficient-java-matrix-library/.\nSection Titles (if exists):  ['7 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: We implemented our model in Tensorflow [URL_CITE] . We tuned the hyper-parameters via the development set.\nFootnote or Reference text (if exists): 3 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiment', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nCitation Context: It is a sparsity-enhanced non-probabilistic topic model. We use the code released by the authors [URL_CITE] . We set the regularization constants as \u03bb = 0.3, \u03c1 = 0.0001 and the maximum number of itera-tions of hierarchical sparse coding, dictionary learning as 100.\nFootnote or Reference text (if exists): 5 http://bigml.cs.tsinghua.edu.cn/jun/stc.shtml/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipediadatabase\nCitation Context: Since we are advocating a high volume topic stream, we require millions of topics. To ensure a high number of topics, we treat the entire English part (4.4 mio articles) of Wikipedia [URL_CITE] as a proxy for a collection of topics and turn it into a stream. Each article is considered to be an unstructured textual representation of a topic time-stamped by its latest verified update.\nFootnote or Reference text (if exists): 2 http://en.wikipedia.org/wiki/Wikipedia database\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2\nCitation Context: For the experiments in this paper, we used a full dump of Wikipedia from September 4, 2010. [URL_CITE] In-cluded in this dump is a total of 10,355,226 articles, of which 1,019,490 have been geotagged. Excluding various types of special-purpose articles used pri-marily for maintaining the site (specifically, redirect articles and articles outside the main namespace), the dump includes 3,431,722 content-bearing arti-cles, of which 488,269 are geotagged.\nFootnote or Reference text (if exists): 2 http://download.wikimedia.org/enwiki/20100904/pages-articles.xml.bz2\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.google.com\nCitation Context: In this section we will describe the various similarity features we use in our model. We utilize page counts and snippets returned by the Google [URL_CITE] search engine for simple text queries to define various similarity scores.\nFootnote or Reference text (if exists): 3 http://www.google.com\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bitterlemons.org\nCitation Context: Our corpus consists of articles published on the bitterlemons website [URL_CITE] . The website is set up to \u201ccontribute to mutual understanding [between Palestinians and Israels] through the open exchange of ideas\u201d.\nFootnote or Reference text (if exists): 1 http://www.bitterlemons.org\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: ELI5 (Fan et al., 2019b) [URL_CITE] is a collection of question-answer-evidence triples where the ques-tions are complex, and the answers are long, ex-planatory, and free-form. For dev and test, we col-lect annotations using Amazon Mechanical Turk, asking evaluators to select which supporting docu-ments from Wikipedia can be used to answer the question. We treat these as gold provenance anno-tations for evaluation (details in section 4).\nFootnote or Reference text (if exists): 8 https://yjernite.github.io/lfqa.html\nSection Titles (if exists):  ['3 Tasks', '3.4 Open Domain Question Answering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wikitravel.org/en/Wikitravel:Articletemplates/Sections\nCitation Context: Each article describes a city or a larger geo-graphic area that is of interest to tourists. In addition, all articles contain sections [URL_CITE] describing different as-pects of a city from a tourism point of view (e.g., places to see, transportation, shopping and eating). For our proof of concept, we performed IE only on the \u2018places to see\u2019 sections.\nFootnote or Reference text (if exists): 7 http://wikitravel.org/en/Wikitravel:Articletemplates/Sections\nSection Titles (if exists):  ['4 Human Subject Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/chaitanyamalaviya/lang-reps\nCitation Context: Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phy-logenetic neighbors. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PravallikaRao/SpellChecker\nCitation Context: \u2022 We create synthetic datasets [URL_CITE] of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus.\nFootnote or Reference text (if exists): 1 https://github.com/PravallikaRao/SpellChecker\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N10-1089\nCitation Context: Korkontzelos and Manandhar (2010) [URL_CITE] reports an improvement in base-phrase chunking by pre-grouping MWEs as words-with-spaces. They fo-cus on compound nouns, adjective-noun construc-tions, and named entities.\nFootnote or Reference text (if exists): Ioannis Korkontzelos and Suresh Manandhar. 2010. Can recognising multiword expressions improve shallow parsing? In Human Language Technolo-gies: The 2010 Annual Conference of the North American Chapter of the Association for Compu-tational Linguistics. Association for Computational Linguistics, Los Angeles, California, pages 636\u2013 644. http://www.aclweb.org/anthology/N10-1089.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wxjiao/UncSamp\nCitation Context: Ex-tensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side. [URL_CITE]\nFootnote or Reference text (if exists): 1 The source code is available at https://github.com/wxjiao/UncSamp\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ntu.edu.au/education/langs/ielex/IE-DATA1\nCitation Context: Since we used automatic tools for preparing our corpus rather than careful linguistic analysis, our cognate list is much noiser in terms of the pres-ence of borrowed words and phonemeic transcrip-tion errors compared to the ones used by previous approaches (Swadesh, 1955; Dyen et al., 1997 [URL_CITE] ). The benefit of our mechanical preprocessing is that more cognate data can easily be made available, allowing us to effectively train richer models.\nFootnote or Reference text (if exists): I. Dyen, J.B. Kruskal, and P. Black. 1997. FILE IE-DATA1. Available at http://www.ntu.edu.au/education/langs/ielex/IE-DATA1.\nSection Titles (if exists):  ['4 Experiments', '4.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wanqiulong0923/TED-CDB\nCitation Context: The dataset and our Chinese annotation guidelines has been made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-personal.umich.edu/~benking/resources/mixed-language-annotations-release-v1.0.tgz\nCitation Context: For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author\u2019s website [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www-personal.umich.edu/\u02dcbenking/resources/mixed-language-annotations-release-v1.0.tgz\nSection Titles (if exists):  ['3 Task Definition', '3.1 Evaluation Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt17/translation-task/preprocessed/\nCitation Context: All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://data.statmt.org/wmt17/translation-task/preprocessed/\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator\nCitation Context: The source of these files is Xinhua newswire. The annotation is carried out on the PDTB annotation tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 http://www.seas.upenn.edu/\u223cpdtb/tools.shtml#annotator\nSection Titles (if exists):  ['4 Annotation experiment', '4.1 Set-up']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt19/translation-task.html\nCitation Context: We also collect additional monolingual data from WMT news-crawl, news-commentary, common-crawl, europarl-v9, news-discussions and wikidump datasets in all 16 lan-guages including English. [URL_CITE] The amount of mono-lingual data varies from 2 million sentences in Zh to 270 million in De. The distribution of our paral-lel and monolingual data is depicted in Figure 1.\nFootnote or Reference text (if exists): 2 Followed the versions recommended by WMT\u201919 shared task, as in http://statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://casmacat.eu/corpus/global-voices.html\nCitation Context: Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, [URL_CITE] EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh.\nFootnote or Reference text (if exists): 10 http://casmacat.eu/corpus/global-voices.html\nSection Titles (if exists):  ['B Data Details for Replication']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebp/scikit-survival\nCitation Context: Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models\u2019 performances dealing with different time-to-events [URL_CITE] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 \u00b1 0.025 0.504 \u00b1 0.032 0.503 \u00b1 0.037 CPH + Feature Set 2 0.621 \u00b1 0.014 0.632 \u00b1 0.033 0.642 \u00b1 0.030 CPH + Hidden Features 0.674 \u00b1 0.023 0.696 \u00b1 0.022 0.710 \u00b1 0.022 MLP + Feature Set 1 0.502 \u00b1 0.023 0.509 \u00b1 0.026 0.509 \u00b1 0.030 MLP + Feature Set 2 0.658 \u00b1 0.010 0.671 \u00b1 0.025 0.685 \u00b1 0.023 MLP + Hidden Features 0.704 \u00b1 0.017 0.726 \u00b1 0.020 0.744 \u00b1 0.019 LSTM + Sequential HF 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 0.501 \u00b1 0.028 0.642 \u00b1 0.030 0.697 \u00b1 0.026 0.501 \u00b1 0.032 0.683 \u00b1 0.008 0.734 \u00b1 0.018 0.742 \u00b1 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 \u00b1 0.115 0.611 \u00b1 0.123 0.618 \u00b1 0.134 BioBert (Lee et al., 2020) 0.701 \u00b1 0.021 0.714 \u00b1 0.027 0.734 \u00b1 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 \u00b1 0.019 0.705 \u00b1 0.023 0.723 \u00b1 0.025 BlueBert (Peng et al., 2019) 0.713 \u00b1 0.019 0.735 \u00b1 0.024 0.755 \u00b1 0.024 CheXbert (Smit et al., 2020) 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 AUC@365 0.620 \u00b1 0.136 0.739 \u00b1 0.028 0.727 \u00b1 0.029 0.756 \u00b1 0.021 0.742 \u00b1 0.023\nFootnote or Reference text (if exists): 1 https://github.com/sebp/ scikit-survival\nSection Titles (if exists):  ['4 Experiments', '4.2 Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-30586-638\nCitation Context: Dimension of human ratings Study Sentence Planning this paper weak positive (\u03c1 = 0.33, WPS) Surface Realisation Domain weak negative (\u03c1 = 0. \u2212 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson\u2019s r = 0.96, NIST ) NLG, weather forecast (Stent et al., 2005) [URL_CITE] weak positive (\u03c1 = 0.47, LSA ) (Liu et al., 2016) weak positive (\u03c1 = 0.35, BLEU -4 ) (Elliott and Keller, 2014) positive (\u03c1 = 0.53, METEOR ) (Kilickaya et al., 2017) positive (\u03c1 = 0.64, SPICE ) (Cahill, 2009) N/A (Espinosa et al., 2010) weak positive (\u03c1 = 0.43, TER ) negative (\u03c1 = \u22120.56, NIST ) paraphrasing of news N/A dialogue/Twitter pairs N/A image caption N/A image caption negative (\u03c1 = \u22120.64, ROUGE ) NLG, German news texts positive (\u03c1 = 0.62, BLEU -4 ) NLG, news texts\nFootnote or Reference text (if exists): Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341\u2013351. https://doi.org/10.1007/978-3-540-30586-638.\nSection Titles (if exists):  ['7 Relation of Human and Automatic Metrics', '8.3 Example-based Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016); Kasai et al. (2017) [URL_CITE] ; CCG: Lewis et al. (2016)).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) 3 Wu and Huang (2015) [URL_CITE] 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) [URL_CITE] (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).\nFootnote or Reference text (if exists): Anthony Kroch and Ann Taylor. 2000. Penn-Helsinki Parsed Corpus of Middle English, second edition. http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/PCEEC-manual/index.htm\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) [URL_CITE] (2.2 million words).\nFootnote or Reference text (if exists): Ann Taylor, Arja Nurmi, Anthony Warner, Susan Pintzuk, and Terttu Nevalainen. 2006. Parsed Corpus of Early English Correspondence. Com-piled by the CEEC Project Team. York: Uni-versity of York and Helsinki: University of Helsinki. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/PCEEC-manual/index.htm.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.linguist.is/icelandic_treebank\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011) [URL_CITE] , French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Joel Wallenberg, Anton Karl Ingason, Einar Freyr Sigursson, and Eirkur Rgnvaldsson. 2011. Icelandic Parsed Historical Corpus (IcePaHC) version 0.4. http://www.linguist.is/icelandic_treebank.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.tycho.iel.unicamp.br/~tycho/corpus/en/index.html\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003), Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010) [URL_CITE] , totaling 4.5 million words.\nFootnote or Reference text (if exists): Charlotte Galves and Pabol Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/\u02dctycho/corpus/en/index.html.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-users.york.ac.uk/~lang22/YCOE/YcoeHome.htm\nCitation Context: 3 Aside from the corpora listed in fn. 1, there are also historical corpora of Old English (Taylor et al., 2003) [URL_CITE] , Ice-landic (Wallenberg et al., 2011), French (Martineau and oth-ers, 2009), and Portuguese (Galves and Faria, 2010), totaling 4.5 million words.\nFootnote or Reference text (if exists): Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003. The York-Toronto-Helsinki Parsed Corpus of Old English Prose. Distributed through the Oxford Text Archive. http://www-users.york.ac.uk/\u02dclang22/YCOE/YcoeHome.htm.\nSection Titles (if exists):  ['2 Corpus description', '2.1 Part-of-speech tags']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ling.upenn.edu/hist-corpora/PPCEME-RELEASE-2/index.html\nCitation Context: 1 The other treebanks in the series cover Early Modern En-glish (Kroch et al., 2004) [URL_CITE] (1.8 million words), Middle Eng-lish (Kroch and Taylor, 2000) (1.2 million words), and Early English Correspondence (Taylor et al., 2006) (2.2 million words).\nFootnote or Reference text (if exists): Anthony Kroch, Beatrice Santorini, and Ariel Diertani. 2004. Penn-Helsinki Parsed Cor-pus of Early Modern English. http: //www.ling.upenn.edu/hist-corpora/ PPCEME-RELEASE-2/index.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representa-tion obtained from CNNs in the same fashion as in the supertagger. We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) [URL_CITE] and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding pre-dicted POS tags boosted performance in Stanford Dependencies.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/anthology/P96-1011\nCitation Context: For our models, we adopt the pruning strate-gies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.00001, and utilize a tag dictio-nary, which maps frequent words to the possible supertags 7 . Unless otherwise stated, we only al-low normal form parses (Eisner, 1996 [URL_CITE] ; Hocken-maier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).\nFootnote or Reference text (if exists): Jason Eisner. 1996. Efficient Normal-Form Parsing for Combinatory Categorial Grammar. In 34th Annual Meeting of the Association for Computational Lin-guistics. http://aclweb.org/anthology/P96-1011.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/mmf\nCitation Context: Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [URL_CITE] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\".\nFootnote or Reference text (if exists): Amanpreet Singh, Vedanuj Goswami, Vivek Natara-jan, Yu Jiang, Xinlei Chen, Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020a. Mmf: A multimodal framework for vision and language research. https://github.com/facebookresearch/mmf.\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/vilbert-multi-task\nCitation Context: Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018); specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) [URL_CITE] . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\".\nFootnote or Reference text (if exists): 5 https://github.com/facebookresearch/ vilbert-multi-task\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://github.com/facebookresearch/detectron\nCitation Context: Faster R-CNN Finally, we consider Faster R-CNN features (Ren et al., 2017), using models trained in the Detectron framework (Girshick et al., 2018) [URL_CITE] ; specifically, we use a ResNeXt-152 back-bone trained on the Visual Genome dataset (Kr-ishna et al., 2016) with the attribute head (Singh et al., 2020a) . The Faster R-CNN features are 2048\u00d7100-dimensional representations, and we re-fer to these features as \u201cFaster R-CNN\".\nFootnote or Reference text (if exists): Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron. https://github.com/facebookresearch/detectron.\nSection Titles (if exists):  [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt\nCitation Context: B.1 Neural Machine Translation For the rich-resource WMT\u201916 En-De dataset, we use the pre-processed data from Ott et al. (2018) [URL_CITE] . For the low-resource datasets, we use byte-pair encoding (Sennrich et al., 2016) with 10,000 merge operations to build the vocabulary for the IWSLT (\u201914, \u201915, \u201916) datasets.\nFootnote or Reference text (if exists): 7 https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [URL_CITE] , forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nCitation Context: ResNeXt WSL We first experiment with im-age representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion pub-lic images (Mahajan et al., 2018), with subse-quent fine-tuning on the ImageNet1K dataset (Rus-sakovsky et al., 2015) [URL_CITE] . The output of this model is a 2048-dimensional vector, and we refer to these representations as \u201cResNeXt WSL\" features.\nFootnote or Reference text (if exists): 4 https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\nSection Titles (if exists): [\\'References\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014), which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016) [URL_CITE] , MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271\u2013 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ccsasuke/man\nCitation Context: Worse still, for some domains there may not be any annotated data at all. In this work, we propose a multino-mial adversarial network [URL_CITE] (MAN) to tackle this real-world problem of multi-domain text clas-sification (MDTC) in which labeled data may exist for multiple domains, but in insufficient amounts to train effective classifiers for one or more of the domains. We provide theo-retical justifications for the MAN framework, proving that different instances of MANs are essentially minimizers of various f-divergence metrics (Ali and Silvey, 1966) among multi-ple probability distributions.\nFootnote or Reference text (if exists): 1 The source code of MAN is available at https://github.com/ccsasuke/man.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/6254-domain-separation-networks.pdf\nCitation Context: As shown in Figure 1, the Multinomial Adver-sarial Network (MAN) adopts the Shared-Private paradigm of Bousmalis et al. (2016) [URL_CITE] and consists of four components: a shared feature extractor F s , a domain feature extractor F d for each labeled domain d i 2 L , a text classifier C, and a domain discriminator D. The main idea of MAN is to ex-plicitly model the domain-invariant features that are beneficial to the main classification task across all domains (i.e. the shared features, extracted by F s ), as well as the domain-specific features that mainly contribute to the classification in its own domain (the domain features, extracted by F d ).\nFootnote or Reference text (if exists): Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Ad-vances in Neural Information Processing Systems 29, Curran Associates, Inc., pages 343\u2013351. http://papers.nips.cc/paper/6254-domain-separation-networks.pdf.\nSection Titles (if exists):  ['2 Model', '2.1 Model Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: And just as binomial adversar-ial networks have been applied to numerous tasks (e.g. image generation (Goodfellow et al., 2014) [URL_CITE] , domain adaptation (Ganin et al., 2016), cross-lingual text classification (Chen et al., 2016)), we anticipate that MANs will make a versatile machine learning framework with applications beyond the MDTC task studied in this work.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/\nCitation Context: Binomial adversarial nets are known to have theoretical connections to the minimization of various f-divergences between two distribu-tions (Nowozin et al., 2016) [URL_CITE] . However, for ad-versarial training among multiple distributions, no theoretical justifications have been provided to our best knowledge, despite that this idea has recently been explored empirically (Liu et al., 2017).\nFootnote or Reference text (if exists): Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. 2016. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., pages 271\u2013 279. http://papers.nips.cc/paper/ 6066-f-gan-training-generative- neural-samplers-using-variational-divergence-minimization.pdf.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: In this paper, we thus propose Multinomial Ad-versarial Networks (henceforth, MANs) for the task of multi-domain text classification. In contrast to standard adversarial networks (Goodfellow et al., 2014) [URL_CITE] , which serve as a tool for minimizing the divergence between two distributions (Nowozin et al., 2016), MANs represent a family of theoret-ically sound adversarial networks that, in contrast, leverage a multinomial discriminator to directly minimize the divergence among multiple proba-bility distributions. And just as binomial adversar-ial networks have been applied to numerous tasks (e.g.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: Our experiments show the effective-ness of our approach, as we improve the previous state-of-the-art in unsupervised machine transla-tion by 5-7 BLEU points in French-English and German-English WMT 2014 and 2016. Our code is available as an open source project at [URL_CITE] https: //github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dadangewp/ACL19-SRW\nCitation Context: Last, we take advantage of the availability of multilingual word embeddings to build a joint-learning approach in the cross-lingual setting. All code and resources are available at [URL_CITE] https://github.com/dadangewp/ACL19-SRW.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/chloebt/discourse/\nCitation Context: We also showed that scores are lowered when considering a realistic setting, relying on predicted tokenization and not assuming gold sen-tences. We make our code available at [URL_CITE] https://bitbucket.org/chloebt/discourse/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['9 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/MiuLab/CLUSE\nCitation Context: The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and dataset are available at http://github.com/MiuLab/CLUSE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/artetxem/monoses\nCitation Context: We thus conclude that, contrary to recent trend, future work in BLI should not focus exclusively in direct retrieval approaches, nor should BLI be the only evaluation task for cross-lingual embeddings. Our code is available at [URL_CITE] https://github.com/artetxem/monoses.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions and future work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://sjmielke.com/papers/syncretism\nCitation Context: We empirically evaluated our method on 5 languages under two evaluation metrics. The code is availabile at [URL_CITE] https://sjmielke.com/papers/syncretism, along with type-disambiguated unigram counts for all lexicons pro-vided by the UniMorph project (100+ languages).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/tuvuumass/task-transferability\nCitation Context: We publicly release our task library, which con-sists of pretrained models and task embeddings for the 33 NLP tasks we study, along with a codebase that computes task embeddings for new tasks and identifies source tasks that will likely yield positive transferability. [URL_CITE]\nFootnote or Reference text (if exists): 2 Library and code available at http://github.com/tuvuumass/task-transferability.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/\nCitation Context: We prove the applicability of such variations by providing empirical justifications on varied text classification tasks. The implementations are available at [URL_CITE] https://github.com/declare-lab/ identifiable-transformers .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/checklist\nCitation Context: In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [URL_CITE] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks.\nFootnote or Reference text (if exists): 1 https://github.com/marcotcr/checklist\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/craigastewart/qe\nCitation Context: In experiments over five settings in three language pairs, we extend a QE pipeline to estimate in-terpreter performance (as approximated by the METEOR evaluation metric) and pro-pose novel features reflecting interpreta-tion strategy and evaluation measures that further improve prediction accuracy. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/craigastewart/qe sim interp Proceedings of the 56th Annual Meeting of the Association for\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rycolab/info-theoretic-probing\nCitation Context: We evaluate on a set of ten typologically diverse languages often under-represented in NLP research\u2014plus English\u2014 totalling eleven languages. Our implementa-tion is available in [URL_CITE] https://github.com/rycolab/info-theoretic-probing.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [URL_CITE] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 4 WMT16: http://www.statmt.org/wmt16/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) [URL_CITE] , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 1 https://wit3.fbk.eu/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://workshop2016.iwslt.org/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) and the 2016 International Workshop on Spoken Language Translation (IWSLT16) [URL_CITE] . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 5 IWSLT16: https://workshop2016.iwslt.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/berkeleylm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 [URL_CITE] (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 6 http://code.google.com/p/berkeleylm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://vectors.nlpl.eu/repository/\nCitation Context: All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [URL_CITE] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines).\nFootnote or Reference text (if exists): 8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes\nCitation Context: Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score [URL_CITE] , , i.e., the software imple-mentation of Normalized Kendall\u2019s \u03c4 as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.\nFootnote or Reference text (if exists): 3 Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.sri.com/projects/srilm/\nCitation Context: \u2022 language models: Akamon can make use of one or many n-gram language models trained by using SRILM [URL_CITE] (Stolcke, 2002) or the Berke-ley language model toolkit, berkeleylm-1.0b3 (Pauls and Klein, 2011). The weights of multi-ple language models are tuned under minimum error rate training (MERT) (Och, 2003).\nFootnote or Reference text (if exists): 5 http://www.speech.sri.com/projects/srilm/\nSection Titles (if exists):  ['2 Akamon Toolkit Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/mecab/\nCitation Context: For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [URL_CITE] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING).\nFootnote or Reference text (if exists): 10 http://taku910.github.io/mecab/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Conditions and Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/\nCitation Context: In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo-rithms, word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), based on the Oct. 2013 Wikipedi-a dump. [URL_CITE] We first compare levels of truncation of word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors based methods for training word2vec vectors.\nFootnote or Reference text (if exists): 1 https://dumps.wikimedia.org/\nSection Titles (if exists):  ['4 Experiments on Word/Phrase Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dennybritz/cnn-text-classification-tf\nCitation Context: The embedding matrix W covers the subset of a vocabulary of interests for a particular NLP task, note that the value of V is known to be very large due to the rich variations in human languages. Here we showcase the em-bedding matrix size of a popular text classification model [URL_CITE] on AG-news dataset (Zhang et al., 2015) in Table 1. From which we can easily observe that the embedding matrix is commonly occupy-ing most of the parameter capacity, which could be the bottleneck in many real-world applications with limited computation resources.\nFootnote or Reference text (if exists): 2 https://github.com/dennybritz/ cnn-text-classification-tf\nSection Titles (if exists):  ['2 Vocabulary Selection', '2.1 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation system [URL_CITE] over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word.\nFootnote or Reference text (if exists): 7 We use Jieba for segmentation: https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['3 Embeddings for Chinese Text']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kaushalshetty/Structured-Self-Attention\nCitation Context: Through the channel-attention network, we can observe the attention matrix A for each input doc-ument. By summing up the attention weights of each word embedding vector, we can visualize the overall attention for that word with the visualiza-tion tool [URL_CITE] . We randomly select three movies from IMDb testing set (See Figure 5).\nFootnote or Reference text (if exists): 5 The visualization tool is provided by https://github.com/kaushalshetty/ Structured-Self-Attention\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.4 More Analysis in Rank-AE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://banner.sourceforge.net\nCitation Context: We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [URL_CITE] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }.\nFootnote or Reference text (if exists): 3 http://banner.sourceforge.net\nSection Titles (if exists):  ['3 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [URL_CITE] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz)\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Oneplus/Tweebank\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [URL_CITE] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 5 https://github.com/Oneplus/Tweebank\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nCitation Context: Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [URL_CITE] . This is similar to the removal of stop words widely used in information retrieval.\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nSection Titles (if exists):  ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nCitation Context: To identify the replaceable entities, we apply named entity recognition (NER) and POS tag-ging using the Stanford CoreNLP toolkit (Man-ning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base [URL_CITE] . This improves the likelihood that the terms are familiar to both headline editors and humor judges.\nFootnote or Reference text (if exists): 1 http://blogs.bing.com/search/2013/03/21/understand-your-world-with-bing/\nSection Titles (if exists):  ['2 The Humor Dataset', '2.1 Task Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cuitools.sourceforge.net\nCitation Context: Our experiments were conducted using CuiTools v0.15, which is freely available from [URL_CITE] http://cuitools.sourceforge.net.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhclark/ducttape\nCitation Context: The toolkit that seems to be most similar to our approach is Ducttape [URL_CITE] , the successor of LonnyBin (Clark and Lavie, 2010). It is well designed and covers many useful points.\nFootnote or Reference text (if exists): 2 https://github.com/jhclark/ducttape\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.marekrei.com/projects/mltagger\nCitation Context: The code used for performing these experi-ments is made available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://www.marekrei.com/projects/mltagger\nSection Titles (if exists):  ['5 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/HKUST-KnowComp/RINANTE\nCitation Context: Our code is available at [URL_CITE] https://github.com/HKUST-KnowComp/RINANTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ilk.uvt.nl/sabine/chunklink/\nCitation Context: 1 Software available at [URL_CITE] http://ilk.uvt.nl/sabine/chunklink/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Unsupervised Relation Extraction Problem', '2.1 Context Vector and Feature Design']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://worrydream.com/ExplorableExplanations/\nCitation Context: Ecco provides tools and interactive explorable explanations [URL_CITE] aiding the examination and intuition of:\nFootnote or Reference text (if exists): 3 http://worrydream.com/ExplorableExplanations/ Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Ecco is open-source software [URL_CITE] and contributions are welcome. Acknowledgments This work was improved thanks to feedback pro-vided by Abdullah Almaatouq, Anfal Alatawi, Christopher Olah, Fahd Alhazmi, Hadeel Al-Negheimish, Hend Al-Khalifa, Isabelle Augen-stein, Jasmijn Bastings, Najwa Alghamdi, Pepa Atanasova, and Sebastian Gehrmann.\nFootnote or Reference text (if exists): 5 https://github.com/jalammar/ecco\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/HKUST-KnowComp/RINANTE\nCitation Context: Our code is available at [URL_CITE] https://github.com/HKUST-KnowComp/RINANTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/bcEysXmR09c\nCitation Context: Ecco is available at https: //www.eccox.io/ . [URL_CITE]\nFootnote or Reference text (if exists): 2 Video demo available at https://youtu.be/ bcEysXmR09c\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.donga.com\nCitation Context: For latent morpheme representations, we used the Donga news article corpus. [URL_CITE] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others.\nFootnote or Reference text (if exists): 3 http://www.donga.com\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.tau.ac.il/~jonatha6/homepage_files/resources/HealthcareGraphs.rar\nCitation Context: For the entailment graph we used the 23 entail-ment graphs published by Berant et al. [URL_CITE] . For the ar-gument taxonomy we employed UMLS \u2013 a database that maps natural language phrases to over one mil-lion unique concept identifiers (CUIs) in the health-care domain.\nFootnote or Reference text (if exists): 2 http://www.cs.tau.ac.il/\u02dcjonatha6/homepage_files/resources/HealthcareGraphs. rar\nSection Titles (if exists):  ['5 Application to the Health-care Domain']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TalLinzen/rnn_agreement\nCitation Context: (2016). [URL_CITE] Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num-ber of attractors, in Table 1.\nFootnote or Reference text (if exists): 1 The dataset and scripts are obtained from https://github.com/TalLinzen/rnn_agreement.\nSection Titles (if exists):  ['2 Number Agreement with LSTM Language Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://web.stanford.edu/~jurafsky/ws97/manual.august1.html\nCitation Context: The SwDA cor-pus conforms to the damsl tag schema. [URL_CITE] We as-sembled the tag sets referring to easy damsl (Iso-mura et al., 2009) into 9 tags (Table 2) in order to consolidate tags with a significantly low fre-quency. The SwDA corpus provides transcriptions of 1, 155 conversations with 219, 297 utterances.\nFootnote or Reference text (if exists): 1 https://web.stanford.edu/\u02dcjurafsky/ws97/manual.august1.html\nSection Titles (if exists):  ['4 Experiment', '4.1 Switch Board Dialogue Act Corpus (SwDA)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nCitation Context: For OOV words, we randomly replace tokens by \"UNK\" token with p = 0.02 during training and use it for testing (\u017dilka and Jurc\u030c\u00edc\u030cek, 2015). We perform hyperparameter search using Comet\u2019s Bayesian search algorithm [URL_CITE] , maximising F1 score for sequence tagging and accuracy for sequence classification on the validation set. The hyperparameter search trials are limited to 20 for all of our experiments.\nFootnote or Reference text (if exists): 4 https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nSection Titles (if exists): [\\'Training details\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe [URL_CITE] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): 8 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014) [URL_CITE] , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/\nCitation Context: Masked Token Type Classification (T OKEN T YPE ): Our fourth objective is a four-way classi-fication, aiming to predict whether a token is a stop word, [URL_CITE] a digit, a punctuation mark, or a content word. Therefore, the task can be seen as a simpli-fied version of POS tagging.\nFootnote or Reference text (if exists): 2 We use the Natural Language Toolkit\u2019s stop word list: https://www.nltk.org/.\nSection Titles (if exists):  ['2 Pretraining Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aspell.net/\nCitation Context: (2010)). We use the Aspell dictionary (v6.06) [URL_CITE] to determine whether a word is IV, and only include in our normalisation dictionary OOV tokens with at least 64 occurrences in the corpus and character length \u2265 4, both of which were determined through empirical observation. For each OOV word type in the corpus, we select the most similar IV type to form (OOV, IV) pairs.\nFootnote or Reference text (if exists): 2 http://aspell.net/\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gorokoba560/norm-analysis-of-transformer\nCitation Context: The codes of our experiments are pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/gorokoba560/ norm-analysis-of-transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JRC1995/Multilingual-BERT-Disaster\nCitation Context: Our code, dataset, and other resources are available on Github. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JRC1995/ Multilingual-BERT-Disaster\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hohocode.github.io/textSimilarityConvNet/\nCitation Context: Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [URL_CITE]\nFootnote or Reference text (if exists): 4 http://hohocode.github.io/textSimilarityConvNet/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use the 100-dimensional GloVe vectors [URL_CITE] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Experiments', '4.1 Answer Selection', '4.1.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tmikolov/word2vec\nCitation Context: The dimension of word embeddings is set to 200, which is initialized with pre-trained word2vec vectors [URL_CITE] . The vocabulary is comprised of the most frequent 31,000 words.\nFootnote or Reference text (if exists): 3 https://github.com/tmikolov/word2vec\nSection Titles (if exists):  ['4 Experiments', '4.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: The word embeddings are pre-trained based on a pool of the top 2,000 documents returned by BM25 for individual queries as suggested by (Diaz et al., 2016). The implementation of Word2Vec [URL_CITE] from (Mikolov et al., 2013) is employed. In par-ticular, we employ CBOW with the dimension set to 300, window size to 10, minimum count to 5, and a subsampling threshold of 10 \u22123 .\nFootnote or Reference text (if exists): 1 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Evaluation', '3.1 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [URL_CITE] . For Twitter and Weibo datasets, we pre-train embeddings on large-scale external data with 99M tweets and 467M Weibo messages, respectively.\nFootnote or Reference text (if exists): 11 http://nlp.stanford.edu/data/glove.6B.zip (200d)\nSection Titles (if exists):  ['3 Experiment Setup', '3.2 Model Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: In addition, if the vocabulary for BoW is available, e.g. IMDb and Wiki10, the Word Embedding component is initialized by Glove [URL_CITE] , a pre-trained word embed-dings of 100 dimensions; if it is not, e.g. Medi-amill, Delicious and RCV, a random initialization is employed.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/\nCitation Context: We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [URL_CITE] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings.\nFootnote or Reference text (if exists): 9 https://github.com/google-research/bert/blob/master/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jekbradbury/revtok\nCitation Context: We tokenize using revtok [URL_CITE] and part-of-speech tag (for the editor) using Stanford CoreNLP (Manning et al., 2014). We fine-tune the smaller, uncased pretrained BERT model by Devlin et al.\nFootnote or Reference text (if exists): 1 https://github.com/jekbradbury/revtok\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt16/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) [URL_CITE] , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 2 https://www.statmt.org/wmt16/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) [URL_CITE] , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 1 https://wit3.fbk.eu/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iwslt.org/\nCitation Context: Dataset statistics are summarized in Table 1. For the low-resource experiments, we use [URL_CITE] : English-Vietnamese from IWSLT\u201915, German-English from IWSLT\u201914, and French-English from IWSLT\u201916. For the rich-resource experiments, we use the English-German dataset from WMT\u201916, which contains about 4.5 million training samples.\nFootnote or Reference text (if exists): 4 https://iwslt.org/\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task\nCitation Context: We evaluate our method on five widely used benchmark tasks: IWSLT14 German\u2192English translation (IWSLT14 De-En) , WMT16 English\u2194Romanian translation (WMT16 En-Ro/Ro-En) , and WMT14 English\u2194German translation (WMT14 En-De/De-En) [URL_CITE] . We strictly follow the dataset configurations of previous works.\nFootnote or Reference text (if exists): 3 https://www.statmt.org/wmt14/ translation-task\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup 5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: More closely related is our prior work (Vyas and Carpuat, 2016) [URL_CITE] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (\u00a76.1), and show that the improvements hold even when exposed to data scarce settings (\u00a76.3).\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: To obtain pattern-based word-pair embeddings, we extracted triples (w 1 ,w 2 ,p) \u2208 T from the Wikipedia corpus, where (w 1 , w 2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe [URL_CITE] (Pen-nington et al., 2014), and p is the co-occurring shortest dependency path . We discarded the triples if p occurred less than five times and sub-sampled the triples based on word-pair probability with a threshold of 5 \u00b7 10 \u22127 , following Joshi et al. (2019).\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/data/glove.6B.zip\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Obtaining Word-Pair Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip\nCitation Context: We built two datasets of adjective-noun phrases for the present research, one with color terms and one with intensional adjectives. [URL_CITE] Color terms. This dataset is populated with a ran-domly selected set of adjective-noun pairs from the space presented above.\nFootnote or Reference text (if exists): 6 Available at http://dl.dropbox.com/u/513347/resources/data-emnlp2012.zip. See Bruni et al. (to appear) for an analysis of the color term dataset from a multi-modal perspective.\nSection Titles (if exists):  ['p = Bv (5)', '3.3 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/bFokI\nCitation Context: We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English. [URL_CITE] We predict semantic similarity as cosine similarity. We make a prediction for a word pair if both words are repre-sented in the semantic space and their vectors have a non-zero similarity.\nFootnote or Reference text (if exists): 2 Downloadable from: http://goo.gl/bFokI\nSection Titles (if exists):  ['5 Experimental Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crowdflower.com\nCitation Context: While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [URL_CITE] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.\nFootnote or Reference text (if exists): 4 http://crowdflower.com\nSection Titles (if exists):  ['4 Crowd-Sourcing Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SussexCompSem/learninghypernyms\nCitation Context: For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs [URL_CITE] from the BLESS dataset (Baroni and Lenci, 2011). These noun-noun word pairs include positive hy-ponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs.\nFootnote or Reference text (if exists): 2 https://github.com/SussexCompSem/ learninghypernyms\nSection Titles (if exists):  ['5 Evaluation', '5.1 Hyponymy with Word2Vec Vectors']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: In Universal Dependencies, Dozat et al. (2017) [URL_CITE] empirically showed that their depen-dency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tag-ger. Indeed, Kasai et al.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P11-1048\nCitation Context: Previous work integrated CCG supertagging and parsing using belief propagation and dual de-composition approaches (Auli and Lopez, 2011) [URL_CITE] . Nguyen et al.\nFootnote or Reference text (if exists): Michael Auli and Adam Lopez. 2011. A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing. In ACL. Association for Computational Linguistics, pages 470\u2013480. http://www.aclweb.org/anthology/P11-1048.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [URL_CITE] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/K17-3014\nCitation Context: Nguyen et al. (2017) [URL_CITE] incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing perfor-mance.\nFootnote or Reference text (if exists): Dat Quoc Nguyen, Mark Dras, and Mark John-son. 2017. A novel neural network model for joint pos tagging and graph-based dependency pars-ing. In CoNLL 2017 Shared Task: Multilin-gual Parsing from Raw Text to Universal Depen-dencies. Association for Computational Linguis-tics, pages 134\u2013142. https://doi.org/10.18653/v1/K17-3014.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.amap.com\nCitation Context: How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked. Thus we use wealthy geographic information about or-ganizations and locations in electronic maps (e.g., Amap [URL_CITE] ) to complete the KG.\nFootnote or Reference text (if exists): 5 https://www.amap.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/~liub/FBS/data.tar.gz\nCitation Context: We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) [URL_CITE] . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons).\nFootnote or Reference text (if exists): 3 Available from http://www.cs.uic.edu/\u02dcliub/FBS/data.tar.gz \u2013 although the original paper works on some unknown subset of this data, so our results are not directly\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ownthink.com\nCitation Context: There are four types of personal information in a Chinese loan application form: \u201cSchool\u201d, \u201cCom-pany\u201d, \u201cResidence\u201d and \u201cBirthPlace\u201d. To generate derived questions, we link all personal information entities to nodes in an existing Chinese KG [URL_CITE] and crawl triplets that are directly related to them. How-ever, owing to the fact that the KG is largely sparse, nearly a half of entities cannot be linked.\nFootnote or Reference text (if exists): 3 https://www.ownthink.com\nSection Titles (if exists):  ['2 Knowledge Graph Constructor']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr\nCitation Context: One is described in (Hahn et al., 2009) and uses a margin based criterion for probabilities estimation. The other is described in (Lavergne et al., 2010) and has been implemented in the software wapiti [URL_CITE] . The latter solution in partic-ular trains the model using two different regulariza-tion factors at the same time:\nFootnote or Reference text (if exists): 1 available at http://wapiti.limsi.fr\nSection Titles (if exists):  ['3 Reranking Framework', '3.1 Conditional Random Fields']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paracrawl.eu/download.html\nCitation Context: There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [URL_CITE] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a).\nFootnote or Reference text (if exists): 6 http://paracrawl.eu/download.html\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: Models Compared: We experiment with Glove (Pennington et al., 2014) [URL_CITE] and ELMo (Peters et al., 2018) embeddings for Multee and compare with following three types of systems: (A) Baselines using entailment as a black-box We use the pre-trained entailment model as a black-box in two ways: concatenate premises (Concat) and aggregate sentence level decisions with a max operation (Max). Both models were also pre-trained on SNLI and MultiNLI datasets and fine-tuned on the target QA datasets with same pre-processing.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: More closely related is our prior work (Vyas and Carpuat, 2016) [URL_CITE] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (\u00a76.1), and show that the improvements hold even when exposed to data scarce settings (\u00a76.3).\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/N10-1138\nCitation Context: Baseline: Kang et al. (2017) extract relevant variables based on a semantic parser - SEMAFOR (Das et al., 2010) [URL_CITE] by filtering causation related frames from news corpora, topics and sentiments from tweets. To overcome the problem of low re-call, they adopt a topic-based knowledge base ex-pansion.\nFootnote or Reference text (if exists): Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic Frame-Semantic Parsing. In Human Language Tech-nologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Com-putational Linguistics, Los Angeles, California, 948\u2013956. https://www.aclweb.org/anthology/N10-1138\nSection Titles (if exists):  ['5 Results', '5.2 Prediction Performance of PCG']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826\nCitation Context: Propensity score based matching aims to control for most confounding variables by using an ex-ternal method for estimating and controlling for the likelihood of outcomes (Olteanu et al., 2017). More recently, (Wang and Blei, 2018) [URL_CITE] showed that with multiple causal factors, it is possible to lever-age the correlation of those multiple causal fac-tors and deconfound using a latent variable model. This setting is similar to the one we consider, and is guaranteed to be truly causal if there is no con-founder which links a single cause and the out-come.\nFootnote or Reference text (if exists): Yixin Wang and David M. Blei. 2018. The Blessings of Multiple Causes. CoRR abs/1805.06826. http: //dblp.uni-trier.de/db/journals/ corr/corr1805.html#abs-1805-06826\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QAQ-v/HetGT\nCitation Context: We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at [URL_CITE] https://github.com/QAQ-v/HetGT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Experiments', '3.2 Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Pretraining Data: We pretrain all models on the English Wikipedia and BookCorpus (Zhu et al., 2015) (WikiBooks) using the datasets library. [URL_CITE] Implementation Details: We pretrain and fine-tune our models with two NVIDIA Tesla V100 (SXM2 - 32GB) with a batch size of 32 for BASE and 64 for MEDIUM and SMALL . We pretrain all our models for up to five days each due to limited access to computational resources and funds for running experiments.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ datasets\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Our models are implemented based on Py-Torch (Paszke et al., 2017). [URL_CITE] To speed up training, we use Nvidia Apex 16 for mixed precision train-ing. Gradient accumulation (Ott et al., 2018) is applied to reduce multi-GPU communication over-heads.\nFootnote or Reference text (if exists): 15 https://pytorch.org/\nSection Titles (if exists):  ['-', 'A.5 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/havakv/pycox\nCitation Context: The training, validation and test splits are 70%, 10%, 20%, respectively. We use pycox and PyTorch to implement the framework [URL_CITE] . The end-to-end training takes about 30 minutes with NVIDIA Tesla P100 16 GB GPU, mainly due to feature extraction.\nFootnote or Reference text (if exists): 2 https://github.com/havakv/pycox\nSection Titles (if exists):  ['4 Experiments', '4.3 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpyang/PreSumm\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm [URL_CITE] . We train our model (with about 400M parameters) one day for 100,000 steps on GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps.\nFootnote or Reference text (if exists): 3 https://github.com/nlpyang/PreSumm\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wanqiulong0923/TED-CDB\nCitation Context: The dataset and our Chinese annotation guidelines has been made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/WeimingWen/CCRV\nCitation Context: We published our code and dataset on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/WeimingWen/CCRV\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: Our Wiktionary datasets and an open-source version of our code are available at [URL_CITE] http://eecs.berkeley.edu/~gdurrett\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: This data is made available under the Creative Commons license, Attribution-Non Commercial-No Derivatives (or the CC BY-NC-ND 4.0 International, https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), all credit for the content goes to the TED organization and the respective authors of the talks. The data it-self can be found at [URL_CITE] http://www.cs.cmu.edu/ \u223c pmichel1/sated/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://creativecommons.org/licenses/by-nc-sa/4.0/\nCitation Context: Copyright Act and ensured that our collection action fell under fair use category. We release our dataset under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [URL_CITE] .\nFootnote or Reference text (if exists): 10 https://creativecommons.org/licenses/by-nc-sa/4.0/\nSection Titles (if exists):  ['8 Ethical Considerations', '8.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://coretex.itc.it/\nCitation Context: Two other related FP5 IST projects are: C ORE - TEX : Improving Core Speech Recognition Tech-nology and E CHO : European CHronicles On-line. C ORETEX ( [URL_CITE] http://coretex.itc.it/), aims at improving core speech recognition technologies, which are central to most applications involv-ing voice technology. In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://twentyone.tpd.tno.nl/olive/\nCitation Context: The European project LE-4 O LIVE : A Multilingual Indexing Tool for Broadcast Material Based on Speech Recognition ( [URL_CITE] http://twentyone.tpd.tno.nl/olive/) addressed methods to automate the disclosure of the infor-mation content of broadcast data thus allowing content-based indexation. Speech recognition was used to produce a time-linked transcript of the audio channel of a broadcast, which was then used to produce a concept index for retrieval.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ee.columbia.edu/dvmm/newDownloads.htm\nCitation Context: In this paper, we propose to jointly incorporate features from both speech (textual) and video (vi-sual) channels for the first time. We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions. With the crawler, we created a bench-mark dataset which is fully annotated with cross-document coreferential events [URL_CITE] .\nFootnote or Reference text (if exists): 1 Dataset can be found at http://www.ee.columbia.edu/dvmm/newDownloads.htm\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.is.cs.cmu.edu/mie\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.im2.ch\nCitation Context: In the past few years, there has been an increasing interest in research on developing systems for effi-cient recording of and access to multimedia meet-ing data [URL_CITE] . This work often results in videos of meetings, transcripts, electronic copies of docu-ments referenced, as well as annotations of various kinds on this data. In order to exploit this work, a user needs to have an interface that allows them to retrieve and browse the multimedia meeting data easily and efficiently.\nFootnote or Reference text (if exists): 1 The IM2 project http://www.im2.ch, the AMI project www.amiproject.org, The Meeting Room Project at Carnegie Mellon University, http://www.is.cs.cmu.edu/mie, and rich transcription of natural and impromptu meetings at ICSI, Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pc-erato2.iei.pi.cnr.it/echo\nCitation Context: In particular the project addresses the development of generic speech recognition technology and methods to rapidly port technology to new domains and languages with limited supervision, and to produce en-riched symbolic speech transcriptions. The E CHO project ( [URL_CITE] http://pc-erato2.iei.pi.cnr.it/echo) aims to develop an infrastructure for access to histori-cal films belonging to large national audiovisual archives. The project will integrate state-of-the-art language technologies for indexing, searching and retrieval, cross-language retrieval capabilities and automatic film summary creation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Recent Research Projects']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: This motivates approaches that directly detect hypernymy in the cross-lingual setting by extend-ing distributional methods for detecting monolin-gual hypernymy, as in our prior work (Vyas and Carpuat, 2016) [URL_CITE] . State-of-the-art distributional ap-proaches (Roller and Erk, 2016; Shwartz et al., 2017) for detecting monolingual hypernymy re-quire syntactic analysis (eg.\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Elbria/utdsm_naacl2018\nCitation Context: We further per-form experiments on two NLP downstream tasks: text classification and paraphrase identification and demonstrate that our learned word represen-tations consistently provide higher performance than single-prototype word embedding models. The code of the present work is publicly avail-able [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Elbria/utdsm_ naacl2018\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Oneplus/Tweebank\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [URL_CITE] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 5 https://github.com/Oneplus/Tweebank\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [URL_CITE] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz)\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rajpurkar.github.io/SQuAD-explorer/\nCitation Context: High-performance deep neural language models such as BERT (Devlin et al., 2018), XLNet (Z. Yang et al., 2019), and GPT-2 (Radford et al., 2019) have brought breakthroughs to a wide range of Natural Language Processing (NLP) tasks including text classification, sentiment analysis, textual entailment, natural language inference, machine translation, and question answering. Their immense ability in capturing various linguistic properties has led these state-of-the-art language models to master different NLP tasks, even surpassing human accuracy on some benchmarks such as SQuAD [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rajpurkar.github.io/SQuAD-explorer/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf\nCitation Context: As an emerging research problem, text-based per-sonality detection has attracted the attention of both NLP and psychological researchers (Cui and Qi, 2017 [URL_CITE] ; Xue et al., 2018; Keh et al., 2019; Jiang et al., 2020; Tadesse et al., 2018; Lynn et al., 2020).\nFootnote or Reference text (if exists): Brandon Cui and Calvin Qi. 2017. Survey analysis of machine learning methods for natural language processing for mbti per-sonality type prediction. Available on-line: http://cs229.stanford.edu/proj2017/final-reports/5242471.pdf (accessed on 26 May 2021).\nSection Titles (if exists):  ['2 Related Work', '2.1 Personality Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/The_Big_Bang_Theory\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, [URL_CITE] available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 3 https://en.wikipedia.org/wiki/The_ Big_Bang_Theory\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.imsdb.com\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). [URL_CITE] We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 4 http://www.imsdb.com\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pushshift/api\nCitation Context: Data was scraped from the two subreddits by querying the Pushshift API. [URL_CITE] 3 years\u2019 worth of posts, ranging from 1 January 2017 to 31 Decem-ber 2019, were collected for each subreddit. To ensure each post had sufficient linguistic content, I excluded any posts containing less than 101 char-acters.\nFootnote or Reference text (if exists): 4 https://github.com/pushshift/api model with all features. The numerical results are given in the appendix (Table 6.)\nSection Titles (if exists):  ['4 Data', '4.1 Data collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Friends\nCitation Context: Data Collection For the dyadic Speaker-Addressee Model we used scripts from the American television comedies Friends [URL_CITE] and The Big Bang Theory, available from Internet Movie Script Database (IMSDb). We collected 13 main characters from the two series in a corpus of 69,565 turns. We split the corpus into train-ing/development/testing sets, with development and testing sets each of about 2,000 turns.\nFootnote or Reference text (if exists): 2 https://en.wikipedia.org/wiki/Friends\nSection Titles (if exists):  ['5 Datasets', '5.3 Television Series Transcripts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/najoungkim/pdtb3\nCitation Context: We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [URL_CITE] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration.\nFootnote or Reference text (if exists): 4 https://github.com/najoungkim/pdtb3\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/translation\nCitation Context: We follow the scripts in Ott et al. (2019) [URL_CITE] for other pre-processing steps.\nFootnote or Reference text (if exists): 8 https://github.com/pytorch/fairseq/tree/master/examples/translation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N/N09/N09-2047\nCitation Context: We follow the protocol of Bangalore et al. (2009) [URL_CITE] , Chung et al. (2016), Kasai et al.\nFootnote or Reference text (if exists): Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Beno\u0131\u0302t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185\u2013188. http://www.aclweb.org/anthology/N/N09/N09-2047.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/JfV6J\nCitation Context: (2019) and parse test captions with Benepar (Kitaev and Klein, 2018). We use the same data preprocess-ing [URL_CITE] as in Shen et al. (2019) and Kim et al.\nFootnote or Reference text (if exists): 4 https://git.io/JfV6J.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016); Kasai et al. (2017) [URL_CITE] ; CCG: Lewis et al. (2016)).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nCitation Context: We resolve spelling errors with a public spell checker [URL_CITE] as preprocessing, as Xie et al. (2016) and Sakaguchi et al. (2017) do.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/spell-check/\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PKUnlp-icler/GAIN\nCitation Context: Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at [URL_CITE] https://github.com/PKUnlp-icler/GAIN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/serenayj/ABCD-ACL2021\nCitation Context: The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. [URL_CITE]\nFootnote or Reference text (if exists): 1 ABCD is available at https://github.com/serenayj/ABCD-ACL2021.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/XinnuoXu/AugNLG\nCitation Context: We fur-ther confirm improved results on the F EW - S HOT SGD data and provide comprehensive analysis results on key components of our sys-tem. Our code and data are available at [URL_CITE] https: //github.com/XinnuoXu/AugNLG .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [URL_CITE] http: //sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dolphin-zs/Doc2EDAG\nCitation Context: Ex-tensive experiments with comprehensive anal-yses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at [URL_CITE] https://github.com/dolphin-zs/Doc2EDAG.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Mixed\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [URL_CITE] ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Vastlab/liblinear.git\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine [URL_CITE] , W-SVM and P I - SVM , and Exploratory learning ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 2 https://github.com/Vastlab/liblinear.git\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/Downloads2015-04\nCitation Context: \u2022 TF*IDF weighting: This simple heuristic was introduced by Luhn (1958). Each sen-tence receives a score from the TF*IDF of its terms. We trained IDFs (Inverse Document Frequencies) on a background corpus [URL_CITE] to im-prove the original algorithm.\nFootnote or Reference text (if exists): 4 We used DBpedia long abstract: http://wiki.dbpedia.org/Downloads2015-04.\nSection Titles (if exists):  ['4 Evaluation', '4.3 End-to-End Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nCitation Context: First we implemented the standard linear SVM [URL_CITE] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.\nFootnote or Reference text (if exists): 6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nSection Titles (if exists):  ['3 Experiments and results', '3.1 Basic SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [URL_CITE] systems in ELS task of SENSEVAL-3.\nFootnote or Reference text (if exists): 3 we SV M light ,used linear available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Experiment 1: LP vs. SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ljain2/libsvm-openset\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM [URL_CITE] , and Exploratory learning ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 3 https://github.com/ljain2/libsvm-openset\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/PepsiChallenge\nCitation Context: In fact, blind taste test experiments have frequently shown how branding directly leads to the success of products and companies. Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge [URL_CITE] , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola\u2019s branding has made it more popular. Even now, Microsoft uses similar blind taste tests to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing\u2019s results, Google\u2019s brand might have strengthened over the years.\nFootnote or Reference text (if exists): 1 http://en.wikipedia.org/wiki/Pepsi Challenge\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wuch15/NRHUB\nCitation Context: We conducted experiments on a real-world news recommendation dataset [URL_CITE] collected from MSN News logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 3 Some publicly available resources can be found at https://github.com/wuch15/NRHUB.\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.msn.com/en-us/news\nCitation Context: We conducted experiments on a real-world news recommendation dataset collected from MSN News [URL_CITE] logs during Dec. 13, 2018 and Jan. 12, 2019. In addition, we crawled the search queries and titles of browsed webpages from the logs of the Bing search engine.\nFootnote or Reference text (if exists): 4 https://www.msn.com/en-us/news\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2003T05\nCitation Context: We also experiment with the English Gigaword corpus, [URL_CITE] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2003T05\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bingiton.com/\nCitation Context: Most notably is a continued study sponsored by Pepsi, known as the Pepsi Challenge , where Pepsi demonstrates how even though people preferred the taste of Pepsi, Coca-Cola\u2019s branding has made it more popular. Even now, Microsoft uses similar blind taste tests [URL_CITE] to compare search en-gines, Bing and Google, showing that although par-ticipants prefer Bing\u2019s results, Google\u2019s brand might have strengthened over the years. These studies all suggest that brand and its associations play impor-tant roles in the customers\u2019 perceptions and deci-sions.\nFootnote or Reference text (if exists): 2 http://www.bingiton.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/freesunshine0316/neural-graph-to-seq-mp\nCitation Context: We release our code and models at [URL_CITE] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DancingSoul/NQ_BERT-DM\nCitation Context: We will release our code and models at [URL_CITE] https: //github.com/DancingSoul/NQ_BERT-DM . Figure 2: System overview.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/disc-span-parser-release\nCitation Context: We release the C++/Python implementation of the parser. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/FilippoC/ disc-span-parser-release\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmaww/SENT\nCitation Context: We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/rtmaww/SENT\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NP-NET-research/Recursive-Semi-Markov-Model\nCitation Context: Our code is released at [URL_CITE] https://github.com/NP-NET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.nltk.org/\nCitation Context: Preprocessing and Hyperparameter Setting. To preprocess the training corpus D, we use the word tokenizer provided by NLTK [URL_CITE] . We also use a phrase mining tool, AutoPhrase (Shang et al., 2017), to extract meaningful phrases such as \u201cgreat wine\u201d and \u201cnumeric keypad\u201d such that they can capture complicated semantics in a single text unit.\nFootnote or Reference text (if exists): 3 https://www.nltk.org/\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: We train a multi-class logistic regression model [URL_CITE] to predict if the application of a paraphrase rule will result in 1) simpler output, 2) more com-plex output, or 3) non-sense output.\nFootnote or Reference text (if exists): 4 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Identifying Simplification Rules', '2.2 Lexical Simplification Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thompsonb/prism\nCitation Context: \u2022 Outperforms or ties with prior metrics and language pairs (Ganitkevitch and Callison-Burch, several contrastive neural methods on the 2014). Paraphrase tables were, in turn, used in MT segment-level WMT 2019 MT metrics task metrics to reward systems for paraphrasing words in every language pair; [URL_CITE] (Banerjee and Lavie, 2005) or phrases (Zhou et al.,\nFootnote or Reference text (if exists): 1 Except for Gujarati, where we had no training data. 2 https://github.com/thompsonb/prism\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt\nCitation Context: The 2012 JHU workshop on Domain Adapta-tion for MT [URL_CITE] proposed phrase sense disambiguation (PSD) for translation model adaptation. In this ap-proach, the context of a phrase helps the system to find the appropriate translation.\nFootnote or Reference text (if exists): 4 http://www.clsp.jhu.edu/workshops/archive/ws-12/groups/dasmt\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yangkevin2/emnlp2020-stream-beam-mt\nCitation Context: Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [URL_CITE] In principle, our method can be applied to any task which sequen-tially processes variable-length data.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://joshua-decoder.com/language-packs/paraphrase/\nCitation Context: This tool can be used to demonstrate and test a model or to hand-tune the model in order to de-termine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the config-uration file, are available at the language pack home page: [URL_CITE] http://joshua-decoder.com/language-packs/paraphrase/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Interactive tool']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tinyurl.com/yblcb5q5\nCitation Context: From a fact-checkers\u2019 point of view, the abun-dance of previously fact-checked claims increases the likelihood that the next claim that needs to be checked would have been fact-checked already by some trusted organization. Indeed, viral claims of-ten come back after a while in social media, and politicians are known to repeat the same claims over and over again. [URL_CITE] Thus, before spending hours fact-checking a claim manually, it is worth first making sure that nobody has done it already.\nFootnote or Reference text (if exists): 7 President Trump has repeated one claim over 80 times: http://tinyurl.com/yblcb5q5.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/\nCitation Context: The increasing popularity of social media has drastically changed how our daily news are pro-duced, disseminated and consumed. [URL_CITE] Without sys-tematic moderation, a large volume of informa-tion based on false or unverified claims (e.g., fake news, rumours, propagandas, etc.) can proliferate online.\nFootnote or Reference text (if exists): 1 The latest Pew Research statistics show that 68% American adults at least occasionally get news on social media. http://www.pewinternet.org/2018/03/01/social-media-use-in-2018/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/clef2018-factcheck/\nCitation Context: Fact checking is very time-consuming, and thus most datasets focus on claims that have been al-ready checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016). As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from En-glish, and focusing on US politics. [URL_CITE] In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly avail-able Arabic fact-checking dataset.\nFootnote or Reference text (if exists): 2 See for example the CLEF-2018 lab on Automatic Iden-tification and Verification of Claims in Political Debates, which features US political debates translated to Arabic: http://alt.qcri.org/clef2018-factcheck/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sshaar/That-is-a-Known-Lie\nCitation Context: \u2022 We create a specialized dataset, which we release to the research community. [URL_CITE] Un-like previous work in fact-checking, which used normalized claims from fact-checking datasets, we work with naturally occurring claims, e.g., in debates or in social media.\nFootnote or Reference text (if exists): 8 Data and code are available at the following URL: https://github.com/sshaar/ That-is-a-Known-Lie\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wapo.st/2rucTq8\nCitation Context: While politicians may refrain themselves from making outright false claims to avoid being fact-checked, oftentimes they even double down after their false claims are debunked. [URL_CITE]\nFootnote or Reference text (if exists): 9 https://wapo.st/2rucTq8\nSection Titles (if exists):  ['2 System Architecture and Components', '2.5 Claim Matcher']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mashable.com/article/snopes-stops-fact-checking-for-facebook/\nCitation Context: On the one hand, it is un-likely fact-checkers are able to check every social media post, due to limited resources and the sheer volume of data. [URL_CITE] On the other hand, a large num-ber of false claims, likely much more than those in traditional media, are being spread through so-cial media. This can be due to the compounded effect of several factors: social media platforms have become increasingly important to public fig-ures and organizations in engaging with voters and citizens; mobile devices have brought an age in which sharing and disseminating information is easy for anyone, including both malicious and un-intentional creators of falsehoods; the falsehoods are further replicated and amplified by social me-dia bots and clickbait articles.\nFootnote or Reference text (if exists): 1 https://mashable.com/article/snopes-stops-fact-checking-for-facebook/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sweaglesw.org/linguistics/ace/\nCitation Context: This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [URL_CITE] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below).\nFootnote or Reference text (if exists): 3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.\nSection Titles (if exists):  ['3 System Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.delph-in.net\nCitation Context: This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000). [URL_CITE] The ERG maps surface strings to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below).\nFootnote or Reference text (if exists): 3 In our experiments, we use the 1212 release of the ERG, in combination with the ACE parser ( http://sweaglesw.org/linguistics/ace/). The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.\nSection Titles (if exists):  ['3 System Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for in-stance, that John knows Mary or John saw an animal. Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descrip-tions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017) [URL_CITE] . Xu et al.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: We perform controlled exper-iments initializing our retrievers with REALM-CCNews (Guu et al., 2020) and varying batch size and keeping all other hyperparameters consistent. In Table 8, we notice a steady increase in perfor-mance as minibatch size is increased, with the largest gains coming by doubling the batch size in Jernite (2020) [URL_CITE] from 512 to 1024. Finally, in pre-liminary experiments we saw no benefit of more intelligent negative sampling schemes.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Batch size R-Prec Recall@5 REALM (pretrained) 6.6 14.9 256 6.2 11.0 512 (Jernite, 2020) [URL_CITE] 6.8 12.6 1024 11.5 21.0 12288 (Ours) 13.3 21.2\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [URL_CITE] that uses gold answers for distant supervision. The key idea is to push the encoded vector for a ques-tion close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Next, we investigate the effect of initialization on the training of C -REALM. Unlike Jernite (2020) [URL_CITE] who initialize their model with BERT, before train-ing we initialize our retriever with a pretrained self-supervised retriever. As a baseline, we initial-ize our model with ICT, a weaker self-supervised retriever introduced in Lee et al.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: This objective is based on contrastive learning, a method that has been used effectively for semi-supervised learning (Chen et al., 2020) and dense retriever training (Karpukhin et al., 2020). Scaling up from Jernite (2020) [URL_CITE] , who used a mini-batch size of 512 and initialized their retriever with BERT, we use much large mini-batches of size 12,288 (and hence, many more negative examples) and initial-ize our retriever with a strong pretrained retriever, the REALM model (Guu et al., 2020) trained on the Common Crawl News (CC-News) corpus. These design decisions greatly improve retriever qual-ity, as we observe in an ablation study (see Ap-pendix A.2).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shawnwun/RNNLG\nCitation Context: \u2022 RNNLG : [URL_CITE] The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface re-alisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far.\nFootnote or Reference text (if exists): 2 https://github.com/shawnwun/RNNLG\nSection Titles (if exists):  ['2 End-to-End NLG Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [URL_CITE] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment.\nFootnote or Reference text (if exists): 2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.\nSection Titles (if exists):  ['4 Experiment', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf\nCitation Context: Atten-tion distribution over the visual input is computed from the last convolutional layer of the network. The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) [URL_CITE] in the recurrent layer. We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.\nFootnote or Reference text (if exists): Orhan Firat and Kyunghyun Cho. 2016. Con-ditional gated recurrent unit with attention mechanism. https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf. Published online, version adbaeea.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multimodal Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.twitter.27B.zip\nCitation Context: Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter [URL_CITE] for each token of ex-tracted discourse arguments from messages. For the distributional representation of discourse ar-guments, we run a Word-level LSTM on the words\u2019 embeddings within each discourse argu-ment and concatenate\u2212\u2192last hidden state vectors of\u2190\u2212forward LSTM (h) and backward LSTM ( h ) which\u2212\u2192is\u2190\u2212suggested by (Ji and Smith, 2017) (DA = [ h ; h ]).\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/data/glove.twitter.27B.zip\nSection Titles (if exists):  ['3 Methods', '3.2 Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5. For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016) [URL_CITE] .\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/dl4mt-tutorial\nCitation Context: \u2022 RNNSearchtentional NMT: a re-implementationsystem (Bahdanauof theet alat-., 2015) with slight changes taken from dl4mt tutorial. [URL_CITE] For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b). It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly gen-erated word.\nFootnote or Reference text (if exists): 8 https://github.com/nyu-dl/ dl4mt-tutorial\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koc\u030cisky\u0301 et al., 2018). Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [URL_CITE] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/document-qa\nSection Titles (if exists):  ['5 Experiments', '5.1 Data and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [URL_CITE] and fairseq .\nFootnote or Reference text (if exists): 1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aka.ms/WikiQA\nCitation Context: We select WikiQA [URL_CITE] as the evaluation data, as it is precisely constructed based on natural language questions and Wikipedia documents, which con-tains 2,118 \u2018question-document\u2019 pairs in the train-ing set, 296 \u2018question-document\u2019 pairs in devel-opment set, and 633 \u2018question-document\u2019 pairs in testing set. Each sentence in the document of a given question is labeled as 1 or 0, where 1 de-notes the current sentence is a correct answer sen-tence, and 0 denotes the opposite meaning.\nFootnote or Reference text (if exists): 6 http://aka.ms/WikiQA\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kermitt2/grobid\nCitation Context: We extracted the text from PDFs using Grobid. [URL_CITE] Several preprocessing steps were necessary before using the articles\u2019 text as features in our model.\nFootnote or Reference text (if exists): 2 https://github.com/kermitt2/grobid\nSection Titles (if exists):  ['4 Methodology', '4.3 Preprocessing and Feature Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/jason/20Newsgroups/\nCitation Context: Text classification. We used the 20NewsGroup [URL_CITE] dataset, which consists of about 20000 docu-ments. Our goal is to classify each document into one of the 20 different newsgroups based on its content.\nFootnote or Reference text (if exists): 6 http://qwone.com/jason/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Setup', '4.5 Downstream NLP Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/scripts\nCitation Context: We extracted 133K articles from Wikidump. [URL_CITE] We used each paragraph in the extracted text as a data sample for our model. We filtered out samples that were either spaces-only or had more than 512 tokens after tokenizing with the pretrained RobertaTokenizer of the roberta-base model.\nFootnote or Reference text (if exists): 2 Timestamp May 9th, 2020. We used the https://github.com/scripts from NVIDIA/DeepLearningExamples/tree/ master/PyTorch/LanguageModeling/BERT# getting-the-data to extract the data.\nSection Titles (if exists):  ['4 Experiments', '4.2 Dataset for Shuffled-Token Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/miso-belica/jusText\nCitation Context: Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [URL_CITE] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences.\nFootnote or Reference text (if exists): 5 https://github.com/miso-belica/ jusText\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.python.org/pypi/newspaper\nCitation Context: We deduplicated the 16,000 extracted URLs into 6,003 unique addressed, then extracted and preprocessed their contents. The newspaper package [URL_CITE] was used to extract article text and the title from the web page. Since we are interested in text articles that can serve as the source text for summarization algorithms, we needed to re-move photos and video links such as those from Instagram and YouTube.\nFootnote or Reference text (if exists): 2 https://pypi.python.org/pypi/newspaper\nSection Titles (if exists):  ['3 Data Extraction and Preprocessing', '3.2 Extracting Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2017). For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) [URL_CITE] during the training phase, yielding the success in the analogy task. For example, a transitive verb su-pertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). Kasai et al.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). Kasai et al.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that gener-alize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analo-gies across induced vector representations of su-pertags (Kasai et al., 2017 [URL_CITE] ; Friedman et al., 2017). The end-to-end TAG parser substantially outper-forms the previously reported best results.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] . For the transition-based parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) during the training phase, yielding the success in the analogy task.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cnap/grammaticality-metrics\nCitation Context: To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [URL_CITE]\nFootnote or Reference text (if exists): 9 https://github.com/cnap/ grammaticality-metrics\nSection Titles (if exists):  ['6 Summary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [URL_CITE] framework.\nFootnote or Reference text (if exists): 1 https://pytorch.org/\nSection Titles (if exists):  ['3 Methodology', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/IQMT\nCitation Context: This approach has proven very effective in recent eval-uation campaigns. Metric computation has been carried out using the IQ MT Framework for Auto-matic MT Evaluation (Gime\u0301nez, 2007) [URL_CITE] . The sim-plicity of this approach (with no training of the metric weighting scheme) ensures that the poten-tial advantages detected in our experiments are not due to overfitting effects.\nFootnote or Reference text (if exists): 1 http://www.lsi.upc.edu/\u02dcnlp/IQMT\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.1 Metric Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service [URL_CITE] (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 11 https://aws.amazon.com/translate\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kpu/kenlm\nCitation Context: PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). [URL_CITE] Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b).\nFootnote or Reference text (if exists): 9 https://github.com/kpu/kenlm\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mjpost/sacrebleu\nCitation Context: PPL and LL scores are extracted from a 5-gram Ken LM model (Heafield, 2011). Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the [URL_CITE] ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS and formality datasets (Briakou et al., 2021b).\nFootnote or Reference text (if exists): 3 https://github.com/mjpost/sacrebleu\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/trescal\nCitation Context: NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set [URL_CITE] . The data statistics of the training set are summarized in Table 1.\nFootnote or Reference text (if exists): 4 http://bit.ly/trescal\nSection Titles (if exists):  ['5 Experiments', '5.1 Knowledge Base Completion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~nlao/\nCitation Context: NELL collects human knowledge from the web and has generated millions of entity-relation triples. We use the data generated from version 165 for training [URL_CITE] , and collect the new triples gen-erated between NELL versions 166 and 533 as the development set and those generated between ver-sion 534 and 745 as the test set . The data statistics of the training set are summarized in Table 1.\nFootnote or Reference text (if exists): 3 http://www.cs.cmu.edu/\u02dcnlao/\nSection Titles (if exists):  ['5 Experiments', '5.1 Knowledge Base Completion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PhilipMay/stsb-multi-mt\nCitation Context: Training Data Table 3 presents statistics on the training data used for supervised and unsuper-vised models across the ST evaluation aspects. For datasets that are only available for EN , we use the already available machine translated re-sources for STS [URL_CITE] and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service (with reported BLEU scores of 37.16 ( BR -\nFootnote or Reference text (if exists): 10 https://github.com/PhilipMay/ stsb-multi-mt\nSection Titles (if exists):  ['4 Experiment Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Experiment Details We adopt the T5 base model from huggingface Transformer library [URL_CITE] for all experiments. T5 closely follows the original encoder-decoder architecture of the Transformer model, with some slight differences such as differ-ent position embedding schemes.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/sentencepiece\nCitation Context: All experiments are performed with the Trans-former architecture (Vaswani et al., 2017) using the open-source Tensorflow-Lingvo implementa-tion (Shen et al., 2019). Specifically, we use the Transformer Big model containing 375M parame-ters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) [URL_CITE] (Kudo and Richardson, 2018). We use a vocabulary size of 32k for the bilingual models and 64k for the multilingual mod-els.\nFootnote or Reference text (if exists): 3 https://github.com/google/sentencepiece\nSection Titles (if exists):  ['3 Experimental Setup', '3.3 Architecture and Optimization']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marian-nmt/marian-examples/blob/master/transformer\nCitation Context: Transformer We used the same training, dev and test sets as in the big RNN model, and the exam-ple configuration for a transformer model from Marian [URL_CITE] adapted to our needs as shown in Table 1. This configuration utilises a six-layer deep encoder and decoder, learning rate warm-up and tied em-beddings for source, target and output layer.\nFootnote or Reference text (if exists): 2 https://github.com/marian-nmt/marian-examples/blob/master/transformer\nSection Titles (if exists):  ['4 Experiment setup', '4.3 Training setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dl.acm.org/citation.cfm?id=1557769.1557821\nCitation Context: Associa-tion for Computational Linguistics, Strouds-burg, PA, USA, ACL \u201907, pages 177\u2013 180. [URL_CITE] http://dl.acm.org/citation.cfm?id=1557769.1557821.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P14-1006\nCitation Context: Association for Compu-tational Linguistics, Baltimore, Maryland, pages 58\u201368. [URL_CITE] http://www.aclweb.org/anthology/P14-1006.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1073083.1073126\nCitation Context: Association for Compu-tational Linguistics, Philadelphia, Pennsylva-nia, USA, pages 255\u2013262. [URL_CITE] https://doi.org/10.3115/1073083.1073126.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1160\nCitation Context: Association for Compu-tational Linguistics, San Diego, California, pages 1346\u20131356. [URL_CITE] http://www.aclweb.org/anthology/N16-1160.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.3115/1118905.1118906\nCitation Context: Computational Linguistics, Stroudsburg, PA, USA, HLT-NAACL-PARALLEL \u201903, pages 1\u201310. [URL_CITE] https://doi.org/10.3115/1118905.1118906.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisisnlp.qcri.org/\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, [URL_CITE] to-gether with two resources from CrisisLex. Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex.\nFootnote or Reference text (if exists): 2 https://crisisnlp.qcri.org/\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Websail-NU/CODAH\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [URL_CITE] for infer-ence.\nFootnote or Reference text (if exists): 3 http://www.gurobi.com/\nSection Titles (if exists):  ['6 Empirical Evaluation', '6.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [URL_CITE] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz)\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crisislex.org/data-collections.html\nCitation Context: To prepare our large multilingual dataset, we ag-gregated several resources from CrisisNLP, to-gether with two resources from CrisisLex. [URL_CITE] Specif-ically, we used Resource #1 (Imran et al., 2016a), Resource #4 (Nguyen et al., 2017), Resource #5 (Alam et al., 2018c), and Resource #7 (Alam et al., 2018a) from CrisisNLP, and CrisisLexT6 (Olteanu et al., 2014) and CrisisLexT26 (Olteanu et al., 2015) from CrisisLex. The original classes in each resource, together with the mapping to the new classes included in our data set, can be seen in Table 1.\nFootnote or Reference text (if exists): 3 https://crisislex.org/data-collections.html\nSection Titles (if exists):  ['3 Aggregated Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://openreview.net/pdf?id=rke2P1BFwS\nCitation Context: We take three static KG models as the static baselines, including Distmult (Yang et al., 2014), TuckER (Balazevic et al., 2019), and COMPGCN (Vashishth et al., 2019). For tKG baselines, we report the performance of TTransE (Leblay and Chekol, 2018), TA-Distmult (Garc\u00eda-Dur\u00e1n et al., 2018), CyGNet (Zhu et al., 2020), DE-SimplE (Goel et al., 2020), TNTComplEx (Lacroix et al., 2020) [URL_CITE] , and RE-Net (Jin et al., 2019). We provide implementation details of baselines and TANGO in Appendix C.\nFootnote or Reference text (if exists): Timothee Lacroix, Guillaume Obozinski, and Nico-las Usunier. 2020. Tensor decompositions for tem-poral knowledge base completion. ICLR preprint https://openreview.net/pdf?id=rke2P1BFwS.\nSection Titles (if exists):  ['4 Experiments', '4.1.3 Baseline Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JiaweiSheng/FAAN\nCitation Context: Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at [URL_CITE] https://github.com/JiaweiSheng/FAAN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/andreasvc/discodop\nCitation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\nFootnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/serenayj/ABCD-ACL2021\nCitation Context: The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. [URL_CITE]\nFootnote or Reference text (if exists): 1 ABCD is available at https://github.com/serenayj/ABCD-ACL2021.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TemporalKGTeam/TANGO\nCitation Context: We evaluate our model by performing future link prediction on five tKG datasets [URL_CITE] . We compare TANGO\u2019s performance with several existing meth-ods and evaluate its potential with inductive link prediction and long horizontal link forecasting.\nFootnote or Reference text (if exists): 1 Code and datasets are available at https://github.com/TemporalKGTeam/TANGO. filtered MRR (%) on ICEWS05-15 with regard to dif-ferent \u2206t. preprint arXiv:2012.08492.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/chrisjbryant/errant\nCitation Context: To identify incorrect spans, we use a binary se-quence tagging model in which tag 0 means the token is in a correct span; while tag 1 means the to-ken is in a grammatically incorrect span that needs to be edited, as shown in Figure 1(a). In order to train the tagging model, we align [URL_CITE] tokens across the source and target sentence in training data. With token alignment, we can identify the text spans that are edited and thus can annotate the edited text spans in the original sentences as erroneous spans.\nFootnote or Reference text (if exists): 1 Alignment can be solved by dynamic programming like Levenshtein distance. We here use ERRANT (https://github.com/chrisjbryant/errant) for alignment.\nSection Titles (if exists):  ['3 Erroneous Span Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hunch.net/~vw/\nCitation Context: Since the topics are fixed, this inference is very fast and can be parallelized on a per-document ba-sis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit [URL_CITE] with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.\nFootnote or Reference text (if exists): 4 http://hunch.net/\u02dcvw/\nSection Titles (if exists):  ['3 High Water Mark for Tandem Anchors', '3.2 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://flan.cs.pitt.edu/~hwa/align/align.html\nCitation Context: In summary, we have presented an annotation envi-ronment for acquiring word alignments between En-glish and Chinese as well as Part-Of-Speech tags for Chinese. The system is in place and the annotation process is underway. [URL_CITE]\nFootnote or Reference text (if exists): 4 The annotation interface is open to public. Please visit http://flan.cs.pitt.edu/~hwa/align/align.html\nSection Titles (if exists):  ['4 Conclusion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sequencecomparison.github.io\nCitation Context: In this test, words drawn from a random-ized sample are repeatedly aligned with each other in order to create a distribution of sound transi-tions for unrelated languages. This distribution is then compared with the actual distribution re-trieved from aligned words in the word list, and a language-specific scoring function is created List (2014b) [URL_CITE] . SCA is very fast in computation, but Lex-Stat has a much higher accuracy.\nFootnote or Reference text (if exists): Johann-Mattis List. 2014b. Sequence compari-son in historical linguistics. D\u00fcsseldorf Uni-versity Press, D\u00fcsseldorf. URL: http://sequencecomparison.github.io.\nSection Titles (if exists):  ['3 Methods', '3.1 Sequence Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: To tackle this challenge, we propose to use a label projection method based on word alignment. Specifically, we first leverage an exist-ing English POS tagger in the spaCy [URL_CITE] library to annotate the English source sentences. We then use the fast_align tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data.\nFootnote or Reference text (if exists): 14 https://spacy.io/\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [URL_CITE] . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data .\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: Previous work found that the performance can be improved by pre-training the word embeddings on large unla-beled data and using the obtained embeddings to make initialization. With the result in Table 2, it is true that these pre-trained word embeddings have a good effect on our performance (we use word2vec [URL_CITE] on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, com-pared to standard pre-training, the influence of het-erogenous data is more evident.\nFootnote or Reference text (if exists): 3 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Experiments', '3.2 Chinese SRL Performance']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/sentence-transformers\nCitation Context: We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [URL_CITE] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model.\nFootnote or Reference text (if exists): 5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.\nSection Titles (if exists):  ['4 Experimental Settings', '4.3 Model Training and Configurations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [URL_CITE] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model.\nFootnote or Reference text (if exists): 5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.\nSection Titles (if exists):  ['4 Experimental Settings', '4.3 Model Training and Configurations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiachenwestlake/Entity_BERT\nCitation Context: Results on the three datasets show that our method outperforms these methods and achieves the best results, which demonstrates the effec-tiveness of the proposed Char-Entity-Transformer structure for integrating entity information in LM pre-training for Chinese NER. To our knowl-edge, we are the first to investigate how to make use of the scale of the input document text for enhancing NER. Our code and NER datasets are released at [URL_CITE] https://github.com/jiachenwestlake/Entity_BERT.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/\nCitation Context: LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word em-beddings based on LSA [URL_CITE] . The vocabulary size is 100,000.\nFootnote or Reference text (if exists): 5 http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/\nSection Titles (if exists):  ['5 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2003T05\nCitation Context: We also experiment with the English Gigaword corpus, [URL_CITE] which contains data from several English newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker.\nFootnote or Reference text (if exists): 6 https://catalog.ldc.upenn.edu/ LDC2003T05\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu.2002\nCitation Context: We use a Maximum Entropy (Berger et al., 1996) classifier with a large number of boolean features, some of which are novel (e.g., the inclusion of words from WordNet definitions). Maximum En-tropy classifiers have been effective on a variety of NLP problems including preposition sense disam-biguation (Ye and Baldwin, 2007), which is some-what similar to noun compound interpretation. We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002) [URL_CITE] .\nFootnote or Reference text (if exists): McCallum, A. K. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.2002.\nSection Titles (if exists):  ['5 Automated Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.d.umn.edu/~tpederse/WSDTutorial.html\nCitation Context: Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005) [URL_CITE] . The hit pages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of a classifier. Features used in classification will be explained in the latter half of this subsection.\nFootnote or Reference text (if exists): Ted Pedersen and Rada. F. Mihalcea, \u201cAdvances in Word Sense Disambiguation,\u201d tutorial at ACL 2005. http://www.d.umn.edu/~tpederse/WSDTutorial.html.\nSection Titles (if exists):  ['2 The proposal']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13\nCitation Context: The larger the collec-tion is, the better would be the estimation. Recently, Google has released the Web 1T dataset [URL_CITE] that pro-vides q(w) estimated on a text collection of one tril-lion tokens. We use it in our experimentation.\nFootnote or Reference text (if exists): 3 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2017-cmapsum-corpus\nCitation Context: We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Priberam/exconsumm\nCitation Context: We also make available a new dataset of oracle compressive summaries de-rived automatically from the CNN/DailyMail reference summaries. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our dataset and code is available at https://github.com/Priberam/exconsumm.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wangcunxiang/Sen-Making-and-Explanation\nCitation Context: Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [URL_CITE] https://github.com/wangcunxiang/Sen-Making-and-Explanation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/serenayj/ABCD-ACL2021\nCitation Context: The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. [URL_CITE]\nFootnote or Reference text (if exists): 1 ABCD is available at https://github.com/serenayj/ABCD-ACL2021.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/complementizer/news-tls\nCitation Context: For a more robust evaluation, we also present a new TLS dataset, which is larger and spans longer time periods than previous datasets. The dataset will be made available at [URL_CITE] https://github.com/complementizer/news-tls.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://voice.etri.re.kr/db/dbpop.asp?code=88\nCitation Context: We annotated predicate-argument structure of verbs in a corpus from the Electronics and Telecommunications Research Institute of Korea (ETRI). [URL_CITE] Our corpus was developed over two years using a specialized annotation tool (Song et al., 2012), resulting in more than 8,000 semanti-cally annotated sentences. As much as possible, annotations followed the PropBank guidelines for English (Bonial et al., 2010).\nFootnote or Reference text (if exists): 1 http://voice.etri.re.kr/db/dbpop.asp?code=88\nSection Titles (if exists):  ['2 A Semantically Annotated Korean Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sle.sharp.co.uk/senseval2/\nCitation Context: We took as data the col-lection of S ENSEVAL -2 English lexical sample WSD corpora. [URL_CITE] We set the NB model parameters in several ways. We optimized JL (using the RFE s).\nFootnote or Reference text (if exists): 2 http://www.sle.sharp.co.uk/senseval2/\nSection Titles (if exists):  ['2 Objective Functions: Naive-Bayes']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amr.isi.edu/download.html\nCitation Context: Annotating natural language with AMR is a complex task and training datasets only exist for English [URL_CITE] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English. We create training data for multilingual AMR-to- Text models, by taking the EUROPARL multilin-gual corpus and automatically annotating the En-glish data with AMRs using the jamr semantic parser.\nFootnote or Reference text (if exists): 1 AMR datasets from the LDC can be found at https://amr.isi.edu/download.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu\nCitation Context: We evaluate the models using a corpus of ESL es-says, annotated [URL_CITE] by native English speakers (Ro-zovskaya and Roth, 2010a). For each preposition (article) used incorrectly, the annotator indicated the correct choice.\nFootnote or Reference text (if exists): 7 The annotation of the ESL corpus can be downloaded from http://cogcomp.cs.illinois.edu.\nSection Titles (if exists):  ['3 Comparison of Algorithms', '3.1 Evaluation Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nCitation Context: All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [URL_CITE] .\nFootnote or Reference text (if exists): 4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia and the British National Corpus [URL_CITE] . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 4 http://www.natcorp.ox.ac.uk/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library [URL_CITE] (Wolf et al., 2020b).\nFootnote or Reference text (if exists): 3 https://github.com/huggingface/ datasets\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/~jrennie/20Newsgroups/\nCitation Context: In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents [URL_CITE] . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al., 2007).\nFootnote or Reference text (if exists): 1 http://people.csail.mit.edu/~jrennie/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com/talks\nCitation Context: The dataset consists of transcripts directly col-lected from [URL_CITE] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don\u2019t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/datasets\nCitation Context: Sim-ilarly, the Rotten Tomatoes (RT) (Pang and Lee, 2005) dataset has 5331 positive and 5331 negative review sentences. We use the processed dataset made available by HuggingFace Dataset library 3 (Wolf et al., 2020b) [URL_CITE] .\nFootnote or Reference text (if exists): Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Fran\u00e7ois Lagunas, Lysandre Debut, Morgan Funtow-icz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Mu\u0161tar, Jeff Boudier, and Anna Tordjmann. 2020b. Datasets. GitHub. Note: https://github.com/huggingface/datasets.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2016/task7/\nCitation Context: Portions of the created lexicon have been used as development and evaluation sets in SemEval-2016 Task 7 \u2018Determining Sentiment Intensity of English and Arabic Phrases\u2019 (Kiritchenko et al., 2016). [URL_CITE] The objective of that task was to test different meth-ods of automatically predicting sentiment associa-tion scores for multi-word phrases.\nFootnote or Reference text (if exists): 5 http://alt.qcri.org/semeval2016/task7/\nSection Titles (if exists):  ['3 Creating a Sentiment Lexicon for Opposing Polarity Phrases']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://vectors.nlpl.eu/repository/\nCitation Context: All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [URL_CITE] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines).\nFootnote or Reference text (if exists): 8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.naist.jp/hiki/ChaSen/\nCitation Context: We tokenized the sentences into words and tagged the part-of-speech information using the Japanese morphological analyzer ChaSen [URL_CITE] 2.3.3 and then labeled the NEs. Unreadable to-kens such as parentheses were removed in to-kenization.\nFootnote or Reference text (if exists): 1 http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/lexicons/effectlexicon/\nCitation Context: +E FFECT (x) and -E FFECT (x): We use the +/-effect sense-level lexicon (Choi and Wiebe, 2014) [URL_CITE] to extract the +/-effect events in each sen-tence. The score of +E FFECT (x) is the fraction of that word\u2019s senses that are +effect senses accord-ing to the lexicon, and the score of -E FFECT (x) is the fraction of that word\u2019s senses that are -effect senses according to the lexicon. If a word does not appear in the lexicon, we do not treat it as a +/-effect event, and thus assign 0 to both +E FFECT (x) and -E FFECT (x).\nFootnote or Reference text (if exists): 7 Available at: http://mpqa.cs.pitt.edu/lexicons/effectlexicon/\nSection Titles (if exists):  ['4 PSL for Sentiment Analysis', '4.4 PSL Augmented with +/-Effect Events (PSL3)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [URL_CITE] . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 3 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.createdebate.com\nCitation Context: The second dataset (createdebate, henceforth) comes from an online debating site [URL_CITE] . It consists of 30 debates containing a total of 2,712 posts.\nFootnote or Reference text (if exists): 3 http://www.createdebate.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [URL_CITE] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.\nFootnote or Reference text (if exists): 3 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://towcenter.org/research/lies-damn-lies-and-viral-content/\nCitation Context: It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by jour-nalists in the context of the Emergent Project (Sil-verman, 2015) [URL_CITE] , and the dataset was first proposed by Ferreira and Vlachos (2016), who only classi-fied the claim w.r.t. the article headline instead of the whole article.\nFootnote or Reference text (if exists): Craig Silverman. 2015. Lies, Damn Lies and Viral Content. http: //towcenter.org/research/ lies-damn-lies-and-viral-content/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nCitation Context: We implement our models using the jiant toolkit, [URL_CITE] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/iterative-search-semparse\nCitation Context: Implementation We ourimplemented model and training algorithms within the AllenNLP (Gardner et al., 2018) toolkit. The code and models are publicly available at [URL_CITE] https://github.com/allenai/iterative-search-semparse.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Experiments', '6.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gitlab.com/sutdnlp/statnlp-core\nCitation Context: We implemented our neural component with the Torch7 library (Collobert et al., 2011). Our com-plete implementation is based on the StatNLP [URL_CITE] structured prediction framework (Lu, 2017).\nFootnote or Reference text (if exists): 12 https://gitlab.com/sutdnlp/statnlp-core\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MT-DNN\nCitation Context: We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [URL_CITE] . Training details are presented in Appendix B.2.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/MT-DNN\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We implement our models using the jiant toolkit, which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. [URL_CITE] Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 4 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2w7nUQK\nCitation Context: We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google\u2019s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [URL_CITE] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences.\nFootnote or Reference text (if exists): 2 This model can be downloaded at https://bit.ly/2w7nUQK.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hanxiao/bert-as-service\nCitation Context: For experiments with the con-textualized representation, we adopt the pre-trained language model BERT (Devlin et al., 2019) for the four datasets. Specifically, we use bert-as-service (Xiao, 2018) [URL_CITE] to generate the contextualized word representation without fine-tuning. Following Luo et al.\nFootnote or Reference text (if exists): Han Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cl-tohoku/bert-japanese\nCitation Context: We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [URL_CITE] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set.\nFootnote or Reference text (if exists): 3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.\nSection Titles (if exists):  ['5 Experiments', '5.2 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: As a text encoder, we employed BERT and a Hi-erarchical Attention Network (HAN) (Yang et al., 2016) for generating sentence and document rep-resentation, respectively. For BERT, we used the pre-trained BERT-base [URL_CITE] (d = 768). We imple-mented the HAN following Yang et al.\nFootnote or Reference text (if exists): 7 https://github.com/google-research/bert\nSection Titles (if exists):  ['3 Experiments', '3.3 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs.umd.edu/miyyer/qblearn/\nCitation Context: LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) [URL_CITE] is taken from Iyyer et al. (2014).\nFootnote or Reference text (if exists): 6 http://cs.umd.edu/miyyer/qblearn/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiesutd/SubwordEncoding-CWS\nCitation Context: In partic-ular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code [URL_CITE] is based on NCRF++ (Yang and Zhang, 2018).\nFootnote or Reference text (if exists): 1 Our code is released at https://github.com/jiesutd/SubwordEncoding-CWS.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nCitation Context: There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST [URL_CITE] MT metrics. We also provide two base-lines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt/resources/scoring.htm\nSection Titles (if exists):  ['2 References to people', '2.6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Detectron.facebookresearch/detectron\nCitation Context: The M AX E NT baseline com-putes features from the sentence and objects de-tected in the paired images. We detect the objects in the images using a Mask R-CNN model (He et al., 2017; Girshick et al., 2018 [URL_CITE] ) pre-trained on the COCO detection task (Lin et al., 2014). We use a detection threshold of 0.5.\nFootnote or Reference text (if exists): Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. https://github.com/Detectron.facebookresearch/detectron.\nSection Titles (if exists):  ['6 Evaluation Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser\nCitation Context: As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [URL_CITE]\nFootnote or Reference text (if exists): 3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.\nSection Titles (if exists):  ['3 Consensus and Minimum Bayes Risk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hltfbk.github.io/Excitement-Open-Platform/\nCitation Context: For this base-line, MCTest is again re-casted as an RTE task by converting each question-answer pair into a statement (using Cucerzan and Agichtein (2005)) and then selecting the answer whose statement has the highest likelihood of being entailed by the story. [URL_CITE] (4) The fourth baseline (called LSTM) is taken from Weston et al. (2015).\nFootnote or Reference text (if exists): 5 The BIUTEE system (Stern and Dagan, 2012) available under the Excitement Open Platform http://hltfbk.github.io/Excitement-Open-Platform/ was used for recognizing textual entailment.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aclweb.org/anthology/D16-1264\nCitation Context: Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) [URL_CITE] . This contains about 23K passages and 100K questions.\nFootnote or Reference text (if exists): Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text pages 2383\u20132392. https://aclweb.org/anthology/D16-1264.\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aip-nlu.gitlab.io/resources/sas-japanese\nCitation Context: We use the Japanese short answer scoring dataset [URL_CITE] introduced by Mizumoto et al. (2019).\nFootnote or Reference text (if exists): 1 https://aip-nlu.gitlab.io/resources/ sas-japanese\nSection Titles (if exists):  ['5 Experiments', '5.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/\nCitation Context: An-swers Comprehensive Questions and Answers cor-pus obtained via Webscope. [URL_CITE] This dataset contains about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from real users in real time.\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: Answers dataset from Yahoo! Labs Webscope [URL_CITE] . Each question in the dataset contains title, description, best an-swer, most voted answers and meta-data like categories, sub categories etc.\nFootnote or Reference text (if exists): 11 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bdhingra/quasar\nCitation Context: Quasar-T [URL_CITE] (Dhingra et al., 2017b) consists of 43, 000 open-domain trivia question, and their an-swers are extracted from ClueWeb09 data source, and the paragraphs are obtained by retrieving 50 sentences for each question from the ClueWeb09 data source using LUCENE.\nFootnote or Reference text (if exists): 1 https://github.com/bdhingra/quasar\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research.facebook.com/research/-babi/\nCitation Context: Bordes et al. (2015) released the SimpleQuestions data set [URL_CITE] , which consists of 108,442 English questions. Each ques-tion (e.g., What does Jimmy Neutron do?)\nFootnote or Reference text (if exists): 11 https://research.facebook.com/research/-babi/\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Websail-NU/CODAH\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/commonsense/conceptnet5/wiki/Downloads\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) [URL_CITE] , CSQA (Talmor et al., 2019) , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 3 https://github.com/commonsense/conceptnet5/wiki/Downloads\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/mirrors.html\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/attardi/wikiextractor\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/soskek/bookcorpus\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tau-nlp.org/commonsenseqa\nCitation Context: Resource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) , CSQA (Talmor et al., 2019) [URL_CITE] , and CODAH (Chen et al., 2019) 5 re-spectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources.\nFootnote or Reference text (if exists): 4 https://www.tau-nlp.org/commonsenseqa 5 https://github.com/Websail-NU/CODAH\nSection Titles (if exists):  ['* Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.answers.com\nCitation Context: Yahoo! An-swers, Answers.com, [URL_CITE] etc. ), which became very popular resources for question answering.\nFootnote or Reference text (if exists): 2 http://www.answers.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers [URL_CITE] , Quora and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora and the StackExchange [URL_CITE] family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 3 http://stackexchange.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://quora.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora [URL_CITE] and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 2 http://quora.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com/\nCitation Context: Yahoo! An-swers, [URL_CITE] Answers.com, etc. ), which became very popular resources for question answering.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, [URL_CITE] and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 6 http://www.opensubtitles.org/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://l10n.kde.org/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, [URL_CITE] movie subtitles, and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 5 http://l10n.kde.org/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ted.com/translate/\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks. [URL_CITE]\nFootnote or Reference text (if exists): 7 http://www.ted.com/translate/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com/toolkit/\nCitation Context: Research has shown that less qualified transla-tors are able to increase their productivity and qual-ity disproportionally when given automatic assis-tance (Koehn and Haddow, 2009). Assistance may be as limited as offering machine translation in a post-editing environment, as for instance provided by Google Translator Toolkit [URL_CITE] (Galvez and Bhansali, 2009) which provides a special function to translate Wikipedia articles.\nFootnote or Reference text (if exists): 8 http://translate.google.com/toolkit/\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Thus our method some-times acts as an efficient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour-aged by the Wikipedia community, [URL_CITE] but from exam-ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors.\nFootnote or Reference text (if exists): 8 https://en.wikipedia.org/wiki/ Wikipedia:Translation#Avoid_machine_ translations\nSection Titles (if exists):  ['6 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/Wikipedia:Translation\nCitation Context: Human translation is also performed in a non-professional environment by generally less quali-fied volunteer translators. To give just a few ex-ample: there are vibrant communities that concern themselves with the translation of Wikipedia arti-cles [URL_CITE] (Kumaran et al., 2008), open source software documentation, movie subtitles, and even material such as the TED conference talks.\nFootnote or Reference text (if exists): 4 http://en.wikipedia.org/wiki/Wikipedia:Translation\nSection Titles (if exists):  ['2 Human Translation', '2.2 Translation Skills']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://eecs.berkeley.edu/~gdurrett\nCitation Context: We present a supervised system that, given only data from Wiktionary, au-tomatically discovers and learns to apply the ortho-graphic transformations governing a language\u2019s in-flectional morphology. [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://eecs.berkeley.edu/~gdurrett for our datasets and code.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/snakeztc/NeuralDialog-LAED\nCitation Context: Our methods have been validated on real-world dialog datasets to discover semantic representa-tions and enhance encoder-decoder mod-els with interpretable generation. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code are available at https://github.com/snakeztc/NeuralDialog-LAED.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rivercold/BERT-unsupervised-OOD\nCitation Context: Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/research/sp\nCitation Context: Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our system and code available at http://statnlp.org/research/sp.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/OnlpLab/morphodetection\nCitation Context: We conclude that bootstrapping datasets for in-flectional morphology in low-resourced languages, is a viable strategy to be adopted and explored. [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available on https://github.com/OnlpLab/morphodetection.\nSection Titles (if exists):  ['table, given a few forms of the same lexeme. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cs.umd.edu/miyyer/qblearn/\nCitation Context: LSTMs have re-cently achieved state-of-the-art results in a vari-ety of tasks due to their ability to model long-term context information as opposed to other neu-ral networks based techniques. (5) The fifth base-line (called QANTA) [URL_CITE] is taken from Iyyer et al. (2014).\nFootnote or Reference text (if exists): 6 http://cs.umd.edu/miyyer/qblearn/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nusnlp/nea\nCitation Context: LSTM T&N model We replicate and evaluate the LSTM model of Taghipour and Ng (2016) [URL_CITE] on ASAP and our synthetic data.\nFootnote or Reference text (if exists): 14 https://github.com/nusnlp/nea\nSection Titles (if exists):  ['5 Model Parameters and Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimental re-sults show that the transition-based linearization sys-tem runs an order of magnitude faster than a state-of-the-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at [URL_CITE] http://sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fred2008/TCMSA\nCitation Context: To our knowledge, we are the first to investigate graph NNs for tree senti-ment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST. We release our code and models at [URL_CITE] https://github.com/fred2008/TCMSA.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser\nCitation Context: As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [URL_CITE]\nFootnote or Reference text (if exists): 3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.\nSection Titles (if exists):  ['3 Consensus and Minimum Bayes Risk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/treelstm\nCitation Context: Our experiments show that Tree-LSTMs outper-form existing systems and sequential LSTM base-lines on both tasks. Implementations of our mod-els and experiments are available at [URL_CITE] https://github.com/stanfordnlp/treelstm.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/research/projects/tao/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [URL_CITE] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/software.php\nCitation Context: We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger [URL_CITE] ) to generate features. For edge the entailment and paraphrase identification tasks.\nFootnote or Reference text (if exists): 3 http://L2R.cs.uiuc.edu/\u02dccogcomp/software.php\nSection Titles (if exists):  ['5 Experiments', '5.2 Textual Entailment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 2005 [URL_CITE] , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously over-looks the arguments-scattering challenge. In con-trast, EE on financial documents, such as ChFi-nAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event.\nFootnote or Reference text (if exists): 2 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hlt.utdallas.edu/\u223cady\nCitation Context: However, the utilization of the ACE corpus for the task of solving event coreference is lim-ited because this resource provides only within-document event coreference annotations using a restricted set of event types such as LIFE , BUSI - NESS , CONFLICT , and JUSTICE . Therefore, as a second dataset, we created the EventCorefBank (ECB) corpus [URL_CITE] to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event corefer-ence resolution. One important step in the cre-ation process of this corpus consists in finding sets of related documents that describe the same semi-nal event such that the annotation of coreferential event mentions across documents is possible.\nFootnote or Reference text (if exists): 6 ECB is available at http://www.hlt.utdallas.edu/\u223cady.\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf\nCitation Context: Given unstructured transcribed CC, we extract en-tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [URL_CITE] :\nFootnote or Reference text (if exists): NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.\nSection Titles (if exists):  ['2 Approach', '2.1 Event Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/ace/2005\nCitation Context: NER systems, how-ever, do not identify nominal NP instances (e.g., \u201ca software manufacturer\u201d or \u201cthe beach\u201d), or han-dle semantic classes that are not associated with proper named entities (e.g., symptoms). ACE mention detection systems (e.g., see (ACE, 2005 [URL_CITE] ; ACE, 2007; ACE, 2008)) require tagging of NPs that correspond to 5-7 general semantic classes. These systems are typically trained with super-vised learning using annotated corpora, although techniques have been developed to use resources for one language to train systems for different lan-guages (e.g., (Zitouni and Florian, 2009)).\nFootnote or Reference text (if exists): ACE. 2005. NIST ACE evaluation website. In http://www.nist.gov/speech/tests/ace/2005.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/collaborations/past-projects/ace\nCitation Context: Besides employing active learning to create spe-cific data set with nested, overlapping, discontinu-ous entity mentions, we notice that there are some off-the-shelf corpora in biomedical domain that we can use to evaluate our proposed methods, al-though, to our knowledge, none of these data sets contains all three kinds of complex mentions, e.g., GENIA (Kim et al., 2003) only contains nested en-tity mentions, and CADEC (Karimi et al., 2015) and SemEval2014 (Pradhan et al., 2014) contain overlapping and discontinuous mentions. In ad-dition, ACE [URL_CITE] and NNE (Ringland, 2016) are newswire corpora with nested entity mentions.\nFootnote or Reference text (if exists): 1 https://www.ldc.upenn.edu/collaborations/past-projects/ace\nSection Titles (if exists):  ['4 Research Plan']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.crowdflower.com\nCitation Context: We used CrowdFlower, [URL_CITE] where annotators were randomly presented test questions for quality con-trol. We collected labels from three annotators for each entity pair, and considered labels when at least two agreed. The resulting annotation con-tains total 2,995 labels on 914 documents, 682 positive, 836 negative and 474 without sentiment, which we discarded.\nFootnote or Reference text (if exists): 6 http://www.crowdflower.com\nSection Titles (if exists):  ['4 Data', '4.2 Sentiment Data Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/\nCitation Context: We randomly select 100 candidates and compare the outputs of different systems. We use Ama-zon Turk [URL_CITE] for human evaluation. Each candidate is rated by three annotators and we report the av-erage scores here.\nFootnote or Reference text (if exists): 8 https://www.mturk.com/ tent preservation. Mean denotes the average of the met-\nSection Titles (if exists):  ['3 Experiments', '3.5 Human Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html\nCitation Context: We sampled [URL_CITE] 100 objects from each of the ten dis-tributions (clusters), and made a dataset of 1,000 ob-jects in total.\nFootnote or Reference text (if exists): 1 We used the random sampling code available at http: //people.kyb.tuebingen.mpg.de/suvrit/work/progs/movmf.html (Banerjee et al., 2005).\nSection Titles (if exists):  ['300']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com/\nCitation Context: We conducted user study via crowdsourcing using the FigureEight [URL_CITE] platform. To ensure the quality of annotators, we used a dedicated expert-worker pool provided by FigureEight. We present the detailed setup of our user studies as follows.\nFootnote or Reference text (if exists): 6 https://www.figure-eight.com/\nSection Titles (if exists):  ['B Human Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html\nCitation Context: We used the WEBIS-CLS-10 corpus [URL_CITE] provided by (Prettenhofer and Stein, 2010) for evaluation. It consists of Amazon product reviews for three product categories (i.e.\nFootnote or Reference text (if exists): 3 http://www.uni-weimar.de/medien/webis/research/corpora/corpus-webis-cls-10.html\nSection Titles (if exists):  ['5 Empirical Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://crowdflower.com\nCitation Context: To answer these questions, we also conduct a human judges study on Crowdflower [URL_CITE] . We se-lect 200 items at random (100 real reviews and 100 from the conditional model, half of each with meta-information), and ask annotators to rate them as real or fake.\nFootnote or Reference text (if exists): 6 https://crowdflower.com\nSection Titles (if exists):  ['4 Experiments', '4.2 Human Judges']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://corpus.korean.go.kr\nCitation Context: We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [URL_CITE] . We split these corpora into three, and each is for training, validation, and test.\nFootnote or Reference text (if exists): 1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2017-cmapsum-corpus\nCitation Context: We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nCitation Context: With our work, we free CDCR research from depending on costly human-annotated training data and open up possibili-ties for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and model available at https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Liang-Qiu/SVRNN-dialogues\nCitation Context: While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is released at https://github.com/Liang-Qiu/SVRNN-dialogues.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wangcunxiang/Sen-Making-and-Explanation\nCitation Context: Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [URL_CITE] https://github.com/wangcunxiang/Sen-Making-and-Explanation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lair.cse.msu.edu/lair/projects/actioneffect.html\nCitation Context: This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples. Third, we have cre-ated a dataset for this task, which is available to the community [URL_CITE] . Our bootstrapping approach can serve as a baseline for future work on this topic.\nFootnote or Reference text (if exists): 1 This dataset is available at http://lair.cse.msu.edu/lair/projects/actioneffect.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hltcoe/golden-horse\nCitation Context: We make our code and the annotated corpus available. [URL_CITE]\nFootnote or Reference text (if exists): 10 https://github.com/hltcoe/golden-horse\nSection Titles (if exists):  ['4 Weibo NER Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2018-novel-metaphors\nCitation Context: We make the annota-tions and scripts to embed them into the original corpus publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/UKPLab/ emnlp2018-novel-metaphors\nSection Titles (if exists):  ['3 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://u.cs.biu.ac.il/~nlp/resources/downloads/\nCitation Context: We ob-serve similar trends to previously published results, and make the dataset splits available for replication. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://u.cs.biu.ac.il/~nlp/resources/downloads/\nSection Titles (if exists):  ['2 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NVIDIA/nv-wavenet\nCitation Context: Our full synthesis model consists of a bidirectional LSTM of 512 dimen-sions, a linear projection down to 128 dimensions, and finally the WaveNet decoder which generates samples at 16 kHz. We use a WaveNet implementa-tion from NVIDIA [URL_CITE] which provides efficient GPU inference. WaveNet hyperparameters can be found in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/NVIDIA/nv-wavenet\nSection Titles (if exists):  ['\u03b4 CCA [i, j] = P S E 0S [i] \u2212 P V E V0 [j]', '3.3 WaveNet Synthesis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/dynet\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit (Cohn et al., 2016), and using the dynet deep learning library [URL_CITE] (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 5 https://github.com/clab/dynet\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano/\nCitation Context: We implement our models and DeClarE with Theano [URL_CITE] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.\nFootnote or Reference text (if exists): 3 http://deeplearning.net/software/theano/\nSection Titles (if exists):  ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Network Training We implement our proposed method based on huggingface Transformers [URL_CITE] . Fol-lowing Wolf et al.\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We used the machine learning package MALLET (McCallum, 2002) [URL_CITE] to conduct the CRF model training and labeling.\nFootnote or Reference text (if exists): McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/language/tree/master/language/realm\nCitation Context: All our models are developed and trained us-ing TensorFlow 1.15 (Abadi et al., 2016) and Tensor2Tensor (Vaswani et al., 2018). Our imple-mentations are based on the open-source codebases of REALM [URL_CITE] and the Routing Transformer. Similar to the REALM implementation, we use separate processes to run the retriever and generate training data (using a MIPS search).\nFootnote or Reference text (if exists): 19 https://github.com/google-research/language/tree/master/language/realm\nSection Titles (if exists):  ['7:453\u2013466.', 'A.1 Training & Model Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/flax\nCitation Context: We have used Flax, an open-source library [URL_CITE] to train neural networks, as the code base for the model training. Our H-Transformer-1D model uses the standard Transformer decoder implemen-tation in Flax as the backbone.\nFootnote or Reference text (if exists): 8 https://github.com/google/flax\nSection Titles (if exists):  ['8 Experiments And Results', '8.2 Language Models Trained on One-Billion Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com\nCitation Context: F p denotes 2 phrase-level features, h PP and h PT . For h PP , bilingual data is used to extrac-t a phrase-based translation table (Koehn et al., 2003), from which paraphrases are extracted (Sec-tion 4.2.1). For h PT , GIZA++ trains word align-ments on 4M \u2018question-answer\u2019 pairs crawled from Yahoo Answers [URL_CITE] , and then a phrase ta-ble is extracted from word alignments using the intersect-diag-grow refinement.\nFootnote or Reference text (if exists): 10 https://answers.yahoo.com\nSection Titles (if exists):  ['7 Experiments', '7.1 Evaluation on QA (English)', '7.1.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tapas\nCitation Context: For T A B ERT we trained the model for WikiSQL using the lookup subset, and for WikiTableQuestions we used the full train-ing set and applied to the lookup subset. For T A P AS we used the trained BASE (reset) models [URL_CITE] for Wik-iSQL and applied to the lookup subsets of the dev and test sets.\nFootnote or Reference text (if exists): 4 https://github.com/google-research/tapas\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/giza/GIZA++.html\nCitation Context: As seen from Table 2, the translation based methods outperforms the baseline signifi-cantly. The models are trained using GIZA++ [URL_CITE] tool with the question and best answer pair as the parallel corpus. For the topic based Q-A topic model and Q-A topic model(s), we implemented the models QATM -PR (Question-Answer Topic Model) Ji et al.\nFootnote or Reference text (if exists): 12 http://www.statmt.org/moses/giza/GIZA++.html\nSection Titles (if exists):  ['8 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/challenge\nCitation Context: \u2022 Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [URL_CITE] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it).\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/challenge\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rivercold/BERT-unsupervised-OOD\nCitation Context: Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Following ZR18 we also consider a more chal-lenging setup where the English source domain consists of user airline (A) reviews (Nguyen, 2015) [URL_CITE] . We use the dataset of ZR18, consisting of 1000 positive and 1000 negative reviews in the la-beled set, and 39396 reviews as the unlabeled set.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/MDfromHTML\nCitation Context: This process is repeated for each URL domain in both datasets. The tools for data prepro-cessing are available here: [URL_CITE] https://github.com/IBM/MDfromHTML.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['URL documents']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dianping.com\nCitation Context: Besides, experts\u2019 prior knowledge or de-velopers\u2019 ingenuity is not always reliable with the variations of domains and languages. For exam-ple, based on the datasets from Dianping site [URL_CITE] , Li et al. (2015) find that the real users tend to review the restaurants nearby, but the spammers are not re-stricted to the geographical location, they may come from anywhere.\nFootnote or Reference text (if exists): 4 http://www.dianping.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html\nCitation Context: We use the publicly available implementation of MSTParser [URL_CITE] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).\nFootnote or Reference text (if exists): 7 http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html\nSection Titles (if exists):  ['6 Experiment', '6.2 Baseline Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.cs.bgu.ac.il/~yoavg/software/projectivize\nCitation Context: Projectivization is based on Yoav Goldberg\u2019s code. [URL_CITE] For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).\nFootnote or Reference text (if exists): 4 https://www.cs.bgu.ac.il/\u02dcyoavg/software/projectivize\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://perso.limsi.fr/aufrant\nCitation Context: For pseudo-projectivization, the M ALT P ARSER 1.9 implementation is used, with the head encoding scheme. For parsing, we use P AN P ARSER (Aufrant and Wisniewski, 2016), our own open source [URL_CITE] implementation of a greedy A RC E AGER parser (using an averaged perceptron and a dynamic oracle).\nFootnote or Reference text (if exists): 5 https://perso.limsi.fr/aufrant\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clearnlp.com,v2.0.2\nCitation Context: The syntactic and semantic features were extracted using the ClearNLP parser. [URL_CITE] We used the default models and options for the parser. We treat this model as a strong baseline to which we will add reference-based features.\nFootnote or Reference text (if exists): 4 http://www.clearnlp.com,v2.0.2\nSection Titles (if exists):  ['3 Models for Short Answer Scoring', '3.1 Response-based']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mstparser\nCitation Context: The token-level features used in the system are the same as those used in MSTParser version 0.4.2 [URL_CITE] . The features include lexical forms and (coarse and fine) POS tags of parent tokens, child tokens, their surrounding tokens, and tokens between the child and the parent.\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/mstparser\nSection Titles (if exists):  ['2 Unlabeled Dependency Parsing using Global Features', '2.3 Local Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mstparser.sourceforge.net\nCitation Context: The parser is basically based on the MSTParser [URL_CITE] using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to im-prove the parser.\nFootnote or Reference text (if exists): 8 It\u2019s freely available at http://mstparser.sourceforge.net.\nSection Titles (if exists):  ['5 Evaluation Results', '5.1 Syntactic Dependency Parsers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jiaweiw/Extract-Edit-Unsupervised-NMT\nCitation Context: Empirical results on popular benchmarks show that exact-edit consistently outperforms the state-of-the-art unsupervised NMT system (Lample et al., 2018b) with back-translation across four dif-ferent languages pairs. In summary, our main con-tributions are three-fold [URL_CITE] :\nFootnote or Reference text (if exists): 1 The source code can be found in this repos-itory: https://github.com/jiaweiw/ Extract-Edit-Unsupervised-NMT\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/multimodal-task.html\nCitation Context: To investigate the effectiveness of informa-tion obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community [URL_CITE] . The best results of NMT model were those of Huang et al.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/multimodal-task.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hiaoxui/D2T-Grounding\nCitation Context: Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zdou0830/DAFE\nCitation Context: In addition, we show that combining our method with back translation can further improve the performance of the model. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at: https://github.com/zdou0830/DAFE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuzhaopeng/NMT-Coverage\nCitation Context: Experiments show that the proposed approach significantly im-proves both translation quality and align-ment quality over standard attention-based NMT. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is publicly available at https://github.com/tuzhaopeng/NMT-Coverage.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: In prior work (Huenerfauth and Kacorri, 2015b), we investigated key methodological con-siderations in conducting a study to evaluate sign language animations with deaf users, including the use of appropriate baselines for comparison, appropriate presentation of questions and instruc-tions, demographic and technology experience factors influencing acceptance of signing avatars, and other factors that we have considered in the design of this current study. Our recent work (Kacorri et al., 2015) [URL_CITE] has established a set of de-mographic and technology experience questions which can be used to screen for the most critical participants in a user study of ASL signers to eval-uate animation. Specifically, we screened for par-ticipants that identified themselves as \u201cdeaf/Deaf\u201d or \u201chard-of-hearing,\u201d who had grown up using ASL at home or had attended an ASL-based school as a young child, such as a residential or daytime school.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\nCitation Context: Our method can be regarded as a general ver-sion of distant supervision since we assume that a reply includes the label information of the cor-responding tweet. There have been many studies about distant supervision for sentiment analysis (Read, 2005; Go et al., 2009 [URL_CITE] ; Davidov et al., 2010; Purver and Battersby, 2012; Mohammad et al., 2013; Tang et al., 2014; dos Santos and Gatti, 2014; Severyn and Moschitti, 2015; Deriu et al., 2016; Mu\u0308ller et al., 2017), but they basically fo-cused on how to use emoticons and hashtags to leverage performance. One exception is the study by (Pool and Nissim, 2016), in which Facebook reactions were used for distant supervision.\nFootnote or Reference text (if exists): Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Dis-tant Supervision. Technical report, Stan-ford Digital Library Technologies Project. https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: There were 8 men and 9 women of ages 19-29 (average age 22.8). In prior work, we (Kacorri et al., 2015) [URL_CITE] have advocated that participants in studies eval-uating sign language animation complete a two standardized surveys about their technology ex-perience (MediaSharing and AnimationAttitude) and that researchers report these values for partici-pants, to enable comparison across studies. In our study, participant scores for MediaSharing varied between 3 and 6, with a mean score of 4.3, and scores for AnimationAttitude varied from 2 to 6, with a mean score of 3.8.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['3 Evaluation', '3.2 User Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/MiuLab/CLUSE\nCitation Context: The proposed approach shows the su-perior quality of sense embeddings evaluated in both monolingual and bilingual spaces. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and dataset are available at http://github.com/MiuLab/CLUSE.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://academicworks.cuny.edu/gc\nCitation Context: A survey of recent work of several researchers on producing animations of sign language with fa-cial expressions appears in (Kacorri, 2015) [URL_CITE] . There is recent interest in data-driven approaches using facial motion-capture of human performances to generate sign language animations: For example, (Schmidt et al., 2013) used clustering techniques to select facial expressions that co-occur with indi-vidual lexical items, and (Gibet et al., 2011) stud-ied how to map facial motion-capture data to ani-mation controls.\nFootnote or Reference text (if exists): Hernisa Kacorri. 2015. Tr-2015001: A sur-vey and critique of facial expression syn-thesis in sign language animation. Tech-nical report, The Graduate Center, CUNY. http://academicworks.cuny.edu/gc cs tr/403.\nSection Titles (if exists):  ['1 Introduction and Motivation', '1.2 Prior Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: Topics from 20 newsgroup We choose the five top-level classes in the 20 newsgroup dataset [URL_CITE] as topics; viz. : Computer, Recreation, Science, Pol-itics, Religion.\nFootnote or Reference text (if exists): 6 http://qwone.com/\u02dcjason/20Newsgroups/\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: Topic categorization. From the 20 Newsgroups [URL_CITE] dataset, we examine four binary classification tasks. We end up with binary classification problems, where we classify a document according to two related categories: comp.sys: ibm.pc.hardware vs. mac.hardware; rec.sport: baseball vs. hockey; sci: med vs. space and alt.atheism vs. soc.religion.christian.\nFootnote or Reference text (if exists): 2 http://qwone.com/~jason/20Newsgroups/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iitp.ac.in/~ai-nlp-ml/resources.html\nCitation Context: For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [URL_CITE] The user-level tweets consist of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 4 All the developed resources are available at http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html\nSection Titles (if exists):  ['4 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doug919/entity_based_narrative_graph\nCitation Context: The evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters\u2019 psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly model-ing the relational structure of entities and events is beneficial. Our code and trained models are pub-licly available [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/doug919/entity_ based_narrative_graph\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.org/Data/QA/QC/\nCitation Context: \u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [URL_CITE] .\nFootnote or Reference text (if exists): 5 http://cogcomp.org/Data/QA/QC/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nCitation Context: : Computer, Recreation, Science, Pol-itics, Religion. The corresponding five down-stream tasks are text classification over the 3\u2013 [URL_CITE] fine-grained classes under each top-level class. Train, test, dev splits were 50%, 40%, 10%.\nFootnote or Reference text (if exists): 5 https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-about.htm\nSection Titles (if exists):  ['Topics and tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [URL_CITE] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: Two standard datasets are tested for NMT tasks: a small-scale English-Vietnamese corpus from the IWSLT 2015 Evaluation Campaign (Cettolo et al., 2015) and a large-scale English-German corpus from the WMT16 Evaluate Campaign [URL_CITE] . Further details of the datasets and the experimental setup are shown in Appendix A.2.\nFootnote or Reference text (if exists): 2 http://statmt.org/wmt16\nSection Titles (if exists):  ['4 Experiments', '4.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt17/\nCitation Context: Following recent unconditional long text genera-tion work (Caccia et al., 2018), we perform exper-iments on EMNLP2017 WMT News dataset [URL_CITE] . All sentences are longer than 20, making the dataset appropriate for testing the exposure bias problem.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt17/\nSection Titles (if exists):  ['4 Experiments', '4.3 Neural Language Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://search.cpan.org/~snowhare/Lingua-Stem-0.83\nCitation Context: \u2022 All above features and their stems. [URL_CITE] (All-Words-Stemmed)\nFootnote or Reference text (if exists): 7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83.\nSection Titles (if exists):  ['6 Local Lexical Context']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\nCitation Context: \u2022 SUBJ: A dataset for classifying a sentence as objective or subjective. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.casmacat.eu/corpus/news-commentary.html\nCitation Context: \u2022 News, which is from News Commentary v11 corpus. [URL_CITE] We use news-test2015 and news-\nFootnote or Reference text (if exists): 9 http://www.casmacat.eu/corpus/news-commentary.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org/wiki/English_grammar#Semantic_gradability\nCitation Context: \u2022 Notgood,quantifiablebetter, best etcgradable. usually [URL_CITE] wordsmake sucha claimas bald, as there is no qualified definition of being good or better.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org/wiki/English_ grammar#Semantic_gradability\nSection Titles (if exists):  ['3 Annotation Scheme']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rowanzellers.com/hellaswag\nCitation Context: We study this question by introducing Hella-Swag, [URL_CITE] a new benchmark for commonsense NLI. We use Adversarial Filtering (AF), a data-collection paradigm in which a series of discrim-inators is used to select a challenging set of gen-erated wrong answers.\nFootnote or Reference text (if exists): 2 Short for Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations. Dataset and code at https://rowanzellers.com/hellaswag.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cl-tohoku/bert-japanese\nCitation Context: We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [URL_CITE] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set.\nFootnote or Reference text (if exists): 3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.\nSection Titles (if exists):  ['5 Experiments', '5.2 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\nCitation Context: Adversarial Networks The idea of adversar-ial networks was proposed by Goodfellow et al. (2014) [URL_CITE] for image generation, and has been applied to various NLP tasks as well (Chen et al., 2016; Yu et al., 2017). Ganin et al.\nFootnote or Reference text (if exists): Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2672\u20132680. http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nSection Titles (if exists):  ['6 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/YerevaNN/WARP\nCitation Context: In this paper, we introduce a novel technique to find optimal prompts. We call our method WARP: Word-level Adversarial RePrograming [URL_CITE] . The method is inspired by adversarial reprogramming (Elsayed et al., 2019) \u2014 a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to perform classification on a task other than the one it was originally trained for.\nFootnote or Reference text (if exists): 1 Our implementation is publicly available at: https: //github.com/YerevaNN/WARP\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jayelm/lsl\nCitation Context: Code, data, and experiments are available at [URL_CITE] https: //github.com/jayelm/lsl and on CodaLab at https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kelp-ml.org\nCitation Context: Learning and classification allow to apply a variety of learning algorithms on vectorial or structured data. Currently KeLP (Filice et al., 2018) [URL_CITE] is integrated in the pipeline. KeLP allows to apply a growing number of kernel-based algo-rithms and kernel functions to perform unsuper-vised, online and batch supervised kernel meth-ods.\nFootnote or Reference text (if exists): 2 http://www.kelp-ml.org\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/valentinhofmann/dga\nCitation Context: The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make all our code and data publicly available at https://github.com/valentinhofmann/dga.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/lsl_acl20\nCitation Context: Code, data, and experiments are available at https: //github.com/jayelm/lsl and on CodaLab at [URL_CITE] https://bit.ly/lsl_acl20. A Model and training details A.1 ShapeWorld f \u03b8 .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/challenge\nCitation Context: Besides the above datasets, we also use a Yelp dataset [URL_CITE] and an Amazon Electronics dataset (He and McAuley, 2016) as auxiliary data to be anno-tated with the mined rules. They are also used to train word embeddings.\nFootnote or Reference text (if exists): 1 https://www.yelp.com/dataset/challenge\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/swift-coreml-transformers\nCitation Context: Models can use adapters to convert models to CoreML weights that are suit-able to be embedded inside a iOS application, to enable on-the-edge machine learning. Code is also made available [URL_CITE] . Similar methods can be used for Android devices.\nFootnote or Reference text (if exists): 3 https://github.com/huggingface/ swift-coreml-transformers\nSection Titles (if exists):  ['5 Deployment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.quest.dcs.shef.ac.uk/\nCitation Context: While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA and Q U E ST [URL_CITE] (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).\nFootnote or Reference text (if exists): 2 http://www.quest.dcs.shef.ac.uk/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/\nCitation Context: Each processing level is independent and can deal with the peculiarities of its type of feature. Machine learning Q U E ST ++ provides scripts to interface the Python toolkit scikit-learn [URL_CITE] (Pedregosa et al., ). This module is indepen-dent from the feature extraction code and uses the extracted feature sets to build and test QE models.\nFootnote or Reference text (if exists): 3 http://scikit-learn.org/\nSection Titles (if exists):  ['2 Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: Metrics from A SIYA . We used the freely avail-able version of the A SIYA toolkit [URL_CITE] in order to ex-tend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. A SIYA (Gime\u0301nez and Ma\u0300rquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic infor-mation.\nFootnote or Reference text (if exists): 4 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 MT Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thu-coai/OpenMEVA\nCitation Context: We also provide an open-source toolkit which implements various metrics, and therefore supports the comparison and analysis of metrics. In addition, the toolkit provides data perturbation techniques for generating customized test cases beyond AU - TOS , which can facilitate fast development of new automatic metrics [URL_CITE] .\nFootnote or Reference text (if exists): 1 All the tools, data, and evaluation scripts are available at https://github.com/thu-coai/OpenMEVA\nSection Titles (if exists):  ['\u2217 Corresponding author']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.lsi.upc.edu/asiya/\nCitation Context: While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: A SIYA [URL_CITE] and Q U E ST (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of partic-ipants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).\nFootnote or Reference text (if exists): 1 http://nlp.lsi.upc.edu/asiya/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nCitation Context: First we implemented the standard linear SVM [URL_CITE] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.\nFootnote or Reference text (if exists): 6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nSection Titles (if exists):  ['3 Experiments and results', '3.1 Basic SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://minorthird.sourceforge.net/\nCitation Context: For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004) [URL_CITE] with the de-fault linear kernel. We report average classification accuracy and average Cohen\u2019s Kappa using 10-fold cross-validation.\nFootnote or Reference text (if exists): William Cohen. 2004. Minorthird: Methods for Iden-tifying Names and Ontological Relations in Text us-ing Heuristics for Inducing Regularities from Data. http://minorthird.sourceforge.net/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: An input pair (A i ,A s ) is then associated with a grade g(A i , A s ) = u T \u03c8(A i , A s ) computed as a lin-ear combination of features. The weight vector u is trained to optimize performance in two scenarios: Regression: An SVM model for regression (SVR) is trained using as target function the grades as-signed by the instructors. We use the libSVM [URL_CITE] im-plementation of SVR, with tuned parameters.\nFootnote or Reference text (if exists): 7 http://www.csie.ntu.edu.tw/\u02dccjlin/libsvm/\nSection Titles (if exists):  ['6 http://Infomap-nlp.sourceforge.net/']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EducationalTestingService/skll\nCitation Context: Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [URL_CITE]\nFootnote or Reference text (if exists): 2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.\nSection Titles (if exists):  ['3 Models for Short Answer Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.waikato.ac.nz/ml/weka/\nCitation Context: Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression [URL_CITE] (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system. We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown cor-pus (Section 5).\nFootnote or Reference text (if exists): 2 Weka software (http://www.cs.waikato.ac.nz/ml/weka/)\nSection Titles (if exists):  ['2 Related Work and Experimental Framework']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/miso-belica/jusText\nCitation Context: Text Extraction To ensure English content, we exclude websites with country-code top-level do-mains; e.g., .fr, .de. We use the jusText [URL_CITE] tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English and incom-plete sentences.\nFootnote or Reference text (if exists): 5 https://github.com/miso-belica/ jusText\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/fast_align\nCitation Context: Specifically, we first leverage an exist-ing English POS tagger in the spaCy library to annotate the English source sentences. We then use the fast_align [URL_CITE] tool (Dyer et al., 2013) to train a word alignment model on the training data for the En-XX language pair, and use the alignment model to obtain the word-level alignment for the test data. We assign the POS tags of the source words in English to their aligned target words in the African language.\nFootnote or Reference text (if exists): 15 https://github.com/clab/fast_align\nSection Titles (if exists):  ['5 Results and Discussion', '5.3 Fine-grained Language Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/fnlp\nCitation Context: We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [URL_CITE] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times.\nFootnote or Reference text (if exists): 9 https://github.com/FudanNLP/fnlp\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/saffsd/langid.py\nCitation Context: We extract texts written by learners who learn only English from Lang-8. A language identification tool langid.py [URL_CITE] (Lui and Baldwin, 2011) is then used to obtain purely English sentences. In addition, we remove noisy source-target sentence pairs in Lang8 where the ratio of the lengths of the source and target sentences is outside [0.5, 2.0], or their word overlap ratio is less than 0.2.\nFootnote or Reference text (if exists): 1 https://github.com/saffsd/langid.py\nSection Titles (if exists):  ['6 Data and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/go-through#go-through\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yanaiela/demog-text-removal\nCitation Context: We explore means for im-proving the effectiveness of the adversarial train-ing procedure (section 5.2). [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://catalog.ldc.upenn.edu/LDC2006T13\nCitation Context: The negative ex-amples are substitute candidates that either are incor-rect for the target word, do not fit into the context or both. We will refer to these substitutes as false substi-tute candidates s f [URL_CITE] . .\nFootnote or Reference text (if exists): 1 http://catalog.ldc.upenn.edu/ LDC2006T13\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Franck-Dernoncourt/pubmed-rct\nCitation Context: PubMed: Medical abstracts of randomized con-trolled trials. Source: [URL_CITE] https://github.com/Franck-Dernoncourt/pubmed-rct.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Data Description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.macmillandictionary.com/dictionary/american/\nCitation Context: In con-trast, the meaning to experience something bad of to go through [a situation] (ibid., (7)), or the sense to do/conduct of to get [something] done (ibid., (5)), are strongly conventionalized, as indicated by their inclusion in dictionaries. [URL_CITE]\nFootnote or Reference text (if exists): 4 e.g., https://www.macmillandictionary.com/dictionary/american/go-through#go-through 7, https://www.macmillandictionary.com/dictionary/american/ get#get 60\nSection Titles (if exists):  ['3 Corpus', '3.3 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [URL_CITE] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/nmt/\nCitation Context: \u2022 Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [URL_CITE] . We again augment one sam-ple from each original training sample.\nFootnote or Reference text (if exists): 2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense\nCitation Context: \u2022 Back-translation (BT): For neural machine trans-lation, we employ a fixed target-to-source transla-tion model trained on the original dataset. For con-versational response generation, we perturb both the input and output text of the original sample pair using two pretrained translation model: an English-to-German model and its backward counterpart, which are obtained using the WMT14 corpus with 4.5M sentence pairs [URL_CITE] . We again augment one sam-ple from each original training sample.\nFootnote or Reference text (if exists): 2 Datasets used in this work can be found at https://nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt15/\nCitation Context: (2016) demonstrated that bpe boosts NMT, so sim-ilar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora [URL_CITE] to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English\u2013Turkish (En\u2013Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt15/\nSection Titles (if exists):  ['4 Experimental Study', '4.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nCitation Context: BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45). [URL_CITE] The basic overview of the VerbNet structure already suggests that measur-ing verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.\nFootnote or Reference text (if exists): 6 https://verbs.colorado.edu/verb-index/vn/break-45.1.php\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/\nCitation Context: To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set (Nelson et al., 2004), and (2) the VerbNet verb lexicon [URL_CITE] (Kipper et al., 2004; Kipper et al., 2008).\nFootnote or Reference text (if exists): 4 http://verbs.colorado.edu/verb-index/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://w3.usf.edu/FreeAssociation/\nCitation Context: To ensure a wide coverage of a variety of syntactico-semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set [URL_CITE] (Nelson et al., 2004), and (2) the VerbNet verb lexicon (Kipper et al., 2004; Kipper et al., 2008).\nFootnote or Reference text (if exists): 3 http://w3.usf.edu/FreeAssociation/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1002/9781444395037.ch5\nCitation Context: Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambda- DCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and Baldridge, 2011) [URL_CITE] (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language se-mantics is described in Abend and Rappoport.\nFootnote or Reference text (if exists): Mark Steedman and Jason Baldridge. 2011. Combina-tory Categorial Grammar, Wiley-Blackwell, pages 181\u2013224. https://doi.org/10.1002/9781444395037.ch5.\nSection Titles (if exists):  ['5 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nCitation Context: VN is or-ganised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class mem-bers. According to the official VerbNet guidelines, [URL_CITE] \u201cVerb Classes are numbered according to shared se-mantics and syntax, and classes which share a top-level number (9-109) have corresponding semantic relationships.\u201d For instance, all verbs from the top-level Class 9 are labelled \u201cVerbs of Putting\u201d, all verbs from Class 30 are labelled \u201cVerbs of Perception\u201d, while Class 39 contains \u201cVerbs of Ingesting\u201d.\nFootnote or Reference text (if exists): 5 http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://verbs.colorado.edu/semlink/\nCitation Context: The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenom-ena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007). [URL_CITE]\nFootnote or Reference text (if exists): 7 https://verbs.colorado.edu/semlink/\nSection Titles (if exists):  ['3 The SimVerb-3500 Data Set', '3.2 Choice of Verb Pairs and Coverage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017) [URL_CITE] . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test.\nFootnote or Reference text (if exists): 3 Scripts publicly available at https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: Training Details Word embeddings were pre-trained with the Gensim [URL_CITE] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact). The di-mensionality of the word embeddings was set to 50.\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [URL_CITE] and fairseq .\nFootnote or Reference text (if exists): 1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2012T21\nCitation Context: To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which in-cludes 6,339,616 documents sampled from the En-glish Gigaword [URL_CITE] dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well.\nFootnote or Reference text (if exists): 4 https://catalog.ldc.upenn.edu/LDC2012T21\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dumps.wikimedia.org/mirrors.html\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/attardi/wikiextractor\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/soskek/bookcorpus\nCitation Context: The original BookCorpus (Zhu et al., 2015) is no longer avail-able, so we collect similar data from Smashwords, the original source of BookCorpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 We collect our data using the Wikipedia XML dump https://dumps.wikimedia.org/mirrors.html and data-processing code https://github.com/attardi/wikiextractor, and a Smashwords crawler https://github.com/soskek/bookcorpus.\nSection Titles (if exists):  ['4 Models, Pretraining, & Fine-Tuning', '4.1 Pretraining']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politicalforum.com\nCitation Context: We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [URL_CITE] . All the posts are in English.\nFootnote or Reference text (if exists): 2 http://www.politicalforum.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt17/translation-task/preprocessed/\nCitation Context: All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://data.statmt.org/wmt17/translation-task/preprocessed/\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://casmacat.eu/corpus/\nCitation Context: Our data comes primarily from WikiMatrix (Schwenk et al., 2019), Global Voices, [URL_CITE] EuroParl\nFootnote or Reference text (if exists): 5 http://casmacat.eu/corpus/ Bootstrap resampling (Koehn, 2004; Graham et al., global-voices.html 2014) is used to estimate confidence intervals for\nSection Titles (if exists):  ['1 Introduction', '4.2 Model Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cordova.apache.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova [URL_CITE] \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 3 https://cordova.apache.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html\nCitation Context: We use the publicly available implementation of MSTParser [URL_CITE] (with modifications to the feature com-putation) and its default settings, so the feature weights of the projective and non-projective parsers are trained by the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2006).\nFootnote or Reference text (if exists): 7 http://www.seas.upenn.edu/\u02dcstrctlrn/MSTParser/MSTParser.html\nSection Titles (if exists):  ['6 Experiment', '6.2 Baseline Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.idpf.org/epub\nCitation Context: We use the EPUB format [URL_CITE] to create what we call a \u201cRead Aloud eBook\" used by MTTR. To link the text in the book to the synchronized audio, we use SMIL (Synchronized Multimedia Integration Lan-guage), as defined in the EPUB Media Overlays specification.\nFootnote or Reference text (if exists): 5 http://www.idpf.org/epub\nSection Titles (if exists): [\\'3 My Turn To Read App\\', \\'3.1 Read Aloud eBooks\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: The whole framework is built on Py-Torch (Paszke et al., 2019). The implementations of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are borrowed from PyTorch Trans-formers (Wolf et al., 2019) [URL_CITE] . All evaluation code is from the PyTorch Transformers as well.\nFootnote or Reference text (if exists): 11 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/forkarinda/MFN\nCitation Context: The extracted ASR-output transcript data is avail-able on [URL_CITE] https://github.com/forkarinda/MFN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['A Appendices', 'A.3 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://readium.org\nCitation Context: Mobile versions of MTTR are built using Apache Cordova \u2013 a cross-platform toolkit \u2013 with platform-specific modifications where nec-essary. The reading and listening components in all versions are built on top of Readium [URL_CITE] , a robust, standards-compliant, and open-source e-reader. Figure 1 shows a screenshot of the iOS version of MTTR.\nFootnote or Reference text (if exists): 4 https://readium.org\nSection Titles (if exists):  ['3 My Turn To Read App']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sodawater/SentenceOrdering\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset [URL_CITE] (Wang and Wan, 2019; Mostafazadeh et al., 2016). It is randomly split by 8:1:1 for the training/validation/test sets.\nFootnote or Reference text (if exists): 4 https://github.com/sodawater/SentenceOrdering\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yuhaozhang/tacred-relation\nCitation Context: We construct the second part of experiments (Sec.4.4) to better understand SENT\u2019s behaviors. Since no labeled training data are available in the distant supervision setting, we construct a noisy dataset with 30% noise from a labeled dataset, TACRED (Zhang et al., 2017) [URL_CITE] . We regard this constructed dataset as noisy-TACRED.\nFootnote or Reference text (if exists): 5 https://github.com/yuhaozhang/tacred-relation\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tingyaohsu/VIST-Edit\nCitation Context: We introduce the first dataset for human edits of machine-generated visual stories and ex-plore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit [URL_CITE] , includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.\nFootnote or Reference text (if exists): 1 VIST-Edit: https://github.com/tingyaohsu/VIST-Edit using human-written stories as references. All the au-tomatic evaluation metrics generate lower scores even when the editing was done by human.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pilehvar.github.io/wic/\nCitation Context: To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on anno-tations curated by experts, for generic evalua-tion of context-sensitive representations. WiC is released in [URL_CITE] https://pilehvar.github.io/wic/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://visionandlanguage.net/VIST/dataset.html\nCitation Context: SIND, ROCStory: SIND is a visual storytelling dataset [URL_CITE] (Huang et al., 2016), which is re-leased as training/validation/testing following the 8:1:1 split. ROCStory is a commonsense story dataset (Wang and Wan, 2019; Mostafazadeh et al., 2016).\nFootnote or Reference text (if exists): 3 http://visionandlanguage.net/VIST/dataset.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N/N09/N09-2047\nCitation Context: This near uniqueness of a parse given a gold sequence of supertags has been con-firmed empirically (TAG: Bangalore et al. (2009) [URL_CITE] ; Chung et al. (2016); Kasai et al.\nFootnote or Reference text (if exists): Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Beno\u0131\u0302t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185\u2013188. http://www.aclweb.org/anthology/N/N09/N09-2047.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: More recent work, however, has shown that data-driven transition-based parsing systems out-perform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017 [URL_CITE] ). Kasai et al.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Indeed, Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.donga.com\nCitation Context: For latent morpheme representations, we used the Donga news article corpus. [URL_CITE] The Donga cor-pus contains 366,636 sentences with 25.09 words on average. The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others.\nFootnote or Reference text (if exists): 3 http://www.donga.com\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homes.cs.washington.edu/~eunsol/project_page/acl16\nCitation Context: We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [URL_CITE] This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demon-strate that the global model significantly improves performance over a pairwise classifier and other strong baselines.\nFootnote or Reference text (if exists): 1 All data will be made publicly available. You can browse it at http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16, and download it from the author\u2019s webpage.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.casmacat.eu/corpus/news-commentary.html\nCitation Context: \u2022 News, which is from News Commentary v11 corpus. [URL_CITE] We use news-test2015 and news-\nFootnote or Reference text (if exists): 9 http://www.casmacat.eu/corpus/news-commentary.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/truprecht/rustomata-eval\nCitation Context: We used the NeGra corpus (German newspaper articles, 20,602 sentences, 355,096 tokens; Skut et al., 1998) to compare our parser to Grammat-ical Framework (Angelov and Ljunglo\u0308f, 2014), rparse (Kallmeyer and Maier, 2013), and disco-dop (van Cranenburgh et al., 2016) with respect to parse time and accuracy. [URL_CITE] Our experiments were conducted on defoliated trees, i.e. we removed the leaves from each tree in the corpus.\nFootnote or Reference text (if exists): 6 The evaluation scripts are available on https://github.com/truprecht/rustomata-eval.\nSection Titles (if exists):  ['5 Evaluation and Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mlg.ucd.ie/datasets/bbc.html\nCitation Context: We use the BBC corpus prepared by Greene and Cunningham (2006). [URL_CITE] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages.\nFootnote or Reference text (if exists): 2 http://mlg.ucd.ie/datasets/bbc.html\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kkma.snu.ac.kr/\nCitation Context: The Domain of this corpus cov-ers typical news articles such as health, entertain-ment, technology, politics, world and others. We ran Kokoma Korean morpheme analyzer [URL_CITE] on each sentence of the Donga corpus to divide words into morphemes to build latent morpheme representa-tions.\nFootnote or Reference text (if exists): 4 http://kkma.snu.ac.kr/\nSection Titles (if exists):  ['6 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com/\nCitation Context: We conducted user study via crowdsourcing using the FigureEight [URL_CITE] platform. To ensure the quality of annotators, we used a dedicated expert-worker pool provided by FigureEight. We present the detailed setup of our user studies as follows.\nFootnote or Reference text (if exists): 6 https://www.figure-eight.com/\nSection Titles (if exists):  ['B Human Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://prolific.ac/\nCitation Context: We employ the Prolific Academic (PA) crowdsourc-ing platform, [URL_CITE] an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.\nFootnote or Reference text (if exists): 11 https://prolific.ac/ (We chose PA for logistic reasons.)\nSection Titles (if exists):  ['4 Word Pair Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mindspore.cn/\nCitation Context: We thank Mindspore [URL_CITE] which is a new deep learning computing framework for the partial support of this work.\nFootnote or Reference text (if exists): 3 https://www.mindspore.cn/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gunviolencearchive.org\nCitation Context: Several projects collect data about gun violence via newspaper teams (Boyle, 2013; Swaine et al., 2015) or volunteer crowds (Burghart, 2014; Wagner, 2014; Kirk and Kois, 2013). Perhaps the largest such ef-fort is the Gun Violence Archive [URL_CITE] . However, none are aimed at the eventual automation of the process.\nFootnote or Reference text (if exists): 5 http://www.gunviolencearchive.org\nSection Titles (if exists):  ['4 Related Efforts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lancers.jp/\nCitation Context: We use a crowdsourcing application, Lancers, [URL_CITE] to perform substitute extraction, substitute evalua-tion, and substitute ranking. In each task, we re-quested the annotators to complete at least 95% of their previous assignments correctly. They were native Japanese speakers.\nFootnote or Reference text (if exists): 3 http://www.lancers.jp/\nSection Titles (if exists):  ['4 Balanced dataset for evaluation of Japanese lexical simplification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gun-violence.org/\nCitation Context: We also thank the students of the University of Pennsylvania\u2019s crowdsourcing class (NETS 213) for their involvement in building and testing a useful crowdsourcing pipeline for in-formation extraction. Finally, we thank the devel-opers at 10clouds for the excellent engineering and design of [URL_CITE] http://gun-violence.org/. Amit Singhal.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 400 units each.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: We implemented our model in Tensorflow [URL_CITE] . We tuned the hyper-parameters via the development set.\nFootnote or Reference text (if exists): 3 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiment', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nCitation Context: Figure 1: RNN (a) and LSTM (b) [URL_CITE]\nFootnote or Reference text (if exists): 1 This figure referred to http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/diegma/neural-dep-srl\nCitation Context: When we stack GCNs on top of LSTM layers, we obtain a substantial improve-ment over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for En-glish and Chinese. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/diegma/neural-dep-srl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andy194673/nlg-sclstm-multiwoz\nCitation Context: The best model for both datasets was found through a grid search over a set of hyper-parameters such as the size of embeddings, learn-ing rate, and number of LSTM layers. [URL_CITE]\nFootnote or Reference text (if exists): 6 The model is publicly available at https://github.com/andy194673/ nlg-sclstm-multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.3 Dialogue-Act-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: This approach has successfully been used in previous work for devising web-based corpora (Scha\u0308fer, 2015; Bie-mann et al., 2007). The data is available at [URL_CITE] https: //github.com/SimonHFL/CWEB . covering a range of topics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sighan.org/bakeoff2005/\nCitation Context: We use the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff [URL_CITE] as the labeled data. We choose the PKU data in our experiment because our baseline meth-ods use the same segmentation standard.\nFootnote or Reference text (if exists): 2 http://www.sighan.org/bakeoff2005/\nSection Titles (if exists):  ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lemurproject.org/clueweb12\nCitation Context: WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [URL_CITE] Similarly, we randomly split into dev and test.\nFootnote or Reference text (if exists): 7 http://lemurproject.org/clueweb12\nSection Titles (if exists):  ['3 Tasks', '3.2 Entity Linking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://banner.sourceforge.net\nCitation Context: We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [URL_CITE] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }.\nFootnote or Reference text (if exists): 3 http://banner.sourceforge.net\nSection Titles (if exists):  ['3 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.psychpark.org\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark ( [URL_CITE] http://www.psychpark.org) (Bai, 2001) and John Tung Foundation (http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/bart\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset and fairseq [URL_CITE] .\nFootnote or Reference text (if exists): 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/abisee/cnn-dailymail\nCitation Context: We used the CNN/Daily Mail dataset 1 (Her-mann et al., 2015), a summary corpus of En-glish news articles, consisting of 287,226 train-ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum-mary sentences have 781 and 56 tokens, respec-tively. For data preprocessing, we followed the in-struction provided in the CNN/Daily Mail dataset [URL_CITE] and fairseq .\nFootnote or Reference text (if exists): 1 CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com/talks\nCitation Context: The dataset consists of transcripts directly col-lected from [URL_CITE] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don\u2019t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nikkei.co.jp\nCitation Context: We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics. [URL_CITE] Each bulletin consists of six to seven news briefs, each with a few sentences. Since a news brief contains nothing to indicate what its longer version might look like, we manually searched the news site for a full-length article that might reasonably be con-sidered a long version of that brief.\nFootnote or Reference text (if exists): 9 http://www.nikkei.co.jp\nSection Titles (if exists):  ['5 Evaluation Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sogou.com/labs/dl/c.html\nCitation Context: Data preparation The data is composed of two parts: source documents and background knowl-edge. For source documents, we use a publicly available Chinese corpus which consists of 17,199 documents and 13,719,428 tokens extracted from Internet news [URL_CITE] including 9 topics: Finance, IT, Health, Sports, Travel, Education, Jobs, Art, Mil-itary. We then randomly but equally select 600 articles as the set of source documents from 9 top-ics without data bias.\nFootnote or Reference text (if exists): 2 http://www.sogou.com/labs/dl/c.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Evaluation as a ranking problem']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016) and Kasai et al. (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 512 units each.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: We also extend the models in Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016) by allowing highway con-nections between LSTM layers.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 400 units each.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/cove\nCitation Context: Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [URL_CITE] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension.\nFootnote or Reference text (if exists): 1 https://github.com/salesforce/cove\nSection Titles (if exists):  ['2 Proposed model: SAN']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: (2016); Kasai et al. (2017) [URL_CITE] ; CCG: Lewis et al. (2016)).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: This concatenation af-ter each layer differs from Kasai et al. (2017) [URL_CITE] and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/andreasvc/discodop\nCitation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\nFootnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [URL_CITE] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/~liub/FBS/data.tar.gz\nCitation Context: We have also tested our approach on the dataset used in (Jindal and Liu, 2006b) [URL_CITE] . We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons).\nFootnote or Reference text (if exists): 3 Available from http://www.cs.uic.edu/\u02dcliub/FBS/data.tar.gz \u2013 although the original paper works on some unknown subset of this data, so our results are not directly\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ai-systems/premise\nCitation Context: For the experiments using BERT and the pro-posed approach, we split the dataset using a 50/20/30 (train/dev/test) split. We run all exper-iments ten times, evaluating on the test set, and report the average Precision, Recall and F1-score. All evaluation data, as well as the experimental pipeline, can be found online [URL_CITE] for reproducibility purposes.\nFootnote or Reference text (if exists): 2 https://github.com/ai-systems/premise selection graph\nSection Titles (if exists):  ['6 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/andreasvc/disco-dop\nCitation Context: We evaluate our parser on the test sets of the three treebanks. We report F-measure and discontinuous F-measure as computed using the disco-dop tool [URL_CITE] with standard parameters in Table 2. ants of our parsers produced exactly the same re-sults in all settings. This may be expected as their cover of the original treebanks are almost similar.\nFootnote or Reference text (if exists): 11 https://github.com/andreasvc/ disco-dop\nSection Titles (if exists):  ['4 Experiments', '4.3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.cornell.edu/~ainur/data.html\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [URL_CITE] , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .\nFootnote or Reference text (if exists): 3 http://www.cs.cornell.edu/~ainur/data. html\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html\nCitation Context: Sentiment classification has been a widely explored task which received a lot of attention. The Movie Review dataset, published by Pang and Lee (2005) [URL_CITE] , has become a standard machine learning bench-mark task for binary sentence classification. Socher et al.\nFootnote or Reference text (if exists): 8 http://www.cs.uic.edu/liub/FBS/sentiment-analysis.html\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.2 Sentiment Polarity Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://snap.stanford.edu/data,June\nCitation Context: For Movies dataset, we used the Amazon review data set (Leskovec and Krevl, 2014) [URL_CITE] . It has over 7.9 million reviews for 250,000 movies.\nFootnote or Reference text (if exists): Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data,June.\nSection Titles (if exists):  ['4 Human Subject Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/checklist\nCitation Context: In this work, we propose CheckList, a new eval-uation methodology and accompanying tool [URL_CITE] for comprehensive behavioral testing of NLP models. CheckList guides users in what to test, by provid-ing a list of linguistic capabilities, which are appli-cable to most tasks.\nFootnote or Reference text (if exists): 1 https://github.com/marcotcr/checklist\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/marcotcr/checklist\nCitation Context: changing names), we ex-pect that collaborative test creation will result in evaluation of NLP models that is much more ro-bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at [URL_CITE] https://github.com/marcotcr/checklist.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: CheckList enabled generating new (perturbed) test samples through abstracting different test types aimed at testing linguistic capabilities. Other studies focused on evaluating robustness to perturbed inputs for machine translation (Belinkov and Bisk, 2018; Niu et al., 2020), perturbation sensitivity analysis for detecting unintended model biases (Prabhakaran et al., 2019), or robustness to adversarial perturbations (Alshemali and Kalita, 2020 [URL_CITE] ; Ebrahimi et al., 2018; Liang et al., 2018). However, a comprehensive methodology for evaluating the performance of NLP models under real-world conditions is still missing.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MT-DNN\nCitation Context: We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [URL_CITE] . Training details are presented in Appendix B.2.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/MT-DNN\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Frozenmad/AMN_SRL\nCitation Context: The model is trained for maximum 20 epochs for the nearly best model based on de-velopment set results. We re-run our model using different initialized parameters for 4 times and re-port the average performance [URL_CITE] .\nFootnote or Reference text (if exists): 4 Our implementation is publicly available at https://github.com/Frozenmad/AMN_SRL.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/domluna/memn2n\nCitation Context: For MemN2N, we use the publicly available im-plementation [URL_CITE] and train it exactly as all other mod-els (same optimizer, total epochs, and early stop-ping criteria) for fairness. While the reported best result for MemN2N is on the version with posi-tion encoding, linear start training, and random-injection of time index noise (Sukhbaatar et al., 2015), the version we use has only position encod-ing. Note that the comparison is still meaningful because linear start training and time index noise are not used in DMN+ (and as a result, neither in our proposed DSMN).\nFootnote or Reference text (if exists): 2 https://github.com/domluna/memn2n\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duncanka/lstm-causality-tagger\nCitation Context: The code for DeepCx is available on GitHub. [URL_CITE] 5.3.1 Dimensionalities The pretrained LSTM parser model uses the same dimensionalities as the original LSTM parser.\nFootnote or Reference text (if exists): 3 https://github.com/duncanka/lstm-causality-tagger.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/treex\nCitation Context: Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and \u017dabokrtsk\u00fd, 2010). [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://ufal.mff.cuni.cz/treex. Domain-independent deep syntax analysis for several languages is included in this framework; the English pipeline used here involves a statistical part-of-speech tagger (Spoustov\u00e1 et al., 2007) and a dependency parser (McDonald et al., 2005), fol-lowed by a rule-based conversion to deep syntax trees.\nSection Titles (if exists):  ['2 Generator Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tensorflow.org/\nCitation Context: The delay in supertagging can be attributed to sev-eral factors, in particular the differences in net-work architectures including the number of bi- LSTM layers (4 vs. 2) and the use of bilin-ear transformation instead of linear one. There are also many implementation differences in our parser (C++ A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ Tensor-Flow (Abadi et al., 2015) [URL_CITE] supertagger and recur-sive neural model in C++ DyNet (Neubig et al., 2017)).\nFootnote or Reference text (if exists): Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane\u0301, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Vie\u0301gas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Sys-tems. Software available from tensorflow.org. http://tensorflow.org/.\nSection Titles (if exists):  ['6 Experiments', '6.3 English Parsing Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser\nCitation Context: As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015). [URL_CITE]\nFootnote or Reference text (if exists): 3 We use the standard data split (02\u201321 for training, 22 for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a different random initialization; this differs from past work on ensembles, which often uses different base model architectures.\nSection Titles (if exists):  ['3 Consensus and Minimum Bayes Risk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/lstm-parser/tree/easy-to-use\nCitation Context: DeepCx is implemented using a refactored version of the LSTM parser codebase that performs iden-tically to the original. [URL_CITE] The neural network frame-work, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02\u201321; development: 22).\nFootnote or Reference text (if exists): 2 https://github.com/clab/lstm-parser/tree/easy-to-use.\nSection Titles (if exists):  ['5 DeepCx neural network architecture', '5.3 Implementation details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xuuuluuu/SynLSTM-for-NER\nCitation Context: Our fur-ther analysis demonstrates that our model can capture longer dependencies compared with strong baselines. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our code publicly available at https://github.com/xuuuluuu/SynLSTM-for-NER.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/research/sp\nCitation Context: Analysis also justifies the effectiveness of us-ing our new dependency-based representa-tion. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make our system and code available at http://statnlp.org/research/sp.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mindspore.cn/\nCitation Context: We thank Mindspore [URL_CITE] which is a new deep learning computing framework for the partial support of this work.\nFootnote or Reference text (if exists): 3 https://www.mindspore.cn/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rivercold/BERT-unsupervised-OOD\nCitation Context: Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/anjalief/unsupervised_gender_bias\nCitation Context: Ulti-mately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and pre-trained models are available at https: //github.com/anjalief/unsupervised_ gender_bias\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.public.asu.edu/~jye02/Software/MALSAR\nCitation Context: Domain-Specific Models Only LS 77.80 77.88 81.63 84.33 80.41 SVM 78.56 78.66 83.03 84.74 81.25 LR 79.73 80.14 84.54 86.10 82.63 MLP 81.70 81.65 85.45 85.95 83.69 Shared Model Only LS 78.40 79.76 84.67 85.73 82.14 SVM 79.16 80.97 85.15 86.06 82.83 LR 80.05 81.88 85.19 86.56 83.42 MLP 82.40 82.15 85.90 88.20 84.66 MAN-L2-MLP 82.05 83.45 86.45 88.85 85.20 MAN-NLL-MLP 81.85 83.10 85.75 89.10 84.95 Shared-Private Models RMTL 1 81.33 82.18 85.49 87.02 84.01 MTLGraph 2 79.66 81.84 83.69 87.06 83.06 CMSC-LS 3 82.10 82.40 86.12 87.56 84.55 CMSC-SVM 3 82.26 83.48 86.76 88.20 85.18 CMSC-LR 3 81.81 83.73 86.67 88.23 85.11 SP-MLP 82.00 84.05 86.85 87.30 85.05 MAN-L2-SP-MLP ( 82 \u00b10 .46 83.98 87.22* 88.53 85.55* .25) (\u00b10.17) (\u00b10.04) (\u00b10.19) (\u00b10.07) MAN-NLL-SP-MLP 82.98* 84.03 87.06 88.57* 85.66* (\u00b10.28) (\u00b10.16) (\u00b10.23) (\u00b10.15) (\u00b10.14) 1 Evgeniou and Pontil (2004) 2 Zhou et al. (2011) [URL_CITE] 3 Wu and Huang (2015) 3 Zhao et al. (2017)\nFootnote or Reference text (if exists): J. Zhou, J. Chen, and J. Ye. 2011. MAL-SAR: Multi-tAsk Learning via StructurAl Regularization. Arizona State University. http://www.public.asu.edu/\u02dcjye02/Software/MALSAR.\nSection Titles (if exists):  ['3 Theories of Multinomial Adversarial Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1007/978-3-540-30586-638\nCitation Context: Dimension of human ratings Study Sentence Planning this paper weak positive (\u03c1 = 0.33, WPS) Surface Realisation Domain weak negative (\u03c1 = 0. \u2212 31, parser) NLG, restaurant/hotel search (Reiter and Belz, 2009) none strong positive (Pearson\u2019s r = 0.96, NIST ) NLG, weather forecast (Stent et al., 2005) [URL_CITE] weak positive (\u03c1 = 0.47, LSA ) (Liu et al., 2016) weak positive (\u03c1 = 0.35, BLEU -4 ) (Elliott and Keller, 2014) positive (\u03c1 = 0.53, METEOR ) (Kilickaya et al., 2017) positive (\u03c1 = 0.64, SPICE ) (Cahill, 2009) N/A (Espinosa et al., 2010) weak positive (\u03c1 = 0.43, TER ) negative (\u03c1 = \u22120.56, NIST ) paraphrasing of news N/A dialogue/Twitter pairs N/A image caption N/A image caption negative (\u03c1 = \u22120.64, ROUGE ) NLG, German news texts positive (\u03c1 = 0.62, BLEU -4 ) NLG, news texts\nFootnote or Reference text (if exists): Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for gener-ation in the presence of variation. In Computa-tional Linguistics and Intelligent Text Processing: 6th International Conference, CICLing 2005, Mex-ico City, Mexico, February 13-19, 2005. Proceed-ings. Springer, Berlin/Heidelberg, pages 341\u2013351. https://doi.org/10.1007/978-3-540-30586-638.\nSection Titles (if exists):  ['7 Relation of Human and Automatic Metrics', '8.3 Example-based Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebp/scikit-survival\nCitation Context: Both C-index and AUC assign a random model 0.5 and a perfect model 1. We measure all-time C-index, C-index at 30 days (C-index@30), and AUC at 30 days and 365 days (AUC@30 and AUC@365) to show the models\u2019 performances dealing with different time-to-events [URL_CITE] . Model C-index C-index@30 AUC@30 AUC@365 CPH + Feature Set 1 0.499 \u00b1 0.025 0.504 \u00b1 0.032 0.503 \u00b1 0.037 CPH + Feature Set 2 0.621 \u00b1 0.014 0.632 \u00b1 0.033 0.642 \u00b1 0.030 CPH + Hidden Features 0.674 \u00b1 0.023 0.696 \u00b1 0.022 0.710 \u00b1 0.022 MLP + Feature Set 1 0.502 \u00b1 0.023 0.509 \u00b1 0.026 0.509 \u00b1 0.030 MLP + Feature Set 2 0.658 \u00b1 0.010 0.671 \u00b1 0.025 0.685 \u00b1 0.023 MLP + Hidden Features 0.704 \u00b1 0.017 0.726 \u00b1 0.020 0.744 \u00b1 0.019 LSTM + Sequential HF 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 0.501 \u00b1 0.028 0.642 \u00b1 0.030 0.697 \u00b1 0.026 0.501 \u00b1 0.032 0.683 \u00b1 0.008 0.734 \u00b1 0.018 0.742 \u00b1 0.023 Model C-index C-index@30 AUC@30 BERT-Base (Devlin et al., 2019) 0.603 \u00b1 0.115 0.611 \u00b1 0.123 0.618 \u00b1 0.134 BioBert (Lee et al., 2020) 0.701 \u00b1 0.021 0.714 \u00b1 0.027 0.734 \u00b1 0.029 ClinicalBert (Alsentzer et al., 2019) 0.692 \u00b1 0.019 0.705 \u00b1 0.023 0.723 \u00b1 0.025 BlueBert (Peng et al., 2019) 0.713 \u00b1 0.019 0.735 \u00b1 0.024 0.755 \u00b1 0.024 CheXbert (Smit et al., 2020) 0.709 \u00b1 0.022 0.733 \u00b1 0.031 0.752 \u00b1 0.033 AUC@365 0.620 \u00b1 0.136 0.739 \u00b1 0.028 0.727 \u00b1 0.029 0.756 \u00b1 0.021 0.742 \u00b1 0.023\nFootnote or Reference text (if exists): 1 https://github.com/sebp/ scikit-survival\nSection Titles (if exists):  ['4 Experiments', '4.2 Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs/src\nCitation Context: The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) \u2013 07:40:54 (NoReC Fine ) Model implementation [URL_CITE] https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/\nCitation Context: ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at [URL_CITE] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/quankiquanki/\nCitation Context: Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)), CLCD English-German S-T D-B M-B B-D M-D B-M D-M English-French All D-B M-B B-D M-D B-M D-M All PBLM Models P+BE 78.7 78.6 80.6 79.2 81.7 78.5 PBLM 70.9 62.9 74.5 66.5 75.0 75.5 Lazy 74.8 74.0 75.1 72.8 73.3 73.7 79.5 81.1 74.7 76.3 75.0 75.1 76.8 76.5 71.0 76.0 67.9 70.3 69.9 67.3 70.4 70.3 73.9 74.2 73.1 75.3 74.4 74.1 72.4 73.9 Autoencoder+pivot Models A-S-SR 68.3 62.5 69.4 69.9 70.2 69 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.2 Airline (English, (Nguyen, 2015) [URL_CITE] ) to Product Review Domains (German or French), CLCD English-German English-French Source-Target A-B A-D A-M All A-B A-D A-M All PBLM Models P+BE 64 6. 65.167.9 62.5 63.6 63.5 66.9 64.8 PBLM 60.9 59.6 60.1 .60 2 60.9 61.9 58.9 60.5 Lazy 66.3 65.0 66.6 66 0.\nFootnote or Reference text (if exists): Quang Nguyen. 2015. The airline review dataset. https://github.com/quankiquanki/skytrax-reviews-dataset. Scraped from www.airlinequality.com.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amr.isi.edu/download.html\nCitation Context: Annotating natural language with AMR is a complex task and training datasets only exist for English [URL_CITE] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English. We create training data for multilingual AMR-to- Text models, by taking the EUROPARL multilin-gual corpus and automatically annotating the En-glish data with AMRs using the jamr semantic parser.\nFootnote or Reference text (if exists): 1 AMR datasets from the LDC can be found at https://amr.isi.edu/download.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svn.nlpl.eu/mrp/2019/public/resources.txt\nCitation Context: Finally, we wish to point out that direct com-parability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT ) whose usage was forbidden to the shared task par-ticipants. [URL_CITE] Nevertheless, we believe that their re-sults are needed here to better contextualize the performances SGL obtains.\nFootnote or Reference text (if exists): 19 Allowed resources are specified at: http://svn.nlpl.eu/mrp/2019/public/resources.txt\nSection Titles (if exists):  ['5 Results', '5.3 UCCA Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jflanigan/jamr\nCitation Context: Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr [URL_CITE] to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET .\nFootnote or Reference text (if exists): 3 https://github.com/jflanigan/jamr\nSection Titles (if exists):  ['4 Experimental Setting', '4.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/cc_net\nCitation Context: Pretraining For encoder pretraining on silver AMR, we take thirty million sentences from the English portion of CCNET [URL_CITE] (Wenzek et al., 2019), a cleaned version of Common Crawl (an open source version of the web). We use jamr to parse En-glish sentences into AMR. For multilingual de-coder pretraining, we take thirty million sentences from each language split of CCNET .\nFootnote or Reference text (if exists): 2 https://github.com/facebookresearch/ cc_net\nSection Titles (if exists):  ['4 Experimental Setting', '4.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/sgl\nCitation Context: Furthermore, we find that mod-els trained under this configuration scale re-markably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its com-petitors by a large margin without even explic-itly seeing non-English to AMR examples at training time and, once these examples are in-cluded as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at [URL_CITE] https: //github.com/SapienzaNLP/sgl.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: VecOff. We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014) [URL_CITE] and represented word pairs as described in Section 2.1.\nFootnote or Reference text (if exists): 1 https://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Evaluation', '4.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe [URL_CITE] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): 8 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014) [URL_CITE] , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use the 100-dimensional GloVe vectors [URL_CITE] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Experiments', '4.1 Answer Selection', '4.1.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We initialized our word representations using publicly available 300-dimensional Glove vec-tors [URL_CITE] (Pennington et al., 2014). For the sentiment classification task, word representations were up-dated during training with a learning rate of 0.1. For the semantic relatedness task, word represen-tations were held fixed as we did not observe any significant improvement when the representations were tuned.\nFootnote or Reference text (if exists): 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/.\nSection Titles (if exists):  ['5 Experiments', '5.3 Hyperparameters and Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nCitation Context: The full implementation can be found in an online tuto-rial. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nSection Titles (if exists):  ['4 Implementing a game']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-eval\nCitation Context: System outputs from the E2E NLG Challenge participants can be found at [URL_CITE] https://github.com/tuetschek/e2e-eval\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-cleaning\nCitation Context: This is enabled by a set of regular expres-sions released by Dus\u030cek et al. (2019a) [URL_CITE] . The reg-ular expressions capture the entire phrase used to express an attribute-value pair, focusing on the con-tent words and attempting as much as possible to leave out the function words, e.g.\nFootnote or Reference text (if exists): 1 https://github.com/tuetschek/ e2e-cleaning\nSection Titles (if exists):  ['2 Frequency of Surface Forms']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuetschek/e2e-cleaning\nCitation Context: It uses modified regu-lar expressions from the e2e-cleaning repos-itory. [URL_CITE] https://github.com/tuetschek/ e2e-cleaning\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/wmaier/rparse\nCitation Context: We run further experiments with rparse [URL_CITE] (Kallmeyer and Maier, 2013) to facilitate a com-parison with a grammar-based parser.\nFootnote or Reference text (if exists): 5 http://github.com/wmaier/rparse\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/daiquocnguyen/R-MeN\nCitation Context: In future work, we plan to extend R-MeN for multi-hop knowledge graph reasoning. Our code is available at: [URL_CITE] https://github.com/daiquocnguyen/R-MeN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu\nCitation Context: Eventually, we would like to expand our data set and relations to cover proper nouns as well. We are hopeful that our current dataset and re-lation definitions, which will be made available via [URL_CITE] http://www.isi.edu will be helpful to other re-searchers doing work regarding text semantics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2017-cmapsum-corpus\nCitation Context: We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasperGuo/Unimer\nCitation Context: We plan to cover more domains and more MRs in U NIMER . We have made U NIMER along with the execution engines publicly available. [URL_CITE] We be-lieve that U NIMER can provide fertile soil for ex-ploring MRs and addressing challenges in semantic parsing.\nFootnote or Reference text (if exists): 3 Our benchmark, execution engines and and our im-plementation can be found on: https://github.com/JasperGuo/Unimer\nSection Titles (if exists):  ['3 Benchmark']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries to extract etymology-related information and Google Trans-late [URL_CITE] to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well.\nFootnote or Reference text (if exists): 5 http://translate.google.com\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dexonline.ro\nCitation Context: We use lemmas for identifying words\u2019 definitions in dictionaries and for computing adequate distances between words and their cognates or etymons. We use the Dex-online [URL_CITE] machine-readable dictionary to lemmatize Romanian words.\nFootnote or Reference text (if exists): 3 http://dexonline.ro\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crowdflower.com\nCitation Context: While ontologies like Open Multi-lingual WordNet (OMW) (Bond and Foster, 2013) and BabelNet (Navigli and Ponzetto, 2012) con-tain cross-lingual links, these resources are semi-automatically generated and hence contain noisy edges. Thus, to get reliable and high-quality test beds, we collect evaluation datasets using Crowd-Flower [URL_CITE] . Our datasets span four languages from distinct families - French (Fr), Russian (Ru), Ara-bic (Ar) and Chinese (Zh) - paired with English.\nFootnote or Reference text (if exists): 4 http://crowdflower.com\nSection Titles (if exists):  ['4 Crowd-Sourcing Annotations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://meta.wikimedia.org/wiki/List_of_Wikipedias\nCitation Context: We used Wikipedia as the main data source for all our experiments. Multilingual BERT was trained on the 104 languages with the largest Wikipedias [URL_CITE] \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.\nFootnote or Reference text (if exists): 11 List of Wikipedias can be found in https://meta.wikimedia.org/wiki/List_of_Wikipedias\nSection Titles (if exists):  ['6 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org\nCitation Context: We focus on analyzing word content and, in order to obtain relevant results, we remove stop words from the datasets. We use the lists of stop words for Roma-nian provided by the Apache Lucene [URL_CITE] text search engine library. In Table 1 we list the total number of stop words from each corpus.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.1 Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sapere.it/sapere/dizionari\nCitation Context: We identify cognate pairs be-tween Romanian and six other languages: Ital-ian, French, Spanish, Portuguese, Turkish and En-glish. We use electronic dictionaries [URL_CITE] to extract etymology-related information and Google Trans-late to translate Romanian words. We are re-stricted in our investigation by the available re-sources, but we plan to extend our method to other related languages as well.\nFootnote or Reference text (if exists): 4 Italian: http://www.sapere.it/sapere/dizionari\nSection Titles (if exists):  ['2 Methodology and Algorithm', '2.2 Algorithm', '2.2.2 Relationships Identification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.business2community.com/infographics/impact-online-reviews-\nCitation Context: The majority of consumers uses such re-views to inform themselves before buying. [URL_CITE] On-line review companies therefore put considerable effort into maintaining this trust, by addressing the greatest threat to consumer trust (and therefore income)\u2014fake reviews.\nFootnote or Reference text (if exists): 1 http://www.business2community.com/infographics/impact-online-reviews-customers-buying-decisions-infographic- 01280945\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/challenge\nCitation Context: \u2022 Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [URL_CITE] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it).\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/challenge\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: Our best neural system in turn outperformed our baseline, achieving an average success rate of 34%. Our code and data, including newly collected human reference outputs for the Yelp and Amazon domains, can be found at [URL_CITE] https://github.com/lijuncen/Sentiment-and-Style-Transfer.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fortune.com/2015/10/19/amazon-fake-reviews/\nCitation Context: Writing fake reviews has become a lucrative business (Streitfeld, 2012), and so there is now an arms race going on between producers and detec-tors (Roberts, 2012) [URL_CITE] . What if fake review writ-ers become aware of the ways to game a detection algorithm?\nFootnote or Reference text (if exists): Jeff John Roberts. 2012. Amazon sues people who charge $5 for fake reviews. Fortune, October 19. http://fortune.com/2015/10/19/amazon-fake-reviews/ Retrieved Feb 27, 2016.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bbc.com/news/technology-24299742\nCitation Context: However, the question remains: is every online review trustful? It has been reported that up to 25% of the reviews on Yelp.com could be fraudu-lent [URL_CITE] . Due to the great profit or reputation, impostors or spammers energetically post fake reviews on the web to promote or defame targeted products (Jindal and Liu, 2008).\nFootnote or Reference text (if exists): 1 http://www.bbc.com/news/technology-24299742\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.princeton.edu/~blei/lda-c/index.html\nCitation Context: We trained an LDA model with 20 top-ics on our document collection using David Blei\u2019s implementation. [URL_CITE] We used this model to re-rank the output of our annotation model according to the three most likely topics in each document.\nFootnote or Reference text (if exists): 3 Available from http://www.cs.princeton.edu/\u02dcblei/lda-c/index.html.\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1038/nn.3331\nCitation Context: The model was implemented using Tensorflow (Abadi et al., 2016) [URL_CITE] . The network weights were randomly initialized using the uniform Glorot initialization method (Glorot and Bengio, 2010) and optimization was performed using AdaDelta (Zeiler, 2012) with learning rate 1.0.\nFootnote or Reference text (if exists): Mart\u0131\u0301n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-war, Paul Tucker, Vincent Vanhoucke, Vijay Va-sudevan, Fernanda Viegas, Oriol Vinyals, Pete War-den, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: Large-Scale Machine Learning on Heterogeneous Dis-tributed Systems. Arxiv preprint arXiv:1603.04467 https://doi.org/10.1038/nn.3331.\nSection Titles (if exists):  ['5 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nCitation Context: All documents use tf-idf term weighting scheme with no feature selection. Source code for different baselines (1-vs-Set Machine , W-SVM and P I - SVM , and Exploratory learning [URL_CITE] ) was provided by the authors of their original papers.\nFootnote or Reference text (if exists): 4 http://www.cs.cmu.edu/~bbd/ExploreEM_package.zip\nSection Titles (if exists):  ['4 Experiments', '4.1 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cuitools.sourceforge.net\nCitation Context: Our experiments were conducted using CuiTools v0.15, which is freely available from [URL_CITE] http://cuitools.sourceforge.net.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: For training, our Our+FULL model is first ini-tialized by pre-training using Our+AVG model, and we set the iteration number over all the train-ing data as 10. For pre-trained word embeddings, we use the 300-dimensional GloVe [URL_CITE] (Pennington et al., 2014) word embeddings learned from 840B Web crawl data.\nFootnote or Reference text (if exists): 6 http://nlp.stanford.edu/data/glove.840B.300d.zip\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ai.mit.edu/people/jrennie/20_newsgroups/\nCitation Context: All methods are initialized by a ran-dom choice of 10 document labels and 10 word la-bels. For simplicity, we follow the widely used cost model (Raghavan and Allan, 2007; Druck et al., 2008; Sindhwani et al., 2009) where features are roughly 5 times cheaper to label than examples, so we assume the cost is [URL_CITE] for a word query and is 5 for a document query. We set \u03b1 = \u03b2 = 5, \u03b3 = 1 for all the following experiments 2 .\nFootnote or Reference text (if exists): 1 http://www.ai.mit.edu/people/jrennie/20_newsgroups/\nSection Titles (if exists):  ['6 Experiments', '6.1 Experiments Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5. For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016) [URL_CITE] .\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Transformer-DyNet\nCitation Context: The dropout rate for the output layer is set to 0.2. For the Transformer, we use Transformer-DyNet [URL_CITE] implementation and extend it for our context-aware NMT model. The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively.\nFootnote or Reference text (if exists): 7 https://github.com/duyvuleo/Transformer-DyNet\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: The texts are tokenized using SpaCy [URL_CITE] and au-tomatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017).\nFootnote or Reference text (if exists): 9 https://spacy.io/\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2w7nUQK\nCitation Context: We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google\u2019s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [URL_CITE] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences.\nFootnote or Reference text (if exists): 2 This model can be downloaded at https://bit.ly/2w7nUQK.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/docs/en/crawl-vectors.html\nCitation Context: We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [URL_CITE] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf.\nFootnote or Reference text (if exists): 7 https://fasttext.cc/docs/en/crawl-vectors.html\nSection Titles (if exists):  ['6 Language Modeling Analysis', '6.1 Predicting Insertion Locations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/explosion/spaCy\nCitation Context: Data Preprocessing For Reddit data (i.e. r/AskScience and r/AskHistorians), we lower-cased, removed redundant spaces/urls, and used the Spacy [URL_CITE] library to tokenize each sentence. For training GloVe embeddings, we substi-tuted \u201c\u201d symbols in WikiText-103 with \u201c\u201d symbols.\nFootnote or Reference text (if exists): 11 https://github.com/explosion/spaCy The word embeddings are trained with GloVe on r/AskScience. The word embeddings are trained with GloVe on WikiText-103. The word embeddings are trained with SGNS on r/AskHistorians. SGNS word embeddings trained with three iterations or 100 dimensions on r/AskHistorians. dings trained with three iterations or 100 dimensions on r/AskHistorians.\nSection Titles (if exists):  ['B Environmental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: We use spaCy [URL_CITE] to get the meta dependency paths of sentences in a document. Following Yao et al.\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Implementation details: The spaCy tool [URL_CITE] is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags. We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encod-ing.\nFootnote or Reference text (if exists): 2 https://spacy.io\nSection Titles (if exists):  ['3 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.biu.ac.il/~nlp/downloads/\nCitation Context: To our knowledge, this is the first such in-depth study. [URL_CITE]\nFootnote or Reference text (if exists): 4 The guidelines and the dataset are available at http://www.cs.biu.ac.il/\u02dcnlp/downloads/\nSection Titles (if exists):  ['3 Motivation and Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.youtube.com/watch?v=zKnP5QEHVAE\nCitation Context: For more detailed demonstration, please refer to the video. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.youtube.com/watch?v= zKnP5QEHVAE\nSection Titles (if exists):  ['3 Design and Internals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.delph-in.net/crawler/\nCitation Context: All numbers reported here reflect this frozen system. [URL_CITE]\nFootnote or Reference text (if exists): 10 The code and data are available from http://www.delph-in.net/crawler/ , for replicability (Fokkens et al., 2013).\nSection Titles (if exists):  ['4 Experiments', '4.1 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sheffieldnlp/fever-baselines\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces and the baseline system [URL_CITE] to stimulate further research on verification.\nFootnote or Reference text (if exists): 3 https://github.com/sheffieldnlp/ fever-baselines\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fever.ai\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website [URL_CITE] , we also publish the annotation interfaces and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 1 http://fever.ai\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/oferbr/BIU-RPI-Event-Extraction-Project\nCitation Context: We consider a re-alistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from J OINT B EAM , we ran the ac-tual system [URL_CITE] used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.\nFootnote or Reference text (if exists): 8 https://github.com/oferbr/ BIU-RPI-Event-Extraction-Project\nSection Titles (if exists):  ['4 Experiments', '4.1 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yym6472/OVSlotTagging\nCitation Context: (3) Experi-ments show that our proposed method consistently outperforms various SOTA baselines, especially in open-vocabulary slot f1. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/yym6472/OVSlotTagging\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2017-cmapsum-corpus\nCitation Context: We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/awslabs/fever\nCitation Context: We also conducted oracle experiments in which com-ponents of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sen-tences containing the evidence. In addition to pub-lishing the data via our website , we also publish the annotation interfaces [URL_CITE] and the baseline system to stimulate further research on verification.\nFootnote or Reference text (if exists): 2 https://github.com/awslabs/fever\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gorokoba560/norm-analysis-of-transformer\nCitation Context: The codes of our experiments are pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/gorokoba560/ norm-analysis-of-transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lit.eecs.umich.edu/downloads.html\nCitation Context: The code used in the experiments described in this paper is publicly available from [URL_CITE] http: //lit.eecs.umich.edu/downloads.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion and Recommendations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cosmozhang/Modular_Neural_CRF\nCitation Context: All of our models beat the state-of-the-art results by a large margin. The source code and experimental setup are avail-able online [URL_CITE] .\nFootnote or Reference text (if exists): 4 https://github.com/cosmozhang/ Modular_Neural_CRF\nSection Titles (if exists):  ['6 Experimental Evaluation', '6.2 Q1: Monolithic vs. Modular Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paracrawl.eu/download.html\nCitation Context: There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [URL_CITE] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a).\nFootnote or Reference text (if exists): 6 http://paracrawl.eu/download.html\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt/\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): 6 http://www.phontron.com/kftt/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://diamt.limsi.fr/eval.html\nCitation Context: We created two contrastive test sets to help com-pare how well different contextual MT mod-els handle (i) anaphoric pronoun translation and (ii) coherence and cohesion. [URL_CITE] For each test set, models are assessed on their ability to rank the cor-rect translation of an ambiguous sentence higher than the incorrect translation, using the disam-biguating context provided in the previous source and/or target sentence. All examples in the test sets are hand-crafted but inspired by real examples from OpenSubtitles2016 (Lison and Tiedemann, 2016) to ensure that they are credible and that vo-cabulary and syntactic structures are varied.\nFootnote or Reference text (if exists): 2 The test sets are freely available at https://diamt.limsi.fr/eval.html.\nSection Titles (if exists):  ['2 Evaluating contextual phenomena', '2.1 Our contrastive discursive test sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mrlyk423/relation_extraction\nCitation Context: Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from [URL_CITE] https://github.com/mrlyk423/relation_extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/RSN\nCitation Context: Experiment results on two real-world datasets show that our frame-work can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at [URL_CITE] https://github.com/thunlp/RSN.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/CG-RL\nCitation Context: Experimental results and in-depth analy-sis on several commonly-used datasets demon-strate the effectiveness and generality of our ap-proach. We have released our code at [URL_CITE] https: //github.com/DeepLearnXMU/CG-RL .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [URL_CITE] http: //sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/CodRED\nCitation Context: We conduct comprehensive ex-periments to show that CodRED is challeng-ing to existing RE methods including strong BERT-based models. We make CodRED and the code for our baselines publicly available at [URL_CITE] https://github.com/thunlp/CodRED.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neukg/GRTE\nCitation Context: Ex-perimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: [URL_CITE] https://github.com/neukg/GRTE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fastnlp/nlp-dataset\nCitation Context: (2019). [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/fastnlp/ nlp-dataset\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.youtube.com/watch?v=zKnP5QEHVAE\nCitation Context: For more detailed demonstration, please refer to the video. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.youtube.com/watch?v= zKnP5QEHVAE\nSection Titles (if exists):  ['3 Design and Internals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.biu.ac.il/~nlp/downloads/\nCitation Context: To our knowledge, this is the first such in-depth study. [URL_CITE]\nFootnote or Reference text (if exists): 4 The guidelines and the dataset are available at http://www.cs.biu.ac.il/\u02dcnlp/downloads/\nSection Titles (if exists):  ['3 Motivation and Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nCitation Context: (2015b). [URL_CITE]\nFootnote or Reference text (if exists): 7 http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nSection Titles (if exists):  ['4 Results and Discussion', '4.1 LE Directionality and Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.englishbaby.com/lessons/4349/slang/fruitcake\nCitation Context: She is a total fruitcake. [URL_CITE]\nFootnote or Reference text (if exists): 12 http://www.englishbaby.com/lessons/4349/slang/ fruitcake\nSection Titles (if exists):  ['5 Task 2: Finding most similar words for slang across languages', '5.2 Baseline and Our Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nCitation Context: For our system, we used the Le Zhang\u2019s maximum entropy modeling toolkit to train the shift-reduce parsing model after extracting 32.6M events from the training data. [URL_CITE] We set the iteration limit to 100. The accuracy on the training data is 90.18%.\nFootnote or Reference text (if exists): 4 http://homepages.inf.ed.ac.uk/lzhang10/maxent.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/unlu\nCitation Context: We drop examples from test sets where we are unable to compute all unique randomizations, typi-cally these are examples with sentences of length of less than 6 tokens. [URL_CITE] Models accept many permuted examples. We find \u2126 max is very high for models trained and evalu-ated on MNLI (in-domain generalization), reaching 98.7% on MNLI dev.\nFootnote or Reference text (if exists): 2 Code, data, and model checkpoints will be available at https://github.com/facebookresearch/unlu. models are trained on OCNLI corpus (Hu et al., 2020a). Bold marks the highest value per metric (red shows the model is insensitive to permutation).\nSection Titles (if exists):  ['5 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Frozenmad/AMN_SRL\nCitation Context: The model is trained for maximum 20 epochs for the nearly best model based on de-velopment set results. We re-run our model using different initialized parameters for 4 times and re-port the average performance [URL_CITE] .\nFootnote or Reference text (if exists): 4 Our implementation is publicly available at https://github.com/Frozenmad/AMN_SRL.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr\nCitation Context: De-spite the reduced set of features, training error rate at both token and sentence level is under 1%. We didn\u2019t carry out optimization for parameters \u03c1 1 and \u03c1 [URL_CITE] of the elastic net (see section 3.1), default values lead in most cases to very accurate models.\nFootnote or Reference text (if exists): 2 available at http://wapiti.limsi.fr\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jaist.ac.jp/~h-yamada/\nCitation Context: We used sec-tion 23 as the test set. Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada [URL_CITE] . During the training phase, we used intact POS and chunk tags .\nFootnote or Reference text (if exists): 2 http://www.jaist.ac.jp/\u02dch-yamada/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/egret-parser\nCitation Context: In previous reports, it has been noted (Kummerfeld et al., 2012) that the la-tent variable model of the Berkeley parser tends to have the higher accuracy of the two, so if the accu-racy of a system using this model is higher then it is likely that parsing accuracy is important for T2S translation. Instead of the Berkeley Parser itself, we use a clone Egret, [URL_CITE] which achieves nearly iden-tical accuracy, and is able to output packed forests for use in MT, as mentioned below. Trees are right-binarized, with the exception of phrase-final punctuation, which is split off before any other el-ement in the phrase.\nFootnote or Reference text (if exists): 4 http://code.google.com/p/egret-parser\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.chokkan.org/software/crfsuite/\nCitation Context: Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007) [URL_CITE] . Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009).\nFootnote or Reference text (if exists): Naoaki Okazaki. 2007. CRFsuite: a fast im-plementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.nyu.edu/evalb/\nCitation Context: We convert the phrase structure trees in the PTB to dependency structure and convert the depen-dency back to phrase structure. We then compare the original PTB trees with the newly-created phrase structure trees, using the standard evalb scoring code (Sekine and Collins, 2008) [URL_CITE] . Xia and Palmer (2001) defined three different algorithms for the conversion, utilizing different heuristics for how to build projec-tion chains, and where to attach dependent subtrees.\nFootnote or Reference text (if exists): Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.\nSection Titles (if exists):  ['4 Results of Dependency to Phrase Structure Conversion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neu-big et al., 2011) [URL_CITE] for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nCitation Context: (2011), as implemented in the Eda toolkit. [URL_CITE] In order to convert dependencies into phrase-structure trees typically used in T2S translation, we use the head rules implemented in the Travatar toolkit. In addition, we also train a latent variable CFG using the Berkeley Parser and use Egret for parsing.\nFootnote or Reference text (if exists): 5 http://plata.ar.media.kyoto-u.ac.jp/tool/EDA\nSection Titles (if exists):  ['3 Parsing', '3.1 Parsing Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nCitation Context: When performing self-training, we constructed the synthetic data by translating the monolingual sen-tences via beam search with beam width 5, and followed Edunov et al. (2018) [URL_CITE] to remove sen-tences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5. The \u201cteacher\u201d NMT model for self-training is the T RANSFORMER -B IG model to ensure the quality of synthetic data.\nFootnote or Reference text (if exists): 7 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nSection Titles (if exists):  ['A Appendix', 'A.1 Synthetic Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cl-tohoku/bert-japanese\nCitation Context: We used pretrained BERT (Devlin et al., 2019) as the em-bedding layer of the model. [URL_CITE] We adopted the same optimization algorithm, learning rate, batch size, and output dimension of the recurrent layer as in Taghipour and Ng (2016). We trained the SAS models for 50 epochs and selected the parameters in the epoch in which the best QWK was achieved for the development set.\nFootnote or Reference text (if exists): 3 We adopted pretrained character-based BERT which is known to be suitable for processing Japanese texts. This is available at https://github.com/cl-tohoku/bert-japanese. for Low-Resource NLP, pages 175\u2013182.\nSection Titles (if exists):  ['5 Experiments', '5.2 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LIAAD/yake\nCitation Context: To select a can-didate phrase set P d , following the literature (Wan and Xiao, 2008b), we consider contiguous nouns in the document d that form a noun phrase satisfying the regular expression ( ADJECTIVE )*( NOUN )+. We then filter the candidates with a stopword list taken from the official YAKE implementation [URL_CITE] (Campos et al., 2020). Finally, for the statistical methods and the graph-based methods based on them (i.e., LexRank and TFIDFRank), we compute citeulike180 6.6 9.5 18.0 15.2 23.0 fao30 17.3 16.0 24.0 20.7 26.0 fao780 9.3 3.2 11.7 10.5 12.4 kdd 11.7 7.0 11.2 11.6 10.6 theses100 5.6 9.4 6.60.9 10.7 wiki20 13.0 13.0 17.0 21.0 13.0 www 12.2 8.1 11.9 12.2 10.6 prior statistics including term frequency (tf), tf-idf, and LDA by Gensim (R\u030cehu\u030ar\u030cek and Sojka, 2010) within each dataset.\nFootnote or Reference text (if exists): 10 https://github.com/LIAAD/yake\nSection Titles (if exists):  ['3 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/\nCitation Context: For the synthetic dataset training method, we con-sider two QA models: finetuning BERT (Devlin et al., 2018) and BiDAF + Self Attention (Clark and Gardner, 2017). [URL_CITE] For the posterior maximisa-tion method, we extract cloze questions from both sentences and sub-clauses, and use the NMT mod-els to estimate p(q|c,a). We evaluate using the standard Exact Match (EM) and F1 metrics.\nFootnote or Reference text (if exists): 5 We use the HuggingFace implementation of BERT, available at https://github.com/huggingface/ pytorch-pretrained-BERT, and the documentQA implementation of BiDAF+SA, available at https://github.com/allenai/document-qa\nSection Titles (if exists):  ['3 Experiments', '3.1 Unsupervised QA Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qanus.com/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS [URL_CITE] , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 6 http://www.qanus.com/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.openchannelfoundation.org/projects/Qanda/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda [URL_CITE] , QANUS , QSQA etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 5 http://www.openchannelfoundation.org/projects/Qanda/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://technologies.kmi.open.ac.uk/aqualog/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog [URL_CITE] , NLBean etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 8 http://technologies.kmi.open.ac.uk/aqualog/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.markwatson.com/opensource/\nCitation Context: are developed that retrieve answers from the Web and other tex-tual sources. Similarly, structured QA systems (Aqualog , NLBean [URL_CITE] etc.) obtain answers from the sub-networks.\nFootnote or Reference text (if exists): 9 http://www.markwatson.com/opensource/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://quora.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora [URL_CITE] and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 2 http://quora.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dzonesoftware.com/products/open-source-question-answer-software/\nCitation Context: On a different line of research, several Textual-based Question Answering (QA) systems (Qanda , QANUS , QSQA [URL_CITE] etc.) are developed that retrieve answers from the Web and other tex-tual sources.\nFootnote or Reference text (if exists): 7 http://www.dzonesoftware.com/products/open-source-question-answer-software/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/LRLS2MhPRE#Ebola\nCitation Context: Tweet @PNHP: 6. Renounce punitive and counterproductive measures such as sealing the borders, [URL_CITE] http://t.co/LRLS2MhPRE #Ebola\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Analysis', '4.1 Exact Match Calculations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fastnlp/nlp-dataset\nCitation Context: (2019). [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/fastnlp/ nlp-dataset\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nCitation Context: (2015b). [URL_CITE]\nFootnote or Reference text (if exists): 7 http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nSection Titles (if exists):  ['4 Results and Discussion', '4.1 LE Directionality and Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://research-lab.yahoo.co.jp/en/software/\nCitation Context: While redistribution of tweets is prohibited, we are planning to publi-cize tweet IDs of this dataset for reproducibility. [URL_CITE]\nFootnote or Reference text (if exists): 2 The tweet IDs will be provided from https://research-lab.yahoo.co.jp/en/software/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://t.co/XExWwxmjnk#travel\nCitation Context: Tweet Are #Airlines doing enough with #Ebola? [URL_CITE] http://t.co/XExWwxmjnk #travel\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/nni\nCitation Context: To make a fair compari-son, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform. [URL_CITE] Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the pre-processing procedures of logical forms.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/nni\nSection Titles (if exists):  ['4 Experimental Setup', '4.2 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Implementations. To implement our models, we rely on MALLET (McCallum, 2002) [URL_CITE] for C-LDA and the Stanford Topic Modeling Toolbox 3 for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters.\nFootnote or Reference text (if exists): Andrew Kachites McCallum. 2002. MAL-LET: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tmt/\nCitation Context: Implementations. To implement our models, we rely on MALLET (McCallum, 2002) for C-LDA and the Stanford Topic Modeling Toolbox [URL_CITE] for L-LDA. In both cases, we run 1000 iterations of Gibbs sam-pling, using default values for all hyperparameters.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/software/tmt/.\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: We generally follow the hyperparameters cho-sen in Dozat and Manning (2017) [URL_CITE] . Specifically, we use BiLSTMs layers with 400 units each.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~och/YASMET.html\nCitation Context: We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://www.isi.edu/\u02dcoch/YASMET.html\nSection Titles (if exists):  ['3 Adjacency Pairs', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: (2009) we use asymmetric \u03b1 and symmetric \u03b2; rather than using fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method. [URL_CITE] We use standard set-tings for the number of training iterations (1000), the length of the burnin period before hyperparameter estimation begins (200 iterations) and the frequency of hyperparameter estimation (50 iterations).\nFootnote or Reference text (if exists): 2 We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.\nSection Titles (if exists):  ['3 Models', '3.1 Latent variable context models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openie.allenai.org/\nCitation Context: We utilize an off-the-shelf toolbox of OpenIE [URL_CITE] to the derive structured answer-relevant relations from sentences as to the point contexts. Relations extracted by OpenIE can be represented either in a triple format or in an n-ary format with several secondary arguments, and we employ the latter to keep the extractions as informative as possible and avoid extracting too many similar relations in dif-ferent granularities from one sentence.\nFootnote or Reference text (if exists): 1 http://openie.allenai.org/\nSection Titles (if exists):  ['2 Framework Description', '2.2 Answer-relevant Relation Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.elastic.co/products/elasticsearch\nCitation Context: For a question with q words Q = {w tQ } qt=1 along with its N answer choices C = {C n } Nn=1 where C n = {w Ct } ct=1 , the essential-term selector chooses a subset of essential terms E \u2282 Q, which are then concatenated with each C n to formulate a query. The query for each answer choice, E+C n , is sent to the retriever (e.g. Elastic Search [URL_CITE] ), and the top K retrieved sentences based on the scores returned by the retriever are then concatenated into the evidence passage P n = {w tP } tK=1 .\nFootnote or Reference text (if exists): 2 https://www.elastic.co/products/elasticsearch\nSection Titles (if exists):  ['3 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freebase.com/\nCitation Context: Given a structured knowledge base, such as Free-base [URL_CITE] , a single relation question Q (in natural language) with its answer can be first parsed into a fact formatted as he sbj ,rel,e obj i, where e sbj denotes a subject entity detected from the question, rel denotes the relationship expressed by the question, e obj denotes an object entity found from the knowledge base based on e sbj and rel. Then we can get hQ,reli pairs.\nFootnote or Reference text (if exists): 5 http://www.freebase.com/\nSection Titles (if exists):  ['4 Response Ranking', '4.5 Relation-level Feature']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: Prediction We use E MBED A LIGN to annotate every word in the training set of the benchmarks above with the posterior mean embedding in con-text. We then average embeddings in a sentence and give that as features to a logistic regression classifier trained with 5-fold cross validation. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://scikit-learn.org/stable/\nSection Titles (if exists):  ['3 Experiments', '3.3 Sentence Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [URL_CITE] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment.\nFootnote or Reference text (if exists): 2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.\nSection Titles (if exists):  ['4 Experiment', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [URL_CITE] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alpage.inria.fr/~sagot/lefff-en.html\nCitation Context: The continuous word representations are used to train a logis-tic regression classifier 2 for each morphological feature: gender and number for noun and adjec-tives; tense for verbs (with labels: present, fu-ture, imperfect, or simple past). Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010) [URL_CITE] . To ensure a fair comparison be-tween context-independent and context-dependent embedding classification, words that are ambigu-ous with respect to a given feature are excluded from the respective classifier\u2019s training and test data.\nFootnote or Reference text (if exists): 3 Lexique des Formes Fle\u0301chies du Franc\u0327ais: http://alpage.inria.fr/\u02dcsagot/lefff-en.html\nSection Titles (if exists):  ['2 Methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.hcu.ox.ac.uk/BNC/\nCitation Context: We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus [URL_CITE] . The English corpus contains about 20 millions of words.\nFootnote or Reference text (if exists): 6 Available at: http://www.hcu.ox.ac.uk/BNC/\nSection Titles (if exists):  ['6 Evaluation on English corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://chywang.github.io/data/emnlp17.zip\nCitation Context: In the experiments, given only 0.6M entities and their respective 2.4M categories in Chinese Wikipedia, our method extracts 1.52M relations with an overall accuracy of 93.6%. The exper-iments also show that our approach outperforms previous methods for both is-a and non-taxonomic relation extraction from Chinese UGCs. The ex-tracted relations and the labeled test set are pub-licly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://chywang.github.io/data/emnlp17.zip\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nCitation Context: The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC [URL_CITE] , containing about 28 millions of Chinese words. Since punc-tuations are rarely used to construct collocations, they were removed from the corpora. To auto-matically estimate the precision of extracted col-locations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations.\nFootnote or Reference text (if exists): 2 Available at: http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2007T03\nSection Titles (if exists):  ['3 Initial Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/noisereduce/\nCitation Context: Audio is recorded from a built-in laptop micro-phone at 16kHz. Background noise is reduced us-ing a spectral gating algorithm, [URL_CITE] and volume is nor-malized across sessions based on peak root-mean-square levels.\nFootnote or Reference text (if exists): 2 https://pypi.org/project/noisereduce/\nSection Titles (if exists):  ['2 Data Collection', '2.3 Recording Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mozilla/DeepSpeech\nCitation Context: Using an au-tomatic transcription allows for much faster and more reproducible comparisons between methods compared to a human evaluation. For our automatic speech recognizer, we use the open source imple-mentation of DeepSpeech from Mozilla [URL_CITE] (Hannun et al., 2014). Running the recognizer on the orig-inal vocalized audio recordings from the test set results in a WER of 9.5%, which represents a lower bound for this evaluation.\nFootnote or Reference text (if exists): 5 https://github.com/mozilla/DeepSpeech\nSection Titles (if exists):  ['4 Experiments', '4.2.2 Automatic Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/wav2vec\nCitation Context: Encoder. We initialize the encoder using the open-sourced [URL_CITE] wav2vec 2.0 large architecture pretrained on unlabelled English-only (XMEF-En) audio from LibriVox (Baevski et al., 2020). For many-to-one experiments, we also experiment with a multi-lingual wav2vec 2.0 (XMEF-X), which was pre-trained on raw audio from 53 languages (Conneau et al., 2020).\nFootnote or Reference text (if exists): 1 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Training']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/text/_modules/torchtext/data/utils.html\nCitation Context: We normalize the text by lower casing, removing special characters, etc. [URL_CITE] For each task, we construct separate 1-Gram vo-cabulary (U) and initialize a trainable randomly sampled token embedding (U \u00d7 d e ) from N (0, 1). Similarly, we randomly initialize a (d s-max \u00d7 d e ) positional embedding.\nFootnote or Reference text (if exists): 9 https://pytorch.org/text/_modules/torchtext/data/utils.html\nSection Titles (if exists):  ['6 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://naudio.codeplex.com/\nCitation Context: The recorder, built using the open-source library NAudio, [URL_CITE] takes the default audio input device as its source and records one channel with a sampling rate of 8 kHz, as the recognition engine we employ is designed for low-quality audio (see Section 4.1).\nFootnote or Reference text (if exists): 8 http://naudio.codeplex.com/\nSection Titles (if exists):  ['5 User interface', '5.1 Audio input and recording']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asmadotgh/neural_chat_web\nCitation Context: For further information and instructions on server configuration, please read the server doc-umentation available at [URL_CITE] https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['F Interactive bot platform details', 'F.1 Website server setup and configuration']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://vectors.nlpl.eu/repository/\nCitation Context: All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository [URL_CITE] (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set (Targeted F 1 for the baselines).\nFootnote or Reference text (if exists): 8 Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100-dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokma\u030al (id 58).\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets?search=kilt\nCitation Context: KILT data and code are available at https://github.com/facebookresearch/KILT. [URL_CITE]\nFootnote or Reference text (if exists): 1 and at https://huggingface.co/datasets? search=kilt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/asmadotgh/neural_chat_web\nCitation Context: The trained models were deployed to inter-act live with human users via a web server that hosts neural network dialog models on GPU for fast, real-time inference: [URL_CITE] https: //github.com/asmadotgh/neural_chat_web .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/freesunshine0316/neural-graph-to-seq-mp\nCitation Context: We release our code and models at [URL_CITE] https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/natashamjaques/neural_chat/tree/master/BatchRL\nCitation Context: After chatting, users were asked to provide a Likert scale rating of the bot\u2019s conversation quality, fluency, diversity, contingency/relatedness, and empathy. The code for the RL models is available in open-source at [URL_CITE] https://github.com/natashamjaques/neural_chat/tree/master/BatchRL. Using the server, we collected a batch of human interaction data containing 46,061 pairs of user input and agent response.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Learning from talking to humans', '4.2 Hosting real-time conversations online']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html\nCitation Context: Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we\u2019ve trained on the switchboard corpus of spoken lan-guage (Godfrey et al., 1992), because it contains, just like our corpus, speech disfluencies. [URL_CITE]\nFootnote or Reference text (if exists): 6 The tagger is available free for academic research from http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.\nSection Titles (if exists):  ['3 Corpus, Features, and Data Creation', '3.1 Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://gutenberg.org/\nCitation Context: For literature, we use the BROWN cor-pus (Francis and Kuc\u030cera, 1979) and the same di-vision as (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006b). We also use raw sen-tences which we downloaded from Project Guten-berg [URL_CITE] as a self-trained corpus. The Switchboard cor-pus ( SWBD ) consists of transcribed telephone con-versations.\nFootnote or Reference text (if exists): 7 http://gutenberg.org/\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/tagger.shtml\nCitation Context: We used the Stanford log-linear part-of-speech tagger (Stanford POS Tagger, Toutanova et al., 2003) to tag each word of each sentence in the cor-pora. [URL_CITE] We then computed the perplexity of each word and averaged them for each tag type sepa-rately. Among the 36 POS tags used by the Stan-ford POS Tagger, we looked at the perplexities of the ten most frequent tags (NN, IN, DT, JJ, RB, NNS, VBZ, VB, PRP, CC), of which we combined NN and NNS into a new tag Noun and VB and VBZ into a new tag Verb.\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/software/tagger.shtml\nSection Titles (if exists):  ['6 Results and Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.brenbarn.net/werewolf/\nCitation Context: The raw data consists of 86 game transcripts col-lected by Barnwell (Barnwell, 2012) [URL_CITE] . The tran-scripts have an average length of 205 messages per transcript, including judge comments.\nFootnote or Reference text (if exists): Brendan Barnwell. 2012. brenbarn.net. http://www.brenbarn.net/werewolf/. [Online; accessed 1-April-2016].\nSection Titles (if exists):  ['3 Experiments', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.clsp.jhu.edu/PIRE/ssr\nCitation Context: We conducted our experiments on the recently re-leased Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisher conversational telephone speech data (Cieri et al., 2004) [URL_CITE] . Advantages of the SSR data include\nFootnote or Reference text (if exists): 1 The Spontaneous Speech Reconstruction corpus can be downloaded from http://www.clsp.jhu.edu/PIRE/ssr.\nSection Titles (if exists):  ['2 Approach', '2.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://compprag.christopherpotts.net/swda.html\nCitation Context: We use the Switch-board Dialogue Act (SwDA) corpus (Calhoun et al., 2010) in our experiments [URL_CITE] . SwDA contains the transcripts of several people discussing a given topic on the telephone. There are 42 dialogue act tags in SwDA, and we list the 10 most frequent tags in Table 3.\nFootnote or Reference text (if exists): 5 http://compprag.christopherpotts.net/swda.html.\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xpqiu/fnlp/\nCitation Context: As described in Section 3, the data are collected from Weibo.com. We randomly select half of the annotated posts as the training data and another half as the test data. FNLP [URL_CITE] is used for Chinese word segmentation.\nFootnote or Reference text (if exists): 7 FNLP (FudanNLP), https://github.com/xpqiu/fnlp/\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: This approach has successfully been used in previous work for devising web-based corpora (Scha\u0308fer, 2015; Bie-mann et al., 2007). The data is available at [URL_CITE] https: //github.com/SimonHFL/CWEB . covering a range of topics.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 CWEB Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.sighan.org/bakeoff2005/\nCitation Context: We use the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff [URL_CITE] as the labeled data. We choose the PKU data in our experiment because our baseline meth-ods use the same segmentation standard.\nFootnote or Reference text (if exists): 2 http://www.sighan.org/bakeoff2005/\nSection Titles (if exists):  ['3 Experiment', '3.1 Data set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lemurproject.org/clueweb12\nCitation Context: WNED-CWEB (Guo and Barbosa, 2018) is a dataset created with the same strategy as WNED-WIKI, but sampling from the ClueWeb 2012 cor-pora annotated with the FACC1 system. [URL_CITE] Similarly, we randomly split into dev and test.\nFootnote or Reference text (if exists): 7 http://lemurproject.org/clueweb12\nSection Titles (if exists):  ['3 Tasks', '3.2 Entity Linking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://banner.sourceforge.net\nCitation Context: We use a standard NLP pipeline based on Stanford CoreNLP 2 (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Rec-ognizer [URL_CITE] (only for PubMed articles) to identify en-tities and phrases. Assume that a document com-prises of introduction sentences {q 1 , q 2 , ...q n }, and the remaining passages {p 1 , p 2 , ..p m }.\nFootnote or Reference text (if exists): 3 http://banner.sourceforge.net\nSection Titles (if exists):  ['3 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.jtf.org.tw\nCitation Context: Then, the CIP randomly selects one seed pattern per run without replacement from the seed set, and iteratively induces relevant patterns from the psychiatry web corpora. The psychiatry web corpora used here include some professional mental health web sites, such as PsychPark (http://www.psychpark.org) (Bai, 2001) and John Tung Foundation ( [URL_CITE] http://www.jtf.org.tw).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experimental Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/megagonlabs/opiniondigest\nCitation Context: Automatic evaluation on Y ELP data shows that our framework outperforms com-petitive baselines. Human studies on two cor-pora verify that O PINION D IGEST produces informative summaries and shows promising customization capabilities [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/megagonlabs/opiniondigest.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tagoyal/sow-reap-paraphrasing\nCitation Context: Our evaluation, both au-tomatic and human, shows that the proposed system retains the quality of the baseline ap-proaches while giving a substantial increase in the diversity of the generated paraphrases. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code are available at https://github.com/tagoyal/sow-reap-paraphrasing\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/successar/FRESH\nCitation Context: In both automatic and manual evaluations we find that variants of this simple framework yield predic-tive performance superior to \u2018end-to-end\u2019 ap-proaches, while being more general and easier to train. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/successar/FRESH\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/lipiji/SongNet\nCitation Context: Extensive experiments conducted on two collected corpora demonstrate that our pro-posed framework generates significantly better results in terms of both automatic metrics and the human evaluation. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code: http://github.com/lipiji/SongNet\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dykang/xslue\nCitation Context: The preprocessed datasets and code are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dykang/xslue\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wanqiulong0923/TED-CDB\nCitation Context: The dataset and our Chinese annotation guidelines has been made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wanqiulong0923/TED-CDB manifestation of intelligence, but more about our na-ture as a living, breathable organism] 1 . [The difference between consciousness and intelligence is very large. You will feel pain even if you are not smart, but only if you have to live] 2 . [(Therefore) RESULT+SPEECHACT\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The dataset and baseline models will be freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yogarshi/bisparse-dep/\nCitation Context: Our embeddings and datasets are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/yogarshi/bisparse-dep/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/wmt17/translation-task/preprocessed/\nCitation Context: All these preprocessed datasets are publicly available, and can be down-loaded from WMT17 official website. [URL_CITE]\nFootnote or Reference text (if exists): 3 http://data.statmt.org/wmt17/translation-task/preprocessed/\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~yww/data/emnlp2013.zip\nCitation Context: The coffee shop dataset is freely available [URL_CITE] for re-search purposes.\nFootnote or Reference text (if exists): 6 http://www.cs.cmu.edu/\u02dcyww/data/emnlp2013.zip\nSection Titles (if exists):  ['Coffee Shops Reviews']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.5281/zenodo.4064408\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.4000/discours.9047\nCitation Context: Guillou and Hardmeier\u2019s (2016) pronoun trans-lation test suite succeeds in overcoming some of these problems by creating an automatic evalua-tion method, with a back-off manual evaluation. Manual evaluation has always been an essential part of evaluating MT quality, and targeted transla-tion allows us to isolate a model\u2019s performance on specific linguistic phenomena; recent work using in-depth, qualitative manual evaluation (Isabelle et al., 2017; Scarton and Specia, 2015 [URL_CITE] ) is very in-formative. Isabelle et al.\nFootnote or Reference text (if exists): Carolina Scarton and Lucia Specia. 2015. A Quan-titative Analysis of Discourse Phenomena in Ma-chine Translation. Discours [online] (16). https: //doi.org/10.4000/discours.9047.\nSection Titles (if exists):  ['2 Evaluating contextual phenomena']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [URL_CITE] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 1 https://translate.google.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [URL_CITE] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 3 https://aws.amazon.com/translate\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bing.com/translator\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [URL_CITE] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 2 https://www.bing.com/translator\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sen-tences of more than 60 words.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/\nCitation Context: On the other hand, the test-on-trans(lation) method trains an English sequence tagger, and utilizes MT to translate the test set of each target language into English in order to make predictions. In this work, we adopt the Mi-crosoft Translator [URL_CITE] , a strong commercial MT sys-tem. Note that for a MT system to work for se-quence tagging tasks, word alignment informa-tion must be available, in order to project word-level annotations across languages.\nFootnote or Reference text (if exists): 4 https://azure.microsoft.com/en-us/services/cognitive-services/translator-text-api/\nSection Titles (if exists):  ['4 Experiments', '4.1 Cross-Lingual Semantic Slot Filling', '4.1.1 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JRC1995/Multilingual-BERT-Disaster\nCitation Context: The exact hyperpa-rameters are available on Github. [URL_CITE] . For significance testing, we used the paired t-test (p \u2264 0.05) (Dror et al., 2018) Note that the CNN baseline is also similar to the model used by Nguyen et al.\nFootnote or Reference text (if exists): 5 https://github.com/JRC1995/ Multilingual-BERT-Disaster\nSection Titles (if exists):  ['4 Methods', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/compare-mt\nCitation Context: For the En\u21d2Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improve-ment with paired bootstrap resampling (Koehn, 2004) using compare-mt [URL_CITE] (Neubig et al., 2019).\nFootnote or Reference text (if exists): 3 https://github.com/neulab/compare-mt\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [URL_CITE] systems in ELS task of SENSEVAL-3.\nFootnote or Reference text (if exists): 3 we SV M light ,used linear available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Experiment 1: LP vs. SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~dbikel/software.html\nCitation Context: In order to establish the statistical significance of results between two parsing experiments in terms of F 1 and UAS, we used a unidirectional t-test for two independent samples [URL_CITE] . The statistical significance between two MWE identification experiments was established by using the McNemar-s test (Gillick and Cox, 1989). The results of the two experiments are considered statistically significant with the com-puted value p < 0.01.\nFootnote or Reference text (if exists): 12 Dan Bikel\u2019s tool available at http://www.cis.upenn.edu/\u02dcdbikel/software.html.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm\nCitation Context: We calculated statistical significance for the main results on the development section using bootstrap random sampling. [URL_CITE] After re-sampling 1000 times, significance was calculated using a paired t-test (999 d.f.). The results indicated that lp-only exceeded the baseline, lp-ngram and lp-syn exceeded lp-only, and the full model exceeded lp-syn, with p < 0.0001 in each case.\nFootnote or Reference text (if exists): 6 Scripts for running these tests are available at http://projectile.sv.cmu.edu/research/public/tools/bootStrap/tutorial.htm\nSection Titles (if exists):  ['4 Evaluation', '4.2 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com\nCitation Context: Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy (https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day ( [URL_CITE] https://business.twitter.com). Thus there is an ex-treme class imbalance between user and non-user tweets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://blog.twitter.com/2011/03/numbers.html\nCitation Context: Over 340 millions of tweets are being generated daily amounting to thousands of tweets per second! [URL_CITE] Twitter\u2019s own search en-gine handles more than 1.6 billion search queries per day. This enormous amount of data renders it in-feasible to browse the entire Twitter network; even if this was possible, it would be extremely difficult for users to find information they are interested in.\nFootnote or Reference text (if exists): 2 In fact, the peak record is 6,939 tweets per second, reported by http://blog.twitter.com/2011/03/numbers.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://twitter.com/about\nCitation Context: As September 2010, Twitter reports that its users publish nearly 95 million tweets per day [URL_CITE] . This makes Twitter an excellent case to analyze misin-formation in social media.\nFootnote or Reference text (if exists): 1 http://twitter.com/about\nSection Titles (if exists):  ['4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://twitter.com\nCitation Context: \u2022 Tweets - Twitter [URL_CITE] is a microblogging plat-form which allows users to post statuses of less than 140 characters. We use two collections for sarcasm detection on tweets.\nFootnote or Reference text (if exists): 2 https://twitter.com\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://business.twitter.com/en/basics/what-is-twitter/\nCitation Context: Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012) [URL_CITE] . It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011).\nFootnote or Reference text (if exists): Twitter. (2012). What is Twitter Retrieved Mar 18, 2012, from https://business.twitter.com/en/basics/what-is-twitter/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://help.twitter.com/en/rules-and-policies/twitter-limits\nCitation Context: User tweets are simply all the tweets from that user\u2019s Twitter account, and measure perhaps in the thousands. Indeed, the number of tweets per user per day is limited to 2400 per day by current Twitter policy ( [URL_CITE] https://help.twitter.com/en/rules-and-policies/twitter-limits). The negative exam-ples consist of all tweets written by other Twitter users, a total of approximately 500 million per day (https://business.twitter.com).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['2 Problem Formulation', '2.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/snakeztc/NeuralDialog-LAED\nCitation Context: Our methods have been validated on real-world dialog datasets to discover semantic representa-tions and enhance encoder-decoder mod-els with interpretable generation. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code are available at https://github.com/snakeztc/NeuralDialog-LAED.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/valentinhofmann/dga\nCitation Context: The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make all our code and data publicly available at https://github.com/valentinhofmann/dga.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mindspore.cn/\nCitation Context: We thank Mindspore [URL_CITE] which is a new deep learning computing framework for the partial support of this work.\nFootnote or Reference text (if exists): 3 https://www.mindspore.cn/\nSection Titles (if exists):  ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Liang-Qiu/SVRNN-dialogues\nCitation Context: While on multi-party dia-logue datasets, our model learns an interac-tive structure demonstrating its capability of distinguishing speakers or addresses, automat-ically disentangling dialogues without explicit human annotation. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is released at https://github.com/Liang-Qiu/SVRNN-dialogues.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nCitation Context: NeuronBlocks provides convenient user interface [URL_CITE] for users to build, train, and test DNN models. The details are described in the following.\nFootnote or Reference text (if exists): 4 https://github.com/microsoft/NeuronBlocks/blob/master/Tutorial.md\nSection Titles (if exists):  ['3 Design', '3.3 User Interface']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.metaversum.com/\nCitation Context: The KomParse system, described in this paper, provides NPCs for a virtual online world named Twinity, a product of the Berlin startup company Metaversum [URL_CITE] . The KomParse NPCs offer vari-ous services through conversation with game users using question-answering and dialog functional-ity. The utilization of Semantic Web technology with RDF-encoded generic and domain-specific ontologies furthermore enables semantic search and inference.\nFootnote or Reference text (if exists): 1 http://www.metaversum.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mecab/files/\nCitation Context: Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [URL_CITE] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task.\nFootnote or Reference text (if exists): 4 http://sourceforge.net/projects/mecab/files/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [URL_CITE] . Both campaigns include two dif-ferent translations exercises: Arabic-to-English (\u2018AE\u2019) and Chinese-to-English (\u2018CE\u2019).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.2 Test Beds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/\nCitation Context: We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. [URL_CITE] To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.\nFootnote or Reference text (if exists): 5 http://www.itl.nist.gov/iad/mig/tests/mt/\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl [URL_CITE] with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 4 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nCitation Context: We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script [URL_CITE] . Model and Hyper-parameters.\nFootnote or Reference text (if exists): 6 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 [URL_CITE] (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 (Isozaki et al., 2010). Follow-ing Cho et al.\nFootnote or Reference text (if exists): 6 http://www.statmt.org/moses/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nCitation Context: Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.1 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1 [URL_CITE] (Isozaki et al., 2010). Follow-ing Cho et al.\nFootnote or Reference text (if exists): 7 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/boots\nCitation Context: For evaluation, we train all the mod-els with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respec-tively, and we select the model which performs the best on the validation set and report its per-formance on the test sets. We measure the case-insensitive/case-sensitive BLEU scores using multi-bleu.perl with the statistical significance test (Koehn, 2004) [URL_CITE] for NIST Zh-En and WMT\u201914 En-De, respectively. For WMT\u201918 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script .\nFootnote or Reference text (if exists): 5 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/analysis/bootstrap-hypothesis-difference-significance. pl\nSection Titles (if exists):  ['4 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wiktionary.org/\nCitation Context: Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [URL_CITE] , a large collaborative-constructed online dictionary. We used WordNet as our core re-source, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wik-tionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the con-struction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word.\nFootnote or Reference text (if exists): 3 https://www.wiktionary.org/\nSection Titles (if exists):  ['2 WiC: the Word-in-Context dataset', '2.1 Construction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC [URL_CITE] , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag-of-words approach and counting co-occurrences within a sentence.\nFootnote or Reference text (if exists): 4 http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Semantic spaces', '4.1 The distributional semantic space']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/futurulus/colors-in-context\nCitation Context: Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public with our code and trained model parameters. [URL_CITE] setting up the data collection platform, and mem-bers of the Stanford NLP group\u2014particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon\u2014for valuable feedback on earlier drafts. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant Nos.\nFootnote or Reference text (if exists): 8 https://github.com/futurulus/ colors-in-context\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wangcunxiang/Sen-Making-and-Explanation\nCitation Context: Besides, we also expect that this work could be instructive on enhancing in-terpretability on commonsense reasoning research and other NLP tasks and on combining expla-nation with language generation. Our dataset is released at [URL_CITE] https://github.com/wangcunxiang/Sen-Making-and-Explanation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://newsblaster.cs.columbia.edu\nCitation Context: We tested the classifiers on data different from that provided by DUC, and also tested human consen-sus on the hearer-new/old distinction. For these pur-poses, we downloaded 45 clusters from one day\u2019s output from Newsblaster [URL_CITE] . We then automatically compiled the list of people mentioned in the ma-chine summaries for these clusters.\nFootnote or Reference text (if exists): 4 http://newsblaster.cs.columbia.edu\nSection Titles (if exists):  ['5 Validating the results on current news']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adymaharana/StoryViz\nCitation Context: We also present correlation experiments of our proposed automated metrics with hu-man evaluations. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data: https://github.com/adymaharana/StoryViz.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cocolab.stanford.edu/datasets/colors.html\nCitation Context: Our contribution includes a new dataset of hu-man utterances in a color reference game in Man-darin Chinese, which we release to the public [URL_CITE] with our code and trained model parameters. setting up the data collection platform, and mem-bers of the Stanford NLP group\u2014particularly Reid Pryzant, Sebastian Schuster, and Reuben Cohn-Gordon\u2014for valuable feedback on earlier drafts.\nFootnote or Reference text (if exists): 7 https://cocolab.stanford.edu/datasets/colors.html\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/bk42ZQ\u201d\nCitation Context: (doubtful) \u201cPresident Barack Obama\u2019s Religion: Christian, Muslim, or Agnostic? - The News of Today (Google): Share With Friend... [URL_CITE] http://bit.ly/bk42ZQ\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://mediabiasfactcheck.com\nCitation Context: (2019), a large corpus of political headlines from 2018 paired with news source rat-ings of political leaning (5-point scale from left- to right-leaning) and factual reliability (5-point scale from least reliable to most reliable). [URL_CITE]\nFootnote or Reference text (if exists): 10 We use the MediaBias/FactCheck ratings: https://mediabiasfactcheck.com. Shoumen, BULGARIA. know\u201d \u2013 Follows: \u201cIt\u2019s devastating to be excluded from a wedding you were invited to\u201d\nSection Titles (if exists):  ['6 Morality & Political Bias']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cs.rochester.edu/nlp/rocstories/\nCitation Context: This subreddit contains a high volume of hateful or disturbing content; we attempt to filter the worst of this using keywords, and also allow annotators to mark dark or disturbing items. 3. rocstories (30k) \u2014 The ROCStories corpus from (Mostafazadeh et al., 2016) [URL_CITE] . ROCStories involve stories about everyday situations, and are generally less contro-versial than the other sources, e.g., \u201cThey weren\u2019t sure either so he started asking friends.\u201d.\nFootnote or Reference text (if exists): Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor-pus and cloze evaluation for deeper understanding of commonsense stories. In NAACL, pages 839\u2013 849, San Diego, California. Association for Com-putational Linguistics. Corpus available at https: //cs.rochester.edu/nlp/rocstories/ .\nSection Titles (if exists):  ['2 Approach']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ff.im/62Evd\u201d\nCitation Context: (12) \u201cSarah Palin Divorce Rumor Debunked on Face-book [URL_CITE] http://ff.im/62Evd\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Data', '4.1 Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.casmacat.eu/corpus/news-commentary.html\nCitation Context: \u2022 News, which is from News Commentary v11 corpus. [URL_CITE] We use news-test2015 and news-\nFootnote or Reference text (if exists): 9 http://www.casmacat.eu/corpus/news-commentary.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bit.ly/9rs6pa\nCitation Context: (rumor) \u201cRT @johnnyA99 Ann Coulter Tells Larry King Why People Think Obama Is A Muslim [URL_CITE] http://bit.ly/9rs6pa #Hussein via @NewsBusters #tcot ..\u201d\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Problem Definition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: The best performance is obtained by the su-pertag representations obtained from the training of the transition-based parser Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017).\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['4 Joint Modeling and Network Representations', '4.2 Syntactic Analogies']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/C10-1094\nCitation Context: Because of variations across formalisms in their representational format for unbounded depden-dencies, past work has conducted manual evalu-ation on this corpus (Rimell et al., 2009; Nivre et al., 2010 [URL_CITE] ). We instead conduct an automatic evaluation using a procedure that converts TAG parses to structures directly comparable to those specified in the unbounded dependency corpus.\nFootnote or Reference text (if exists): Joakim Nivre, Laura Rimell, Ryan McDonald, and Car-los Go\u0301mez Rodr\u0131\u0301guez. 2010. Evaluation of depen-dency parsers on unbounded dependencies. In COL-ING. Coling 2010 Organizing Committee, Beijing, China, pages 833\u2013841. http://www.aclweb.org/anthology/C10-1094.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.2 Unbounded Dependency Recovery']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017), and Fried-man et al. (2017) [URL_CITE] ; we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/translation\nCitation Context: We follow the scripts in Ott et al. (2019) [URL_CITE] for other pre-processing steps.\nFootnote or Reference text (if exists): 8 https://github.com/pytorch/fairseq/tree/master/examples/translation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: We apply three types of transformation from Xu et al. (2017) [URL_CITE] to interpret the TAG parses.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['A.1 Transformations from PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rowanzellers.com/hellaswag\nCitation Context: We study this question by introducing Hella-Swag, [URL_CITE] a new benchmark for commonsense NLI. We use Adversarial Filtering (AF), a data-collection paradigm in which a series of discrim-inators is used to select a challenging set of gen-erated wrong answers.\nFootnote or Reference text (if exists): 2 Short for Harder Endings, Longer contexts, and Low-shot Activities for Situations With Adversarial Generations. Dataset and code at https://rowanzellers.com/hellaswag.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py\nCitation Context: We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [URL_CITE] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks.\nFootnote or Reference text (if exists): 1 https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: We used 300, 100, 100 sentences from ASPEC-JE 2 for train-ing, development and test data, respectively. [URL_CITE] Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128.\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yanaiela/demog-text-removal\nCitation Context: We explore means for im-proving the effectiveness of the adversarial train-ing procedure (section 5.2). [URL_CITE]\nFootnote or Reference text (if exists): 1 The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/c/asap-aes/\nCitation Context: At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neu-ral AES approaches with respect to adversarial in-put related to local aspects of coherence. For our experiments, we use the Automated Student As-sessment Prize (ASAP) dataset, [URL_CITE] which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4).\nFootnote or Reference text (if exists): 3 https://www.kaggle.com/c/asap-aes/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://demo.patrickpantel.com/\nCitation Context: The inherent semantic similarity between the errors can be leveraged to quickly clean up the expan-sion. For example, given a manually tagged error \u201casteroid\u201d, a distributional similarity thesaurus such as (Lin 1998) [URL_CITE] can identify comet as similar to asteroid and therefore potentially also as an error. This method has its limitations since a manually tagged error such as Earth would correctly remove Moon and Sun, but it would also incorrectly re-move Mars, Venus and Jupiter since they are also similar to Earth .\nFootnote or Reference text (if exists): 2 See http://demo.patrickpantel.com/ for a demonstration of the distributional thesaurus.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://isomap.stanford.edu/\nCitation Context: For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word \u201cuse\u201d in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [URL_CITE] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space.\nFootnote or Reference text (if exists): 5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.4 An Example: Word \u201cuse\u201d']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://labs.google.com/sets\nCitation Context: Even for state of the art methods, expansion er-rors inevitably occur and manual refinements are necessary for most practical uses requiring high precision (such as for query interpretation at com-mercial search engines). Looking at expansions from state of the art systems such as GoogleSets [URL_CITE] , we found systematic errors such as those resulting from ambiguous seed instances. For example, con-sider the following seed instances for the target set Roman Gods:\nFootnote or Reference text (if exists): 1 http://labs.google.com/sets the North American Chapter of the ACL, pages 290\u2013298,\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/salesforce/cove\nCitation Context: Both passage and question use a shared two-layers BiLSTM as the contextual encoding layer, which projects the lexicon embeddings to contextual embeddings. We concatenate a pre-trained 600-dimensional CoVe vectors [URL_CITE] (McCann et al., 2017) trained on German-English machine translation dataset, with the aforementioned lexicon embeddings as the fi-nal input of the contextual encoding layer, and also with the output of the first contextual encoding layer as the input of its second encoding layer. To reduce the parameter size, we use a maxout layer (Goodfellow et al., 2013) at each BiLSTM layer to shrink its dimension.\nFootnote or Reference text (if exists): 1 https://github.com/salesforce/cove\nSection Titles (if exists):  ['2 Proposed model: SAN']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deansong/contextLSTMCNN\nCitation Context: In many cases, however, the context available is of significant size. We therefore introduce a new method, Context-LSTM-CNN [URL_CITE] , which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence. The method consis-tently improves over results obtained from either LSTM alone, CNN alone, or these two combined, with little increase in training time.\nFootnote or Reference text (if exists): 1 The code is publicly available at https://github.com/deansong/contextLSTMCNN\nSection Titles (if exists):  ['angus.roberts@kcl.ac.uk']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://demo.patrickpantel.com/\nCitation Context: Consider the set of seed terms S and an errone-ous expanded instance e. In the SIM method of Section 3.1 all set elements that have a feature vec-tor (i.e., context vector) similar to e are removed. The Feature Modification Method (FMM) instead tries to identify the subset of features of the error e which represent the unintended sense of the seed terms S. For example, let S = {Minerva, Neptune, Baccus, Juno, Apollo}. Looking at the contexts of these words in a large corpus, we construct a cen-troid context vector for S by taking a weighted av-erage of the contexts of the seeds in S. In Wikipedia articles we see contexts (i.e., features) such as [URL_CITE] :\nFootnote or Reference text (if exists): 4 The full feature vector for these and all other terms in Wiki-pedia can be found at http://demo.patrickpantel.com/..\nSection Titles (if exists):  ['3 Dynamic Similarity Modeling', '3.2 Feature Modification Method (FMM)']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cis.temple.edu/~edragut/DebugSL.webm\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at https://github.com/atschneid/DebugSL and a screencast can be viewed at [URL_CITE] https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atschneid/DebugSL\nCitation Context: We present two main scenarios to demonstrate the practical usefulness of DebugSL, screenshots of which are shown in Figure 3. The source code is available at [URL_CITE] https://github.com/atschneid/DebugSL.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Demonstration']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://textessence.github.io\nCitation Context: A case study on COVID-19 sci-entific literature illustrates the utility of the sys-tem. TextEssence can be found at [URL_CITE] https: //textessence.github.io.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/jacobe/naacl09.html\nCitation Context: Source code is available at [URL_CITE] http://people.csail.mit.edu/jacobe/naacl09.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Elbria/xformal-FoST-meta\nCitation Context: \u2022 Our analysis code and meta-evaluation files with system outputs are made public to facil-itate further work in developing better auto-matic metrics for ST : [URL_CITE] https://github.com/Elbria/xformal-FoST-meta.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/atschneid/DebugSL\nCitation Context: In this paper we have presented the system DebugSL and described its usage. The project source code is available at [URL_CITE] https://github.com/atschneid/DebugSL and a screencast can be viewed at https://cis.temple.edu/\u02dcedragut/DebugSL.webm. The sys-tem will be deployed online for use by the public.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.opensubtitles.org\nCitation Context: Models are trained and tested on fan-produced parallel subtitles from OpenSubtitles2016 [URL_CITE] (Lison and Tiedemann, 2016). The data is first corrected using heuristics (e.g.\nFootnote or Reference text (if exists): 6 http://www.opensubtitles.org\nSection Titles (if exists):  ['4 Experiments', '4.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: To provide more diversity and comprehensiveness we augmented our pre-defined lists of known attribute values with their synonyms and hyponyms. [URL_CITE]\nFootnote or Reference text (if exists): 5 Available at https://github.com/Anna146/ CHARM\nSection Titles (if exists):  ['4 Dataset', '4.2 Document collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.umiacs.umd.edu/~jbg/static/downloads_and_media.html\nCitation Context: We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus. [URL_CITE]\nFootnote or Reference text (if exists): 6 We followed Pang and Lee\u2019s method for creating a nu-merical score between 0 and 1 from a star rating. We then converted that to an integer by multiplying by 100; this was done because initial data preprocessing assumed integer values (although downstream processing did not as-sume integer values). The German movie review corpus is available at http://www.umiacs.umd.edu/\u02dcjbg/static/downloads_and_media.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Sentiment Prediction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.imdb.com/\nCitation Context: Thus, our models include movie genre as an additional feature. Genres for each movie were obtained from IMDb [URL_CITE] and transformed into a multi-hot encoding.\nFootnote or Reference text (if exists): 3 https://www.imdb.com/\nSection Titles (if exists):  ['3 Method', '3.3 Role of Movie Genre']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/SGM\nCitation Context: We include it as a baseline by using its publicly released code. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/lancopku/SGM\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mfaruqui/crosslingual-cca\nCitation Context: For all experiments, we use the Word2Vec imple-mentation in Gensim to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) [URL_CITE] in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).\nFootnote or Reference text (if exists): 3 https://github.com/mfaruqui/ crosslingual-cca proaches on various languages. Since about one-third of the test mentions are non-trivial, a baseline is 66.67 for all languages, if we pick the most common title given the mention. Bold signi-fies highest score for each column.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: Training Details Word embeddings were pre-trained with the Gensim [URL_CITE] implementation of word2vec (Mikolov et al., 2013) on the English GigaWord corpus (with case left intact). The di-mensionality of the word embeddings was set to 50.\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Word2vec We average word embeddings trained with word2vec. [URL_CITE] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.\nFootnote or Reference text (if exists): 2 The code is available from https://code.google.com/archive/p/word2vec/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/askerlee/topicvec\nCitation Context: \u2022 TopicVec (Li et al., 2016). A model incorpo-rates generative word embedding model with LDA [URL_CITE] . We also use default values for the pa-rameters.\nFootnote or Reference text (if exists): 8 https://github.com/askerlee/topicvec\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: For semi-supervised learning, we sample addi-tional English monolingual data from News Crawl 2017. [URL_CITE] We randomly sample 5K, 10K, 20K, 50K, and 100K sentences, which are about half, equal, double, 5-times, 10-times the size of the parallel training set. For transfer and multilingual train-ing experiments, we use 12K, 23K, or 58K X-En (X=Czech/German/Russian/Chinese) parallel examples, which are equal, double, and 5-times the size of Chr-En training set.\nFootnote or Reference text (if exists): 18 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['B Experimental Details', 'B.1 Data and Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: For all experiments, we use the Word2Vec imple-mentation in Gensim [URL_CITE] to learn the skip-gram model with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings is from Faruqui and Dyer (2014) in which the ratio parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).\nFootnote or Reference text (if exists): 2 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/seraphinatarrant/plan-write-revise\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cwc-story.isi.edu\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nCitation Context: Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmaww/SENT\nCitation Context: We further design an iterative training algorithm to take full advantage of these data-refining processes, which significantly boost performance. Our codes are publicly available at Github [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/rtmaww/SENT\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://youtu.be/-hGd2399dnA\nCitation Context: Nor do they enable iteration; a user cannot revise earlier sen-tences and have the system update later genera-tions. We develop a system [URL_CITE] that allows a user to interact in all of these ways that were limita-tions in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged.\nFootnote or Reference text (if exists): 1 The live demo is at http://cwc-story.isi.edu, with a video at https://youtu.be/-hGd2399dnA. Code and models are available at https://github.com/seraphinatarrant/plan-write-revise.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/CodRED\nCitation Context: We conduct comprehensive ex-periments to show that CodRED is challeng-ing to existing RE methods including strong BERT-based models. We make CodRED and the code for our baselines publicly available at [URL_CITE] https://github.com/thunlp/CodRED.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/zgen/\nCitation Context: Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-first baseline. We publicly release our code at [URL_CITE] http: //sourceforge.net/projects/zgen/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sjcfr/CWVAE\nCitation Context: Experiments on the Event2Mind and Atomic dataset show that our proposed approach outper-forms baseline methods in both the accuracy and diversity of inferences. The code is released at [URL_CITE] https://github.com/sjcfr/CWVAE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/SDLM-pytorch\nCitation Context: Experiments on language modeling and the downstream application of headline gener-ation demonstrate the significant effectiveness of SDLM. Source code and data used in the experiments can be accessed at [URL_CITE] https://github.com/thunlp/SDLM-pytorch.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rivercold/BERT-unsupervised-OOD\nCitation Context: Our em-pirical evaluations of related methods on two datasets validate that our method greatly im-proves out-of-domain detection ability in a more general scenario. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code is available at https://github.com/rivercold/BERT-unsupervised-OOD.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ShyamSubramanian/HESM\nCitation Context: Our ex-perimental results show that HESM outper-forms 7 state-of-the-art methods for fact ex-traction and claim verification. Our source code is available at [URL_CITE] https://github.com/ShyamSubramanian/HESM.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.apptek.com/\nCitation Context: (2017). AppTek [URL_CITE] also uses Sisyphus internally.\nFootnote or Reference text (if exists): 3 http://www.apptek.com/\nSection Titles (if exists):  ['6 Real-World Usage']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nCitation Context: \u2022 Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [URL_CITE] .\nFootnote or Reference text (if exists): 3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/nlp/dpo3/\nCitation Context: 3. A free distribution of our implementation. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://groups.csail.mit.edu/nlp/dpo3/\nSection Titles (if exists):  ['3. A free distribution of our implementation. 2']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.org/Data/QA/QC/\nCitation Context: \u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [URL_CITE] .\nFootnote or Reference text (if exists): 5 http://cogcomp.org/Data/QA/QC/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://search.carrot2.org/stable/search\nCitation Context: With respect to implementation, we used the Carrot2 APIs [URL_CITE] which are freely available for STC, LINGO and the classical BIK. It is worth notic-ing that all implementations in Carrot2 are tuned to extract exactly 10 clusters.\nFootnote or Reference text (if exists): 4 http://search.carrot2.org/stable/search [Last access: 15/05/2013].\nSection Titles (if exists):  ['4 Evaluation', '4.3 Comparative Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.python.org/\nCitation Context: \u2022 Minimal requirements on the underlying server structure: Sisyphus only requires Python 3 [URL_CITE] with a few basic packages and a Unix-type operating system.\nFootnote or Reference text (if exists): 1 https://www.python.org/\nSection Titles (if exists):  ['1 Introduction', '1.2 Design Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://web.stanford.edu/%7ejurafsky/ws97/\nCitation Context: (2000) is used in our experiments. There are 1,115 dia-logues in the training set and 19 dialogues in the test set [URL_CITE] . We also randomly split the original train-ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).\nFootnote or Reference text (if exists): 7 http://web.stanford.edu/%7ejurafsky/ws97/\nSection Titles (if exists):  ['4 Experiments', '4.2 Dialogue Act Analysis', '4.2.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhanghainan/TailoredSeq2Seq2DifferentConversationScenarios\nCitation Context: We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/zhanghainan/TailoredSeq2Seq2 DifferentConversationScenarios\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Settings', '5.1.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pan.webis.de/clef16/pan16-web/author-profiling.html\nCitation Context: Dataset Three tasks are conducted on three pub-lic sentence classification datasets. Author profiling (Age dataset [URL_CITE] ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset ) is to predict the number of stars the user assigned to by analysis their reviews.\nFootnote or Reference text (if exists): 1 https://pan.webis.de/clef16/pan16-web/author-profiling.html\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://www.cs.cornell.edu/~ainur/data.html\nCitation Context: Sentiment analysis. The sentiment analysis datasets we examined include movie reviews (Pang and Lee, 2004; Zaidan and Eisner, 2008) [URL_CITE] , floor speeches by U.S. Congressmen deciding \u201cyea\"/\u201cnay\" votes on the bill under discussion (Thomas et al., 2006) 3 and product reviews from Amazon (Blitzer et al., 2007) .\nFootnote or Reference text (if exists): 3 http://www.cs.cornell.edu/~ainur/data. html\nSection Titles (if exists): [\\'4 Experiments\\', \\'4.1 Datasets\\']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lijuncen/Sentiment-and-Style-Transfer\nCitation Context: We evaluate our approach on three datasets for sen-timent transfer with positive and negative reviews: Yelp review dataset, Amazon review dataset pro-vided by Li et al. (2018), [URL_CITE] and the IMDb movie review dataset provided by Dai et al. (2019).\nFootnote or Reference text (if exists): 2 https://github.com/lijuncen/ Sentiment-and-Style-Transfer\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/challenge\nCitation Context: \u2022 Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge [URL_CITE] . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016 (Pontiki et al., 2016) and SemEval-2015 (Pontiki et al., 2015), where each sentence is labeled with aspect and sentiment po-larity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it).\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/challenge\nSection Titles (if exists):  ['5 Evaluation', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.allenai.org/scitail/\nCitation Context: SciTail [URL_CITE] is an entailment dataset based on multiple-choice science exams and web sentences. Each record is a premise-hypothesis-label triple.\nFootnote or Reference text (if exists): 4 http://data.allenai.org/scitail/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.yelp.com/dataset/download\nCitation Context: Author profiling (Age dataset ) is to predict the age range of the user by giving their tweets. Sentiment analysis (Yelp dataset [URL_CITE] ) is to predict the number of stars the user assigned to by analysis their reviews. Tex-tual entailment (SNLI dataset 3 ) is to tell whether the semantics in the two sentences are entailment or contradiction or neutral.\nFootnote or Reference text (if exists): 2 https://www.yelp.com/dataset/download\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.1 Self-attentive Sentence Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nCitation Context: We generated translations using beam search with beam width 5, and followed Edunov et al. (2018) [URL_CITE] to filter the generated sentence pairs (See Ap-pendix A.1).\nFootnote or Reference text (if exists): 4 https://github.com/pytorch/fairseq/tree/master/examples/backtranslation\nSection Titles (if exists):  ['2 Observing Monolingual Uncertainty', '2.3 Effect of Uncertain Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://taku910.github.io/mecab/\nCitation Context: For the rest (i.e., Japanese words), we initialized them using random-valued vectors. We used MeCab [URL_CITE] to tokenize the feedback comments. With these word embeddings, we trained the networks on the training set of the respective subsets (PART-TIME JOB and SMOKING).\nFootnote or Reference text (if exists): 10 http://taku910.github.io/mecab/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Conditions and Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model\nCitation Context: Our baseline is the original Transformer [URL_CITE] . For AMR-to-text generation, Transformer takes lin-earized graphs as inputs.\nFootnote or Reference text (if exists): 2 Parameters were chosen following the OpenNMT FAQ: http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i- use-the-transformer-model\nSection Titles (if exists):  ['3 Experiments', '3.3 Metrics and Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xiaochang13/AMR-generation\nCitation Context: Since incorporating unaligned words will introduce noise, we rank the translation candi-dates for each AMR fragment by their counts in the training data, and select the top N candidates. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code for grammar induction can be downloaded from https://github.com/xiaochang13/AMR-generation\nSection Titles (if exists):  ['2 Method', '2.3 Rule Acquisition']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://odur.let.rug.nl/\u223cvannoord/TextCat/\nCitation Context: Each webpage is stripped of HTML tags, tokenized, and sentence segmented. Each docu-ment is language identified by the software TextCat [URL_CITE] which implements the paper by Cavnar and Trenkle (1994). We retain only English documents.\nFootnote or Reference text (if exists): 4 http://odur.let.rug.nl/\u223cvannoord/TextCat/\nSection Titles (if exists):  ['4 Building Noun Similarity Lists', '4.1 Web Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/newspaper3k/\nCitation Context: 2. We download these web pages, remove excess markup and detect publication dates with the newspaper3k framework [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://pypi.org/project/newspaper3k/(cf. Pustejovsky et al. (2003)) and mentions using\nSection Titles (if exists):  ['from Common Crawl 2 and apply it to create the']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/brmson/dataset-factoid-webquestions\nCitation Context: WebQuestions [URL_CITE] (Berant et al., 2013b) is de-signed for answering questions from the Free-base knowledge base, which is built by crawl-ing questions through the Google Suggest API and the paragraphs are retrieved from the English Wikipedia using .\nFootnote or Reference text (if exists): 5 https://github.com/brmson/ dataset-factoid-webquestions\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: We use spaCy [URL_CITE] to get the meta dependency paths of sentences in a document. Following Yao et al.\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dmoz.org/\nCitation Context: We set up a spider to download roughly 70 million web pages from the Internet. Initially, we use the links from Open Directory project [URL_CITE] as seed links for our spider. Each webpage is stripped of HTML tags, tokenized, and sentence segmented.\nFootnote or Reference text (if exists): 3 http://www.dmoz.org/\nSection Titles (if exists):  ['4 Building Noun Similarity Lists', '4.1 Web Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/fnlp\nCitation Context: We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) [URL_CITE] for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words and words occurring less than 3 times.\nFootnote or Reference text (if exists): 9 https://github.com/FudanNLP/fnlp\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~ark/TweetNLP/\nCitation Context: We preprocessed Twitter dataset with Twitter NLP tool [URL_CITE] (Gimpel et al., 2011; Owoputi et al., 2013) for tokenization. For Weibo dataset, we used NLPIR tool 10 (Zhang et al., 2003) for Chi-nese word segmentation.\nFootnote or Reference text (if exists): 9 http://www.cs.cmu.edu/\u02dcark/TweetNLP/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/parsing/preprocessing.html\nCitation Context: As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation. In addition, for each dataset, we maintain a vocab-ulary built based on the training set with removal of stop words [URL_CITE] and words occurring less than 3 times. The inputs of topic models x BoW are con-structed based on this vocabulary following com-mon topic model settings (Blei et al., 2003; Miao et al., 2016).\nFootnote or Reference text (if exists): 10 https://radimrehurek.com/gensim/parsing/preprocessing.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.6B.zip\nCitation Context: We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews datasets, we use pre-trained GloVe embed-dings (Pennington et al., 2014) [URL_CITE] . For Twitter and Weibo datasets, we pre-train embeddings on large-scale external data with 99M tweets and 467M Weibo messages, respectively.\nFootnote or Reference text (if exists): 11 http://nlp.stanford.edu/data/glove.6B.zip (200d)\nSection Titles (if exists):  ['3 Experiment Setup', '3.2 Model Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nCitation Context: We randomly split each of the two datasets into two portions, where 90% of the ut-terances are used for training the punctuation predic-tion models, and the remaining 10% for evaluating the prediction performance. For all the experiments, we use the default segmentation of Chinese as pro-vided, and English texts are preprocessed with the Penn Treebank tokenizer [URL_CITE] . We list the statistics of the two datasets after processing in Table 3.\nFootnote or Reference text (if exists): 2 http://www.cis.upenn.edu/\u223ctreebank/tokenization.html\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/utils.html\nCitation Context: 20% of randomly selected training instances are used to form development set. We preprocess our English datasets, i.e., Snippets, TagMyNews, and Twit-ter, with gensim tokenizer [URL_CITE] for tokenization. As to the Chinese Weibo dataset, we use FudanNLP toolkit (Qiu et al., 2013) for word segmentation.\nFootnote or Reference text (if exists): 8 https://radimrehurek.com/gensim/utils.html\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We trained Nile and tested both methods on the Japanese-English alignments provided with the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] (430k parallel sentences, 1074 manually aligned training sentences, and 120 manually aligned test sentences). As creating manual alignment data is costly, we also created two training sets that con-sisted of 1/4 and 1/16 of the total data to test if we can achieve an effect with smaller amounts of manually annotated data. The details of data size and alignment accuracy are shown in Table 3.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.cs.kuleuven.be/\u223civan.vulic/software/\nCitation Context: In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. [URL_CITE] . We have used 5 extra nouns with 20 sentences each as a development set to tune the parameters of our models.\nFootnote or Reference text (if exists): 1 Available at http://people.cs.kuleuven.be/\u223civan.vulic/software/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [URL_CITE] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlpado.de/~sebastian/data.shtml\nCitation Context: Our projection on the English side results in 53K V and 35K T sentences, of which 731 are labeled as both T and V. Finally, from the English labeled sentences we ex-tracted a training set with 72 novels (63K sentences) and a test set with 21 novels (15K sentences). [URL_CITE]\nFootnote or Reference text (if exists): 4 The corpus can be downloaded for research purposes from http://www.nlpado.de/~sebastian/data.shtml.\nSection Titles (if exists):  ['2 A Parallel Corpus of Literary Texts', '2.3 T/V Gold Labels for English Utterances']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.org/Data/QA/QC/\nCitation Context: \u2022 TREC: A dataset for classifying types of ques-tions with 6 classes [URL_CITE] .\nFootnote or Reference text (if exists): 5 http://cogcomp.org/Data/QA/QC/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-split\nCitation Context: \u2022 Public release of the English WikiSplit dataset, containing one million rewrites: [URL_CITE] http://goo.gl/language/wiki-split\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nCitation Context: \u2022 We use Comet\u2019s Bayes algorithm [URL_CITE] , which bal- Learning rate\nFootnote or Reference text (if exists): 6 https://www.comet.ml/docs/python-sdk/introduction-optimizer/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://search.cpan.org/~snowhare/Lingua-Stem-0.83\nCitation Context: \u2022 All above features and their stems. [URL_CITE] (All-Words-Stemmed)\nFootnote or Reference text (if exists): 7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83.\nSection Titles (if exists):  ['6 Local Lexical Context']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://scikit-learn.org/stable/\nCitation Context: \u2022 Logistic regression classifier: [URL_CITE] http://scikit-learn.org/stable/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: We focused on small datasets, as they are the most affected by cover-age mistakes. We use the IWSLT 2014 corpus for D E -E N , the KFTT corpus for J A -E N (Neu-big, 2011) [URL_CITE] , and the WMT 2016 dataset for R O - E N . The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt08/shared-task.html\nCitation Context: This PT was obtained processing the training data of the English-Spanish Europarl corpus used in the WMT 2008 shared task [URL_CITE] . We used a 2,000 sentence test set of the same shared evaluation for experi-menting with the querying phase.\nFootnote or Reference text (if exists): 5 http://www.statmt.org/wmt08/shared-task.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.statmt.org/wmt14/translation-task.html\nCitation Context: 4 We use WMT14 EN-DE translation dataset as the EN-DE sentence-level parallel dataset which consists of 4.4M sentence pairs. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://www.statmt.org/wmt14/translation-task.html\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/resources/\nCitation Context: Table 1 shows the distribution over the stance labels, [URL_CITE] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.\nFootnote or Reference text (if exists): 7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/sls/downloads/\nCitation Context: Table 1 shows the distribution over the stance labels, [URL_CITE] which turns out to be very similar to that for the FNC dataset. We can see that there are very few documents disagreeing with true claims (about 0.5%), which suggests that stance is pos-itively correlated with factuality. However, the number of documents agreeing with false docu-ments is larger than the number of documents dis-agreeing with them, which illustrates one of the main challenges when trying to predict the factu-ality of news based on stance.\nFootnote or Reference text (if exists): 7 The corpus is available at http://groups.csail.mit.edu/sls/downloads/ and also at http://alt.qcri.org/resources/\nSection Titles (if exists):  ['3 The Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.snopes.com/fact-check-ratings/\nCitation Context: We collected 1,000 suitable tweets as In-put claims, and we paired them with the corre-sponding claim that the page is about as the Ver-Claim claim. We further extracted from the article its Title, and the TruthValue of the Input claim (a rating of the claims assigned from Snopes [URL_CITE] ).\nFootnote or Reference text (if exists): 13 http://www.snopes.com/fact-check-ratings/\nSection Titles (if exists):  ['4 Datasets', '4.2 Snopes Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: Progress on the aforementioned tasks has bene-fited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently con-ducted Fake News Challenge (Pomerleau and Rao, 2017) [URL_CITE] with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 as-sociated news articles which is orders of magni-tude smaller than those used for TE and QA.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge [URL_CITE] , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al.\nFootnote or Reference text (if exists): 2 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://fakenewschallenge.org/\nCitation Context: The Fake News challenge (Pomerleau and Rao, 2017) [URL_CITE] modelled verification as stance classifica-tion: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, com-bining 300 claims with 2,582 articles.\nFootnote or Reference text (if exists): Dean Pomerleau and Delip Rao. 2017. Fake news chal-lenge. http://fakenewschallenge.org/.\nSection Titles (if exists):  ['2 Related Works']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://evalai.cloudcv.org/web/challenges/challenge-page/689/leaderboard/1908\nCitation Context: The recently proposed KILT benchmark (Petroni et al., 2020), which compares retrieval-augmented models across a variety of knowledge-intensive tasks including ELI5, automatically evaluates LFQA models by the quality of both generated an-swers (ROUGE-L against reference answers) and retrieved documents (R-precision against human-annotated relevant documents). In this paper, we build a state-of-the-art system [URL_CITE] for ELI5 by using a sparse Transformer variant (Roy et al., 2020) to condition over Wikipedia paragraphs returned by a REALM-style retriever (Guu et al., 2020).\nFootnote or Reference text (if exists): 2 State-of-the-art as of April 3, 2021 \u2014 the \u201cGoogle Research & UMass Amherst\u201d team entry on https: //evalai.cloudcv.org/web/challenges/ challenge-page/689/leaderboard/1908\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepset-ai/FARM\nCitation Context: We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [URL_CITE] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/deepset-ai/FARM\nSection Titles (if exists):  ['3 Language models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909\nCitation Context: To compare the same systems on shorter length outputs, we also tried finetuning the pretrained model on Wizard of Wikipedia (Dinan et al., 2019), an unconstrained dialogue generation task with single sentence dialogues (much shorter than ELI5). As seen on the public KILT leaderboard, [URL_CITE] our system has lower ROUGE-L scores than the BART / RAG baselines. Another possible explanation is issues with ROUGE-L itself, as discussed in Section 3.3.\nFootnote or Reference text (if exists): 12 https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909\nSection Titles (if exists):  ['3 Analysis', '3.1 Are generations grounded in retrieval?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: One of our contributions is scaling up a distantly supervised objective for training retrievers on ELI5, originally described in Jernite (2020) [URL_CITE] . This method uses in-batch negative sampling, making mini-batch size a critical hyperparameter for better con-strastive learning.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['7:453\u2013466.', 'A.2 Ablation Study of C -REALM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: Since the ELI5 dataset does not include gold retrievals, we train our retriever by scaling up a method recently introduced by Jernite (2020) [URL_CITE] that uses gold answers for distant supervision. The key idea is to push the encoded vector for a ques-tion close to a vector representation of its ground-truth answer(s), but away from all other answer vectors in the mini-batch (negative examples).\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system', '2.1 Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://yjernite.github.io/lfqa.html\nCitation Context: In this section, we describe our proposed LFQA system, which conditions answer generation on Wikipedia articles identified by a pretrained re-triever. We use a dense retriever trained by scaling up a distantly supervised algorithm from Jernite (2020) [URL_CITE] . Since retrieved articles can be quite long and often exceed the maximum sequence length of pretrained models like BERT (Devlin et al., 2019), we use a sparse-attention variant of the Transformer to allow modeling over longer sequences.\nFootnote or Reference text (if exists): Yacine Jernite. 2020. Explain anything like i\u2019m five: A model for open domain long form question answer-ing. https://yjernite.github.io/lfqa.html.\nSection Titles (if exists):  ['2 A state-of-the-art LFQA system']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2008/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlpir.nist.gov/projects/duc/\nCitation Context: English document summarization experiments are run using the summarization test collection provided in the framework of the Document Understanding Con-ference (DUC). In particular, we use the data set of 567 news articles made available during the DUC 2002 evaluations (DUC, 2002) [URL_CITE] , and the correspond-ing 100-word summaries generated for each of these documents. This is the single document summariza-tion task undertaken by other systems participating in\nFootnote or Reference text (if exists): DUC. 2002. Document understanding conference 2002. http://www-nlpir.nist.gov/projects/duc/.\nSection Titles (if exists):  ['3 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/jason/20Newsgroups/\nCitation Context: Text classification. We used the 20NewsGroup [URL_CITE] dataset, which consists of about 20000 docu-ments. Our goal is to classify each document into one of the 20 different newsgroups based on its content.\nFootnote or Reference text (if exists): 6 http://qwone.com/jason/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Setup', '4.5 Downstream NLP Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://simple.wikipedia.org\nCitation Context: Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [URL_CITE] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each.\nFootnote or Reference text (if exists): 10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com\nSection Titles (if exists):  ['5 Experiment Details', 'Details of Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://newsela.com\nCitation Context: Hence, we decided to create our own. Our dataset consists of a diverse set of 30 texts, from Simple English Wikipedia (10 articles), En-glish Wikipedia (8 articles), and online news ar-ticles (12 articles) [URL_CITE] . We did not wish to over-burden the readers, so we kept the size of texts to approximately 200 words each.\nFootnote or Reference text (if exists): 10 The sources for the articles were https://simple.wikipedia.org, https://en.wikipedia.org, and https://newsela.com\nSection Titles (if exists):  ['5 Experiment Details', 'Details of Texts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://tac.nist.gov/2009/Summarization/\nCitation Context: Datasets We use two multi-document summa-rization datasets from the Text Analysis Confer-ence (TAC) shared task: TAC-2008 and TAC-2009. [URL_CITE] TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words.\nFootnote or Reference text (if exists): 1 http://tac.nist.gov/2009/Summarization/, http://tac.nist.gov/2008/Summarization/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/liblinear/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://liblinear.bwaldvogel.de/\nCitation Context: (2013), we do not use any weight-ing in the training if a substitute has been supplied by many annotators, as we could not observe any improve-ments. Additionally, we use logistic regression (Fan et al., 2008) as classifier [URL_CITE] .\nFootnote or Reference text (if exists): 2 We use a Java port of LIBLINEAR (http://www.csie.ntu.edu.tw/\u02dccjlin/liblinear/) available from http://liblinear.bwaldvogel.de/\nSection Titles (if exists):  ['3 Method']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/maxparser/\nCitation Context: Self-Training: Following Reichart and Rap-poport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5 [URL_CITE] http://sourceforge.net/projects/maxparser/ add the entire auto-parsed trees to the man-ually labeled data in a single step without checking their parsing quality, and retrain the parser.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Experiments', '3.2 Comparison Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/treex\nCitation Context: Deep-syntax annotation of sentences in the training set is needed to train the sentence plan-ner, but we assume automatic annotation and reuse an existing deep-syntactic analyzer from the Treex NLP framework (Popel and \u017dabokrtsk\u00fd, 2010). [URL_CITE]\nFootnote or Reference text (if exists): 2 See http://ufal.mff.cuni.cz/treex. Domain-independent deep syntax analysis for several languages is included in this framework; the English pipeline used here involves a statistical part-of-speech tagger (Spoustov\u00e1 et al., 2007) and a dependency parser (McDonald et al., 2005), fol-lowed by a rule-based conversion to deep syntax trees.\nSection Titles (if exists):  ['2 Generator Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml\nCitation Context: Since the constraint type is determined by syntactic and dependency parse features, this component does not need to be retrained for new domains. In this work, we trained this classifier based on a manually annotated set of 80 sentences describing classes in the small UCI Zoo dataset (Lichman, 2013) [URL_CITE] , and used this model for all experiments.\nFootnote or Reference text (if exists): M. Lichman. 2013. UCI machine learning repository. http://archive.ics.uci.edu/ml.\nSection Titles (if exists):  ['3 Learning Classifiers from Language', '3.1 Mapping language to constraints', '3.1.1 Semantic Parser components']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-dl/SearchQA\nCitation Context: SearchQA [URL_CITE] (Dunn et al., 2017) is a large-scale open domain question answering dataset, which consists of question-answer pairs crawled from J! Archive, and the paragraphs are obtained by retrieving 50 webpages for each question from Google Search API.\nFootnote or Reference text (if exists): 2 https://github.com/nyu-dl/SearchQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nCitation Context: TrecQA [URL_CITE] is a answer sentence selection dataset designed for the open-domain question answering setting. We use the raw version TrecQA, questions with no answers or with only positive/negative an-swers are included.\nFootnote or Reference text (if exists): 5 https://github.com/castorini/NCE-CNN-Torch/tree/master/data/TrecQA\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/document-qa\nCitation Context: Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koc\u030cisky\u0301 et al., 2018). Using the same preprocessing as Clark and Gardner (2018) for TriviaQA-Wiki [URL_CITE] , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al.\nFootnote or Reference text (if exists): 6 https://github.com/allenai/document-qa\nSection Titles (if exists):  ['5 Experiments', '5.1 Data and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.microsoft.com/en-us/download/details.aspx?id=52419\nCitation Context: WikiQA [URL_CITE] is a retrieval-based question answer-ing dataset based on Wikipedia. We follow the data split of original paper.\nFootnote or Reference text (if exists): 6 https://www.microsoft.com/en-us/download/details.aspx?id=52419\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/moses/\nCitation Context: Our SMT implementation is based on Moses [URL_CITE] , and we use the KenLM (Heafield et al., 2013) tool included in it to estimate our 5-gram language model with modified Kneser-Ney smoothing. Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint refinement procedure.\nFootnote or Reference text (if exists): 10 http://www.statmt.org/moses/\nSection Titles (if exists):  ['5 Experiments and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yangkevin2/emnlp2020-stream-beam-mt\nCitation Context: Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [URL_CITE] In principle, our method can be applied to any task which sequen-tially processes variable-length data.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/thebeast\nCitation Context: MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich re-lations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast [URL_CITE] to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP infer-ence for Markov logic.\nFootnote or Reference text (if exists): 5 http://code.google.com/p/thebeast\nSection Titles (if exists):  ['4 Models for Bridging Resolution', '4.2 Markov Logic Networks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ivri/dmorph\nCitation Context: (2017) against the reported performance of their encoder-decoder model. [URL_CITE] Notice Vylomova et al. (2017) show that providing the LSTM with the POS of the deriva-tive increases performance.\nFootnote or Reference text (if exists): 8 The dataset is available at https://github.com/ivri/dmorph. While Vylomova et al. (2017) take morpho-orthographic changes into account, we only predict affixes, not the accompanying changes in orthography (Section 4.1).\nSection Titles (if exists):  ['4 Experiments', '4.3 Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://kheafield.com/code/kenlm/\nCitation Context: This pruning is done efficiently through a varia-tion of cube-pruning (Chiang, 2007). We use KenLM [URL_CITE] (Heafield, 2011) for computing the target language model score. Decoding is made more efficient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heafield et al., 2011)) and rest-cost estima-tions(Heafield et al., 2012).\nFootnote or Reference text (if exists): 3 http://kheafield.com/code/kenlm/\nSection Titles (if exists):  ['4 Decoding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use the 100-dimensional GloVe vectors [URL_CITE] (Pen-nington et al., 2014) to initialize our word embed-dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well.\nFootnote or Reference text (if exists): 3 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['4 Experiments', '4.1 Answer Selection', '4.1.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/word2vec/\nCitation Context: We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em-beddings to initialize the word embedding ma-trices of the neural networks. English and Chi-nese Wikipedia documents were used to train the word embeddings by Word2Vec tool [URL_CITE] proposed in (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 3 Available at http://code.google.com/p/word2vec/ Why does unsupervised pre-training help deep\nSection Titles (if exists):  ['3 Experiments', '3.2 Training Strategy']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2016-supersense-embeddings\nCitation Context: \u2022 We are the first to provide a joint word-and supersense-embedding model, which we make publicly available [URL_CITE] for the research com-munity. This provides an insight into the word and supersense positions in the vector space\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2016-supersense-embeddings\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [URL_CITE] (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.\nSection Titles (if exists):  ['3 Neural Readability Ranking Model for Words and Phrases', '3.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nCitation Context: We use the open-domain word embeddings [URL_CITE] for the initialization of word vectors. We initialize other model parameters from a uniform distribu-tion U(-0.05, 0.05).\nFootnote or Reference text (if exists): 1 https://github.com/mmihaltz/word2vec-GoogleNews-vectors\nSection Titles (if exists):  ['6 Experiments', '6.3 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhanglu-cst/ClassKG\nCitation Context: Our code has already been released. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/zhanglu-cst/ ClassKG\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/liushulinle/event\nCitation Context: We publish our code for further study by the NLP community. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/liushulinle/event detection without triggers\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://aws.amazon.com/translate\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, [URL_CITE] (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 3 https://aws.amazon.com/translate\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.bing.com/translator\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, [URL_CITE] (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 2 https://www.bing.com/translator\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://translate.google.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, [URL_CITE] (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 1 https://translate.google.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://matrix.statmt.org/matrix\nCitation Context: Table 4 shows the overall results on 12 transla-tion directions. We also provide the results from WMT17 winning systems [URL_CITE] . Notice that unlike the Transformer and our model, these winner systems typically use model ensemble, system combina-tion and large-scale monolingual corpus.\nFootnote or Reference text (if exists): 4 http://matrix.statmt.org/matrix\nSection Titles (if exists):  ['5 Experiments', '5.2 WMT17 Translation Tasks', '5.2.1 Translation Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gabrielStanovsky/mt_gender\nCitation Context: We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod-els are significantly prone to translate based on gender stereotypes rather than more mean-ingful context. Our data and code are pub-licly available at [URL_CITE] https://github.com/gabrielStanovsky/mt_gender.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.systransoft.com\nCitation Context: MT systems We test six widely used MT mod-els, representing the state of the art in both commercial and academic research: (1) Google Translate, (2) Microsoft Translator, (3) Amazon Translate, (4) SYSTRAN, [URL_CITE] (5) the model of Ott et al. (2018), which recently achieved the best per-formance on English-to-French translation on the WMT\u201914 test set, and (6) the model of Edunov et al.\nFootnote or Reference text (if exists): 4 http://www.systransoft.com\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh\nCitation Context: Node Number Terms C18.452.297 diabetes mellitus insulin-dependent diabetes C18.452.297.267 mellitus C18.452.297.267.960 wolfram syndrome Table 1. Subtree of MeSH [URL_CITE] tree. Node numbers represent hierarchical structure of terms\nFootnote or Reference text (if exists): 1 MeSH is available at http://www.nlm.nih.gov/mesh. MeSH 2003 was used in this research.\nSection Titles (if exists):  ['1. Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: The MeSH layer contains entities from the hier-archical medical ontology MeSH (Medical Subject Headings). [URL_CITE] The MeSH annotations on Figure 1 are overlapping (share the word cell) and hierarchical both ways: spanning, since blood cell (with MeSH id D001773) orthographically spans the word cell (id A11), and ontologically, since blood cell is a kind of cell and cell death (id D016923) is a kind of Bio-logical Phenomena.\nFootnote or Reference text (if exists): 8 http://www.nlm.nih.gov/mesh/meshhome.html\nSection Titles (if exists):  ['3 The Layered Query Language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov\nCitation Context: \u201cmetabolic diseases(C18.452)\u201d node is root of the subtree, and the subtree consists of 436 disease names which are target terms of specificity measuring. A set of journal abstracts was extracted from MEDLINE [URL_CITE] database using the disease names as quires. Therefore, all the abstracts are related to some of the disease names.\nFootnote or Reference text (if exists): 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine, USA. (http://www.nlm.nih.gov)\nSection Titles (if exists):  ['4. Experiment and Evaluation', '4.1. Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh\nCitation Context: In this paper we use the MeSH (Medical Subject Head-ings) lexical hierarchy [URL_CITE] , but the approach should be equally applicable to other domains using other thesauri and ontologies. In MeSH, each concept is assigned one or more alphanumeric descriptor codes corresponding to particular positions in the hierarchy.\nFootnote or Reference text (if exists): 1 http://www.nlm.nih.gov/mesh\nSection Titles (if exists):  ['2 MeSH and Medline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/\nCitation Context: Most of the labels [URL_CITE] are self-explanatory: Pa-tient (PAT), Actor (ACT), Time (TWHEN), Effect (EFF), Location (LOC), Manner (MANN), Ad-dressee (ADDR), Extent (EXT). CPHR marks the nominal part of a complex predicate, as in \u201cto have [a plan] CPHR \u201d, and DIR3 indicates destination.\nFootnote or Reference text (if exists): 2 http://ufal.mff.cuni.cz/ \u223c toman/pcedt/en/functors.html\nSection Titles (if exists):  ['5 Results', '5.2 Argument Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/mesh/meshhome.html\nCitation Context: These nodes have children, for example, Abdomen (A01.047) and Back (A01.176) are level [URL_CITE] children of Body Regions. The longer the ID of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous\nFootnote or Reference text (if exists): 1 http://www.nlm.nih.gov/mesh/meshhome.html; the work reported in this paper uses MeSH 2001.\nSection Titles (if exists):  ['3 The Lexical Hierarchy: MeSH']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dbamman/litbank\nCitation Context: In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them. The dataset is freely available for down-load under a Creative Commons ShareAlike 4.0 li-cense at [URL_CITE] https://github.com/dbamman/litbank.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.bethard.info/data/fables-100-temporal-dependency.xml\nCitation Context: Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyre-Lapata collection and measured inter-annotator agree-ment by Krippendorff\u2019s Alpha for nominal data (Krip-pendorff, 2004; Hayes and Krippendorff, 2007). For the resulting annotated corpus annotators achieved Alpha of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering rela-tion labels. Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables [URL_CITE] could be used to train a temporal dependency parsing model.\nFootnote or Reference text (if exists): 2 Available from http://www.bethard.info/data/fables-100-temporal-dependency.xml\nSection Titles (if exists):  ['3 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/trangvu/alil-dream\nCitation Context: Our contribution are as follows: (i) we propose a sample-efficient AL policy learning method to make the best use of the annotation budget to im-prove both the student learner and the AL policy directly on the target task of interest; (ii) we pro-vide comprehensive experimental results compar-ing our method to strong heuristic-based and data-driven AL query strategy learning-based methods on cross-lingual and cross-domain text classifica-tion, and cross annotation scheme named entity recognition tasks [URL_CITE] . The experiment results demon-strate the ability of our method to quickly learn a good policy directly on the task of interest. Com-pared to the previous work (Fang et al., 2017; Liu et al., 2018a) which transfers a policy learned on a source task to target task, our dream-based AL query policies are consistently more effective even when the data domain and annotation scheme of target task are different from the source task.\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/trangvu/alil-dream\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf\nCitation Context: We present in this work a new dataset of en-tity annotations for a wide sample of 210,532 to-kens from 100 literary texts to help address these issues and help advance computational work on literature. These annotations follow the guide-lines set forth by the ACE 2005 entity tagging task (LDC, 2005) [URL_CITE] in labeling all nominal entities (named and common alike), including those with nested structure. In evaluating the stylistic dif-ference between the texts in ACE 2005 (primar-ily news) and the literary texts in our new dataset, we find considerably more attention dedicated to people and settings in literature; this attention di-rectly translates into substantially improved accu-racies for those classes when models are trained on them.\nFootnote or Reference text (if exists): LDC. 2005. Ace (automatic content extrac-tion) English annotation guidelines for enti-ties. https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-entities-guidelines-v5.6.6.pdf.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.answers.com\nCitation Context: For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [URL_CITE] (Fader et al., 2014). The Yahoo!\nFootnote or Reference text (if exists): 5 http://wiki.answers.com\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers [URL_CITE] and a crawl of WikiAnswers (Fader et al., 2014). The Yahoo!\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We used the machine learning package MALLET (McCallum, 2002) [URL_CITE] to conduct the CRF model training and labeling.\nFootnote or Reference text (if exists): McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tensorflow_constrained_optimization\nCitation Context: Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/google-research/ tensorflow_constrained_optimization\nSection Titles (if exists):  ['3 Proposed Approaches', '3.2 Bias-constrained Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lpsolve.sourceforge.net/\nCitation Context: We used lp solve [URL_CITE] to solve our ILP optimization problems. We ran experiments on two datasets.\nFootnote or Reference text (if exists): 3 From http://lpsolve.sourceforge.net/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dcferreira/multilingual-joint-embeddings\nCitation Context: We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org/wsd\nCitation Context: A live demo featuring several dis-ambiguation models is available online. [URL_CITE]\nFootnote or Reference text (if exists): 2 http://jobimtext.org/wsd\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hiaoxui/D2T-Grounding\nCitation Context: Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/2700406\nCitation Context: It seems unclear that this is a good simulation of the kind of shift in distribution that one is likely to en-counter in practice. An exception to this is Esuli and Sebastiani (2015) [URL_CITE] , who test their method on the RCV1-v2 corpus, also splitting by time. They perform a large number of experiments, but un-fortunately, nearly all of their experiments involve only a very small difference in label proportions between the source and target (with the vast ma-jority < 0.01), which limits the generalizability of their findings.\nFootnote or Reference text (if exists): Andrea Esuli and Fabrizio Sebastiani. 2015. Optimiz-ing text quantifiers for multivariate loss functions. ACM Trans. Knowl. Discov. Data 9(4). https: //doi.org/10.1145/2700406.\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/m-amr2text\nCitation Context: Our pretrained mod-els are available for download. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://github.com/facebookresearch/ m-amr2text\nSection Titles (if exists):  ['4 Experimental Setting', '4.2 Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nCitation Context: Replication Our code (part of the TextGrounder system), our processed version of Wikipedia, and in-structions for replicating our experiments are avail-able on the TextGrounder website. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://code.google.com/p/textgrounder/wiki/WingBaldridge2011\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/\nCitation Context: LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word em-beddings based on LSA [URL_CITE] . The vocabulary size is 100,000.\nFootnote or Reference text (if exists): 5 http://www.lingexp.uni-tuebingen.de/z2/LSAspaces/\nSection Titles (if exists):  ['5 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-atomic-edits\nCitation Context: Our full corpus is available for download at [URL_CITE] http: //goo.gl/language/wiki-atomic-edits . The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Description of Data Release']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google/sentencepiece\nCitation Context: All experiments are performed with the Trans-former architecture (Vaswani et al., 2017) using the open-source Tensorflow-Lingvo implementa-tion (Shen et al., 2019). Specifically, we use the Transformer Big model containing 375M parame-ters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) [URL_CITE] (Kudo and Richardson, 2018). We use a vocabulary size of 32k for the bilingual models and 64k for the multilingual mod-els.\nFootnote or Reference text (if exists): 3 https://github.com/google/sentencepiece\nSection Titles (if exists):  ['3 Experimental Setup', '3.3 Architecture and Optimization']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://joshua-decoder.com/language-packs/paraphrase/\nCitation Context: Several different size language packs are available for download. [URL_CITE] The components of the language packs are described below. Grammar Our approach to sentential paraphras-ing is analogous to machine translation.\nFootnote or Reference text (if exists): 1 http://joshua-decoder.com/language-packs/paraphrase/\nSection Titles (if exists):  ['2 Language pack description']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt/\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): 6 http://www.phontron.com/kftt/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf\nCitation Context: There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1\u20135 for accuracy and fluency [URL_CITE] . We used 4 human judges, each judging 25 sentence pairs.\nFootnote or Reference text (if exists): 3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.\nSection Titles (if exists):  ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf\nCitation Context: The 769 sentences were translated by using five commercial MT systems to investigate the relation-ship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy. The instruction for the subjective evaluation based on fluency and ad-equacy followed that given in the TIDES specifi-cation (TIDES, 2002) [URL_CITE] . The subjective evaluation based on yes/no questions was done by manually answering each question for each translation.\nFootnote or Reference text (if exists): TIDES. 2002. Linguistic Data Annotation Specifi-cation: Assessment of Fluency and Adequacy in Arabic-English and Chinese-English Translations. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf.\nSection Titles (if exists):  ['2 Test Set for Evaluating Machine Translation Quality', '2.1 Test Set']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/jweb1t/\nCitation Context: We use the Google Web1T to generate n-gram features as we expect this open domain resource to have consid-erable coverage for most specific domains as well. For accessing the resource, we use JWeb1T [URL_CITE] (Giuliano et al., 2007).\nFootnote or Reference text (if exists): 5 https://code.google.com/p/jweb1t/\nSection Titles (if exists):  ['4 Resources', '4.3 Google Web1T']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/\nCitation Context: These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) [URL_CITE] and law, medical OPUS datasets. For this setting, we con-sider two language pairs, namely Czech and Ger-man to English.\nFootnote or Reference text (if exists): Kevin Duh. 2018. The multitarget ted talks task. http://www.cs.jhu.edu/\u02dckevinduh/a/multitarget-tedtalks/.\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/\nCitation Context: Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia [URL_CITE] , English Gigaword (Parker et al., 2011) and Newscrawl as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task.\nFootnote or Reference text (if exists): 5 https://en.wikipedia.org/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nCitation Context: In the third experiment denoted by \u2019\u201cWSJ-to- G\u201d, we used WSJ corpus as the source domain and Genia corpus (G) [URL_CITE] as the target domain. Following Plank and van Noord (2011), we used the train-ing data in CoNLL 2008 shared task (Surdeanu et al., 2008) which are also from WSJ sections 2-21 but converted into dependency structure by the LTH converter (Johansson and Nugues, 2007).\nFootnote or Reference text (if exists): 4 Genia distribution in Penn Treebank format is avail-able at http://bllip.cs.brown.edu/download/genia1.0-division-rel1.tar.gz\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and Tools']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: Except the sythetic data, all the data can be found at the website of the BEA-19 shared task. The synthetic data is generated from English Wikipedia , English Gigaword (Parker et al., 2011) and Newscrawl [URL_CITE] as the previous work (Ge et al., 2018a; Zhang et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019) did, using back trans-lation and sentence corruption. Specifically, we train a transformer (base) model (Vaswani et al., 2017) for back translation using the training data of the restricted track in the BEA-19 shared task.\nFootnote or Reference text (if exists): 6 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/XinnuoXu/\nCitation Context: For further analysis and studies, we re-domains. However, GPT-2 is pre-trained on Open- lease the data from all intermediate steps for WebText and the language style and topics thereof each domain at [URL_CITE] https://github.com/XinnuoXu/ are quite different from those of daily conversa- AugNLG/tree/master/augmented_data .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Mixed\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/yamcha/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/TinySVM/\nCitation Context: During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM [URL_CITE] ,and trained the SVMs classifiers using a third-order polynomial kernel. The other parameters are set to default.\nFootnote or Reference text (if exists): 6 http://chasen.org/\u02dctaku-ku/software/TinySVM/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku-ku/software/yamcha/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger (Tsuruoka and Tsujii, 2005) and YamCha chunker [URL_CITE] (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 5 http://chasen.org/\u02dctaku-ku/software/yamcha/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/postagger/\nCitation Context: During the training phase, we used intact POS and chunk tags . During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka\u2019s tagger [URL_CITE] (Tsuruoka and Tsujii, 2005) and YamCha chunker (Kudo and Matsumoto, 2001). We used an SVMs package, TinySVM ,and trained the SVMs classifiers using a third-order polynomial kernel.\nFootnote or Reference text (if exists): 4 http://www-tsujii.is.s.u-tokyo.ac.jp/\u02dctsuruoka/postagger/\nSection Titles (if exists):  ['3 Minimally Lexicalized Dependency Parsing', '3.1 Base Dependency Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.chasen.org/~taku/software/yamcha/\nCitation Context: There were 33 NE classes (eight categories * four chunk-ing states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + x~ \u00b7 y~) [URL_CITE] and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s n (x) with \u03b2 n =1.0 and Viterbi search to the SVMs\u2019 outputs. These parameters were exper-imentally chosen using the test set.\nFootnote or Reference text (if exists): 2 http://www.chasen.org/\u02dctaku/software/yamcha/\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chasen.org/~taku/software/TinySVM/\nCitation Context: The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classi-fication decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM [URL_CITE] sup-port vector machine (SVM) implementation in conjunction with YamCha 7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking.\nFootnote or Reference text (if exists): 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/\nSection Titles (if exists):  ['6 Classifier Parameters']"
                },
                {
                    "role": "assistant",
                    "content": "This is the same citation context as the first one. The answer remains the same:\n\n ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/moving-qatar/posts/can-i-obtain-driving-license-my-qid-written-employee\nCitation Context: A simplified example is shown in Figure 1, [URL_CITE] where answers 2 and 4 are good, answer 1 is po-tentially useful, and answer 3 is bad. In this paper, we focus on a 2-class variant of the above Sub-task A, which is closer to a real CQA application.\nFootnote or Reference text (if exists): 3 http://www.qatarliving.com/moving-qatar/posts/can-i- obtain-driving-license-my-qid-written-employee\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://stackoverflow.com/\nCitation Context: A typical example for CQA is shown in Table 1. In this example, Answer [URL_CITE] is a good answer, be-cause it provides helpful information, e.g., \u201ccheck it to the traffic dept\u201d. Although Answer 2 is rele-vant to the question, it does not contain any useful information so that it should be regarded as a bad answer.\nFootnote or Reference text (if exists): 1 https://stackoverflow.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.qatarliving.com/forum\nCitation Context: We use the CQA-QL corpus from Subtask A of SemEval-2015 Task 3 on Answer Selection in CQA. The corpus contains data from the Qatar Living forum, [URL_CITE] and is publicly available on the task\u2019s website. The dataset consists of ques-tions and a list of answers for each question, i.e., question-answer threads.\nFootnote or Reference text (if exists): 1 http://www.qatarliving.com/forum\nSection Titles (if exists):  ['2 The Task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.fakenewschallenge.org/\nCitation Context: While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-2016 Task 6 (Mohammad et al., 2016) and from the Fake News Challenge (FNC). [URL_CITE] De-spite its name, the latter has no annotations for fac-tuality, but consists of article-claim pairs labeled for stance: agrees, disagrees, discusses, and unre-lated. In contrast, we retrieve documents for each claim, which yields an arguably more natural dis-tribution of stance labels compared to FNC.\nFootnote or Reference text (if exists): 3 http://www.fakenewschallenge.org/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackoverflow.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora , StackOverflow [URL_CITE] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 4 http://stackoverflow.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cqa.iyas.qcri.org/cQA-Arabic-Demo\nCitation Context: A second example is a cQA demo [URL_CITE] in Arabic, which retrieves data from multiple medical forums from middle-east. In this case physicians answer to patients\u2019 questions: the left panel shows a ques-tion from a user and the right panel similar ques-tions with the answers from the expert. In general there is only one (good) answer from the doctor, so this is mostly a question re-ranking task.\nFootnote or Reference text (if exists): 4 http://cqa.iyas.qcri.org/ cQA-Arabic-Demo\nSection Titles (if exists):  ['5 Software Package']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/research/umls\nCitation Context: To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS [URL_CITE] meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i .\nFootnote or Reference text (if exists): 6 http://www.nlm.nih.gov/research/umls\nSection Titles (if exists):  ['3 Linguistic Resources', '3.3 Terminology Reference Lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freelang.net/dictionary\nCitation Context: These dictionaries provided high-quality manually edited word translations. In addition, we used large Eng-lish-Italian and English-Portuguese bilingual lexi-cons available from FreeLang site ( [URL_CITE] http://www.freelang.net/dictionary) as well as an English-Chinese bilingual word list available from LDC (Linguistic Data Consortium). Compiled without professional editing, these bilingual word lists contain errors and inaccurate translations, and hence they introduced noise into the mapping pro-cess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Generating Multilingual Semantic Lexi-cons by Automatic Mapping']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://meta.wikimedia.org/wiki/List_of_Wikipedias\nCitation Context: We used Wikipedia as the main data source for all our experiments. Multilingual BERT was trained on the 104 languages with the largest Wikipedias [URL_CITE] \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.\nFootnote or Reference text (if exists): 11 List of Wikipedias can be found in https://meta.wikimedia.org/wiki/List_of_Wikipedias\nSection Titles (if exists):  ['6 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: More and more studies utilize Wikidata [URL_CITE] as a knowledge resource due to its high quality and large quantity. We utilize the September 2019 En-glish dump in experiments.\nFootnote or Reference text (if exists): 1 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Dataset Design', '3.1 Data Preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/multilingual.md\nCitation Context: We used Wikipedia as the main data source for all our experiments. Multilingual BERT [URL_CITE] was trained on the 104 languages with the largest Wikipedias \u2014of these, we subsampled a diverse set of 18 for our experiments: Afrikaans, Arabic, Bengali, English, Estonian, Finnish, Hebrew, Indonesian, Icelandic, Kannada, Malayalam, Marathi, Persian, Portuguese, Tagalog, Turkish, Tatar, and Yoruba.\nFootnote or Reference text (if exists): 10 Information about multilingual BERT can be found in: https://github.com/google-research/bert/blob/master/multilingual.md\nSection Titles (if exists):  ['6 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elra.info/\nCitation Context: The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M0033 available from the ELRA catalogue [URL_CITE] . This re-source is a general language dictionary which con-tains only a few terms related to the medical do-main.\nFootnote or Reference text (if exists): 5 http://www.elra.info/\nSection Titles (if exists):  ['3 Linguistic Resources', '3.2 Bilingual Dictionary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.codeproject.com/KB/recipes/englishparsing.aspx\nCitation Context: Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the \u201copenNLP\u201d software (Northedge, 2005) [URL_CITE] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters.\nFootnote or Reference text (if exists): R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx\nSection Titles (if exists):  ['4 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~mbweb/\nCitation Context: We use the Toronto Book Corpus [URL_CITE] to train word embeddings. This corpus contains 74,004,228 already pre-processed sentences in total, which are made up of 1,057,070,918 tokens, originating from 7,087 unique books. In our experiments, we consider tokens appearing 5 times or more, which leads to a vocabulary of 315,643 words.\nFootnote or Reference text (if exists): 1 The corpus can be downloaded from http://www.cs.toronto.edu/\u02dcmbweb/;cf. (Zhu et al., 2015).\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: We obtain phrase-structure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model [URL_CITE] trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.\nFootnote or Reference text (if exists): 8 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.2 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py\nCitation Context: Text pre-processing For all the RoBERTa-based models, we used the RoBERTa roberta-base\u2019s tokenizer provided in the transformers library. [URL_CITE] We did not perform any additional pre-processing in our experiments.\nFootnote or Reference text (if exists): 13 https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_roberta.py.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mlg.ucd.ie/datasets/bbc.html\nCitation Context: We use the BBC corpus prepared by Greene and Cunningham (2006). [URL_CITE] Unlike the IMDB corpus, this corpus contains news articles which are almost always written in a formal style. By evaluating the pro-posed approaches on both the IMDB and BBC corpora, we can tell whether the benefits from larger context exist in both informal and formal languages.\nFootnote or Reference text (if exists): 2 http://mlg.ucd.ie/datasets/bbc.html\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\nCitation Context: Both with the IMDB and BBC corpora, we did not do any preprocessing other than tokenization. [URL_CITE] Penn Treebank We evaluate a normal recurrent language model, count-based n-gram language model as well as the proposed RLM-BoW-EF-n and RLM-BoW-LF-n with varying n = 1, 2, 4, 8 on the Penn Treebank Corpus. We preprocess the corpus according to (Mikolov et al., 2011) and use a vocabulary of 10k words from the training cor-pus.\nFootnote or Reference text (if exists): 3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Our analysis further suggests that the CLARE can be used to improve the robustness of the down-stream models, and improve their accuracy when the available training data is limited. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/declare-lab/MIME\nCitation Context: We demonstrate the importance of these factors to empathetic re-sponse generation using both automatic- and human-based evaluations. The implementa-tion of MIME is publicly available at [URL_CITE] https: //github.com/declare-lab/MIME .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py\nCitation Context: We present complete effectiveness graphs and details of human evaluation in Appendix B and C. BAE is implemented [URL_CITE] in TextAttack (Morris et al., 2020), a popular suite of NLP adversarial attacks.\nFootnote or Reference text (if exists): 1 https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/syxu828/CSRL_dataset\nCitation Context: Our proposed training objectives could also help the model to better learn predicate-aware token repre-sentations and structure-aware utterance represen-tations. Our code is publicly available at [URL_CITE] https: //github.com/syxu828/CSRL_dataset.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nCitation Context: With our work, we free CDCR research from depending on costly human-annotated training data and open up possibili-ties for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and model available at https://github.com/UKPLab/emnlp2021-hypercoref-cdcr\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cookielee77/CLARE\nCitation Context: Human eval-uation shows significant advantages of CLARE in terms of textual similarity, fluency and gram-maticality. We release our code and models at [URL_CITE] https://github.com/cookielee77/CLARE.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nclt.computing.dcu.ie/~jfoster/resources/\nCitation Context: Finally, we use a small number of sentences from the British National Corpus ( BNC ) (Foster and van Genabith, 2008). [URL_CITE] The sentences were chosen randomly, so each one is potentially from a different domain. On the other hand, BNC can be thought of as its own domain in that it contains significant lex-ical differences from the American English used in our other corpora.\nFootnote or Reference text (if exists): 10 http://nclt.computing.dcu.ie/\u02dcjfoster/resources/, downloaded January 8th, 2009.\nSection Titles (if exists):  ['6 Experiments', '6.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.elsevier.com\nCitation Context: Breast cancer corpus This comparable corpus is composed of documents collected from the Elsevier website [URL_CITE] . The documents were taken from the medical domain within the sub-domain of \u201cbreast cancer\u201d.\nFootnote or Reference text (if exists): 2 http://www.elsevier.com\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jobimtext.org\nCitation Context: The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexical-syntactic patterns from all corpora by applying PattaMaika [URL_CITE] , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).\nFootnote or Reference text (if exists): 2 http://jobimtext.org: The PattaMaika compo-nent is based on UIMA RUTA (Kluegl et al., 2016).\nSection Titles (if exists):  ['3 Taxonomy Refinement using Hyperbolic Word Embeddings', '3.1 Domain-specific Poincare\u0301 Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://manikvarma.org/downloads/XC/XMLRepository.html\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database and the rest five datasets are downloaded from the extreme classification repository [URL_CITE] . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 3 http://manikvarma.org/downloads/XC/XMLRepository.html\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: We use the IMDB Movie Review Corpus (IMDB) prepared by Maas et al. (2011). [URL_CITE] This cor-pus has 75k training reviews and 25k test reviews.\nFootnote or Reference text (if exists): 1 http://ai.stanford.edu/\u02dcamaas/data/sentiment/\nSection Titles (if exists):  ['5 Experimental Settings', '5.2 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://iwslt.org/\nCitation Context: Dataset statistics are summarized in Table 1. For the low-resource experiments, we use [URL_CITE] : English-Vietnamese from IWSLT\u201915, German-English from IWSLT\u201914, and French-English from IWSLT\u201916. For the rich-resource experiments, we use the English-German dataset from WMT\u201916, which contains about 4.5 million training samples.\nFootnote or Reference text (if exists): 4 https://iwslt.org/\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/~amaas/data/sentiment/\nCitation Context: 2. IMDB movie review dataset [URL_CITE] , a binary senti-ment classification dataset consisting of movie review comments with positive/negative senti-ment labels (Maas et al., 2011).\nFootnote or Reference text (if exists): 2 http://ai.stanford.edu/~amaas/data/sentiment/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2005T23\nCitation Context: To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0. [URL_CITE] We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2005T23\nSection Titles (if exists):  ['3 Experiments', '3.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.imdb.com/\nCitation Context: Dataset. Our experiments are conducted on six extreme multi-label datasets and their character-istics are shown in Table 1, among which IMDb is crawled from online movie database [URL_CITE] and the rest five datasets are downloaded from the extreme classification repository . For datasets from the repository, we adopt the provided train/test split, and for IMDb we randomly choose 20% of the data as test set and the rest of 80% as training set.\nFootnote or Reference text (if exists): 2 https://www.imdb.com/\nSection Titles (if exists):  ['3 Experiments & Analysis', '3.1 Dataset & Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gorokoba560/norm-analysis-of-transformer\nCitation Context: The codes of our experiments are pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/gorokoba560/ norm-analysis-of-transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nijianmo/recsysjustification.git\nCitation Context: Our data and code are available online. [URL_CITE]\nFootnote or Reference text (if exists): 8 https://github.com/nijianmo/recsysjustification.git\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/acl2017-non-factoid-qa\nCitation Context: The source-code of our system is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/UKPLab/ acl2017-non-factoid-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhanglu-cst/ClassKG\nCitation Context: Our code has already been released. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/zhanglu-cst/ ClassKG\nSection Titles (if exists):  ['4 Experiments', '4.3 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adhigunasurya/distillation_parser.git\nCitation Context: The code to reproduce our results is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/adhigunasurya/distillation_parser.git\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/java/\nCitation Context: We performed experiments on a set of 1449 questions from TREC-99-03. Using the search en-gine [URL_CITE] , we retrieved around 5 top-ranked candi-date sentences from a large newswire corpus for each question to compile around 7200 q/a pairs. We manually labeled each candidate sentence as true or false entailment depending on the contain-ment of the true answer string and soundness of the entailment to compile quality training set.\nFootnote or Reference text (if exists): 2 http://lucene.apache.org/java/\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nCitation Context: \u2022 Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset [URL_CITE] .\nFootnote or Reference text (if exists): 3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nCitation Context: This need arises from the fact that not every sentence in an article contains ex-plicit links to the entities it mentions, as Wikipedia guidelines only allow one link to another article in the article\u2019s text. [URL_CITE] The algorithm takes a sentence as input and iterates over n-grams that are not yet linked to an entity. If an n-gram matches a surface form of an entity, we establish a link between the n-gram and the entity.\nFootnote or Reference text (if exists): 2 http://en.Wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking\nSection Titles (if exists):  ['4 Explaining Entity Relationships', '4.1 Extracting candidate sentences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe [URL_CITE] (Pen-nington et al., 2014), and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): 8 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: We use as word representation the concatena-tion of word vectors initialized to GloVe (Pen-nington et al., 2014) [URL_CITE] , and randomly initialized pre-fix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al. (2016). All affixes ap-pearing less than two times in the training data are mapped to \u201cUNK\u201d.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Nat-ural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['6 Experiments', '6.1 English Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.google.com/apis\nCitation Context: Here, we give an example illustrating the prin-ciple of clustering web search results in the FGS. In submitting TREC 2004 test question 1.1 \u201dwhen was the first Crip gang started?\u201d to Google ( [URL_CITE] http://www.google.com/apis), we extract n(= 8) different candidates from the top m(= 30) Google snippets. The Google snippets containing the same candidates are aligned snippets, and thus the 12 re-tained snippets are grouped into 8 clusters, as listed in Table 2.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 The U-SVM', '3.1 Clustering Web Search Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/research/projects/tao/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wapiti.limsi.fr/\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti [URL_CITE] (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): 6 Wapiti can be found at http://wapiti.limsi.fr/. It was con-figured as follows: rprop algorithm, default L1-penalty value (0.5), default L2-penalty value (0.00001), default stopping cri-terion value (0.02%).\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~unitex\nCitation Context: In all exper-iments, we varied the set of features: endo are all en-dogenous features; coll and lex include all endoge-nous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) [URL_CITE] to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011).\nFootnote or Reference text (if exists): S. Paumier. 2011. Unitex 3.9 documentation. http://igm.univ-mlv.fr/\u02dcunitex.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nCitation Context: Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [URL_CITE] . This is similar to the removal of stop words widely used in information retrieval.\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nSection Titles (if exists):  ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mcs.anl.gov/petsc/\nCitation Context: The part-of-speech tagger used to extract POS features was lgtagger (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm [URL_CITE] as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 8 We used the following mathematical libraries PETSc et TAO, freely available at http://www.mcs.anl.gov/petsc/ and http://www.mcs.anl.gov/research/projects/tao/\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo\nCitation Context: The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [URL_CITE] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures.\nFootnote or Reference text (if exists): 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nCitation Context: All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [URL_CITE] .\nFootnote or Reference text (if exists): 4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://en.wikipedia.org\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus , a mid-2009 dump of the English Wikipedia [URL_CITE] and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 3 http://en.wikipedia.org\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wacky.sslmit.unibo.it/\nCitation Context: Our source corpus is the concatenation of the ukWaC corpus [URL_CITE] , a mid-2009 dump of the English Wikipedia and the British National Corpus . The corpus is tokenized, POS-tagged and lemmatized with TreeTagger (Schmid, 1995) and contains about 2.8 billion tokens. We extracted all statistics at the lemma level, ignoring inflectional information.\nFootnote or Reference text (if exists): 2 http://wacky.sslmit.unibo.it/\nSection Titles (if exists):  ['3 Method', '3.1 Semantic space', '3.1.1 Source corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.natcorp.ox.ac.uk\nCitation Context: The Italian language vectors for the cross-lingual ex-periments of Section 6 were trained on 1.6 bil-lion tokens from itWaC. [URL_CITE] A word token is a word-form + POS-tag string. We extract both word vec-tors and the observed phrase vectors which are required for the training procedures.\nFootnote or Reference text (if exists): 5 Corpus sources: http://wacky.sslmit.unibo. it, http://www.natcorp.ox.ac.uk\nSection Titles (if exists):  ['4 Evaluation setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09\nCitation Context: However, we identify several limitations in the schemas produced by their system. [URL_CITE] Their schemas often lack coherence: mixing unrelated events and having actors whose entities do not play the same role in the schema. Table 2 shows an event schema from Chambers that mixes the events of fire spread-ing and disease spreading.\nFootnote or Reference text (if exists): 2 Available at http://www.usna.edu/Users/cs/nchamber/data/schemas/acl09\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We have created a Rel-grams database with 1.1M entries and a set of over 2K event schemas from a corpus of 1.8M New York Times articles. Both are freely available to the research community [URL_CITE] and may prove useful for a wide range of NLP applications.\nFootnote or Reference text (if exists): 9 available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['7 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/oferbr/BIU-RPI-Event-Extraction-Project\nCitation Context: We consider a re-alistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from J OINT B EAM , we ran the ac-tual system [URL_CITE] used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.\nFootnote or Reference text (if exists): 8 https://github.com/oferbr/ BIU-RPI-Event-Extraction-Project\nSection Titles (if exists):  ['4 Experiments', '4.1 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://relgrams.cs.washington.edu\nCitation Context: We release our open domain event schemas and the Rel-grams database [URL_CITE] for further use by the NLP community.\nFootnote or Reference text (if exists): 3 Available at http://relgrams.cs.washington.edu\nSection Titles (if exists):  ['1 Introduction', '1.1 Contributions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/clab/rnng\nCitation Context: Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a dis-criminative model [URL_CITE] as our proposal distribution. As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity de-spite substantially worse performance than RN-NGs in number agreement with multiple attrac-tors, suggesting that lower perplexity is not neces-sarily correlated with number agreement success.\nFootnote or Reference text (if exists): 11 https://github.com/clab/rnng\nSection Titles (if exists):  ['3 Number Agreement with RNNGs', '3.3 Further Analysis']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.usna.edu/Users/cs/\nCitation Context: We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009). [URL_CITE] Chambers\u2019 schemas are less expressive than ours \u2013 they do not associate types with actors and each schema has a constant pre-specified number of relations. For a fair comparison we use a similarly expressive ver-sion of our schemas that strips off argument types and has the same number of relations per schema (six) as their highest quality output set.\nFootnote or Reference text (if exists): 8 Available at http://www.usna.edu/Users/cs/\nSection Titles (if exists):  ['5 Evaluation', '5.2 Schemas Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [URL_CITE] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/jalammar/ecco\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Mallet (McCallum, 2002) [URL_CITE] is more oriented to NLP applications. It is entirely in Java and in-cludes feature extraction tools for converting text into vectors and statistical analysis tools for docu-ment classification, clustering, topic modeling, in-formation extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on Lib-SVM, and obviously inherit its limits.\nFootnote or Reference text (if exists): Andrew Kachites McCallum. 2002. Mal-let: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lucene.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene [URL_CITE] . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 3 http://lucene.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.answers.com\nCitation Context: For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers and a crawl of WikiAnswers [URL_CITE] (Fader et al., 2014). The Yahoo!\nFootnote or Reference text (if exists): 5 http://wiki.answers.com\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nCitation Context: For experiments we used 2 publicly available CQA datasets: Yahoo! Answers Comprehensive Ques-tions and Answers [URL_CITE] and a crawl of WikiAnswers (Fader et al., 2014). The Yahoo!\nFootnote or Reference text (if exists): 4 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/gorokoba560/norm-analysis-of-transformer\nCitation Context: The codes of our experiments are pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/gorokoba560/ norm-analysis-of-transformer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.reddit.com\nCitation Context: Our experiments are based on Reddit [URL_CITE] , one of the world\u2019s largest public discussion forums. On Red-dit, registered users initiate a post and people re-spond with comments, either to the original post or one of its associated comments.\nFootnote or Reference text (if exists): 2 http://www.reddit.com\nSection Titles (if exists):  ['2 Popularity Prediction and Tracking']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/P16-1101\nCitation Context: Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5. For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016) [URL_CITE] .\nFootnote or Reference text (if exists): Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In ACL. Association for Computational Linguistics, Berlin, Germany, pages 1064\u20131074. http://www.aclweb.org/anthology/P16-1101.\nSection Titles (if exists):  ['2 Our Models', '2.1 Supertagging Model', '2.1.2 Deep Highway BiLSTM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: We use two sets of 300-dimensional pre-trained embeddings, word2vec 1 and GloVe [URL_CITE] , forming two channels for our network. For all datasets, we use 100 convolution filters each for window sizes of 3, 4, 5. Rectified Linear Units (ReLU) is chosen as the nonlinear function in the convolutional layer.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: Both LSTMs are trained via a grid search over the following hyperparameters: learning rate, hid-den dimension, batch size, number of layers, and L2-penalty (a.k.a. weight decay). The networks are written in Pytorch [URL_CITE] and optimized with Adam (Kingma and Ba, 2014) .\nFootnote or Reference text (if exists): 20 https://pytorch.org/\nSection Titles (if exists):  ['5 Predicting changes in analysts\u2019 post-call forecasts', '5.3 Models', '5.3.3 Turn-by-turn models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dblp.uni-trier.de/db/journals/corr/corr1412.html\nCitation Context: Network parameters for CNN, LSTM & GRU: In the fully connected layers we use 50 and 10 neurons , respectively for the two hidden layers. We use Relu activations (Glorot et al., 2011) for intermediate layers and tanh activation in the final layer. We employ 20% Dropout (Srivastava et al., 2014) in the fully connected layers as a measure of regularization and Adam optimizer (Kingma and Ba, 2014) [URL_CITE] for optimization.\nFootnote or Reference text (if exists): Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza-tion. CoRR abs/1412.6980. http://dblp.uni-trier.de/db/journals/corr/corr1412.html.\nSection Titles (if exists):  ['2 Proposed Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: For better comparison, we implement three methods as baselines, which have been proved ef-fective in relevance evaluation: (1) Vector Space Model (VSM), (2) Word Embedding (WE), and (3) Latent Dirichlet Allocation (LDA). Note that our model captures the distributional semantics of triples with LDA, while WE serves as a baseline only, where the word embeddings are acquired over the same corpus mentioned previously with the publicly available tool word2vec [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['5 Experiments', '5.1 Evaluation as a ranking problem']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.840B.300d.zip\nCitation Context: The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used through-out our model. For word embedding, we use pre-trained case-sensitive GloVe embeddings [URL_CITE] (Pen-nington et al., 2014) for both questions and pas-sages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to com-pute character-level embeddings and 3 layers of bi-directional GRU to encode questions and pas-sages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment.\nFootnote or Reference text (if exists): 2 Downloaded from http://nlp.stanford.edu/data/glove.840B.300d.zip.\nSection Titles (if exists):  ['4 Experiment', '4.1 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ex-periment with both type-level word embeddings (GloVe.6B.300d; Pennington et al., 2014) and contextual embeddings (BERT large; Devlin et al., 2019). [URL_CITE] In both cases, we keep the embeddings fixed, so the vast majority of the learnable pa-rameters are in the WFSAs. We train mod-els using GloVe embeddings on all datasets.\nFootnote or Reference text (if exists): 8 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/askerlee/topicvec\nCitation Context: \u2022 TopicVec (Li et al., 2016). A model incorpo-rates generative word embedding model with LDA [URL_CITE] . We also use default values for the pa-rameters.\nFootnote or Reference text (if exists): 8 https://github.com/askerlee/topicvec\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/projects/glove/\nCitation Context: Word embedding layer (W E \u2208 R N\u00d7300 ): Sup-posing the word number of the vocabulary is N, this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/projects/glove/\nSection Titles (if exists):  ['3 Neural Sparse Topical Coding', '3.3 Neural Sparse Topical Coding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://radimrehurek.com/gensim/\nCitation Context: \u2022 DSobtainedwordviaembeddingsLatent Semantic: DS embeddingsAnalysis (LSAare) and via retraining word2vec on the test data sets using the implementation in gensim [URL_CITE] . DS embeddings via LSA are denoted by LSA and DS embeddings via word2vec are de-noted by DSw2v.\nFootnote or Reference text (if exists): 3 https://radimrehurek.com/gensim/\nSection Titles (if exists):  ['3 Domain Adapted Word Embeddings for Improved Sentiment Classification', '3.2 Experimental evaluation and results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: For the syntax-based NMT, we take syntac-tic trees of source texts as inputs. We evaluate our model on both English-German (En-De) and English-Czech (En-Cs) News Commentary v11 datasets from the WMT16 translation task [URL_CITE] . Both sides are tokenized and split into subwords using BPE with 8000 merge operations.\nFootnote or Reference text (if exists): 1 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['3 Experiments', '3.1 Data and preprocessing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.codeproject.com/KB/recipes/englishparsing.aspx\nCitation Context: Similar to SLM (Chelba and Jelinek, 2000), af-ter the parses undergo headword percolation and binarization, each model component of WORD-PREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the \u201copenNLP\u201d software (Northedge, 2005) [URL_CITE] to parse a large amount of sentences in the LDC English Gi-gaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initial-ize model parameters.\nFootnote or Reference text (if exists): R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishparsing.aspx\nSection Titles (if exists):  ['4 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: In training, we simply take the greedy maximum probability to predict the parent of each word. In the testing phase, we use the heuristics formulated by Dozat and Manning (2017) [URL_CITE] to ensure that the resulting parse is single-rooted and acyclic.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.2 Biaffine Parser']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nCitation Context: For English sentences, we per-formed the same preprocessings described on the WAT \u201916 Website. [URL_CITE] To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the train-ing data. Specifically, among the million pre-processed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sen-tence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sen-tence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sen-tence is larger than 20 (3% of whole data).\nFootnote or Reference text (if exists): 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/dataPreparationJE.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2013T19\nCitation Context: It is worth noting that to attain our best perfor-mance, we require the use of both an NER system, indirectly using labelled data from OntoNotes 5, and a constituency parser for extracting sub-clauses, trained on the Penn Treebank (Marcus et al., 1994). [URL_CITE] Moreover, a language-specific wh* heuristic was used for training the best perform-ing NMT models. This limits the applicability and flexibility of our best-performing approach to do-mains and languages that already enjoy extensive linguistic resources (named entity recognition and treebank datasets), as well as requiring some hu-man engineering to define new heuristics.\nFootnote or Reference text (if exists): 7 Ontonotes 5: https://catalog.ldc.upenn.edu/LDC2013T19\nSection Titles (if exists):  ['5 Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/\nCitation Context: The sense of word based similarity is infused to SCQA by using BM25 ranking algorithm. Lucene [URL_CITE] is used to cal-culate the BM25 scores for question pairs. The score from similarity metric of SCQA is com-bined with the BM25 score.\nFootnote or Reference text (if exists): 10 https://lucene.apache.org/\nSection Titles (if exists):  ['6 Siamese Neural Network with Textual Similarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://disi.unitn.it/moschitti/Tree-Kernel.htm\nCitation Context: Reranking models based on SVM and PTK have been trained with \u201cSVM-Light-TK\u201d [URL_CITE] . Kernel param-eters M and SVM parameter C have been optimized on the development set, as well as thresholds for the WRR (see section 4.2).\nFootnote or Reference text (if exists): 3 available at http://disi.unitn.it/moschitti/Tree-Kernel.htm\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anserini.io/\nCitation Context: At inference time, we retrieve k text segments (one of the above conditions) using the question as a \u201cbag of words\u201d query. We use a post-v0.3.0 branch of Anserini, [URL_CITE] with BM25 as the ranking function (Anserini\u2019s default parameters).\nFootnote or Reference text (if exists): 1 http://anserini.io/\nSection Titles (if exists):  ['3 System Architecture', '3.1 Anserini Retriever']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nCitation Context: We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [URL_CITE] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1).\nFootnote or Reference text (if exists): 4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nSection Titles (if exists):  ['4 Experiments', '4.4 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nCitation Context: Relevance (R) We use the Lucene implementa-tion [URL_CITE] of the BM25 IR model (Robertson et al., 2009) to estimate the relevance of each justifica-tion sentence to a given question and candidate answer. In particular, we form a query that concate-nates the question and candidate answer, and use as underlying document collection (necessary to com-pute document statistics such as inverse document frequencies (IDF)) either: sentences in the entire KB (for ARC), or all sentences in the correspond-ing passage in the case of reading comprehension (MultiRC). The arithmetic mean of BM25 scores over all sentences in a given justification set gives the value of R for the entire set.\nFootnote or Reference text (if exists): 4 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html\nSection Titles (if exists):  ['3 Approach', '3.1 Ranking of Candidate Justification Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/castorini/birch\nCitation Context: The final score used by Birch for re-reranking is: \u03b3\u00b7S BM25 +(1\u2212\u03b3)\u00b7P i \u03ba i \u00b7S(x q , x i ) where S BM25 is the baseline BM25 score and \u03b3, \u03ba i are weight coefficients. We use the published mod-els [URL_CITE] and coefficient values for evaluation.\nFootnote or Reference text (if exists): 3 https://github.com/castorini/birch\nSection Titles (if exists):  ['5 Experiments', '5.1 Tasks and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/K17-3002\nCitation Context: (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) [URL_CITE] that allows for global training and inference.\nFootnote or Reference text (if exists): Timothy Dozat, Peng Qi, and Christopher D. Man-ning. 2017. Stanford\u2019s graph-based neural depen-dency parser at the CoNLL 2017 shared task. In CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Asso-ciation for Computational Linguistics, Vancouver, Canada, pages 20\u201330. http://www.aclweb.org/anthology/K17-3002.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Kasai et al. (2017) [URL_CITE] and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.aclweb.org/anthology/D17-1180\nCitation Context: Figures 2 and 3 illustrate the relative perfor-mance of the feed-forward neural network shift-reduce TAG parser (Kasai et al., 2017) [URL_CITE] and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions.\nFootnote or Reference text (if exists): Jungo Kasai, Robert Frank, Tom McCoy, Owen Rambow, and Alexis Nasr. 2017. TAG pars-ing with neural networks and vector representa-tions of supertags. In EMNLP. Association for Computational Linguistics, Copenhagen, Denmark, pages 1713\u20131723. https://www.aclweb.org/anthology/D17-1180.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] demonstrated that their unlexicalized neural network TAG parsers that only get as in-put predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model', '2.2.1 Input Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6213\nCitation Context: (2017) and Friedman et al. (2017) [URL_CITE] achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.\nFootnote or Reference text (if exists): Dan Friedman, Jungo Kasai, R. Thomas McCoy, Robert Frank, Forrest Davis, and Owen Rambow. 2017. Linguistically rich vector representations of supertags for TAG parsing. In TAG+. Associa-tion for Computational Linguistics, Umea\u030a, Sweden, pages 122\u2013131. http://www.aclweb.org/anthology/W17-6213.\nSection Titles (if exists):  ['2 Our Models', '2.2 Parsing Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/bzhangXMU/transformer-aan\nCitation Context: We conduct a series of experiments on WMT17 translation tasks, where on 6 dif-ferent language pairs, we obtain robust and consistent speed-ups in decoding. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code is available at https://github.com/bzhangXMU/transformer-aan.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Henry-E/reliable_\nCitation Context: Main experiment repository All the experi-ments are done with python modules and bash scripts. These are available in our main repos-itory [URL_CITE] https://github.com/Henry-E/reliable_ neural_nlg Experiment steps\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt11/\nCitation Context: Next we evaluated our method on a word transla-tion task, introduced in (Mikolov et al., 2013b) and used in (Gouws et al., 2015). The words were ex-tracted from the publicly available WMT11 [URL_CITE] cor-pus. The experiments were done for two sets of translation: English to Spanish and Spanish to En-glish.\nFootnote or Reference text (if exists): 2 http://www.statmt.org/wmt11/\nSection Titles (if exists):  ['5 Experiments', '5.2 P@k Word Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/e-bug/nmt-difficulty\nCitation Context: We then present the first systematic and con-trolled study of cross-lingual translation dif-ficulties using modern neural translation sys-tems. Code for replicating our experiments is available online at [URL_CITE] https://github.com/e-bug/nmt-difficulty.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cadillacforums.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles [URL_CITE] to gadgets such as those of Mac or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 3 http://www.cadillacforums.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://discussions.apple.com/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac [URL_CITE] or Samsung . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 4 https://discussions.apple.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.alexa.com/siteinfo/stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally [URL_CITE] . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 2 http://www.alexa.com/siteinfo/stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.stackoverflow.com\nCitation Context: Discussion forums have become a popular knowl-edge source for finding solutions to common prob-lems. StackOverflow [URL_CITE] , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally . Now, there are discussion fo-rums for almost every major product ranging from automobiles 3 to gadgets such as those of Mac 4 or Samsung 5 .\nFootnote or Reference text (if exists): 1 http://www.stackoverflow.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wpdev.uservoice.com/forums/110705-universal-windows-platform\nCitation Context: 1) Customer posts from a publicly accessible suggestion forums for the products Feedly mobile app , and Windows App studio [URL_CITE] . We crawled the suggestion for improvement posts for these products, and labeled only a subset of them due to the annotation costs.\nFootnote or Reference text (if exists): 3 https://wpdev.uservoice.com/forums/110705-universal- windows-platform\nSection Titles (if exists):  ['3 Research Methodology', '3.2 RQ2: Suggestion Detection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.galaxyforums.net/\nCitation Context: StackOverflow 1 , a popular discussion forum for programmers is among the top-100 most vis-ited sites globally 2 . Now, there are discussion fo-rums for almost every major product ranging from automobiles to gadgets such as those of Mac or Samsung [URL_CITE] . These typically start with a registered user posting a question/problem to which other users respond.\nFootnote or Reference text (if exists): 5 http://www.galaxyforums.net/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 1: Stability of word2vec as a property of fre-quency in the PTB. Stability is measured across ten randomized embedding spaces trained on the training portion of the PTB (determined using language model-ing splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (x-axis), and each column (fre-quency bucket) is normalized.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right y-axis).\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['3 Defining Stability']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: Figure 3: Stability of both word2vec and GloVe as properties of the starting word position in the training data of the PTB. Stability is measured across ten ran-domized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Boxes with more than 0.02% of the total vocabulary mass are outlined.\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['5 Lessons Learned: What Contributes to the Stability of an Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs/src\nCitation Context: The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) \u2013 07:40:54 (NoReC Fine ) Model implementation [URL_CITE] https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Word2vec We average word embeddings trained with word2vec. [URL_CITE] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.\nFootnote or Reference text (if exists): 2 The code is available from https://code.google.com/archive/p/word2vec/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.isca-speech.org/archive/\nCitation Context: approx 32 0.6 ( Neighbors ( 1024 9998 Corpus 100 2 Particular 32 9998 Nearest 1024 0.4 Training 200 in 2 Words 32 of in 1024 # 0.2 % Frequency 800 99982 32 1024 max 9998 0 20 40 60 80 100 % Stability Figure 6: Stability of word2vec on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined us-ing language modeling splits (Mikolov et al., 2010) [URL_CITE] ). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of\nFootnote or Reference text (if exists): Tomas Mikolov, Martin Karafia\u0301t, Lukas Burget, Jan Cernocky\u0300, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech. volume 2, page 3. https: //www.isca-speech.org/archive/ interspeech 2010/i10 1045.html.\nSection Titles (if exists):  ['5 Lessons Learned: What Contributes to the Stability of an Embedding']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://snowball.tartarus.org/credits.php\nCitation Context: 4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970) [URL_CITE] , and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content words were excluded.\nFootnote or Reference text (if exists): Martin Porter and Richard Boulton. 1970. Snowball stemmer. http://snowball.tartarus.org/credits.php.\nSection Titles (if exists):  ['4 Experiments', '4.1 Matching on Multilingual Topics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mounicam/lexical_simplification\nCitation Context: We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators. [URL_CITE] Despite the actual larger En-glish vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus was effective for simplifica-tion purposes (see experiments in \u00a74) as our neural ranking model (\u00a73) can estimate the complexity of any word or phrase even out-of-vocabulary.\nFootnote or Reference text (if exists): 3 Download at https://github.com/mounicam/ lexical_simplification\nSection Titles (if exists):  ['2 Constructing A Word-Complexity Lexicon with Human Judgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nCitation Context: Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-fier (JJ), selected from the LDC POS set [URL_CITE] . This is similar to the removal of stop words widely used in information retrieval.\nFootnote or Reference text (if exists): 2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf\nSection Titles (if exists):  ['2 Related Work', '4.3 Clustering vs. LDA Based LM Adaptation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/research/umls\nCitation Context: To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS [URL_CITE] meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i .\nFootnote or Reference text (if exists): 6 http://www.nlm.nih.gov/research/umls\nSection Titles (if exists):  ['3 Linguistic Resources', '3.3 Terminology Reference Lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/~jason/20Newsgroups/\nCitation Context: We use the well-known 20 Newsgroups dataset (20 NEWS ) used in previous interactive topic mod-eling work: 18,846 Usenet postings from 20 dif-ferent newgroups in the early 1990s. [URL_CITE] We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes. We then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.\nFootnote or Reference text (if exists): 1 http://qwone.com/\u02dcjason/20Newsgroups/\nSection Titles (if exists):  ['3 High Water Mark for Tandem Anchors', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1017/CBO9780511809071.007\nCitation Context: Only a small percentage of the words appearing in news can be used for meaningful information extraction and analysis (Manning et al., 1999; Ho-vold, 2005). Specifically, we eliminated too fre-quent (at least once in more than 50% of the days) or too rare (appearing in less than 100 articles) (Manning et al., 2008) [URL_CITE] . Many common English nouns, adjectives and verbs, whose contribution to semantics is minimal (Forman, 2003) were also removed from the vocabulary.\nFootnote or Reference text (if exists): Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schtze. 2008. Scoring, term weighting, and the vector space model. Cambridge University Press, 100123. https://doi.org/10.1017/CBO9780511809071.007\nSection Titles (if exists):  ['3 Predictive Causal Graph', '3.1 Selecting Informative Words:']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/electra\nCitation Context: Experiment settings The ELECTRA-small model we implemented follow all official settings [URL_CITE] except that it is fully-trained on one GTX 1080Ti GPU for 6 days. The ELECTRA-small model has 12 layers with 4 heads in every layer\u2019s attention.\nFootnote or Reference text (if exists): 11 https://github.com/google-research/electra\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.gstatic.com/gumdrop/sustainability/google-2019-environmental-report.pdf\nCitation Context: Our final models were trained using 64 Google Cloud TPUs for a total of 32 hours. As men-tioned in the Google 2019 environment report, [URL_CITE] \u201cTPUs are highly efficient chips which have been specifically designed for machine learning applica-tions\u201d. These accelerators run on Google Cloud, which has \u201cmatched 100% of its electricity con-sumption with renewable energy purchases, and has committed to fully decarbonize its electricity supply by 2030\u201d (https://cloud.google.com/sustainability).\nFootnote or Reference text (if exists): 18 https://www.gstatic.com/gumdrop/sustainability/google-2019-environmental-report.pdf\nSection Titles (if exists):  ['Ethical Considerations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fchollet/keras\nCitation Context: All models are implemented with Keras (Chol-let et al., 2015) [URL_CITE] using Tensorflow as backend, and are trained on servers with multiple CPU cores. In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant.\nFootnote or Reference text (if exists): Franc\u0327ois Chollet et al. 2015. Keras. https://github.com/fchollet/keras.\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: As for training cost, our system processes around 4000 tokens per second on a single GTX 670 GPU. As an example, this amounts to [URL_CITE] minute per epoch on the TREC dataset, converging within 50 epochs.\nFootnote or Reference text (if exists): 1 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details and Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Parameters: We employ pre-trained \u2018albert-xxlarge-v2\u2019 2 and reuse the implementation of PreSumm . We train our model (with about 400M parameters) one day for 100,000 steps on [URL_CITE] GPUs(Nvidia Tesla V100, 32G) with gradient accumulation every two steps. We select the top-3 checkpoints according to the evaluation loss on val-idation set and report the averaged results on the test set.\nFootnote or Reference text (if exists): 2 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments Setting', '4.2 Evaluation Metric & Parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/electra\nCitation Context: We compare the number of train-ing steps per second. For direct comparison, we modify the ELECTRA reference code [URL_CITE] . For TPU v3 with 8 TPU cores, ELECTRA and SCRIPT achieve 31.3 and 22.7 training iterations per sec-ond with a mean MXU utilization of 14.93% and 17.91% for small models, respectively.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ electra\nSection Titles (if exists):  ['3 Experiments', '3.3 Computational Efficiency']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT (Bayer, 2015) [URL_CITE] is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system.\nFootnote or Reference text (if exists): Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: Five of those tools support document-level annotations: MAT, MyMiner, tagtog, prodigy, and LightTag. MAT [URL_CITE] (Bayer, 2015) is designed for what they call the \u201ctag-a-little, learn-a-little (TALLAL) loop\u201d to in-crementally build up an annotation corpus, but it is only a research prototype and is not ready to be used in production. MyMiner (Salgado et al., 2012) is an online-only tool without a login or user system.\nFootnote or Reference text (if exists): 1 http://mat-annotation.sourceforge.net/\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.figure-eight.com\nCitation Context: We take advantage of the annotation platform in Figure Eight. [URL_CITE] The annotators are randomly eval-uated through a pre-defined set of test questions, and they have to maintain an accuracy threshold to be able to continue their annotations. If an an-notator\u2019s accuracy drops below a threshold, their previous annotations are labeled as untrusted and will be added to the pool of annotations again.\nFootnote or Reference text (if exists): 5 https://www.figure-eight.com\nSection Titles (if exists):  ['4 Dataset', '4.1 Annotation using Crowd Workers']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/maxparser/\nCitation Context: Self-Training: Following Reichart and Rap-poport (2007), we train a parser with the union of the source and target labeled data, parse the unlabeled data in the target domain, 5 [URL_CITE] http://sourceforge.net/projects/maxparser/ add the entire auto-parsed trees to the man-ually labeled data in a single step without checking their parsing quality, and retrain the parser.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Experiments', '3.2 Comparison Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/W17-6214\nCitation Context: Xu et al. (2017) [URL_CITE] provided a set of linguistically-motivated transformations to use TAG derivation trees to solve the PETE task. We follow their pro-cedures and evaluation for our new parsers.\nFootnote or Reference text (if exists): Pauli Xu, Robert Frank, Jungo Kasai, and Owen Rambow. 2017. TAG parser evaluation us-ing textual entailments. In TAG+. Association for Computational Linguistics, Umea\u030a, Sweden, pages 132\u2013141. http://www.aclweb.org/anthology/W17-6214.\nSection Titles (if exists):  ['5 Downstream Tasks', '5.1 PETE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mat-annotation.sourceforge.net/\nCitation Context: This process can then be repeated until the ma-chine learning model is performing well enough to be partly or fully automated. This is similar to the \u201ctag-a-little, learn-a-little loop\u201d from MAT (Bayer, 2015) [URL_CITE] . If combined with enabling pre-annotations, it is also very similar to the annotation process of RapTAT (Gobbel et al., 2014).\nFootnote or Reference text (if exists): Samuel Bayer. 2015. MITRE Annotation Toolkit (MAT). http://mat-annotation.sourceforge.net/.\nSection Titles (if exists):  ['4 Annotation Generator', '4.2 Active Learning Process']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/\nCitation Context: We further augment this data with 20 participants who took the TOEIC Listening and Reading test within the same four years range, resulting in a total of 53 external proficiency scores. The TOEIC scores were converted to the TOEFL scale by fitting a third degree polynomial on an unofficial score conversion table [URL_CITE] between the tests. The converted scores were then divided by two.\nFootnote or Reference text (if exists): 2 http://theedge.com.hk/conversion-table-for-toefl-ibt-pbt-cbt-tests/ Although both TOEFL and TOEIC are administered by the same company (ETS), to the best of our knowledge there is no publicly available official conversion table between the two tests.\nSection Titles (if exists):  ['2 Experimental Setup', '2.3 Standardized English Tests']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Babylonpartners/corrsim\nCitation Context: Note that the dataset for the STS13 SMT subtask is no longer publicly available, so the mean Pear-son correlation for STS13 reported in our exper-iments has been re-calculated accordingly. The code for our experiments builds on the SentEval toolkit (Conneau and Kiela, 2018) and is available on GitHub [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/Babylonpartners/ corrsim\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://davidmlane.com/hyperstat/A34739.html\nCitation Context: The Pearson\u2019s correlation coefficient [URL_CITE] measures the strength and direction of a linear relationship be-tween any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.\nFootnote or Reference text (if exists): 5 For a quick overview of the Pearson\u2019s coefficient, see: http://davidmlane.com/hyperstat/A34739.html.\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luyaojie/text2event\nCitation Context: We conducted experiments [URL_CITE] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:\nFootnote or Reference text (if exists): 1 Our source codes are openly available at https://github.com/luyaojie/text2event\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nCitation Context: One of the quirks of the TED talks is that the speaker annotation correlates with the topic of their talk to a high degree. Although the topics that a speaker talks about can be considered as a man-ifestation of speaker traits, we also perform a con-trol experiment on a different dataset to verify that our model is indeed learning more than just topical information. Specifically, we train our models on a speaker annotated version of the Europarl corpus (Rabinovich et al., 2017), on the en-de language pair [URL_CITE] .\nFootnote or Reference text (if exists): 6 available here: https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age/version/1\nSection Titles (if exists):  ['5 Experiments', '5.3 Further experiments on the Europarl corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.efset.org/research/~/media/centralefcom/efset/pdf/\nCitation Context: First, as mentioned previously, in our dataset the MET and reported TOEFL scores have a Pearson\u2019s r correlation of 0.74. We further note an external study conducted by the testing com-pany Education First (EF) which measured the correlation of their flagship standardized English proficiency test EFSET-PLUS with TOEFL-iBT (Luecht, 2015) [URL_CITE] . Using 384 participants who took both tests, the study found a Pearson\u2019s r of 0.63 for the reading comprehension and 0.69 for the listen-ing comprehension sections of these tests.\nFootnote or Reference text (if exists): Richard M Luecht. 2015. EFSET PLUS - TOEFL iBT correlation study report. https://www.efset.org/research/\u02dc/media/centralefcom/efset/pdf/ EFSET_TOEFL_correlational_report_ Sep_v1.pdf.\nSection Titles (if exists):  ['4 English Proficiency Scoring Based on Eye Movements in Reading', '4.1 Correlation with MET and TOEFL']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/p/word2vec/\nCitation Context: After fil-tering HTML tags and noise we POS-tagged the corpus and extracted \u2248 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single to-kens using the word2vec [URL_CITE] bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the com-pounds\u2019 component words.\nFootnote or Reference text (if exists): 5 https://code.google.com/p/word2vec/\nSection Titles (if exists):  ['3 Representation of Words and Compounds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N16-1142\nCitation Context: More closely related is our prior work (Vyas and Carpuat, 2016) [URL_CITE] where we used lexical context based embeddings to detect cross-lingual lexical entailment. In contrast, the focus of this work is on hypernymy, a more well-defined relation than entailment. Also, we improve upon our previ-ous approach by using dependency based embed-dings (\u00a76.1), and show that the improvements hold even when exposed to data scarce settings (\u00a76.3).\nFootnote or Reference text (if exists): Yogarshi Vyas and Marine Carpuat. 2016. Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment. Association for Computational Linguistics, San Diego, California, pages 1187\u2013 1197. http://www.aclweb.org/anthology/N16-1142.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf\nCitation Context: Given unstructured transcribed CC, we extract en-tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005) [URL_CITE] :\nFootnote or Reference text (if exists): NIST. 2005. The ace 2005 evaluation plan. http: //www.itl.nist.gov/iad/mig/tests/ ace/ace05/doc/ace05-evaplan.v3.pdf.\nSection Titles (if exists):  ['2 Approach', '2.1 Event Extraction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.dss.2013.02.006\nCitation Context: Existing word em-bedding techniques are not suited to learn relation-ships between words appearing in different docu-ments and contexts (Le and Mikolov, 2014). Ex-isting work on stock price prediction using news have typically relied on extracting features from financial news (Falinouss, 2007; Hagenau et al., 2013 [URL_CITE] ), or sentiments expressed on Twitter (Mao et al., 2011; Rao and Srivastava, 2012; Bernardo et al., 2018), or by focusing on features present in a single document (Kalyani et al., 2016; Shynke-vich et al., 2015). However, relationships between events affecting stock prices can be quite com-plex, and their mentions can be spread across mul-tiple documents.\nFootnote or Reference text (if exists): Michael Hagenau, Michael Liebmann, and Dirk Neu-mann. 2013. Automated news reading: Stock price prediction based on financial news using context-capturing features. Decision Support Systems 55, 3 (2013), 685 \u2013 697. https://doi.org/10.1016/j.dss.2013.02.006\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/\nCitation Context: Sil et al. (2010) extract sequences of verb constructions from webpages to learn common 8 See [URL_CITE] https://en.wikipedia.org/wiki/ preconditions of actions and events, and Cham- Wikipedia:Notability_(events) be transferable to the cross-document case. on Empirical Methods in Natural Language Process-To the best of our knowledge, we are the first to ing: System Demonstrations, pages 205\u2013215, On-propose a cheap and high-quality data extraction approach specifically for cross-document event ex-traction and coreference which does not depend on pre-existing resources, combining past work on event extraction from raw newswire text and min-ing of hyperlinks.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Discussion and Future Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Other"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.irit.fr/STAC/\nCitation Context: In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [URL_CITE] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set.\nFootnote or Reference text (if exists): 1 https://www.irit.fr/STAC/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/valentinhofmann/dga\nCitation Context: The model produces embeddings that capture information about the compatibility of af-fixes and stems in derivation and can be used as pretrained input to other NLP applications. [URL_CITE]\nFootnote or Reference text (if exists): 1 We make all our code and data publicly available at https://github.com/valentinhofmann/dga.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nCitation Context: only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de-pendency parsing (INDP) [URL_CITE] .\nFootnote or Reference text (if exists): 1 The source code is available at http://homepage.fudan.edu.cn/zhengxq/deeplearning/\nSection Titles (if exists):  ['1 Introduction and Motivation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mrlyk423/relation_extraction\nCitation Context: Experimental results on real-world datasets show that, as com-pared with baselines, our model achieves significant and consistent improvements on knowledge base completion and re-lation extraction from text. The source code of this paper can be obtained from [URL_CITE] https://github.com/mrlyk423/relation_extraction.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Zikangli/SOM-NCSCM\nCitation Context: In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answer-ing system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. [URL_CITE] Experimental results show that our SOM-NCSCM can significantly ben-efit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.\nFootnote or Reference text (if exists): 1 Our dataset and code will be available at: https://github.com/Zikangli/SOM-NCSCM.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset\nCitation Context: The proposed model learns relation representations along the shortest dependency path (SDP) extracted from the structure regularized de-pendency tree, which has the benefits of re-ducing the complexity of the whole model. Experimental results show that the proposed method significantly improves the F 1 score by 10.3, and outperforms the state-of-the-art ap-proaches on Chinese literature text [URL_CITE] .\nFootnote or Reference text (if exists): 1 The Chinese literature text corpus for relation classi-fication, developed and used by this paper, is available at https://github.com/lancopku/Chinese-Li terature-NER-RE-Dataset\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.cs.kuleuven.be/\u223civan.vulic/software/\nCitation Context: In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. [URL_CITE] . We have used 5 extra nouns with 20 sentences each as a development set to tune the parameters of our models.\nFootnote or Reference text (if exists): 1 Available at http://people.cs.kuleuven.be/\u223civan.vulic/software/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf\nCitation Context: There were 118 sentences, out of which 94 had at least one modification after the rewrite pro-cess. We selected 50 of these 94 sentences at ran-dom and asked 2 human judges to rate each sen-tence and its rewritten form on a scale of 1\u20135 for accuracy and fluency [URL_CITE] . We used 4 human judges, each judging 25 sentence pairs.\nFootnote or Reference text (if exists): 3 We followed the DARPA/LDC guidelines from http://ldc.upenn.edu/Projects/TIDES/Translation/TranAssessSpec.pdf. For fluency, the scale was 5:Flawless, 4:Good, 3:Non-native, 2:Disfluent, 1:Incomprehensible. The accuracy scale for information covered (comparing with human translation) was 5:All, 4:Most, 3:Much, 2:Little, 1:None.\nSection Titles (if exists):  ['3 Arbitrary noun phrases', '3.5 Manual Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pstadler1990/nmt_paper21_appendix\nCitation Context: We created 10 sentences per phe-nomenon, resulting in a total of 585 sentences, ex-amining 49 phenomena organised in 13 categories. The raw test items, as well as the translations eval-uated can be found in our repository [URL_CITE] . The phe-nomena selected for this experiment are a subset of the ones of German\u2192English MT, as described in Macketanz et al.\nFootnote or Reference text (if exists): 1 https://github.com/pstadler1990/nmt_ paper21_appendix\nSection Titles (if exists):  ['4 Experiment setup', '4.1 Test suite setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Mixed\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://qwone.com/jason/20Newsgroups/\nCitation Context: Text classification. We used the 20NewsGroup [URL_CITE] dataset, which consists of about 20000 docu-ments. Our goal is to classify each document into one of the 20 different newsgroups based on its content.\nFootnote or Reference text (if exists): 6 http://qwone.com/jason/20Newsgroups/\nSection Titles (if exists):  ['4 Experimental Setup', '4.5 Downstream NLP Tasks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dcferreira/multilingual-joint-embeddings\nCitation Context: We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. [URL_CITE]\nFootnote or Reference text (if exists): 5 Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/\nCitation Context: We used the \u201cModApte\u201d split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987) [URL_CITE] . The corpus has 9,603 training (not to be confused with D) and 3,299 test documents (which represents D u ).\nFootnote or Reference text (if exists): Lewis, D. et al. (1987). Reuters-21578. http: //www.daviddlewis.com/resources/\nSection Titles (if exists):  ['5 Results', '5.1 Reuters-21578']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://di.unipi.it/~gulli/AG_corpus_of_news_articles.html\nCitation Context: We use five datasets for text classification follow-ing Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR Tweets (Sarker et al., 2015); (iv) AG News; [URL_CITE] and (v) MIMIC Anemia (Johnson et al., 2016). See Table 1 for detailed data statistics.\nFootnote or Reference text (if exists): 9 https://di.unipi.it/\u02dcgulli/AG_corpus_of_news_articles.html\nSection Titles (if exists):  ['6 Experiments and Results', '6.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.daviddlewis.com/resources/testcollections/reuters21578/\nCitation Context: We used the Reuters-21578 dataset [URL_CITE] to evaluate the proposed methods. The dataset has several splits of a training set and a test set.\nFootnote or Reference text (if exists): 3 Available from David D. Lewis\u2019s page: http://www.daviddlewis.com/resources/testcollections/reuters21578/\nSection Titles (if exists):  ['5 Experimental Results and Discussion', '5.1 Test Set Collection']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://archive.ics.uci.edu/ml/\nCitation Context: Two multiclass document classification datasets were used: Reuters Transcribed and Mini News-groups, distributed at [URL_CITE] http://archive.ics.uci.edu/ml/. The properties of the datasets are summarized in Ta-ble 2.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Experiments', '7.2 Document classification', '7.2.1 Task and dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://git.io/en-eo-dict-issue\nCitation Context: 1a fol-lows the main setup of Lample et al. (2018), who tested six language pairs using Wiki-Embs [URL_CITE] . Af-ter ` 1 refinement, M USE -` 1 , JA-M USE -` 1 , and V EC M AP -` 1 all significantly (p < 0.01) outper-form their corresponding base algorithms, with an average 1.1% performance gain over M USE , 1.1% over JA-M USE , and 0.5% over V EC M AP .\nFootnote or Reference text (if exists): 4 Note that we are unable to report the result of English to Esperanto as the corresponding dictionary is missing, see https://git.io/en-eo-dict-issue.\nSection Titles (if exists):  ['5 Results', '5.1 Bilingual Lexicon Induction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pan.webis.de\nCitation Context: For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [URL_CITE] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).\nFootnote or Reference text (if exists): 3 http://pan.webis.de\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/NL2code\nCitation Context: Experiments (\u00a7 5) on two Python code gener-ation tasks show 11.7% and 9.3% absolute im-provements in accuracy against the state-of-the-art system (Ling et al., 2016). Our model also gives competitive performance on a standard semantic parsing benchmark [URL_CITE] .\nFootnote or Reference text (if exists): 1 Implementation available at https://github.com/neulab/NL2code\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cindyxinyiwang/TrDec_pytorch\nCitation Context: Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cis.upenn.edu/dbikel/software.html\nCitation Context: The sec-ond largest gain was on PPs, where we improved by 1.7% over 3,775 constituents. We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel\u2019s randomized parsing evaluation com-parator [URL_CITE] and found that both the precision and recall gains were significant at p \u2264 0.01.\nFootnote or Reference text (if exists): 6 Available at http://www.cis.upenn.edu/dbikel/software.html\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: For Japanese-English, we evaluated on the NTCIR-10 PatentMT task data (patents) (Goto et al., 2013) and compared our system with the official baseline scores. For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC [URL_CITE] corpus and com-pared against the same baseline system as for Japanese-English. The corpora contain 3M parallel sentences for Japanese-English and 670K for Japanese-Chinese.\nFootnote or Reference text (if exists): 5 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['6 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://encarta.msn.com/default.asp\nCitation Context: For each of the words, we extracted about 200 sentences containing the word from the Encarta [URL_CITE] English corpus and labeled those sentences with Chinese translations ourselves. We used the labeled sentences as test data and the remaining sentences as unclassified data in English.\nFootnote or Reference text (if exists): 4 http://encarta.msn.com/default.asp\nSection Titles (if exists):  ['5 Word Translation Disambiguation', '6.2 Experiment 2: Yarowsky\u2019s Words']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.\nSection Titles (if exists):  ['3 Neural Readability Ranking Model for Words and Phrases', '3.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nCitation Context: Currently we use Japanese and English resources to learn word embeddings, but plan to add more lan-guages in the future. The bilingual sentence-parallel resource used is the ASPEC corpus [URL_CITE] , which fea-tures sentence-aligned text from scientific paper ab-stracts. For monolingual data, we use subsets of the Japanese and English Wikipedia.\nFootnote or Reference text (if exists): 2 http://orchid.kuee.kyoto-u.ac.jp/ASPEC/\nSection Titles (if exists):  ['3 Crosslingual Vector-Based Writing Assistance (CroVeWA)', '3.1 Inducing Crosslingually Constrained Word Representations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.npm.gov.tw\nCitation Context: We use the Chinese/English web pages of the National Palace Museum [URL_CITE] as our underlying parallel corpus. It contains about 30,000 sentences in each language.\nFootnote or Reference text (if exists): 2 http://www.npm.gov.tw\nSection Titles (if exists):  ['3 Experimental Results', '3.1 Experimental Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: While earlier versions only included linguistic moves by players, the latest version of STAC is a multi-modal corpus of multi-party chats between players of an online game (Asher et al., 2016; Hunter et al., 2018 [URL_CITE] ). It includes 2,593 dialogues (each with a weakly connected DAG discourse structure), 12,588 \u201clinguistic\u201d DUs, 31,811 \u201cnon-linguistic\u201d DUs and 31,251 semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/jacobandreas/neuralese\nCitation Context: We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmat-ics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward rel-ative to players with a common language. [URL_CITE]\nFootnote or Reference text (if exists): 1 We have released code and data at http://github.com/jacobandreas/neuralese. denotes a gated recurrent unit (Cho et al., 2014). Dashed lines represent recurrent connections.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://en.wikipedia.org/wiki/Yes,_and\nCitation Context: Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers no way of continuing the conversation. [URL_CITE] two agents using a 4-layer LSTM encoder-decoder trained on the OpenSubtitles dataset. The first turn (index 1) is input by the authors.\nFootnote or Reference text (if exists): 1 A similar rule is often suggested in improvisational comedy: https://en.wikipedia.org/wiki/Yes,_and...\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nCitation Context: All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [URL_CITE] .\nFootnote or Reference text (if exists): 4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mturk.com\nCitation Context: To look at more perceptual measures of dialogue quality, we used Amazon Mechanical Turk [URL_CITE] to an-notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants. Details on the annotation pro-cess can be found in (Gravano et al., 2011).\nFootnote or Reference text (if exists): 2 http://www.mturk.com\nSection Titles (if exists):  ['5 Entrainment and dialogue quality', '5.1 Entrainment correlates of dialogue charac-teristics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dx.doi.org/10.3765/sp.11.10\nCitation Context: These player ut-terances are the \u201clinguistic\u201d turns. In addition the corpus contains information given visually in the game interface but transcribed in the corpus into Server or interface messages, \u201cnon-linguistic\u201d turns (Hunter et al., 2018) [URL_CITE] . All turns are seg-mented into DUs, and these units are then con-nected by semantic relations.\nFootnote or Reference text (if exists): Julie Hunter, Nicholas Asher, and Alex Lascarides. 2018. A formal semantics for situated conver-sation. Semantics and Pragmatics, 11. DOI: http://dx.doi.org/10.3765/sp.11.10.\nSection Titles (if exists):  ['3 The STAC Annotated Corpus', '3.1 Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [URL_CITE] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras 1 . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 [URL_CITE] . For the GloVe model, we use the pre-trained Stanford GloVe model trained on data from Wikipedia and Gigaword which includes around 6 billion word vectors with a dimension of 100 (Pennington et al., 2014).\nFootnote or Reference text (if exists): 2 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://keras.io/\nCitation Context: Additionally, we also use word embeddings from pre-trained word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models, represent-ing each utterance summing all of the vectors for the component words. Each utterance is repre-sented with these pretrained embeddings in the embedding layers of our models, which are im-plemented in Keras [URL_CITE] . For the word2vec model, we use the Google News model which includes about 100 billion word vectors with a dimension of 300 2 .\nFootnote or Reference text (if exists): 1 https://keras.io/\nSection Titles (if exists):  ['3 Models', '3.2 Neural model architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Our three models are implemented in Python using TensorFlow [URL_CITE] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing.\nFootnote or Reference text (if exists): 9 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.\nSection Titles (if exists):  ['3 Neural Readability Ranking Model for Words and Phrases', '3.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zenodo.org/record/4421380\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec [URL_CITE] and GloVe, for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 3 https://zenodo.org/record/4421380\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.ted.com/talks\nCitation Context: The dataset consists of transcripts directly col-lected from [URL_CITE] https://www.ted.com/talks, and contains roughly 271K sentences in each language distributed among 2324 talks. We pre-process the data by removing sentences that don\u2019t have any translation or are longer than 60 words, lower-casing, and tokenizing (using the Moses tokenizer (Koehn et al., 2007)).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Speaker Annotated TED Talks Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://fasttext.cc/docs/en/crawl-vectors.html\nCitation Context: We use a 256-dimensional 2-layer biLSTM encoder, initial-ized with FastText 300-dimensional word vectors (Mikolov et al., 2018; Grave et al., 2018). [URL_CITE] We hold out 50K and 10K insertion edits for each lan-guage as development and test sets, and use the re-maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex-amples for training in each language (cf.\nFootnote or Reference text (if exists): 7 https://fasttext.cc/docs/en/crawl-vectors.html\nSection Titles (if exists):  ['6 Language Modeling Analysis', '6.1 Predicting Insertion Locations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/D14-1162\nCitation Context: The Glove 100-dimensional pre-trained word em-beddings (Pennington et al., 2014) [URL_CITE] are used for all experiments (d = 100). The maximum lengths of utterances and queries are m = 92 and n = 126, and the maximum number of utterances is k = 25.\nFootnote or Reference text (if exists): Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www.aclweb.org/anthology/D14-1162.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N/N09/N09-2047\nCitation Context: Since POS tagging, supertagging, and parsing decisions are made independently for each word in a sentence, our system can parallelize computation once the sentence is encoded in the BiLSTM layers. Our current implementation pro-cesses 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009) [URL_CITE] . 5\nFootnote or Reference text (if exists): Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Beno\u0131\u0302t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185\u2013188. http://www.aclweb.org/anthology/N/N09/N09-2047.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909\nCitation Context: To compare the same systems on shorter length outputs, we also tried finetuning the pretrained model on Wizard of Wikipedia (Dinan et al., 2019), an unconstrained dialogue generation task with single sentence dialogues (much shorter than ELI5). As seen on the public KILT leaderboard, [URL_CITE] our system has lower ROUGE-L scores than the BART / RAG baselines. Another possible explanation is issues with ROUGE-L itself, as discussed in Section 3.3.\nFootnote or Reference text (if exists): 12 https://eval.ai/web/challenges/challenge-page/689/leaderboard/1909\nSection Titles (if exists):  ['3 Analysis', '3.1 Are generations grounded in retrieval?']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/NLPIR-team/NLPIR\nCitation Context: If use the maximum length of all conversations as the input length for en-coders, padding the inputs will lead to a sparse ma-trix. Therefore, for long conversations (with more than 10 messages), we use KLSum (Haghighi and Vanderwende, 2009) to produce summaries with a length of [URL_CITE] messages and then encode the pro-duced summaries. In contrast, we do not sum-marize Twitter conversations because their length range is much narrower (from 4 to 1,035 words).\nFootnote or Reference text (if exists): 10 https://github.com/NLPIR-team/NLPIR\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stream.twitter.com\nCitation Context: We created a new data set consisting of two streams (document and topic stream). The document stream consists of 52 million tweets gathered through Twitter\u2019s streaming API [URL_CITE] . The tweets are order by their time-stamps.\nFootnote or Reference text (if exists): 1 http://stream.twitter.com\nSection Titles (if exists):  ['Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/tweets/\nCitation Context: Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track [URL_CITE] . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its \u201cin reply to status id\u201d included.\nFootnote or Reference text (if exists): 4 http://trec.nist.gov/data/tweets/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://dev.twitter.com/\nCitation Context: The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs [URL_CITE] . The statistics are shown in Table 1.\nFootnote or Reference text (if exists): 3 https://dev.twitter.com/\nSection Titles (if exists):  ['3 Experiments and Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.createdebate.com\nCitation Context: The second dataset (createdebate, henceforth) comes from an online debating site [URL_CITE] . It consists of 30 debates containing a total of 2,712 posts.\nFootnote or Reference text (if exists): 3 http://www.createdebate.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.iitp.ac.in/~ai-nlp-ml/resources.html\nCitation Context: For experiments we categorize the datasets into three kinds: training, test and user-level. Train-ing set consists of 27k tweets, whereas the test set is manually annotated with 741 tweets. [URL_CITE] The user-level tweets consist of \u224810 million tweets from 5,191 users mapped to their user-level features.\nFootnote or Reference text (if exists): 4 All the developed resources are available at http://www.iitp.ac.in/\u02dcai-nlp-ml/resources.html\nSection Titles (if exists):  ['4 Data Sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/IBM/twitter-customer-care-document-prediction\nCitation Context: Two of these datasets are internal: one from an internal customer support service on Mac devices (Mac-Support) and another from an external client in the telecommunication domain (Telco-Support). We also release a new Twitter dataset, containing conversations between users and CC agents in 25 organizations on the Twitter platform [URL_CITE] . We summarize the statistics of the three datasets in Table 2.\nFootnote or Reference text (if exists): 3 The Twitter dataset is available at: https://github.com/IBM/ twitter-customer-care-document-prediction based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, tion set of Telco-Support dataset for dialog context se-quence input handling.\nSection Titles (if exists):  ['2 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cnts.ua.ac.be/conll2000/chunking/\nCitation Context: To test the CRF-based model also with sparse features, we followed Sha and Pereira (2003) in applying CRFs to the noun phrase chunking task on the CoNLL-2000 dataset [URL_CITE] . We split the origi-nal training set into a dev set (top 1,000 sent.)\nFootnote or Reference text (if exists): 4 http://www.cnts.ua.ac.be/conll2000/chunking/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We used the machine learning package MALLET (McCallum, 2002) [URL_CITE] to conduct the CRF model training and labeling.\nFootnote or Reference text (if exists): McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://igm.univ-mlv.fr/~mconstan/research/software/\nCitation Context: The CRF recognizer relies on the software Wapiti (Lavergne et al., 2010) to train and apply the model, and on the software Unitex (Paumier, 2011) to apply lexical resources. The part-of-speech tagger used to extract POS features was lgtagger [URL_CITE] (Constant and Sigogne, 2011). To train the reranker, we used a MaxEnt al-gorithm as in (Charniak and Johnson, 2005).\nFootnote or Reference text (if exists): 7 Available at http://igm.univ-mlv.fr/\u02dcmconstan/research/software/.\nSection Titles (if exists):  ['6 Evaluation', '6.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://metamap.nlm.nih.gov/\nCitation Context: Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013 [URL_CITE] ), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) of words in a window of size 3. We also use conjunctions of the features.\nFootnote or Reference text (if exists): Url2. 2013. Metamap (http://metamap.nlm.nih.gov/) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cogcomp.cs.illinois.edu/page/softwareview/\nCitation Context: Our Approach In the first step, we identify the concept boundaries using a CRF (with BIO encod-ing). Features used by the CRF include the con-stituents given by MetaMap (Aronson and Lang, 2010; Url2, 2013), shallow parse constituents, sur-face form and part-of-speech (Url3, 2013) [URL_CITE] of words in a window of size 3. We also use conjunctions of the features.\nFootnote or Reference text (if exists): Url3. 2013. Illinois part-of-speech tagger (http://cogcomp.cs.illinois.edu/page/softwareview/ pos) (accessed july 1, 2013).\nSection Titles (if exists):  ['2 Identifying Medical Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://crfpp.sourceforge.net/\nCitation Context: The CRF model we compare with was trained with the CRF++ tool, available at [URL_CITE] http://crfpp.sourceforge.net/. The model is equiva-lent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic cat-egories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.2 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://psy.takelab.fer.hr/datasets/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora [URL_CITE] is another dataset collected from Reddit, where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 6 https://psy.takelab.fer.hr/datasets/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://personalitycafe.com/forum\nCitation Context: We choose two public MBTI datasets for evalua-tions, which have been widely used in recent stud-ies (Tadesse et al., 2018; Hernandez and Knight, 2017; Majumder et al., 2017; Jiang et al., 2020; Gjurkovic\u0301 et al., 2020). The Kaggle dataset is col-lected from PersonalityCafe, [URL_CITE] where people share their personality types and discussions about health, behavior, care, etc. There are a total of 8675 users in this dataset and each user has 45-50 posts.\nFootnote or Reference text (if exists): 5 http://personalitycafe.com/forum\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politicalforum.com\nCitation Context: We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [URL_CITE] . All the posts are in English.\nFootnote or Reference text (if exists): 2 http://www.politicalforum.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.createdebate.com\nCitation Context: The second dataset (createdebate, henceforth) comes from an online debating site [URL_CITE] . It consists of 30 debates containing a total of 2,712 posts.\nFootnote or Reference text (if exists): 3 http://www.createdebate.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nCitation Context: The dataset contains 2500 similarity judgements, provided by 25 participants, and is pub-licly available. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt\nSection Titles (if exists):  ['5 Evaluation', '5.1 Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com/\nCitation Context: There are a total of 8675 users in this dataset and each user has 45-50 posts. Pan-dora is another dataset collected from Reddit, [URL_CITE] where personality labels are extracted from short descriptions of users with MBTI results to intro-duce themselves. There are dozens to hundreds of posts for each of the 9067 users in this dataset.\nFootnote or Reference text (if exists): 7 https://www.reddit.com/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cnap/grammaticality-metrics\nCitation Context: To facilitate GEC evaluation, we have set up an online platform for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available. [URL_CITE]\nFootnote or Reference text (if exists): 9 https://github.com/cnap/ grammaticality-metrics\nSection Titles (if exists):  ['6 Summary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/redpony/cdec\nCitation Context: \u2022 cdecerarchical(Dyerphrase-basedet al., 2010):SMTan opensystemsource(Chi-hi-ang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://github.com/redpony/cdec\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.purl.com/net/ArabicSentiment\nCitation Context: The resources created as part of this project (Arabic sentiment lex-icons, Arabic sentiment annotations of social me-dia posts, and English sentiment annotations of their translations) are made freely available. [URL_CITE]\nFootnote or Reference text (if exists): 5 http://www.purl.com/net/ArabicSentiment\nSection Titles (if exists):  ['9 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/optimization/\nCitation Context: We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation met-ric. To solve the AGTSP, we use Or-tool [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://developers.google.com/optimization/\nSection Titles (if exists):  ['3 Experiments', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/lex4all/lex4all\nCitation Context: We have developed lex4all as a desktop applica-tion for Microsoft Windows, since it relies on the Microsoft Speech Platform (MSP) as explained in Section 4.1. The application and its source code are freely available via GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://github.com/lex4all/lex4all\nSection Titles (if exists):  ['3 System overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanford-qa.com\nCitation Context: F1 measures the overlap between the prediction and ground truth answers which takes the max-imum F1 over all of the ground truth answers. The scores on dev set are evaluated by the offi-cial script [URL_CITE] . Since the test set is hidden, we are re-quired to submit the model to Stanford NLP group to obtain the test scores.\nFootnote or Reference text (if exists): 3 Downloaded from http://stanford-qa.com\nSection Titles (if exists):  ['4 Experiment', '4.2 Main Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/20717\nCitation Context: F 1 scores for intra- and inter-sentence entity pairs are also reported. Evaluation on the test set is done through CodaLab [URL_CITE] .\nFootnote or Reference text (if exists): 3 https://competitions.codalab.org/competitions/20717\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\nCitation Context: We report performance in F 0.5 -measure, as calculated by the m2scorer\u2014 the official implementation of the scoring metric in the shared task. [URL_CITE] Given system outputs and gold-standard edits, m2scorer computes the F 0.5 measure of a set of system edits against a set of gold-standard edits.\nFootnote or Reference text (if exists): 1 http://www.comp.nus.edu.sg/\u02dcnlp/sw/m2scorer.tar.gz\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset and Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcg-www.uia.ac.be/conll2000/chunking\nCitation Context: A standard software program has been provided (which is available from [URL_CITE] http://lcg-www.uia.ac.be/conll2000/chunking) to compute the performance of each algorithm. For each chunk, three figures of merit are computed: precision (the percentage of detected phrases that are correct), recall (the percentage of phrases in the data that are found), and the \u201eL nL% metric the recall.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 CoNLL-2000 chunking task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/72.410363\nCitation Context: In most attention-based sys-tems, the attention amount is calculated in ref-erence to some external information. For exam-ple, in machine translation the attention values are found based on a representation of the out-put that has already been generated (Bahdanau et al., 2015); in question answering, the attention weights are calculated in reference to the input question (Hermann et al., 2015) [URL_CITE] . In our task there is no external information to be used, therefore we predict the attention values directly based on h i , by passing it through a separate feedforward layer: where W e , b e , W e eand e and e i results e e in a single scalar value.\nFootnote or Reference text (if exists): Karl Moritz Hermann, Toma\u0301s\u030c Koc\u030cisky\u0301, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching Ma-chines to Read and Comprehend. In Advances in Neural Information Processing Systems (NIPS 2015). pages 1\u201314. https://doi.org/10.1109/72.410363.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/sempre/\nCitation Context: Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric [URL_CITE] .\nFootnote or Reference text (if exists): 6 We used the official evaluation script from http://www-nlp.stanford.edu/software/sempre/.\nSection Titles (if exists):  ['4 Experiments', '4.1 Data & evaluation metric']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ZurichNLP/emnlp2018-imitation-learning-for-neural-morphology\nCitation Context: We make our code and predictions publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/ZurichNLP/emnlp2018-imitation- learning-for-neural-morphology\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dodgejesse/sparsifying_regularizers_for_RRNNs\nCitation Context: We publicly release our code. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pkhdipraja/towards-incremental-transformers\nCitation Context: Our implementation is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/pkhdipraja/ towards-incremental-transformers\nSection Titles (if exists):  ['4 Experimental Setup', '4.3 Implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/wasiahmad/PolicyIE\nCitation Context: We release the code and data to facilitate research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/wasiahmad/ PolicyIE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dgaddy/silent_speech\nCitation Context: We hope that our public release of data will encourage others to further improve models for this task. [URL_CITE]\nFootnote or Reference text (if exists): 6 Our dataset can be downloaded from https://doi.org/10.5281/zenodo.4064408 and code is available at https://github.com/dgaddy/silent_speech. Samples of predicted outputs can be found in the supplementary material.\nSection Titles (if exists):  ['5 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Our experimental code is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['Origin Locale Expensiveness']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://cuitools.sourceforge.net\nCitation Context: Our experiments were conducted using CuiTools v0.15, which is freely available from [URL_CITE] http://cuitools.sourceforge.net.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JasonForJoy/SPD\nCitation Context: All codes were implemented under the Ten-sorFlow framework (Abadi et al., 2016) and are published along with the dataset to help replicate our results. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JasonForJoy/SPD\nSection Titles (if exists):  ['5 Experiments', '5.2 Training Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: In all the experiments, we use PyTorch [URL_CITE] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 2 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nju-websoft/GLRE\nCitation Context: We implemented our GLRE with PyTorch 1.5. The source code and datasets are available online. [URL_CITE] In this section, we report our experimental results.\nFootnote or Reference text (if exists): 1 https://github.com/nju-websoft/GLRE\nSection Titles (if exists):  ['4 Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://publish.illinois.edu/yirenwang/emnlp16source/\nCitation Context: All our implementations are based on Theano (Theano De-velopment Team, 2016) and run on one K40 GPU. All the source codes and datasets can be down-loaded at [URL_CITE] https://publish.illinois.edu/yirenwang/emnlp16source/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://deeplearning.net/software/theano/\nCitation Context: We implement our models and DeClarE with Theano [URL_CITE] , and use the original codes of other base-lines. As DeClarE is not yet open-source, we con-sult with its developers for our implementation.\nFootnote or Reference text (if exists): 3 http://deeplearning.net/software/theano/\nSection Titles (if exists):  ['5 Experiments and Results', '5.2 Experiments on Veracity-based Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/higher\nCitation Context: In all the experiments, we use PyTorch (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package [URL_CITE] (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 3 https://github.com/facebookresearch/ higher\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: In all the experiments, we use PyTorch [URL_CITE] (Paszke et al., 2019) as the backend. All the experiments are conducted on NVIDIA V100 32GB GPUs. We use the Higher package (Grefenstette et al., 2019) to implement the proposed algorithm.\nFootnote or Reference text (if exists): 2 https://pytorch.org/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jungokasai/graph_parser\nCitation Context: We implement all of our models in TensorFlow (Abadi et al., 2016). [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bitbucket.org/TomKenter/siamese-cbow\nCitation Context: We use Theano (Theano Development Team, 2016) to implement our network. [URL_CITE] We ran our ex-periments on GPUs in the DAS5 cluster (Bal et al., 2016).\nFootnote or Reference text (if exists): 4 The code for Siamese CBOW is available under an open-source license at https://bitbucket.org/TomKenter/siamese-cbow.\nSection Titles (if exists):  ['3 Experimental Setup', '3.4 Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: For the hyper-parameters in our CNN-based model, we set d 0 =100, d p =10, w=3, n 1 =200, n 2 =500, \u03bb=10 -4 , p=0.8. The embeddings of the to-kens in ordinary sentences (as word sequences) are initialized by Word2Vec [URL_CITE] (Mikolov et al., 2013).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/word2vec/\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The recommended hyper-parameter configuration of the model on each dataset are presented in Table 3. To diminish the effects of randomness in training, the results of our model are averaged with [URL_CITE] random initializa-tions. For data preprocessing, we use the tok-enizer 6 from BERT to preprocess the sentences.\nFootnote or Reference text (if exists): 5 Code for metrics: https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: Word2vec We average word embeddings trained with word2vec. [URL_CITE] We use both architec-tures, Skipgram and CBOW, and apply default settings: minimum word frequency 5, word embedding size 300, context window 5, sample threshold 10 -5 , no hierarchical softmax, 5 negative examples.\nFootnote or Reference text (if exists): 2 The code is available from https://code.google.com/archive/p/word2vec/.\nSection Titles (if exists):  ['3 Experimental Setup', '3.2 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alchemy.cs.washington.edu/\nCitation Context: We implemented our method as an extension to the Alchemy system (Kok et al., 2007) [URL_CITE] . Since our learn-ing uses sampling, all results are the average of five runs using different random seeds.\nFootnote or Reference text (if exists): Kok, S.; Singla, P.; Richardson, M.; Domingos, P.; Sumner, M.; Poon, H. & Lowd, D. 2007. The Alchemy system for statistical relational AI. http://alchemy.cs.washington.edu/.\nSection Titles (if exists):  ['6 Experiments', '6.1 System']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: Our three models are implemented in Python using TensorFlow [URL_CITE] . For both datasets, we use the pre-trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train-ing.\nFootnote or Reference text (if exists): 9 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiments', '4.1 Data and Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/stanfordnlp/GloVe\nCitation Context: For all other hyper-parameters, we use the default values of previous implementations. [URL_CITE] For each corpus-algorithm pair, we train k = 32 word embedding models using different random seeds.\nFootnote or Reference text (if exists): 6 For SGNS, we use Gensim 3.8.3 (R\u030cehu\u030ar\u030cek and Sojka, 2010) with a window size of 5, a minimum word count of 5 and 5 iterations. For GloVe, we use the official implemen-tation https://github.com/stanfordnlp/GloVe, with a window size of 15, a minimum word count of 5 and 15 iterations. Because we do not fine-tune hyper-parameters, our results do not necessarily indicate which algorithm itself is of better reliability. To investigate the potential impact of this decision, we also include an explorative study on the influence of hyper-parameters in Appendix E.\nSection Titles (if exists):  ['5 Experiments', '5.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://anthology.aclweb.org/P16-2028\nCitation Context: Association for Computational Linguistics, Berlin, Germany, pages 169\u2013174. [URL_CITE] http://anthology.aclweb.org/P16-2028.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.clsp.jhu.edu/workshops/18-workshop/\nCitation Context: This work was mostly conducted at the 2018 Fred-erick Jelinek Memorial Summer Workshop on Speech and Language Technologies, [URL_CITE] hosted and sponsored by Johns Hopkins University. Shruti Palaskar received funding from Facebook and Ama-zon grants.\nFootnote or Reference text (if exists): 1 https://www.clsp.jhu.edu/workshops/18-workshop/\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/AIPHES/ACL20-Reference-Free-MT-Evaluation\nCitation Context: We make our MT evaluation code available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/AIPHES/ ACL20-Reference-Free-MT-Evaluation\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MT-DNN\nCitation Context: We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [URL_CITE] . Training details are presented in Appendix B.2.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/MT-DNN\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.toronto.edu/~aditya/g2p-tl-rr\nCitation Context: Our experiments aim at comprehensive evaluation of the reranking approach on both MTL and G2P tasks, employing various supplemental representa-tions. Relevant code and scripts associated with our experimental results are available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www.cs.toronto.edu/\u02dcaditya/ g2p-tl-rr\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/doug919/multi_relational_script_learning\nCitation Context: We leave the details about hyperparameter tuning in the appendix. The source code and pre-trained models are publicly available [URL_CITE] .\nFootnote or Reference text (if exists): 2 https://github.com/doug919/multi_ relational_script_learning\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq\nCitation Context: We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pre-trained models provided by the Fairseq toolkit. [URL_CITE]\nFootnote or Reference text (if exists): 5 https://github.com/pytorch/fairseq\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pallets/flask\nCitation Context: It uses JQuery on the client side and Python on the server side. The ap-plication is built with the Flask (Ronacher, 2018) [URL_CITE] framework. For text preprocessing we use SpaCy (Honnibal and Johnson, 2015) and NLTK (Loper and Bird, 2002).\nFootnote or Reference text (if exists): Armin Ronacher. 2018. Flask. https://github.com/pallets/flask.\nSection Titles (if exists):  ['3 System', '3.3 Technical implementation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: Preprocessing Each dataset is tokenized using the spaCy [URL_CITE] tokenizer, converted to lowercase, and non-\nFootnote or Reference text (if exists): 1 https://spacy.io\nSection Titles (if exists):  ['4 Model Training', '4.1 Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: We use spaCy [URL_CITE] to get the meta dependency paths of sentences in a document. Following Yao et al.\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experiments', '3.2 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: We do sentence segmentation and tokenization with the Stanford Core NLP [URL_CITE] . Annotations are mapped to the extracted tokens.\nFootnote or Reference text (if exists): 4 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/kentonl/e2e-coref\nCitation Context: We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments. The code for replicating these results is publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/kentonl/e2e-coref\nSection Titles (if exists):  ['5 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: For all English processing we will use tools available in the NLTK toolkit (Bird et al., 2009) or SpaCy. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://spacy.io/\nSection Titles (if exists):  ['3 Experimental Framework', '3.3 Tools and Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wiktionary.org/\nCitation Context: Contextual sentences in WiC were extracted from example usages provided for words in three lexi-cal resources: (1) WordNet (Fellbaum, 1998), the standard English lexicographic resource; (2) Verb-Net (Kipper-Schuler, 2005), the largest domain-independent verb-based resource; and (3) Wik-tionary [URL_CITE] , a large collaborative-constructed online dictionary. We used WordNet as our core re-source, exploiting BabelNet\u2019s mappings (Navigli and Ponzetto, 2012) as a bridge between Wik-tionary and VerbNet to WordNet. Lexicographer examples constitute a reliable base for the con-struction of the dataset, as they are curated in a way to be clearly distinguishable across different senses of a word.\nFootnote or Reference text (if exists): 3 https://www.wiktionary.org/\nSection Titles (if exists):  ['2 WiC: the Word-in-Context dataset', '2.1 Construction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [URL_CITE] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SapienzaNLP/mwsd-datasets\nCitation Context: In order to assess its performance, we tested Syn-tagRank on the five English all-words WSD evalu-ation datasets standardized according to WordNet 3.0 in the framework of Raganato et al. (2017a), namely: Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Nav-igli et al., 2013), and SemEval-2015 (Moro and Navigli, 2015). As regards the appraisal of Synta-gRank in a multilingual setting, we used the Ger-man, Spanish, French and Italian annotations avail-able in the amended version of the SemEval-2013 and SemEval-2015 evaluation datasets [URL_CITE] , which is accordant with the BabelNet API 4.0.1 graph and enables testing on a larger number of instances than hitherto.\nFootnote or Reference text (if exists): 10 Made available at https://github.com/SapienzaNLP/mwsd-datasets.\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www/eml-research.de/nlp/download/wikirelations.php\nCitation Context: To determine the supersense and isa relation we use WordNet 3.0, and a set of 7,578,112 isa rela-tions extracted by processing the page and cate-gory network of Wikipedia [URL_CITE] (Nastase and Strube, 2008). The collocations extracted from BNC con-tain numerous named entities, most of which are not part of WordNet. If an isa relation be-tween a collocate from the corpus w j and a pos-sible sense of a PMW s i cannot be established us-ing supersense information (for the supersenses) or through transitive closure in the hypernym-hyponym hierarchy in WordNet (for company and organization) for any sense of w j , it is tried against the Wikipedia-based links.\nFootnote or Reference text (if exists): 1 http://www/eml-research.de/nlp/download/wikirelations.php\nSection Titles (if exists):  ['4 Representation', '4.2 Selectional preference features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/nasari/\nCitation Context: N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in [URL_CITE] http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://babelnet.org\nCitation Context: To cope with these issues, we exploit Babel-Net, [URL_CITE] a huge multilingual semantic network ob-tained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Nav-igli and Ponzetto, 2012), and create the Babel-Net subgraph induced by the WordNet vertices. The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to Word-Net (such as hyperlinks in a Wikipedia article con-necting it to other articles). The greatest contribu-tion of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related ar-ticles (e.g., the English Wikipedia Bus article is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).\nFootnote or Reference text (if exists): 2 http://babelnet.org\nSection Titles (if exists):  ['3 Creating a Denser and Multilingual Semantic Network']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/GuyAllard/markov_clustering\nCitation Context: Further-more, it reduces the number of similarity compu-tations between pairs of articles considerably. We run MCL on this graph and obtain clusters by iden-tifying the connected components in the resulting connectivity matrix [URL_CITE] .\nFootnote or Reference text (if exists): 4 We use the implementation and default parameters from https://github.com/GuyAllard/markov_ clustering\nSection Titles (if exists):  ['3 Strategies for Timeline Summarization', 'Clustering']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs/src\nCitation Context: The computations were performed on resources scheme (y-axis) on the evaluation metrics (x-axis) for MultiB EU . percentage points for MultiB CA . GPU Infrastructure NVIDIA P100, 16 GiB RAM CPU Infrastructure Intel Xeon-Gold 6138 2.0 GHz Training duration 00:31:43 (MultiB EU ) \u2013 07:40:54 (NoReC Fine ) Model implementation [URL_CITE] https://github.com/jerbarnes/sentiment_graphs/src Hyperparameter Best assignment embedding Word2Vec SkipGram 100D contexualized embedding mBERT embeddings trainable False number of epochs 100 batch size 50 beta1 0 beta2 0.95 l2 3e-09 hidden lstm 200 hidden char lstm 100 layers lstm 3 dim mlp 200 dim embedding 100 dim char embedding 80 early stopping 0 pos style xpos attention bilinear model interpolation 0.5 loss interpolation 0.025 lstm implementation drop connect char implementation convolved emb dropout type replace bridge dpa+ dropout embedding 0.2 dropout edge 0.2 dropout label 0.3 dropout main recurrent 0.2 dropout recurrent char 0.3 dropout main ff 0.4 dropout char ff 0.3 dropout char linear 0.3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['8 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/changzhisun/AntNRE\nCitation Context: Different from previous GCN-based models (Shang et al., 2018; Zhang et al., 2018), the adjacency matrix of graph is based on the output of binary rela-tion classification, which makes the proposed ad-jacency matrix more explanatory. To summarize, the main contributions of this work are [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https://github.com/changzhisun/AntNRE.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.gurobi.com/\nCitation Context: We preprocessed the para-graphs with the Stanford CoreNLP pipeline ver-sion 3.4 (Manning et al., 2014) and Illinois SRL (Punyakanok et al., 2008; Clarke et al., 2012). We used the Gurobi optimization package [URL_CITE] for infer-ence.\nFootnote or Reference text (if exists): 3 http://www.gurobi.com/\nSection Titles (if exists):  ['6 Empirical Evaluation', '6.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ananthnyu/faithful-causal-rep/\nCitation Context: We see that in the cosine-similarity based RoBERTa, the causal word pairs had a mean distance of 0.48, while in the quasi-pseudo metric based Faithful-RoBERTa, the mean distance between the causal word pairs reduced to 0.28. The distances are nor-malized between 0 and [URL_CITE] based on the maximum and minimum values of distances (cosine or d M ) in the sampled word-pairs.\nFootnote or Reference text (if exists): 1 https://github.com/ananthnyu/faithful-causal-rep/ and Antal van den Bosch. 2007. ILK: Machine learning of semantic relations with shallow features and almost no data. In Proceedings of the Fourth\nSection Titles (if exists):  ['5 Results', '5.3 Re-alignment towards causation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ofai.at/\nCitation Context: For comparison, we also tested two recently pro-posed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) [URL_CITE] , and the one (Suzuki et al., 2012) based on graph Laplacian kernels. Since the Laplacian kernels are defined for graph nodes, we computed them by taking the co-sine similarity matrix K as the weighted adjacency (affinity) matrix of a graph. For Laplacian kernels, we computed both the regularized Laplacian ker-nel (Chebotarev and Shamis, 1997; Smola and Kon-dor, 2003) with several parameter values, as well as the commute-time kernel (Saerens et al., 2004), but present only the best results among these kernels.\nFootnote or Reference text (if exists): 2 We used the Matlab script downloaded from http://www.ofai.at/ \u223c dominik.schnitzer/mp/.\nSection Titles (if exists):  ['7 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://edition.cnn.com/specials/world/fast-facts\nCitation Context: Ground-Truth Timelines: We obtain ground-truth timelines from CNN Fast Facts [URL_CITE] , which has a collection of several hundred timelines grouped in categories, e.g., \u2018people\u2019 or \u2018disasters\u2019. We pick all timelines of the \u2018people\u2019 category and a small number from other categories.\nFootnote or Reference text (if exists): 6 http://edition.cnn.com/specials/world/fast-facts\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://cloud.google.com/bigquery\nCitation Context: We obtain all Reddit posts from the popular sub-reddits r/worldnews and r/politics from January 2017 to May 2018 using Google Big-Query [URL_CITE] . Each of these posts is a headline from a news source.\nFootnote or Reference text (if exists): 2 https://cloud.google.com/bigquery\nSection Titles (if exists):  ['2 The Humor Dataset', '2.2 Collecting Headlines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.gdeltproject.org/events/index.html\nCitation Context: We crawled and parsed the GDELT Even-t Database [URL_CITE] containing news articles published in May 2014. We manually annotated one-week da-ta containing 101,654 documents and identified 77 storylines for evaluation.\nFootnote or Reference text (if exists): 1 http://data.gdeltproject.org/events/index.html\nSection Titles (if exists):  ['4 Experiments', '4.1 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dykang/cgraph\nCitation Context: The news dataset [URL_CITE] we used for stock price predic-tion contains news crawled from 2010 to 2013 us-ing Google News APIs and New York Times data from 1989 to 2007. We construct PCG from the time series representation of its 12,804 unigrams and 25,909 bigrams over the entire news corpora of more than 23 years, as well as the 10 stock prices from 2010 to 2012 for training and 2013 as test data for prediction.\nFootnote or Reference text (if exists): 1 https://github.com/dykang/cgraph\nSection Titles (if exists):  ['5 Results', '5.1 Data and Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://open-platform.theguardian.com/\nCitation Context: Input Articles: For each entity from the ground-truth timelines, we search for news articles using TheGuardian API [URL_CITE] . We use this source because it provides access to all published articles starting from 1999.\nFootnote or Reference text (if exists): 7 http://open-platform.theguardian.com/\nSection Titles (if exists):  ['4 Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). It contains 3,361 articles that have been downloaded from the BBC News website [URL_CITE] . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 3 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.politicalforum.com\nCitation Context: We use three different datasets. The first dataset (politicalforum, henceforth) consists of 5,743 posts collected from a political forum [URL_CITE] . All the posts are in English.\nFootnote or Reference text (if exists): 2 http://www.politicalforum.com\nSection Titles (if exists):  ['2 Related Work', '2.4 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: It contains 3,361 documents that have been down-loaded from the BBC News website. [URL_CITE] Each doc-ument comes with an image that depicts some of its content. The images are usually 203 pixels wide and 152 pixels high.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/word2vec/\nCitation Context: We use the 300-dimensional embeddings pretrained on the Google News corpus, which is released as part of the word2vec package. [URL_CITE]\nFootnote or Reference text (if exists): 7 https://code.google.com/archive/p/word2vec/ ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional rep-resentations by applying Gaussian radial basis functions.\nSection Titles (if exists):  ['3 Neural Readability Ranking Model for Words and Phrases', '3.2 Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homepages.inf.ed.ac.uk/s0677528/data.html\nCitation Context: As mentioned earlier, we use the dataset created in Feng and Lapata (2008). [URL_CITE] It contains 3,361 articles that have been downloaded from the BBC News website . Each article contains a news image which in turn is associated with a caption.\nFootnote or Reference text (if exists): 2 Available from http://homepages.inf.ed.ac.uk/s0677528/data.html.\nSection Titles (if exists):  ['3 Problem Formulation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/mecab/files/\nCitation Context: Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab [URL_CITE] for Japanese. For the FE language pair, we used stan-dard data for the WMT 2014 translation task.\nFootnote or Reference text (if exists): 4 http://sourceforge.net/projects/mecab/files/\nSection Titles (if exists):  ['5 Experiments', '5.1 Setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [URL_CITE] . Both campaigns include two dif-ferent translations exercises: Arabic-to-English (\u2018AE\u2019) and Chinese-to-English (\u2018CE\u2019).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.2 Test Beds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.cs.berkeley.edu\nCitation Context: We used the English and Chinese parsers in Petrov and Klein (2007) [URL_CITE] to generate all k-best lists and as our evaluation baseline. Because our bilin-gual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).\nFootnote or Reference text (if exists): 5 Available at http://nlp.cs.berkeley.edu.\nSection Titles (if exists):  ['6 Statistical Parsing Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/slavpetrov/berkeleyparser\nCitation Context: We choose NIST MT 06 dataset (1664 sentence pairs) as our develop-ment set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respec-tively) as our test sets. To get the source syn-tax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser [URL_CITE] (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005). We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.\nFootnote or Reference text (if exists): 6 https://github.com/slavpetrov/ berkeleyparser\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1146/annurev.neuro.26.041002.131047\nCitation Context: We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015) [URL_CITE] , sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017). In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.\nFootnote or Reference text (if exists): Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In International Conference on Learning Rep-resentations. https://doi.org/10.1146/annurev.neuro.26.041002.131047.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1.1.56.7752\nCitation Context: The main system takes as input a sentence, sep-arated into tokens, and outputs a binary predic-tion as the label of the sentence. We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) [URL_CITE] architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017).\nFootnote or Reference text (if exists): Sepp Hochreiter and Ju\u0308rgen Schmidhuber. 1997. Long Short-term Memory. Neural Computation 9. https://doi.org/10.1.1.56.7752.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/\nCitation Context: ATIS and SNIPS, respectively), named entity recog- The results in Table 1 (above) support the observa-nition, part-of-speech tagging and semantic role tion that, in general, bidirectional models do have labeling (Weischedel et al., 2013); and four for a better non-incremental performance than LSTMs sentence classification: intent (Hemphill et al., (except for IntentATIS and ProsCons) and that there marginal difference in the evaluation metrics. We thus present 4 The code is available at [URL_CITE] https://github.com/ the results using the pre-trained model only, and leave more briemadu/inc-bidirectional. For more details on exploration of fine-tuning for future work.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1161\nCitation Context: Many NLP tasks, including part-of-speech tagging, named entity recognition, chunking, and error detec-tion, are often formulated as variations of se-quence labeling. Recent state-of-the-art models make use of bidirectional LSTM architectures (Ir-soy and Cardie, 2014), character-based represen-tations (Lample et al., 2016), and additional ex-ternal features (Peters et al., 2017) [URL_CITE] . Optimiza-tion of these models requires appropriate training data where individual tokens are manually labeled, which can be time-consuming and expensive to obtain for each different task, domain and target language.\nFootnote or Reference text (if exists): Matthew E. Peters, Waleed Ammar, Chandra Bhaga-vatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language mod-els. In ACL 2017. https://doi.org/10.18653/v1/P17-1161.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/decomp-sem/neural-sprl\nCitation Context: achieves state-of-the-art performance for SPRL. [URL_CITE] As depicted in Figure 1, our model\u2019s architecture is an extension of the bidirectional LSTM, cap-turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end-to-end training. We include a thorough quanti-tative analysis highlighting the contrasting errors between the proposed model and previous (non-neural) state-of-the-art.\nFootnote or Reference text (if exists): 2 Implementation available at https://github.com/decomp-sem/neural-sprl.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.18653/v1/P17-1070\nCitation Context: We use a bidirectional LSTM (Hochreiter and Schmidhu-ber, 1997) architecture for sentence classification, with dynamic attention over words for construct-ing the sentence representations. Related archi-tectures have been successful for machine trans-lation (Bahdanau et al., 2015), sentence summa-rization (Rush and Weston, 2015), entailment de-tection (Rockta\u0308schel et al., 2016), and error cor-rection (Ji et al., 2017) [URL_CITE] . In this work, we modify the attention mechanism and training objective in order to make the resulting network suitable for also inferring binary token labels, while still per-forming well as a sentence classifier.\nFootnote or Reference text (if exists): Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen Gong, Steven Truong, and Jianfeng Gao. 2017. A Nested Attention Neural Hybrid Model for Gram-matical Error Correction. In ACL 2017. pages 753\u2013762. https://doi.org/10.18653/v1/P17-1070.\nSection Titles (if exists):  ['2 Network Architecture']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://deepgraphlearning.github.io/project/wikidata5m\nCitation Context: For the Knowledge Base, we use the version of Wikidata 3 provided by Wang et al. (2019b) (in par-ticular the transductive split [URL_CITE] ), containing approxi-mately 5 million entities. Similarly to Freebase, we remove all links between pairs in the test set from the resulting KB, which contains approximately 20 million triples after pruning.\nFootnote or Reference text (if exists): 4 https://deepgraphlearning.github.io/project/wikidata5m\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://developers.google.com/freebase\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/\nCitation Context: For the choice of the Knowledge Base, we use a subset of Freebase [URL_CITE] that includes 3 million entities with the most connections, similar to Xu and Barbosa (2019). For all pairs appearing in the test set of NYT 10 (both positive and negative), we remove all links in the subset of Freebase to ensure that we will not memorise any relations between them (Weston et al., 2013). The resulting KB contains approximately 24 million triples.\nFootnote or Reference text (if exists): 2 https://developers.google.com/freebase 3 https://www.wikidata.org/\nSection Titles (if exists):  ['3 Experimental Settings', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: The Amazon dataset contains 2000 samples for each of the four domains: book, DVD, electron-ics, and kitchen, with binary labels (positive, neg-ative). Following Wu and Huang (2015) [URL_CITE] , we con-duct 5-way cross validation. Three out of the five folds are treated as the training set, one serves as the validation set, while the remaining is the test set.\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['4 Experiments', '4.1 Multi-Domain Text Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xwhan/One-shot-Relational-Learning\nCitation Context: We conduct experiments on two public benchmark datasets: NELL and Wiki [URL_CITE] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks.\nFootnote or Reference text (if exists): 1 https://github.com/xwhan/ One-shot-Relational-Learning\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.wikidata.org/wiki/Wikidata:List_of_properties\nCitation Context: For Freebase we select 5, 000 triples as the validation set, while for Wikidata we use the validation set provided in the transductive setting (5, 136 triples). A.4 W IKI D ISTANT Relation Categories Since W IKI D ISTANT contains 454 relations, their labels are used directly from the WikiData proper-ties [URL_CITE] . Here, we add explanations about the top 10 most frequent categories used in Figures 4c, 4d.\nFootnote or Reference text (if exists): 9 https://www.wikidata.org/wiki/Wikidata: List_of_properties\nSection Titles (if exists):  ['A Appendix', 'A.3 Hyper-parameter Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Almost all the character-level perturbations presented here were already tested in adversarial attack scenarios (Heigold et al., 2018; Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ), but were not yet implemented in a non-adversarial testing framework, except the misspelling perturbation implemented by CheckList. Among the word-level perturbations, Deletion, Repetition, Singular/plural verbs, Word order, and Verb tense were not already used to test the robustness. However, Negation was included in CheckList, and Replacement with Synonyms was used for adversarial attack (Dong et al., 2020; Ren et al., 2019).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['4 Perturbation methods']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020; Ren et al., 2019; Zhang et al., 2020 [URL_CITE] ). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.knosys.2019.105210\nCitation Context: This can help to identify biases and errors in NLP models. Adversarial perturbations have been widely studied to assess the robustness of NLP systems against adversarial samples crafted to fool a model (Alshemali and Kalita, 2020 [URL_CITE] ; Ren et al., 2019; Zhang et al., 2020). However, adversarial samples resemble a very specific type of noise.\nFootnote or Reference text (if exists): Basemah Alshemali, and Jugal Kalita. 2020. Improving the Reliability of Deep Neural Networks in NLP: A Review. Knowledge-Based Systems, 191: 105210. https://doi.org/10.1016/j.knosys.2019.105210.\nSection Titles (if exists):  ['6 Related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1145/3374217\nCitation Context: Some types of perturbation utilized in this work were already tested in previous work on adversarial attacks on NLP systems (Zeng et al., 2020; Zhang et al., 2020 [URL_CITE] ). However, adversarial perturbations are considered worst-case scenarios that do not occur frequently in real-world situations, representing a very specific type of noise (Fawzi et al., 2016).\nFootnote or Reference text (if exists): Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol., 11(3): Article 24. https://doi.org/10.1145/3374217. Max sequence length 64 64 Batch size 12 12 Learning rate 2e-5 1e-5 Num epochs 20 15 Classification, SA: Sentiment Analysis, QA: Question Answering, NER: Named Entity Recognition, SS: Semantic Similarity.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.perspectiveapi.com\nCitation Context: Hosseini et al. (2017) show that simple modifications, such as adding spaces or dots between characters, can drasti-cally change the toxicity score from Google\u2019s perspective API [URL_CITE] . Belinkov and Bisk (2018) show that character-level machine translation sys-tems are overly sensitive to random character ma-nipulations, such as keyboard typos.\nFootnote or Reference text (if exists): 1 https://www.perspectiveapi.com\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1016/j.engappai.2020.103641\nCitation Context: Since it has been proven that sentences that contain few typos, misspellings, or minor character-level errors can be still fully understandable to humans (Belinkov and Bisk, 2018; Xu and Du, 2020 [URL_CITE] ), character-level perturbations are not expected to change the text\u2019s meaning in most cases. Therefore, they can be automatically produced and used for testing the robustness of NLP systems.\nFootnote or Reference text (if exists): Jincheng Xu, and Qingfeng Du. 2020. TextTricker: Loss-based and gradient-based adversarial attacks on text classification models. Engineering Applications of Artificial Intelligence, 92: 103641. https://doi.org/10.1016/j.engappai.2020.103641.\nSection Titles (if exists):  ['5 Experimental results', '5.1 Performance on perturbed inputs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Paper\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/corenlp.shtml\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP [URL_CITE] , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 1 http://nlp.stanford.edu/software/corenlp.shtml\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SAG-KeLP\nCitation Context: The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github [URL_CITE] . Furthermore it can be imported via Maven.\nFootnote or Reference text (if exists): 4 https://github.com/SAG-KeLP\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sag.art.uniroma2.it/demo-software/kelp/\nCitation Context: Furthermore it can be imported via Maven. A detailed documentation of K E LP with helpful examples and use cases is available on the website of the Semantic Analytics Group [URL_CITE] of the University of Roma, Tor Vergata.\nFootnote or Reference text (if exists): 5 http://sag.art.uniroma2.it/demo-software/kelp/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jalammar/ecco\nCitation Context: Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the trans-parency of Transformer-based language mod-els, we present Ecco \u2013 an open-source [URL_CITE] li-brary for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and in-teractively explore inner mechanics of these models.\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/jalammar/ecco\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: Mallet (McCallum, 2002) [URL_CITE] is more oriented to NLP applications. It is entirely in Java and in-cludes feature extraction tools for converting text into vectors and statistical analysis tools for docu-ment classification, clustering, topic modeling, in-formation extraction, and other machine learning applications to text. Regarding the kernel-based learning both Weka and Mallet leverage on Lib-SVM, and obviously inherit its limits.\nFootnote or Reference text (if exists): Andrew Kachites McCallum. 2002. Mal-let: A machine learning for language toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://opennlp.apache.org/\nCitation Context: K E LP is a machine learning library completely written in Java. The Java language has been cho-sen in order to be compatible with many Java NLP/IR tools that are developed by the commu-nity, such as Stanford CoreNLP , OpenNLP [URL_CITE] or Lucene . K E LP is released as open source soft-ware under the Apache 2.0 license and the source code is available on github .\nFootnote or Reference text (if exists): 2 https://opennlp.apache.org/\nSection Titles (if exists):  ['2 Framework Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com\nCitation Context: We created an Amazon Mechanical Turk [URL_CITE] project to investigate the possible scalability of our anno-tation using non-trained annotators.\nFootnote or Reference text (if exists): 7 https://www.mturk.com\nSection Titles (if exists):  ['4 Annotation Experiment', '4.2 Crowdsourcing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.mturk.com\nCitation Context: In addition to providing a new QA corpus, we hope our description of the data collection process will provide insight, resources and motivation for further research and projects using similar collection methods. We collected our corpus through Amazon Mechanical Turk service [URL_CITE] (MTurk). The MTurk infrastructure allowed us to distribute our tasks to a multitude of workers around the world, without the burden of advertising.\nFootnote or Reference text (if exists): 4 http://www.mturk.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://translate.google.com\nCitation Context: Fortu-nately, due to the development of machine trans-lation techniques, a few online machine transla-tion services can be used for review translation. We adopt Google Translate [URL_CITE] for review transla-tion. After review translation, the training re-views and the test reviews are now in the same language, and any regression algorithm (e.g.\nFootnote or Reference text (if exists): 1 http://translate.google.com\nSection Titles (if exists):  ['3 Problem Definition and Baseline Ap-proaches']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.4000/discours.9047\nCitation Context: Guillou and Hardmeier\u2019s (2016) pronoun trans-lation test suite succeeds in overcoming some of these problems by creating an automatic evalua-tion method, with a back-off manual evaluation. Manual evaluation has always been an essential part of evaluating MT quality, and targeted transla-tion allows us to isolate a model\u2019s performance on specific linguistic phenomena; recent work using in-depth, qualitative manual evaluation (Isabelle et al., 2017; Scarton and Specia, 2015 [URL_CITE] ) is very in-formative. Isabelle et al.\nFootnote or Reference text (if exists): Carolina Scarton and Lucia Specia. 2015. A Quan-titative Analysis of Discourse Phenomena in Ma-chine Translation. Discours [online] (16). https: //doi.org/10.4000/discours.9047.\nSection Titles (if exists):  ['2 Evaluating contextual phenomena']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://doi.org/10.1109/ICDM.2015.68\nCitation Context: We then validate the effectiveness of MAN in experiments on two MDTC data sets. We find first that MAN significantly outperforms the state-of-the-art CMSC method (Wu and Huang, 2015) [URL_CITE] on the widely used multi-domain Amazon review dataset, and does so without relying on external re-sources such as sentiment lexica (\u00a74.1). When ap-plied to the second dataset, FDU-MTL (\u00a74.3), we obtain similar results: MAN achieves substantially higher accuracy than the previous top-performing method, ASP-MTL (Liu et al., 2017).\nFootnote or Reference text (if exists): Fangzhao Wu and Yongfeng Huang. 2015. Collabora-tive multi-domain sentiment classification. In 2015 IEEE International Conference on Data Mining. pages 459\u2013468. https://doi.org/10.1109/ICDM.2015.68.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hub.microsofttranslator.com\nCitation Context: To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service [URL_CITE] to train a cus-tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida-tion process, first using crowd-sourced evaluations with Amazon\u2019s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.\nFootnote or Reference text (if exists): 2 http://hub.microsofttranslator.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ygraham/significance-williams\nCitation Context: As part of this research, we have made avail-able an open-source implementation of statis-tical tests tailored to the assessment of MT metrics available at [URL_CITE] https://github.com/ygraham/significance-williams.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Evaluation and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/nile\nCitation Context: As our baseline aligner, we use the GIZA++ im-plementation of the IBM models (Och and Ney, 2003) with the default options. To test the effect of improved alignment accuracy, we use the dis-criminative alignment method of Riesa and Marcu (2010) as implemented in the Nile toolkit. [URL_CITE] This method has the ability to use source- and target-side syntactic information, and has been shown to improve the accuracy of S2T translation.\nFootnote or Reference text (if exists): 6 http://code.google.com/p/nile\nSection Titles (if exists):  ['4 Alignment', '4.1 Alignment Overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: We propose a novel demonstration (URL: [URL_CITE] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://competitions.codalab.org/competitions/12731\nCitation Context: To facilitate GEC evaluation, we have set up an online platform [URL_CITE] for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.\nFootnote or Reference text (if exists): 8 https://competitions.codalab.org/competitions/12731\nSection Titles (if exists):  ['6 Summary']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/uhh-lt/wsd\nCitation Context: Implementation of the system is open source. [URL_CITE] A live demo featuring several dis-ambiguation models is available online.\nFootnote or Reference text (if exists): 1 https://github.com/uhh-lt/wsd\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.dcs.shef.ac.uk/nlp/meter\nCitation Context: A prototype browser-based demo of both the GST algorithm and alignment program, allow-ing users to test arbitrary text pairs for simi-larity, is now available [URL_CITE] and will continue to be enhanced. helpful comments on earlier drafts.\nFootnote or Reference text (if exists): 4 See http://www.dcs.shef.ac.uk/nlp/meter.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa\nCitation Context: The MPQA corpus contains news articles man-ually annotated for opinions and private states. [URL_CITE] Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level. We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative).\nFootnote or Reference text (if exists): 4 http://www.cs.pitt.edu/mpqa\nSection Titles (if exists):  ['4 Evaluation', '4.2 Extrinsic: Identifying phrase polarity']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/corpora/\nCitation Context: MPQA 3.0 is a recently developed corpus with entity/event-level sentiment annotations (Deng and Wiebe, 2015). [URL_CITE] It is built on the basis of MPQA 2.0 (Wiebe et al., 2005b; Wilson, 2007), which includes editorials, reviews, news reports, and scripts of interviews from different news agen-cies, and covers a wide range of topics.\nFootnote or Reference text (if exists): 4 Available at http://mpqa.cs.pitt.edu/corpora/\nSection Titles (if exists):  ['3 Task Definition', '3.1 Gold Standard Corpus: MPQA 3.0']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mpqa.cs.pitt.edu/\nCitation Context: \u2022 MPQA: An unbalanced dataset for polarity detection of opinions [URL_CITE] .\nFootnote or Reference text (if exists): 4 http://mpqa.cs.pitt.edu/\nSection Titles (if exists):  ['A Experimental Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://alt.qcri.org/semeval2015/task12/\nCitation Context: To evaluate the effectiveness of SDRN, we conduct extensive experiments on five benchmark datasets from SemEval 2014 (Pontiki et al., 2014), Se-mEval 2015 [URL_CITE] (Pontiki et al., 2015), MPQA ver-sion 2.0 corpus (Wiebe et al., 2005), and J.D. Power and Associates Sentiment Corpora (JDPA) (Kessler et al., 2010).\nFootnote or Reference text (if exists): 5 http://alt.qcri.org/semeval2015/task12/\nSection Titles (if exists):  ['4 Experiments', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.pitt.edu/mpqa/\nCitation Context: For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering (MPQA) [URL_CITE] corpus (Wiebe et al., 2005) version 1.2, which contains 535 newswire documents that are manually annotated with phrase-level subjectivity and intensity. We use the expression-level boundary markings in MPQA to extract phrases.\nFootnote or Reference text (if exists): 6 http://www.cs.pitt.edu/mpqa/\nSection Titles (if exists):  ['3 Experimental Methodology']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.quora.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora [URL_CITE] , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 3 http://www.quora.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackoverflow.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao , Quora , StackOverflow [URL_CITE] etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 4 http://stackoverflow.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://answers.yahoo.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers [URL_CITE] , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 1 https://answers.yahoo.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stackexchange.com/\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers , Quora and the StackExchange [URL_CITE] family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 3 http://stackexchange.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://answers.yahoo.com\nCitation Context: This is changing, however, with the popularity of community-based question answering (CQA) sites such as Yahoo! Answers [URL_CITE] , Quora and the StackExchange family of forums. The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.\nFootnote or Reference text (if exists): 1 http://answers.yahoo.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://zhidao.baidu.com/\nCitation Context: Community Question Answering (cQA) services like Yahoo! Answers , Baidu Zhidao [URL_CITE] , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions.\nFootnote or Reference text (if exists): 2 http://zhidao.baidu.com/\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://propaganda.qcri.org/emnlp20-tutorial\nCitation Context: Here is an outline of the tutorial. More informa-tion and materials are available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://propaganda.qcri.org/ emnlp20-tutorial\nSection Titles (if exists):  ['2 Outline of the Tutorial']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.youtube.com/watch?v=zKnP5QEHVAE\nCitation Context: For more detailed demonstration, please refer to the video. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.youtube.com/watch?v= zKnP5QEHVAE\nSection Titles (if exists):  ['3 Design and Internals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ufal.mff.cuni.cz/udpipe\nCitation Context: ).] [URL_CITE]\nFootnote or Reference text (if exists): 3 http://ufal.mff.cuni.cz/udpipe\nSection Titles (if exists):  ['3 Discourse segmentation', '3.1 Binary task']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nCitation Context: The full implementation can be found in an online tuto-rial. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://colab.research.google.com/github/facebookresearch/EGG/blob/master/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb\nSection Titles (if exists):  ['4 Implementing a game']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012\nCitation Context: A detailed derivation is provided in the supplementary file. [URL_CITE]\nFootnote or Reference text (if exists): 8 The supplementary file is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012.\nSection Titles (if exists):  ['5 Parameter Estimation', '5.1 Optimizing the Mapping A']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statnlp.org/tutorials/\nCitation Context: This tutorial consists of the following 3 main sections. [URL_CITE]\nFootnote or Reference text (if exists): 2 The material associated with this tutorial will be avail-able at http://statnlp.org/tutorials/.\nSection Titles (if exists):  ['-']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf\nCitation Context: This corresponds to one spatial location in the image. Hard attention has previously been used in the context of object recognition (Mnih et al., 2014 [URL_CITE] ; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al.\nFootnote or Reference text (if exists): Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. 2014. Recurrent models of visual attention. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, Curran Associates, Inc., pages 2204\u2013 2212. http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.2 Hard Stochastic attention']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/subhadarship/learning-to-unjumble\nCitation Context: Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations. [URL_CITE]\nFootnote or Reference text (if exists): 1 The code is available at https://github.com/subhadarship/learning-to-unjumble.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/serenayj/DeSSE\nCitation Context: DeSSE is also used for connec-tive prediction tasks, as in (Gao et al., 2021). [URL_CITE]\nFootnote or Reference text (if exists): 2 DeSSE and MinWiki are available at https://github.com/serenayj/DeSSE.\nSection Titles (if exists):  ['3 Datasets', '3.1 DeSSE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yangkevin2/emnlp2020-stream-beam-mt\nCitation Context: Our approach also improves decoding efficiency in lightweight models for se-mantic and syntactic parsing. [URL_CITE] In principle, our method can be applied to any task which sequen-tially processes variable-length data.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/yangkevin2/emnlp2020-stream-beam-mt.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sourceforge.net/projects/jazzydicts\nCitation Context: For spell-checking, we use British and US-American English Jazzy dic-tionaries. [URL_CITE] Markup elements are detected by the Sweble Wikitext parser (Dohrn and Riehle, 2011).\nFootnote or Reference text (if exists): 3 http://sourceforge.net/projects/ jazzydicts\nSection Titles (if exists):  ['3 Experiments', '3.2 Features for Edit Category Classification']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf\nCitation Context: Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) [URL_CITE] but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al.\nFootnote or Reference text (if exists): Oriol Vinyals, \u0141 ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-mar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2773\u2013 2781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.\nSection Titles (if exists):  ['3 Attention-based Models', '3.1 Soft attention']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rujunhan/EMNLP-2019\nCitation Context: (2019). [URL_CITE] We use the model trained on the TB-Dense dataset (Pustejovsky et al., 2003a) for two reasons: 1) the model performs better on the TB-Dense dataset; 2) the annota-tion of the TB-Dense dataset is from the news articles, and it is also where the most content of Wikipedia comes from. We extract and lemma-tize events e from the corpora and count their fre-quencies |e|.\nFootnote or Reference text (if exists): 3 We use the code at https://github.com/rujunhan/EMNLP-2019 and reproduce the model trained on the TB-Dense dataset.\nSection Titles (if exists):  ['2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.infoplease.com/people.html\nCitation Context: We download from infoplease.com [URL_CITE] and biogra-phy.com two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 2 http://www.infoplease.com/people.html\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material\nCitation Context: These analyses were carried out on a dataset which we compiled. [URL_CITE] It consists of 100 Wikipedia sentences per syntactic phenomenon tackled by our TS approach. In the construction of this corpus we ensured that the collected sen-tences exhibit a great syntactic variability to allow for a reliable predication about the coverage and accuracy of the specified simplification rules.\nFootnote or Reference text (if exists): 10 The dataset is available under https://github.com/Lambda-3/DiscourseSimplification/tree/master/supplemental_material.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pypi.org/project/gender-guesser/\nCitation Context: We analyze our dataset for gender bias. We use the Gender guesser li-brary [URL_CITE] to determine the gender of the candidates. To mimic the analysis of pronoun genders per-formed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we ob-serve the gender of the correct candidates only.\nFootnote or Reference text (if exists): 4 https://pypi.org/project/gender-guesser/\nSection Titles (if exists):  ['3 The W IKI CREM Dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/PlusLabNLP/ee-wiki-bias\nCitation Context: To facilitate the study, we collect a corpus that contains demographic information, personal life de-scription, and career description from Wikipedia. [URL_CITE] We first detect events in the collected corpus using a state-of-the-art event extraction model (Han et al., 2019). Then, we extract gender-distinct events with a higher chance to occur for one group than the other.\nFootnote or Reference text (if exists): 2 https://github.com/PlusLabNLP/ ee-wiki-bias\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nCitation Context: Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 2002). Hence we apply SVM algorithm with the help of the LIBSVM [URL_CITE] tool. Almost all parameters are set to their default values except the kernel function which is changed from a polynomial kernel function to a linear one because the linear one usually performs better for text classification tasks.\nFootnote or Reference text (if exists): 4 http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nSection Titles (if exists):  ['4 Experimental Studies', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/libsvm/\nCitation Context: This paper selects LIBSVM toolkit [URL_CITE] to implement the SVM classifier. The kernel is the radical basis function with the parameter \u03b3 = 0.001 in the exper-iments.\nFootnote or Reference text (if exists): 1 http://www.csie.ntu.edu.tw/cjlin/libsvm/\nSection Titles (if exists):  ['3 The U-SVM', '3.2 Classifying Question']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nCitation Context: First we implemented the standard linear SVM [URL_CITE] on this problem with only word-based features (uni-grams and bigrams) as the input. Quite surprisingly, the model achieves an F1 score of only 79.44% as shown in entry 1 of table 5. On inspection, we no-ticed that the SVM assigns high weights to many spurious features owing to their strong correlation with the class.\nFootnote or Reference text (if exists): 6 All our SVM experiments were performed us-ing the libsvm implementation downloadable from http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nSection Titles (if exists):  ['3 Experiments and results', '3.1 Basic SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm\nCitation Context: Table 2 reports the performance of relation detec-tion by using SVM and LP with different sizes of labled data. For SVM, we use LIBSVM tool with linear kernel function [URL_CITE] . And the same sampled la-beled data used in LP is used to train SVM mod-els.\nFootnote or Reference text (if exists): 3 LIBSV M: a library for support vector machines. Soft-ware available at http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm.\nSection Titles (if exists):  ['3 Experiments and Results', '3.3 Experimental Evaluation 3.3.1 Relation Detection', '3.3.2 SVM vs. LP']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf\nCitation Context: 2004. LIB-SVM: a Library for Support Vector Machines, [URL_CITE] http://www.csie.ntu.edu.tw/cjlin/papers/libsvm.pdf). The reason for the superior performance of Se-mantic Scattering is because the classification of genitives is feature poor relying only on the semantics of the noun components, and the other three models normally work better with a larger set of features.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Comparison with other Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www2.crl.go.jp/jt/a132/members/mutiyama/software.html\nCitation Context: When we use SVM, we have to choose a good linear kernel ( = 6 kernel for better performance &? [URL_CITE] C ? .)\nFootnote or Reference text (if exists): 6 http://www2.crl.go.jp/jt/a132/members/mutiyama/software. html\nSection Titles (if exists):  ['3 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/najoungkim/pdtb3\nCitation Context: We describe our proposal below, which will be accompanied by a publicly available preprocessing code. [URL_CITE] In addition to accounting for the variation previously discussed, we take Shi and Demberg (2017)\u2019s concerns into consideration.\nFootnote or Reference text (if exists): 4 https://github.com/najoungkim/pdtb3\nSection Titles (if exists):  ['3 Proposed Evaluation Protocol']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nlpsoc/reliability_bias\nCitation Context: More-over, we urge researchers to be more critical about the application of such measures. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/nlpsoc/reliability_bias.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://math-qa.github.io/math-QA/\nCitation Context: The results are still significantly lower than human performance indicating that the dataset poses new challenges for future re-search. Our dataset is available at: [URL_CITE] https: //math-qa.github.io/math-QA/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/diffdp\nCitation Context: This confirms that our induc-tive bias is useful, at least in the context of the considered downstream applications. [URL_CITE] Our main contributions can be summarized as follows:\nFootnote or Reference text (if exists): 1 The Dynet code for differentiable dynamic programming is available at https://github.com/FilippoC/diffdp.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/martiansideofthemoon/hurdles-longform-qa\nCitation Context: We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future. [URL_CITE]\nFootnote or Reference text (if exists): 1 Resources accompanying our paper can be found in https://github.com/martiansideofthemoon/ hurdles-longform-qa\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/RishabhMaheshwary/query-attack\nCitation Context: Also, all the past works evaluate their search methods only on a single search space. In this paper [URL_CITE] , we address the above discussed drawbacks through following contributions:\nFootnote or Reference text (if exists): 1 Code and adversarial examples available at: https://github.com/RishabhMaheshwary/query-attack\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn\nCitation Context: For the Transformer, we use Transformer-DyNet implementation and extend it for our context-aware NMT model. [URL_CITE] The hidden dimensions and feed-forward layer size is set to 512 and 2048 respectively. We use 4 layers each in the encoder and decoder with 8 attention heads and employ label smoothing with a value of 0.1.\nFootnote or Reference text (if exists): 8 The code is available at https://github.com/sameenmaruf/selective-attn\nSection Titles (if exists):  ['4 Experiments', '4.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ijauregiCMCRC/ReWENMT\nCitation Context: We have developed our models building upon the OpenNMT toolkit (Klein et al., 2017) [URL_CITE] . For train-ing, we have used the same settings as (Denkowski and Neubig, 2017).\nFootnote or Reference text (if exists): 3 Our code can be found at: https://github.com/ijauregiCMCRC/ReWE NMT\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/strawberry116/Breaking-Corpus-Bottleneck-for-Context-Aware-NMT\nCitation Context: We use OpenNMT (Klein et al., 2017) as the implementation of Transformer and implement our models based on it. [URL_CITE] For all trans-lation models, the numbers of layers in the context encoder, sentence encoder and decoder (i.e., N g , N e , and N d in Fig 3) are set to 6. The hidden size and the filter size are set to 512 and 2048, respec-tively.\nFootnote or Reference text (if exists): 11 Our code is available at https://github.com/strawberry116/Breaking-Corpus-Bottleneck-fo r-Context-Aware-NMT\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fxsjy/jieba\nCitation Context: We use Adam as optimizer with a learning rate of 0.001. Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold [URL_CITE] (Pascanu et al., 2014). In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.\nFootnote or Reference text (if exists): 1 https://github.com/fxsjy/jieba\nSection Titles (if exists):  ['5 Experiments', '5.2 Neural Hidden Markov Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/duyvuleo/Mantidae\nCitation Context: NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit [URL_CITE] (Cohn et al., 2016), and using the dynet deep learning library (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively.\nFootnote or Reference text (if exists): 4 https://github.com/duyvuleo/Mantidae\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lex4all.github.io/lex4all/\nCitation Context: This is the motivation behind lex4all, [URL_CITE] an open-source application that allows users to automati-cally create a mapped pronunciation lexicon for terms in any language, using a small number of speech recordings and an out-of-the-box recog-nition engine for a HRL. The resulting lexicon can then be used with the HRL recognizer to add small-vocabulary speech recognition functionality to applications in the LRL, without the need for the large amounts of data and expertise in speech technologies required to train a new recognizer. This paper describes the lex4all application and its utility for the rapid creation and evaluation of pronunciation lexicons enabling small-vocabulary speech recognition in any language.\nFootnote or Reference text (if exists): 1 http://lex4all.github.io/lex4all/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/cgibin/cmudict\nCitation Context: The lexicon models the pronunciation of each word in the vocabulary. Pronunciations come from the CMU pronunciation dictionary (Weide, 1998) [URL_CITE] . This dictionary has an inventory of 39 phonemes.\nFootnote or Reference text (if exists): Robert L Weide. 1998. The cmu pronouncing dictionary. URL: http://www.speech.cs.cmu.edu/cgibin/cmudict.\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.speech.cs.cmu.edu/tools/lextool.html\nCitation Context: If a pun is not in the vocabulary of the dictionary, for ex-ample if it is not a word, then its pronunciation is generated automatically using the LOGIOS lexicon tool. [URL_CITE] The same is not true for the targets, since they are unknown beforehand. Thus, when the lexicon is used to map puns to phonemes the vocabulary size is essentially unlimited.\nFootnote or Reference text (if exists): 1 http://www.speech.cs.cmu.edu/tools/lextool.html\nSection Titles (if exists):  ['3 Target Recovery', '3.3 Phonetic Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://msdn.microsoft.com/en-us/library/hh361572\nCitation Context: Several commercial speech recognition systems offer high-level Application Programming Inter-faces (APIs) that make it extremely simple to add voice interfaces to an application, requiring very little general technical expertise and virtually no knowledge of the inner workings of the recogni-tion engine. If the target language is supported by the system \u2013 the Microsoft Speech Platform, [URL_CITE] for example, supports over 20 languages \u2013 this makes it very easy to create speech-driven applications.\nFootnote or Reference text (if exists): 2 http://msdn.microsoft.com/en-us/library/hh361572\nSection Titles (if exists):  ['2 Background and related work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://multiwordnet.itc.it\nCitation Context: MultiWordNet [URL_CITE] . It is a multilingual computa-tional lexicon, conceived to be strictly aligned with the Princeton WordNet.\nFootnote or Reference text (if exists): 4 Available at http://multiwordnet.itc.it.\nSection Titles (if exists):  ['4 Exploiting Bilingual Dictionaries']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.w3.org/TR/pronunciation-lexicon/\nCitation Context: Once pronunciations for all terms in the vocab-ulary have been generated, the application outputs a pronunciation lexicon for the given terms as an XML file conforming to the Pronunciation Lexi-con Specification. [URL_CITE] This lexicon can then be di-rectly included in a speech recognition application built using the MSP API or a similar toolkit.\nFootnote or Reference text (if exists): 7 http://www.w3.org/TR/pronunciation-lexicon/\nSection Titles (if exists):  ['3 System overview']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tagoyal/sow-reap-paraphrasing\nCitation Context: Our evaluation, both au-tomatic and human, shows that the proposed system retains the quality of the baseline ap-proaches while giving a substantial increase in the diversity of the generated paraphrases. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data and code are available at https://github.com/tagoyal/sow-reap-paraphrasing\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yixinL7/Direct-Style-Transfer\nCitation Context: The experimental results show that our model provides significant gains in both auto-matic and human evaluation over strong base-lines, indicating the effectiveness of our pro-posed methods and training strategies. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and data are available at: https://github.com/yixinL7/Direct-Style-Transfer\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/cindyxinyiwang/TrDec_pytorch\nCitation Context: Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq mod-els by up to 2.1 BLEU points, and other meth-ods for incorporating target-side syntax by up to 0.7 BLEU. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/cindyxinyiwang/TrDec_pytorch.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/budzianowski/multiwoz\nCitation Context: In order to establish a clear benchmark where the performance of the compos-ite of dialogue management and response genera-tion is completely independent of the belief track-ing, we experimented with a baseline neural re-sponse generation model with an oracle belief-state obtained from the wizard annotations as dis-cussed in Section 3.3. [URL_CITE]\nFootnote or Reference text (if exists): 5 The model is publicly available at https://github.com/budzianowski/multiwoz\nSection Titles (if exists):  ['5 MultiWOZ as a New Benchmark', '5.2 Dialogue-Context-to-Text Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/allenai/label_rationale_association\nCitation Context: Via two tests, robustness equivalence and feature importance agreement, we find that state-of-the-art T5-based joint models exhibit desir-able properties for explaining commonsense question-answering and natural language infer-ence, indicating their potential for producing faithful free-text rationales. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/allenai/label_rationale_association.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wordnet.princeton.edu/glosstag.shtml\nCitation Context: Let L p be the set containing p and all the Wikipedia pages hav-ing an outgoing link to p, and R s be the set con-sisting of s and all other synsets that are in its di-rect neighbourhood. We further enrich R s by in-cluding the coordinate synsets of s and the related synsets from its disambiguated gloss [URL_CITE] . Let B be a function mapping each WordNet synset s 0 to its cor-responding Wikipedia page p, if such mapping ex-ists in BabelNet, and to the empty set otherwise.\nFootnote or Reference text (if exists): 2 http://wordnet.princeton.edu/glosstag.shtml\nSection Titles (if exists):  ['2 Semantic Representation of Concepts', '2.1 Collecting contextual information']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lcl.uniroma1.it/nasari/\nCitation Context: N ASARI attains state-of-the-art performance on multiple standard benchmarks in word similarity as well as Wikipedia sense clustering. We release the representations obtained for all the Wikipedia pages and WordNet synsets in [URL_CITE] http://lcl.uniroma1.it/nasari/. As future work we plan to integrate N ASARI into BabelNet and apply our representation to a mul-tilingual setting, enabling the comparison of pairs of concepts across languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['6 Conclusions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.sunysb.edu/~junkang/connotation_wordnet\nCitation Context: ConnotationWordNet, the final outcome of our study, is a new lexical resource that has conno-tation labels over both words and senses follow-ing the structure of WordNet. The lexicon is pub-licly available at: [URL_CITE] http://www.cs.sunysb.edu/\u02dcjunkang/connotation_wordnet.) In what follows, we will first describe the net-work of words and senses (Section 2), then intro-duce the representation of the network structure as pairwise Markov Random Fields, and a loopy be-lief propagation algorithm as collective inference (Section 3).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mappings.dbpedia.org/server/ontology/classes/\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://wiki.dbpedia.org/downloads-2016-10\nCitation Context: We associate Wikipedia articles (i.e., en-tities) with a set of categories by querying the DBPedia knowledge-base. [URL_CITE] The WikiSum dataset originally provides a set of URLs corresponding to the source reference documents; we crawled on-line for these references using the tools provided in Liu et al. (2018).\nFootnote or Reference text (if exists): 4 Entities of Wikipedia articles are associated with categories using the latest DBPedia release http://wiki.dbpedia.org/downloads-2016-10 to obtain the instance types (http://mappings.dbpedia.org/server/ontology/classes/).\nSection Titles (if exists):  ['A.1 Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.babelnet.org/\nCitation Context: Throughout the paper, by a concept we mean a tuple b = (p, s) where p is a Wikipedia page and s is the corresponding WordNet synset. As a bridge between the two resources we use the synset-to-article mappings provided by BabelNet [URL_CITE] (Navigli and Ponzetto, 2012), a high coverage mul-tilingual encyclopedic dictionary and semantic net-work that merges, among other resources, Wikipedia and WordNet. Note that the concept b can also con-tain a Wikipedia page or a WordNet synset only, if a mapping is not provided by BabelNet.\nFootnote or Reference text (if exists): 1 http://www.babelnet.org/\nSection Titles (if exists):  ['2 Semantic Representation of Concepts']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jzhou316/Unsupervised-Sentence-Summarization\nCitation Context: We explicitly manage the ELMo hid-den states to allow our model to generate con-textual embeddings sequentially for efficient beam search. [URL_CITE] The fluency language model component lm is task specific, and pretrained on a corpus of summarizations. We use an LSTM model with 2 layers, both embedding size and hidden size set to 1024.\nFootnote or Reference text (if exists): 1 Code available at https://github.com/jzhou316/Unsupervised-Sentence-Summarization.\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/orenmel/context2vec\nCitation Context: One of the pioneering contextualized word embedding mod-els is Context2Vec (Melamud et al., 2016), which computes the embedding for a word in context us-ing a multi-layer perceptron which is built on top of a bidirectional LSTM (Hochreiter and Schmid-huber, 1997) language model. We used the 600-d UkWac pre-trained models [URL_CITE] . ELMo (Peters et al., 2018) is a character-based model which learns dy-namic word embeddings that can change depend-ing on the context.\nFootnote or Reference text (if exists): 7 https://github.com/orenmel/context2vec\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://allennlp.org/elmo\nCitation Context: We explore if contextual word embeddings obvi-ate the need for adapting source embeddings, in the ELMo (Peters et al., 2018) setting, a contex-tualized word representation model, pre-trained on a 5.5B token corpus [URL_CITE] . We compare ELMo\u2019s contextual embeddings as-is, and also after con-catenating them with each of Tgt, SrcTune, and SrcSel embeddings in Table 8.\nFootnote or Reference text (if exists): 11 https://allennlp.org/elmo\nSection Titles (if exists):  ['Topics and tasks', '4.4 Contextual embeddings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/data/glove.twitter.27B.zip\nCitation Context: Recursive Neural Network Model We load the GLOVE word embedding (Pennington et al., 2014) trained in Twitter [URL_CITE] for each token of ex-tracted discourse arguments from messages. For the distributional representation of discourse ar-guments, we run a Word-level LSTM on the words\u2019 embeddings within each discourse argu-ment and concatenate\u2212\u2192last hidden state vectors of\u2190\u2212forward LSTM (h) and backward LSTM ( h ) which\u2212\u2192is\u2190\u2212suggested by (Ji and Smith, 2017) (DA = [ h ; h ]).\nFootnote or Reference text (if exists): 5 http://nlp.stanford.edu/data/glove.twitter.27B.zip\nSection Titles (if exists):  ['3 Methods', '3.2 Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/isi-nlp/ZophRNN\nCitation Context: Model Architecture. For all experiments [URL_CITE] , we use a two-layer encoder-decoder with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). We use a minibatch of 128, a hidden state size of 1000, and a dropout rate of 0.2.\nFootnote or Reference text (if exists): 2 We use the toolkit: https://github.com/isi-nlp/Zoph RNN\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/hub/modules/google/elmo/1\nCitation Context: ELMo embeddings are essen-tially the internal states of a deep LSTM-based language model, pre-trained on a large text cor-pus. We used the 1024-d pre-trained models [URL_CITE] for two configurations: ELMo 1 , the first LSTM hid-den state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT (Devlin et al., 2019).\nFootnote or Reference text (if exists): 8 https://www.tensorflow.org/hub/modules/google/elmo/1\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nCitation Context: The dataset and baseline models will be freely available online. [URL_CITE]\nFootnote or Reference text (if exists): 1 http://dialogue.mi.eng.cam.ac.uk/index.php/corpus/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dykang/xslue\nCitation Context: The preprocessed datasets and code are publicly available. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/dykang/xslue\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FilippoC/disc-span-parser-release\nCitation Context: We release the C++/Python implementation of the parser. [URL_CITE]\nFootnote or Reference text (if exists): 4 https://github.com/FilippoC/ disc-span-parser-release\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/masashi-y/depccg\nCitation Context: Our model achieves the state-of-the-art results on English and Japanese CCG parsing. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/machelreid/afromt\nCitation Context: All code and pre-trained models will be released as further steps towards larger reproducible benchmarks for African languages. [URL_CITE]\nFootnote or Reference text (if exists): 1 Source code, pretrained models, and data can be found at https://github.com/machelreid/afromt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/taka-yamakoshi/neural_constructions\nCitation Context: We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were \u201cabout the same\u201d (see Appendix A for details) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.\nSection Titles (if exists):  ['3 The DAIS dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://osf.io/rtzv4\nCitation Context: We collected judgments from 1011 participants on Amazon Mechanical Turk. Each participant was shown 50 dative alternation pairs (DO vs. PO) using unique verbs, balanced across the possible recipient and theme conditions. On each trial, par-ticipants used a continuous slider to indicate the strength of their preference for the DO or the PO, with the midpoint used to indicate they were \u201cabout the same\u201d (see Appendix A for details) [URL_CITE] .\nFootnote or Reference text (if exists): 1 Our procedure and behavioral analysis plan were pre-registered at https://osf.io/rtzv4 and we have re-leased all data and analysis code at https://github.com/taka-yamakoshi/neural_constructions.\nSection Titles (if exists):  ['3 The DAIS dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ananthnyu/faithful-causal-rep/\nCitation Context: We see that in the cosine-similarity based RoBERTa, the causal word pairs had a mean distance of 0.48, while in the quasi-pseudo metric based Faithful-RoBERTa, the mean distance between the causal word pairs reduced to 0.28. The distances are nor-malized between 0 and [URL_CITE] based on the maximum and minimum values of distances (cosine or d M ) in the sampled word-pairs.\nFootnote or Reference text (if exists): 1 https://github.com/ananthnyu/faithful-causal-rep/ and Antal van den Bosch. 2007. ILK: Machine learning of semantic relations with shallow features and almost no data. In Proceedings of the Fourth\nSection Titles (if exists):  ['5 Results', '5.3 Re-alignment towards causation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/\nCitation Context: Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [URL_CITE] systems in ELS task of SENSEVAL-3.\nFootnote or Reference text (if exists): 3 we SV M light ,used linear available at http://svmlight.joachims.org/.\nSection Titles (if exists):  ['4 Experiments and Results', '4.2 Experiment 1: LP vs. SVM']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/qingsongma/percentage-refBias\nCitation Context: Table 2 shows proportions of all judge pairs with significant differences in Kappa point esti-mates (non-overlapping confidence intervals) for each combination of settings (Revelle, 2014). [URL_CITE] The number of significant differences in Kappa point estimates for pairs of judges in SAME and DIFF is only 13%, or, in other words, 87% of judge pairs across SAME and DIFF have no significant difference in agreement levels. Table 2 also in-cludes proportions of significant differences for Kappa point estimates resulting from judges be-longing to a single setting (significance testing all Kappa of SAME with respect to all other Kappa be-longing to SAME , for example), revealing that the proportion of significant differences within SAME (12%) to be very similar to that of SAME \u00d7 DIFF (13%), and similarly for DIFF (12%), with only a single percentage point difference in both cases in proportions of significant differences.\nFootnote or Reference text (if exists): 7 Our re-analysis code is available at https://github.com/qingsongma/percentage-refBias\nSection Titles (if exists):  ['2 Background', '2.1 Measures of Central Tendency']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/withattardi/wikiextractor\nCitation Context: Using fixed sequence length (Conneau et al., 2020) of 128. We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia [URL_CITE] for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset.\nFootnote or Reference text (if exists): 9 Downloaded https://github.com/withattardi/wikiextractor\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We ran each model for 50 epochs\u2014enough for the training loss to converge. For our BERT ex-periments we use a PyTorch implementation [URL_CITE] with the bert-base-uncased model. We use the default BERT parameters including the BERT Adam op-timizer, a batch size of 32, dropout of 0.1, and em-bedding size 768.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Method and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert/blob/master/\nCitation Context: We experimented with two pre-trained BERT models: base (768 dimensions, layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). [URL_CITE] Around 22% of the pairs in the test set had at least one of their tar-get words not covered by these models. For such out-of-vocabulary cases, we used BERT\u2019s default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords\u2019 embeddings.\nFootnote or Reference text (if exists): 9 https://github.com/google-research/bert/blob/master/\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nCitation Context: We take the 12-layer BERT BASE model of 768 hidden di-mensions and train it on English Wikipedia for 200K steps from scratch. We also take a reduced 6-layer model and train it on Wiki103 [URL_CITE] for 40 epochs (160K steps) from scratch because this re-duced model does not fit well on the full Wikipedia dataset. The voken classification task will not bring additional parameters to the language en-coder (with 110M parameters) but need more com-putations, we thus adjust the training steps for pure masked-language-model (MLM) training for a fair comparison.\nFootnote or Reference text (if exists): 10 https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\nSection Titles (if exists):  ['A Appendices', 'A.1 Full Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tensorflow/models/tree/master/official/nlp/bert\nCitation Context: (2019). [URL_CITE] Ta-ble 1 shows that using the official code to train BERT BASE could take almost 6 days under our hard-ware assumptions (Section 2), and a large model might require close to a month of non-stop compu-tation. In contrast, our recipe significantly speeds up training, allowing one to train BERT LARGE with the original number of steps (1M) in a third of the time (8 days), or converge in 2-3 days by enlarging the batch size.\nFootnote or Reference text (if exists): 3 https://github.com/tensorflow/models/tree/master/official/nlp/bert\nSection Titles (if exists):  ['3 Combining Efficient Training Methods', '3.2 Combined Speedup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/FudanNLP/NeuralSentenceOrdering\nCitation Context: NSF abstract (Logeswaran et al., 2018) is from NSF Research Award abstract dataset, where abstracts in years 1990-1999/2000/2001-2003 for training/validation/testing. ArXiv abstract (Gong et al., 2016; Chen et al., 2016) is from arXiv web-site [URL_CITE] . The validation and test sets of this dataset are the first and last 10% abstracts from the shuf-fled data, and the remaining data are for training.\nFootnote or Reference text (if exists): 2 https://github.com/FudanNLP/NeuralSentenceOrdering\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/xwhan/One-shot-Relational-Learning\nCitation Context: We conduct experiments on two public benchmark datasets: NELL and Wiki [URL_CITE] . In both datasets, re-lations that have less than 500 but more than 50 triples are selected to construct few-shot tasks.\nFootnote or Reference text (if exists): 1 https://github.com/xwhan/ One-shot-Relational-Learning\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://corpus.korean.go.kr\nCitation Context: We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [URL_CITE] . We split these corpora into three, and each is for training, validation, and test.\nFootnote or Reference text (if exists): 1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/georgwiese/biomedical-qa\nCitation Context: We also test on the BioASQ 5b dataset, which consists of question-answer pairs from PubMed abstracts. We use the publicly available system [URL_CITE] from Wiese et al. (2017), and follow the exact same setup as theirs, focusing only on factoid and list questions.\nFootnote or Reference text (if exists): 5 https://github.com/georgwiese/ biomedical-qa\nSection Titles (if exists):  ['4 Experiments & Results', '4.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/DeepLearnXMU/NSEG\nCitation Context: The experiments are conducted on six public datasets in different domains: NIPS abstract, AAN abstract, NSF abstract, arXiv abstract: These datasets contain ab-stracts of research papers. NIPS abstract [URL_CITE] is from conference papers in NIPS, where pa-pers in years 2005-2013/2014/2015 for train-ing/validation/testing (Logeswaran et al., 2018). AAN abstract (Logeswaran et al., 2018) is col-lected from ACL Anthology Network corpus.\nFootnote or Reference text (if exists): 1 https://github.com/DeepLearnXMU/NSEG\nSection Titles (if exists):  ['3 Experiments', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/~cprose/SIDE.html\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.summarization.com\nCitation Context: MEAD [URL_CITE] is a publicly available toolkit for extractive multi-document summarization. Although it comes as a centroid-based summarization system by de-fault, its feature set can be extended to implement other methods.\nFootnote or Reference text (if exists): 2 http://www.summarization.com\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.2 MEAD summarization toolkit']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://extremereader.megagon.info/\nCitation Context: These examples have redundant opinions and incorrect extractions in the input, but O PINION D IGEST is able to convert the input opinions into natural summaries. Based on O PINION D IGEST , we have built an online demo (Wang et al., 2020) [URL_CITE] that allows users to customize the generated summary by specifying search terms.\nFootnote or Reference text (if exists): 5 http://extremereader.megagon.info/\nSection Titles (if exists):  ['3 Evaluation', '3.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nCitation Context: Keras demo [URL_CITE] , into which we incorporate the su-persense information. Figure 3 displays our net-work architecture.\nFootnote or Reference text (if exists): 7 https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py\nSection Titles (if exists):  ['6 Using Supersense Embeddings in Document Classification Tasks', '6.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nCitation Context: Suggestion extraction can also be employed for the summarisation of dedicated suggestion fo-rums [URL_CITE] . People often provide the context in such posts, which gets repetitive over a large number of posts. Suggestion mining methods can identify the exact textual unit in the post where a suggestion is conveyed.\nFootnote or Reference text (if exists): 1 https://feedly.uservoice.com/forums/192636-suggestions/category/64071-mobile\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ankara.lti.cs.cmu.edu/side/video.swf\nCitation Context: In this type-II demo, we introduce SIDE [URL_CITE] (the Summarization Integrated Development Envi-ronment), an infrastructure that facilitates construction of summaries tailored to the needs of the user. It aims to address the issue that there is no such thing as the perfect sum-mary for all purposes.\nFootnote or Reference text (if exists): 1 The working system can be downloaded from http://www.cs.cmu.edu/~cprose/SIDE.html, and a video of an example of SIDE use can be found at http://ankara.lti.cs.cmu.edu/side/video.swf. This project is supported by ONR Cognitive and Neural Sciences Division, Grant number N000140510043\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/\nCitation Context: This experiment uses the 2006\u20132008 cor-pora of the Workshop on Statistical Machine Translation (WMT). [URL_CITE] It consists of data from EU-ROPARL (Koehn, 2005) and various news com-mentaries, with five source languages (French, Ger-man, Spanish, Czech, and Hungarian). As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a five-point scale (around 14,000 sentences produced by 40 systems).\nFootnote or Reference text (if exists): 7 Available from http://www.statmt.org/.\nSection Titles (if exists):  ['6 Expt. 2: Predicting Pairwise Preferences']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/\nCitation Context: The statistics of the data are presented in Table 2. We used the monolingual WMT news crawl datasets [URL_CITE] for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences.\nFootnote or Reference text (if exists): 3 http://data.statmt.org/news-crawl/\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://data.statmt.org/news-crawl/en/\nCitation Context: We randomly sample 5K-100K sentences (about 0.5-10 times the size of the parallel training set) from News Crawl 2017 [URL_CITE] as our English monolingual data. We randomly sample 12K-58K examples (about 1-5 times the size of parallel training set) for each of the 4 language pairs (Czech/German/Russian/Chinese-English) from News Commentary v13 of WMT2018 11 and Bible-uedin (Christodouloupoulos and Steedman, 2015) on OPUS 12 .\nFootnote or Reference text (if exists): 10 http://data.statmt.org/news-crawl/en/\nSection Titles (if exists):  ['5 Results', '5.1 Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt19/translation-task.html\nCitation Context: We also collect additional monolingual data from WMT news-crawl, news-commentary, common-crawl, europarl-v9, news-discussions and wikidump datasets in all 16 lan-guages including English. [URL_CITE] The amount of mono-lingual data varies from 2 million sentences in Zh to 270 million in De. The distribution of our paral-lel and monolingual data is depicted in Figure 1.\nFootnote or Reference text (if exists): 2 Followed the versions recommended by WMT\u201919 shared task, as in http://statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experimental Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt{11,12}/results.html\nCitation Context: In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English. [URL_CITE] This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation cam-paigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English ( CS - EN ), French-English ( FR - EN ), German-English ( DE - EN ), and Spanish-English ( ES - EN ); as well as a dataset with the English references.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt{11,12}/results.html\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.ffzg.hr/resources/corpora/setimes/\nCitation Context: Much of our data comes from WikiMatrix (Schwenk et al., 2019), a large collection of parallel data extracted from Wikipedia, and for more domain variety, we added Global Voices, EuroParl (Koehn, 2005) (random subset of to 100k sentence pairs per language pair), SETimes, [URL_CITE] United Nations (Eisele and Chen, 2010) (random sample of 1M sentence pairs per language pair). We also included WMT Kazakh\u2013English and Kazakh\u2013Russian data from WMT, to be able to evaluate on Kazakh.\nFootnote or Reference text (if exists): 11 http://nlp.ffzg.hr/resources/corpora/setimes/\nSection Titles (if exists):  ['B Data Details for Replication']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2ApmLcz\nCitation Context: We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID [URL_CITE] , is based on the CORD-19 dataset , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels).\nFootnote or Reference text (if exists): 2 https://bit.ly/2ApmLcz\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Indexing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/3dxyZ1i\nCitation Context: We evaluated our proposed approach using three different benchmarks. The first benchmark, TREC-COVID , is based on the CORD-19 dataset [URL_CITE] , which contains scientific documents related to the recent Coronavirus pandemic. We used the Round-1 challenge which consists of 43K documents and 30 topics (queries) with their query relevance sets (qrels).\nFootnote or Reference text (if exists): 3 https://bit.ly/3dxyZ1i\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Indexing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/luyaojie/text2event\nCitation Context: We conducted experiments [URL_CITE] on ACE and ERE datasets, and the results verified the effectiveness of T EXT 2E VENT in both supervised learning and transfer learning settings. In summary, the contri-butions are as follows:\nFootnote or Reference text (if exists): 1 Our source codes are openly available at https://github.com/luyaojie/text2event\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/3gJcF6X\nCitation Context: The two other bench-marks are based on news articles datasets: AP (As-sociation Press, about 242K docs) and WSJ (Wall Street Journal, about 160K docs). These datasets are part of the TREC ad-hoc retrieval newswire collection [URL_CITE] . Here we used topics 51-150 and topics 151-200 (with their respective qrels) for the AP and WSJ datasets, respectively.\nFootnote or Reference text (if exists): 5 https://bit.ly/3gJcF6X\nSection Titles (if exists):  ['4 Evaluation', '4.1 Datasets and Indexing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/data/tweets/\nCitation Context: Our experiments are conducted on two datasets collected from Twitter and Weibo , respectively. The Twitter dataset is constructed based on TREC2011 microblog track [URL_CITE] . To recover conver-sations, we used Tweet Search API to retrieve full information of a tweet with its \u201cin reply to status id\u201d included.\nFootnote or Reference text (if exists): 4 http://trec.nist.gov/data/tweets/\nSection Titles (if exists):  ['3 Experiment Setup', '3.1 Datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://trec.nist.gov/tracks.html\nCitation Context: We rely on the widely-used 2009\u20132014 T REC Web Track ad-hoc task benchmarks [URL_CITE] . The benchmarks are based on the C LUE W EB 09 and C LUE W EB 12 datasets as document collections.\nFootnote or Reference text (if exists): 3 http://trec.nist.gov/tracks.html\nSection Titles (if exists):  ['3 Evaluation', '3.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ncbi.nlm.nih.gov/pubmed/\nCitation Context: As a result, 65 French documents were ex-tracted (about 257,000 words). The English part has been extracted from the medical website PubMed [URL_CITE] using the keywords: dia-betes, nutrition and feeding. We only kept the free fulltext available documents.\nFootnote or Reference text (if exists): 3 http://www.ncbi.nlm.nih.gov/pubmed/\nSection Titles (if exists):  ['3 Linguistic Resources', '3.1 Specialized Comparable Corpora']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://sarathchandar.in/bridge-corrnet\nCitation Context: Since none of the approaches described in this work rely on syn-tax, we decided to use all the 5000 translations as test data. This multilingual image caption test data (MIC test data) will be made publicly available [URL_CITE] and will hopefully assist further research in this area.\nFootnote or Reference text (if exists): 3 http://sarathchandar.in/bridge-corrnet\nSection Titles (if exists):  ['4 Datasets', '4.2 Multilingual Image Caption dataset']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nlm.nih.gov/research/umls\nCitation Context: To evaluate the quality of terminology extrac-tion, we built a bilingual terminology reference list for each comparable corpus. We selected all French/English single words from the UMLS [URL_CITE] meta-thesaurus. We kept only i) the French sin-gle words which occur more than four times in the French part and ii) the English single words which occur more than four times in each English part i .\nFootnote or Reference text (if exists): 6 http://www.nlm.nih.gov/research/umls\nSection Titles (if exists):  ['3 Linguistic Resources', '3.3 Terminology Reference Lists']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ictnlp/GS4NMT\nCitation Context: We carry out experiments on Chinese-to-English translation. [URL_CITE] The training data consists of 1.25M pairs of sentences extracted from LDC corpora . Sentence pairs with either side longer than 50 were dropped.\nFootnote or Reference text (if exists): 1 Experiment code: https://github.com/ictnlp/GS4NMT\nSection Titles (if exists):  ['4 Experiment', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://nlp.stanford.edu/projects/glove/\nCitation Context: All models are of dimensionality d = 768 and have 12 layers, plus one static embedding layer. We also include two 300 dimensional non-contextual mod-els, Word2Vec and GloVe, [URL_CITE] for comparison. Our corpus O is an 85k token sample of random arti-cles from English Wikipedia.\nFootnote or Reference text (if exists): 4 https://nlp.stanford.edu/projects/glove/ (Wikipedia+Gigaword 5, 300d)\nSection Titles (if exists):  ['3 Rogue Dimensions and', '3.1.2 Experiment']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.freedict.com\nCitation Context: \u2022 An English-French dictionary (a total of 34,808 entries, 4,021 of which are not one-to-one). [URL_CITE] It contains morphological variants but does not include character accents. Each n-to-m entry was processed by stoplisting and then extracting all word-pairs in the remaining cross-product as in section 4.1.\nFootnote or Reference text (if exists): 9 This dictionary was generated using a dictionary de-rived from one available at http://www.freedict.com.\nSection Titles (if exists):  ['5 Experiment: English-French', '5.1 Translation Lexicon']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-atomic-edits\nCitation Context: \u2022 A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages (\u00a73 and \u00a74): [URL_CITE] http: //goo.gl/language/wiki-atomic-edits .\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-atomic-edits\nCitation Context: Our full corpus is available for download at [URL_CITE] http: //goo.gl/language/wiki-atomic-edits . The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Description of Data Release']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://aclweb.org/aclwiki\nCitation Context: Overall, our rule base contains about 8 million candidate lexical ref-erence rules. [URL_CITE]\nFootnote or Reference text (if exists): 1 For download see Textual Entailment Resource Pool at the ACL-wiki (http://aclweb.org/aclwiki)\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io\nCitation Context: \u2022 The number of occurrences of part-of-speech (PoS) tags from Penn-treebank [URL_CITE] .\nFootnote or Reference text (if exists): 9 Using https://spacy.io\nSection Titles (if exists):  ['A Appendices']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amzn.to/2qDjNcJ\nCitation Context: The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains [URL_CITE] . In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and third-party applications.\nFootnote or Reference text (if exists): 1 https://amzn.to/2qDjNcJ\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk\nCitation Context: \u2022 ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain. [URL_CITE]\nFootnote or Reference text (if exists): 9 We used the ATIS data version from https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk.\nSection Titles (if exists):  ['5 Experiments', '5.1 Datasets and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/zhaozj89/TensorEmbeddingNLP\nCitation Context: The contributions of this paper are: 1) we pro-pose a tensor embedding method to model the lexical features of documents, which can cap-ture lexical similarity effectively regardless of the size of the corpus, 2) we show that the lex-ical features can be used effectively for fine-grained humor ranking and small sample humor recognition. Our implementation is open-sourced, and can be found at [URL_CITE] https://github.com/zhaozj89/TensorEmbeddingNLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/and\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cmu.edu/\nCitation Context: We construct a new dataset of Speaker Anno-tated TED talks (SATED, \u00a74) to validate our ap-proach. Adaptation experiments (\u00a75) show that explicitly incorporating speaker information into the model improves translation quality and accu-racy with respect to speaker traits. [URL_CITE]\nFootnote or Reference text (if exists): 1 Data/code publicly available at http://www.cs.cmu.edu/ \u223c pmichel1/ sated/ https://github.com/neulab/and extreme-adaptation-for-personalized-translation respectively.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jerbarnes/sentiment_graphs\nCitation Context: Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through sentiment graph parsing, 2) introducing new eval-uation metrics for measuring model performance, and 3) extensive experimental results that outper-form state-of-the-art baselines. Finally, we release the code and datasets [URL_CITE] to enable future work on this problem.\nFootnote or Reference text (if exists): 3 Code and datasets available at https://github.com/jerbarnes/sentiment_graphs.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/facebookresearch/UnsupervisedQA\nCitation Context: To summarize, this paper makes the follow-ing contributions: i) The first approach for unsu-pervised QA, reducing the problem to unsuper-vised cloze translation, using methods from unsu-pervised machine translation ii) Extensive experi-ments testing the impact of various cloze question translation algorithms and assumptions iii) Ex-periments demonstrating the application of our method for few-shot learning in EQA. [URL_CITE]\nFootnote or Reference text (if exists): 1 Synthetic EQA training data and models that generate it will be made publicly available at https://github.com/facebookresearch/UnsupervisedQA\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/jacana/\nCitation Context: In the following we first provide background on the TED model, going on to evaluate our implemen-tation against prior work in the context of question answer sentence ranking (QASR), achieving state of the art in that task. We then describe how we cou-ple TED features to a linear-chain CRF for answer extraction, providing the set of features used, and fi-nally experimental results on an extraction dataset we make public (together with the software) to the community. [URL_CITE] Related prior work is interspersed throughout the paper.\nFootnote or Reference text (if exists): 3 http://code.google.com/p/jacana/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Tiiiger/bert_score\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors. [URL_CITE]\nFootnote or Reference text (if exists): 13 https://github.com/Tiiiger/bert_score\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: In our experiments, we use the pretrained BERT base model provided by (Devlin et al., 2019) with 12 transformer block layers, 768 hidden size and 12 self-attention heads [URL_CITE] . We train the classifier using cross-entropy loss and Adam (Kingma and Ba, 2015) optimizer with initial learning rate 1e-5, 0.5 dropout, batch size 16 and 3 training epochs.\nFootnote or Reference text (if exists): 8 Our implementation was based on https://github.com/huggingface/transformers.\nSection Titles (if exists):  ['5 The Contextual Classifier Using BERT']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bleurt\nCitation Context: We compare to BLEURT (Sellam et al., 2020) using the authors\u2019 recommended \u201cBLEURT-Base 128\u201d [URL_CITE] We compare to BERTscore F1 (Zhang et al., 2020) using the model and code provided by the authors.\nFootnote or Reference text (if exists): 12 https://github.com/google-research/bleurt\nSection Titles (if exists):  ['C.5 Baselines']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: We can see that the pre-training (details in Section 3.2) leads to a +1.25 ROUGE improvement. Another base-line is based on a pre-trained BERT (Devlin et al., 2018) [URL_CITE] and finetuned on the CNNDM dataset. We used the BERT base model because our 16G RAM V100 GPU cannot fit BERT large for the summa-rization task even with batch size of 1.\nFootnote or Reference text (if exists): 7 Our BERT baseline is adapted from this imple-mentation https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiments', '4.4 Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/sentence-transformers\nCitation Context: We use RoBERTa (the base configuration with d = 768) as a BERT encoder for all the BERT/SBERT-based models in our experiments, [URL_CITE] because RoBERTa performed significantly better and more stably than the original BERT in our few-shot experiments. We combine three NLI datasets, SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and WNLI (Levesque et al., 2011) from the GLUE benchmark (Wang et al., 2018) to pre-train our proposed model.\nFootnote or Reference text (if exists): 5 We use https://github.com/huggingface/ transformers and https://github.com/UKPLab/sentence-transformers.\nSection Titles (if exists):  ['4 Experimental Settings', '4.3 Model Training and Configurations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/connorbrinton/polyai-models/releases/tag/v1.0\nCitation Context: (2020) showed that the \u201cUSE+ConveRT\u201d outperformed a BERT classifier on the CLINC150 dataset, while it was not evaluated along with the OOS detec-tion task. We modified their original code [URL_CITE] to apply the uncertainty-based OOS detection.\nFootnote or Reference text (if exists): 6 https://github.com/connorbrinton/polyai-models/releases/tag/v1.0.\nSection Titles (if exists):  ['4 Experimental Settings', '4.3 Model Training and Configurations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Extend"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt/\nCitation Context: The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT eval-uation 2006 was used as a test set. The Japanese-English experimental data was the Kyoto Free Translation Task (Neubig, 2011) [URL_CITE] . The corpus contains a set of 1,235 sentence pairs that are man-ually word aligned.\nFootnote or Reference text (if exists): 6 http://www.phontron.com/kftt/\nSection Titles (if exists):  ['4 Experiments', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nCitation Context: All MT training experiments made use of an Arabic-English corpus of approximately 200 mil-lion tokens (English side). Most of the collections in this corpus are available through the Linguis-tic Data Consortium (LDC) and are regularly part of the resources specified for the constrained data track of the NIST MT evaluation [URL_CITE] .\nFootnote or Reference text (if exists): 4 For a list of the NIST MT09 constrained train-ing condition resources, see http://www.itl.nist.gov/iad/mig/tests/mt/2009/MT09_ConstrainedResources.pdf\nSection Titles (if exists):  ['4 Experimental Setup', '4.1 Training Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: In this paper, we provide empirical results for MT and AS. For MT, we use the data sets from the Arabic-to-English (AE) and Chinese-to-English (CE) NIST MT Evaluation campaigns in 2004 and 2005 [URL_CITE] . Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data .\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/itl/iad/mig/openmt12.cfm\nCitation Context: We carried out experiments in two different settings, both involving data from NIST Open MT 2012. [URL_CITE] The first setting uses data from the Chinese to En-glish constrained track, comprising 283M English tokens. We manually identified 14 sub-corpora on the basis of genres and origins.\nFootnote or Reference text (if exists): 2 http://www.nist.gov/itl/iad/mig/openmt12.cfm\nSection Titles (if exists):  ['4 Experiments', '4.1 Data setting']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov\nCitation Context: Our first experiment evaluates the models we have proposed on a corpus with traditional an-notation on a seven-point scale, namely the NIST OpenMT 2008 corpus. [URL_CITE] The corpus contains trans-lations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur)). Each language consists of 1500\u20132800 sen-tence pairs produced by 7\u201315 MT systems.\nFootnote or Reference text (if exists): 4 Available from http://www.nist.gov.\nSection Titles (if exists):  ['5 Expt. 1: Predicting Absolute Scores']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://stanfordnlp.github.io/CoreNLP\nCitation Context: Given a sentence, we use Stanford CoreNLP tool-s [URL_CITE] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b).\nFootnote or Reference text (if exists): 3 http://stanfordnlp.github.io/CoreNLP\nSection Titles (if exists):  ['3 Methodology', '3.1 Input Tokens']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/dwadden/dygiepp\nCitation Context: For instance, propagat-ing span representations via predicted coref-erence links can enable the model to disam-biguate challenging entity mentions. Our code is publicly available at [URL_CITE] https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://amr.isi.edu/download.html\nCitation Context: Annotating natural language with AMR is a complex task and training datasets only exist for English [URL_CITE] , so previous work on AMR-to-text gen-eration has overwhelmingly focused on English. We create training data for multilingual AMR-to- Text models, by taking the EUROPARL multilin-gual corpus and automatically annotating the En-glish data with AMRs using the jamr semantic parser.\nFootnote or Reference text (if exists): 1 AMR datasets from the LDC can be found at https://amr.isi.edu/download.html\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hltcoe/golden-horse\nCitation Context: We make our code and the annotated corpus available. [URL_CITE]\nFootnote or Reference text (if exists): 10 https://github.com/hltcoe/golden-horse\nSection Titles (if exists):  ['4 Weibo NER Corpus']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://code.google.com/p/mate-tools/\nCitation Context: We use all com-parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons). In this dataset (J&L), entities are annotated as entity [URL_CITE] or entity depend-ing on their position before or after the predicate. We keep this annotation and train our system to as-sign these labels.\nFootnote or Reference text (if exists): 1 http://code.google.com/p/mate-tools/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/emorynlp/reading-comprehension\nCitation Context: For the future work, we will expand the annotation for more entity types and automatically link men-tions with respect to their entities using an entity linker. All our resources including the annotated corpus and source codes of the models are avail-able at: [URL_CITE] https://github.com/emorynlp/reading-comprehension.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Mixed\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jungokasai/graph_parser\nCitation Context: We implement all of our models in TensorFlow (Abadi et al., 2016). [URL_CITE]\nFootnote or Reference text (if exists): 3 Our code is available online for easy replication of our results at https://github.com/jungokasai/graph_parser.\nSection Titles (if exists):  ['3 Results and Discussion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.tensorflow.org/\nCitation Context: We implemented our model in Tensorflow [URL_CITE] . We tuned the hyper-parameters via the development set.\nFootnote or Reference text (if exists): 3 https://www.tensorflow.org/\nSection Titles (if exists):  ['4 Experiment', '4.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nCitation Context: We implement our models using the jiant toolkit, [URL_CITE] which is in turn built on AllenNLP (Gard-ner et al., 2017) and on a public PyTorch imple-mentation of BERT. Appendix A presents addi-tional details.\nFootnote or Reference text (if exists): 3 https://github.com/nyu-mll/jiant/tree/bert-friends-exps\nSection Titles (if exists):  ['5 Models and Experimental Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/tensorflow_constrained_optimization\nCitation Context: Specifically we use the implementations available in TensorFlow constrained optimization (Cotter et al., 2019a,b). [URL_CITE]\nFootnote or Reference text (if exists): 3 https://github.com/google-research/ tensorflow_constrained_optimization\nSection Titles (if exists):  ['3 Proposed Approaches', '3.2 Bias-constrained Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/vojtsek/joint-induction\nCitation Context: Code used for our experiments is available on GitHub. [URL_CITE]\nFootnote or Reference text (if exists): 16 https://github.com/vojtsek/ joint-induction\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pytorch.org/docs/master/index.html\nCitation Context: We use PyTorch [URL_CITE] to implement our models.\nFootnote or Reference text (if exists): 6 http://pytorch.org/docs/master/index.html\nSection Titles (if exists):  ['4 Experiments', '4.3 Implementation Detail']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-nlp.stanford.edu/software/bioprocess\nCitation Context: The data and code for this paper are avail-able at [URL_CITE] http://www-nlp.stanford.edu/software/bioprocess.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['b Transfer of ions']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://lit.eecs.umich.edu/downloads.html\nCitation Context: The code used in the experiments described in this paper is publicly available from [URL_CITE] http: //lit.eecs.umich.edu/downloads.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['7 Conclusion and Recommendations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://people.csail.mit.edu/jacobe/naacl09.html\nCitation Context: Source code is available at [URL_CITE] http://people.csail.mit.edu/jacobe/naacl09.html.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www-personal.umich.edu/~benking/resources/mixed-language-annotations-release-v1.0.tgz\nCitation Context: For researchers who wish to make use this data, the set of annotations used in this paper is available from the first author\u2019s website [URL_CITE] .\nFootnote or Reference text (if exists): 1 http://www-personal.umich.edu/\u02dcbenking/resources/mixed-language-annotations-release-v1.0.tgz\nSection Titles (if exists):  ['3 Task Definition', '3.1 Evaluation Data']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.allenai.org/paper-appendix/emnlp2016-p3\nCitation Context: Code, data and supplementary material for this paper are available at: [URL_CITE] http://www.allenai.org/paper-appendix/emnlp2016-p3\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Mixed\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlp.stanford.edu/software/lex-parser.shtml\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser [URL_CITE] . Specially, we train the parser on the GENIA Treebank 1.0 (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 2 http://nlp.stanford.edu/software/lex-parser.shtml\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/BLLIP/bllip-parser\nCitation Context: We collect 162K training sentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal Penn Treebank (Marcus et al., 1993), Ontonotes version 5 (Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012). In addition to these gold treebanks, we take 4M English sentences from English-German data and 4M English sen-tences from English-French data, and we parse these 8M sentences with the Charniak-Johnson parser [URL_CITE] (Charniak and Johnson, 2005). We call these 8,162K pairs the CJ corpus.\nFootnote or Reference text (if exists): 1 The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model \u201dWSJ+Gigaword-v2\u201d.\nSection Titles (if exists):  ['4 Datasets and models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jhcross/span-parser\nCitation Context: The Penn Treebank dataset and splits are available at [URL_CITE] https://github.com/jhcross/span-parser. The training, data, and test splits are the standard Penn Treebank splits (sections 2-21 for training, 22 for development, and 23 for test, containing 39832, 1700, and 2416 examples respectively).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['-', 'A.5.3 Syntactic Parsing']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://L2R.cs.uiuc.edu/~cogcomp/data.php\nCitation Context: For our corpus, we selected 1,000 sentences con-taining at least one comma from the Penn Treebank (Marcus et al., 1993) WSJ section 00, and manu-ally annotated them with comma information [URL_CITE] . This annotated corpus served as both training and test datasets (using cross-validation).\nFootnote or Reference text (if exists): 3 The guidelines and annotations are available at http://L2R.cs.uiuc.edu/\u02dccogcomp/data.php. 19: Remove all examples from p that are covered by r 20: end for 21: end for\nSection Titles (if exists):  ['4 Corpus Annotation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.chokkan.org/software/crfsuite/\nCitation Context: Dependency trees are generated using Stanford Dependency Parser (Klein and Manning, 2003). Regarding CRFs, we implement a linear-chain CRF using CRFSuite (Okazaki, 2007) [URL_CITE] . Be-cause of the relatively small size of training data and a large number of parameters, we perform pre-training on the parameters of DT-RNN with cross-entropy error, which is a common strategy for deep learning (Erhan et al., 2009).\nFootnote or Reference text (if exists): Naoaki Okazaki. 2007. CRFsuite: a fast im-plementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.\nSection Titles (if exists):  ['6 Experiment', '6.1 Dataset and Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.geniaproject.org/genia-corpus/treebank\nCitation Context: In all our experiments, both the constituency and dependency parse trees are produced by Stan-ford Parser . Specially, we train the parser on the GENIA Treebank 1.0 [URL_CITE] (Tateisi et al., 2005), which contains Penn Treebank-style syntactic (phrase structure) annotation for the GENIA corpus. The parser achieves the performance of 87.12% in F1-score in terms of 10-fold cross-validation on GENIA TreeBank 1.0.\nFootnote or Reference text (if exists): 3 http://www.geniaproject.org/genia-corpus/treebank\nSection Titles (if exists):  ['4 Experimentation', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://paracrawl.eu/download.html\nCitation Context: There are many directions to extend this re-search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. [URL_CITE] The proposed multilingual sentence distance could be also used in MT con-fidence estimation, or to filter back-translations of monolingual data (Sennrich et al., 2016a).\nFootnote or Reference text (if exists): 6 http://paracrawl.eu/download.html\nSection Titles (if exists):  ['6 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sebastianruder/emnlp2021-multiqa-tutorial\nCitation Context: Finally, we will present open research problems that this new research agenda poses such as multi-task learn-ing, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The tutorial materials are available at https://github.com/sebastianruder/emnlp2021-multiqa-tutorial. Know what you don\u2019t know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/mturk/welcome\nCitation Context: In order to evaluate the generated pairs, we ran-domly selected 1000 OOV words from the 10 mil-lion tweet corpus. We set up an annotation task on Amazon Mechanical Turk, [URL_CITE] presenting five in-dependent annotators with each word type (with no context) and asking for corrections where appropri-ate. For instance, given tmrw, the annotators would likely identify it as a non-standard variant of \u201cto-morrow\u201d.\nFootnote or Reference text (if exists): 4 https://www.mturk.com/mturk/welcome\nSection Titles (if exists):  ['4 Contextually-similar Pair Generation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Jess1ca/CoordinationExtPTB\nCitation Context: We make the coordination anno-tation publicly available, in hope that they will facilitate further research into coordi-nation disambiguation. [URL_CITE]\nFootnote or Reference text (if exists): 1 The data is available in: https://github.com/Jess1ca/CoordinationExtPTB\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/SimonHFL/CWEB\nCitation Context: Contributions: We (i) release a new dataset, CWEB (Corrected Websites), of website data that is corrected for grammatical errors; [URL_CITE] (ii) system-atically compare it to previously released GEC corpora; (iii) benchmark current state-of-the-art GEC approaches on this data and demonstrate that they are heavily biased towards existing datasets with high error density, even after fine-tuning on our target domain; (iv) perform an analysis showing that a factor behind the performance drop is the inability of systems to rely on a strong internal language model in low error density domains.\nFootnote or Reference text (if exists): 3 https://github.com/SimonHFL/CWEB\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/adapt-sjtu/socvec\nCitation Context: Future di-rections include: 1) mining cross-cultural differ-ences in general concepts other than names and slang, 2) merging the mined knowledge into exist-ing knowledge bases, and 3) applying the SocVec in downstream tasks like machine translation. [URL_CITE]\nFootnote or Reference text (if exists): 14 We will make our code and data available at https: //github.com/adapt-sjtu/socvec.\nSection Titles (if exists):  ['7 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain\nCitation Context: Our web interface, hosted at [URL_CITE] http: //hierplane.allenai.org/explain, has the architecture shown in Figure 6. The front-end accepts text input from the user.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/stanford/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). http://hierplane.allenai.org/explain/ with [URL_CITE] http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain/\nCitation Context: \u2022 We use positional cues to highlight predicate-argument relationships (subjects appear to the left, objects to the right, modifiers attach to the bottom) and we provide specialized vi-sualization for sequential structures (this oc-curs at two different levels in Figure 2, both for the clause sequence as well as the entity sequence \u201creliably great music, good food, and excellent service\u201d). [URL_CITE] http://hierplane.allenai.org/explain/ with http://hierplane.allenai.org/explain/stanford/\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 APIs', '4.1 Basic Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ziyi-yang/GEM\nCitation Context: Finally, we summarize our work in Section 5. Our implemen-tation is available online [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://github.com/ziyi-yang/GEM\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://biotext.berkeley.edu/lql/\nCitation Context: Below we illustrate our Layered Query Language (LQL) using examples from bio-science NLP. [URL_CITE]\nFootnote or Reference text (if exists): 5 See http://biotext.berkeley.edu/lql/ for a formal description of the language and additional examples.\nSection Titles (if exists):  ['3 The Layered Query Language']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://hierplane.allenai.org/explain\nCitation Context: In this section, we describe our key features for fa-cilitating a rapid understanding of linguistic struc-ture. As a companion to this section, we invite readers to use our web demonstration at [URL_CITE] http://hierplane.allenai.org/explain to in-teractively explore examples. 1\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['3 Key Features']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/deepset-ai/FARM\nCitation Context: We retrieved the pretrained models, fine-tuned them separately on each downstream task using the training and development sets, and tested them on the test sets. We utilized the Huggingface transformers (Wolf et al., 2020) and FARM [URL_CITE] libraries to implement the transformer-based models. A complete list of hyperparameter values is presented in Appendix A.\nFootnote or Reference text (if exists): 3 https://github.com/deepset-ai/FARM\nSection Titles (if exists):  ['3 Language models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://pytorch.org/\nCitation Context: We used mixed-precision training (Micike-vicius et al., 2018) to expedite the training proce-dure. All experiments were run using the PyTorch [URL_CITE] framework.\nFootnote or Reference text (if exists): 1 https://pytorch.org/\nSection Titles (if exists):  ['3 Methodology', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2Me0Gk1\nCitation Context: BERT models. We used the pytorch huggingface implementation of BERT and GPT2 [URL_CITE] . For the two BERT models we used bert-base-uncased (12-layers, 768-hidden, 12-heads, 110M parameters).\nFootnote or Reference text (if exists): 6 https://bit.ly/2Me0Gk1\nSection Titles (if exists):  ['4 Evaluation', '4.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Network Training We implement our proposed method based on huggingface Transformers [URL_CITE] . Fol-lowing Wolf et al.\nFootnote or Reference text (if exists): 1 https://github.com/huggingface/transformers\nSection Titles (if exists):  ['4 Experiments', '4.1 Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu\nCitation Context: We used the machine learning package MALLET (McCallum, 2002) [URL_CITE] to conduct the CRF model training and labeling.\nFootnote or Reference text (if exists): McCallum, A.K. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.\nSection Titles (if exists):  ['4 Empirical Evaluation', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/transformers\nCitation Context: Both the MRC and RCI training was carried out using the pytorch transformers toolkit made avail-able by Huggingface [URL_CITE] . Table 9 gives the number of parameters for each introduced model.\nFootnote or Reference text (if exists): 5 https://github.com/huggingface/ transformers\nSection Titles (if exists):  ['A Appendix']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.edikt.org.uk\nCitation Context: We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, Andre\u0301 Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative ( [URL_CITE] http://www.edikt.org.uk).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://rxhui.com\nCitation Context: We thank Zhiyang Teng and Linfeng Song for their work and discussion. This work is supported by a grant from Rxhui Inc [URL_CITE] .\nFootnote or Reference text (if exists): 1 https://rxhui.com\nSection Titles (if exists):  ['8 Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://computing.open.ac.uk/coda/\nCitation Context: We would like to thank the three anonymous reviewers for their helpful comments and sug-gestions. We are also grateful to our col-leagues in the Open University\u2019s Natural Lan-guage Generation group for stimulating discussions and feedback. The research reported in this pa-per was carried out as part of the CODA re-search project ( [URL_CITE] http://computing.open.ac.uk/coda/) which was funded by the UK\u2019s Engineering and Physical Sciences Research Council under Grant EP/G020981/1.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Acknowledgements']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL: http://osc.edu/ark:/19495/f5s1ph73\nCitation Context: The authors would like to thank the anonymous re-viewers for their helpful comments. Computations for this project were partly run on the Ohio Super-computer Center (1987) [URL_CITE] . This research was par-tially funded by the Defense Advanced Research Projects Agency award HR0011-15-2-0022.\nFootnote or Reference text (if exists): The Ohio Supercomputer Center. 1987. Ohio Supercomputer Center. \\\\url{http://osc.edu/ark:/19495/f5s1ph73}.\nSection Titles (if exists): ['Acknowledgments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.umn.edu~cluto\nCitation Context: of Computer Science, University of Minnesota. Available at [URL_CITE] http://www.cs.umn.edu\u02dcclutoS. Ploux and H. Ji.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ecdf.ed.ac.uk\nCitation Context: We would like to thank Phil Blunsom, Prachya Boonkwan, Christos Christodoulopoulos, Stephen Clark, Michael Collins, Chris Dyer, Timothy Fowler, Mark Granroth-Wilding, Philipp Koehn, Terry Koo, Tom Kwiatkowski, Andre\u0301 Martins, Matt Post, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts, and the anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); the EuroMatrixPlus project funded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility ( [URL_CITE] http://www.ecdf.ed.ac.uk). The ECDF is partially supported by the eDIKT initiative (http://www.edikt.org.uk).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com/\nCitation Context: We randomly select 100 candidates and compare the outputs of different systems. We use Ama-zon Turk [URL_CITE] for human evaluation. Each candidate is rated by three annotators and we report the av-erage scores here.\nFootnote or Reference text (if exists): 8 https://www.mturk.com/ tent preservation. Mean denotes the average of the met-\nSection Titles (if exists):  ['3 Experiments', '3.5 Human Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf\nCitation Context: Both include two translations exercises: for the 2005 campaign we contacted each participant individually and asked for permission to use their data 3 . In our experiments, we take the sum of ad-equacy and fluency, both in a 1-5 scale, as a global measure of quality (LDC, 2005) [URL_CITE] . Thus, human as-sessments are in a 2-10 scale.\nFootnote or Reference text (if exists): LDC. 2005. Linguistic Data Annotation Spec-ification: Assessment of Adequacy and Flu-ency in Translations. Revision 1.5. Tech-nical report, Linguistic Data Consortium. http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess04.pdf.\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.1 Data sets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Document\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.lsi.upc.edu/~nlp/Asiya\nCitation Context: As for evaluation measures, for MT we have used a rich set of 64 measures provided within the ASIYA Toolkit (Gime\u0301nez and Ma\u0300rquez, 2010) [URL_CITE] . This in-cludes measures operating at different linguistic lev-els: lexical, syntactic, and semantic.\nFootnote or Reference text (if exists): 5 http://www.lsi.upc.edu/\u02dcnlp/Asiya\nSection Titles (if exists):  ['4 Data Sets and Measures', '4.2 Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Frozenmad/AMN_SRL\nCitation Context: The model is trained for maximum 20 epochs for the nearly best model based on de-velopment set results. We re-run our model using different initialized parameters for 4 times and re-port the average performance [URL_CITE] .\nFootnote or Reference text (if exists): 4 Our implementation is publicly available at https://github.com/Frozenmad/AMN_SRL.\nSection Titles (if exists):  ['3 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nist.gov/speech/tests/mt\nCitation Context: We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005) [URL_CITE] . Both campaigns include two dif-ferent translations exercises: Arabic-to-English (\u2018AE\u2019) and Chinese-to-English (\u2018CE\u2019).\nFootnote or Reference text (if exists): 2 http://www.nist.gov/speech/tests/mt\nSection Titles (if exists):  ['3 Metrics and Test Beds', '3.2 Test Beds']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mturk.com\nCitation Context: Once post-edits had been created, DA was em-ployed in two separate runs on Amazon Mechani-cal Turk, [URL_CITE] once for GEN - REF and once for POST - EDIT . Besides employing distinct reference trans-lations in the assessment, all other set-up crite-ria were identical for both evaluation settings, in-cluding the conventional segment-level DA set-ting, where a minimum of 15 human assessments are combined into a mean DA score for a given translation, after strict quality control measures and score standardization have been applied.\nFootnote or Reference text (if exists): 11 https://www.mturk.com\nSection Titles (if exists):  ['3 Alternate Reference Bias Investigation', '3.1 Reference Bias Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://Skylion007.github.io/OpenWebTextCorpus\nCitation Context: Table 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText corpus (Gokaslan and Cohen, 2019) [URL_CITE] for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs over a set of random seeds.\nFootnote or Reference text (if exists): Aaron Gokaslan and Vanya Cohen. 2019. Openweb-text corpus. http://Skylion007.github.io/OpenWebTextCorpus.\nSection Titles (if exists):  ['3 Experiments', '3.2 Efficient Pseudo-Log-Likelihood Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nCitation Context: We utilize BLEU score (Papineni et al., 2002) to measure n-grams overlaps between ground-truth and generated re-sponse. Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to com-pute BLEU scores using smoothing techniques (smoothing 7) [URL_CITE] . For each testing context, we sam-ple 10 responses from the models and compute their BLEU scores, i.e., n-gram precision (BLEU-Precision), n-gram recall (BLEU-Recall), and n-gram F1 (BLEU-F1).\nFootnote or Reference text (if exists): 4 http://www.nltk.org/_modules/nltk/translate/bleu_score.html\nSection Titles (if exists):  ['4 Experiments', '4.4 Evaluation Metrics']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/sameenmaruf/selective-attn/tree/master/data\nCitation Context: (2019). [URL_CITE] Simi-lar to fine-tuning datasets, the pre-processing steps consist of word segmentation, tokenization, long document split. Then we segment the words into subwords using the BPE models trained on pre-training datasets.\nFootnote or Reference text (if exists): 10 https://github.com/sameenmaruf/selective-attn/tree/master/data\nSection Titles (if exists):  ['4 Experimentation', '4.1 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://gluebenchmark.com/\nCitation Context: Open-WebTextCorpus containes 38GB of text data from 8,013,769 documents. The pretrained model is then finetuned and evaluated on GLUE benchmark [URL_CITE] . GLUE contains a variety of tasks covering tex-tual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), senti-ment (SST), and linguistic acceptability (CoLA).\nFootnote or Reference text (if exists): 10 https://gluebenchmark.com/\nSection Titles (if exists):  ['B Additional Experimental Details', 'B.3 ELECTRA']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/neulab/cmu-multinlp\nCitation Context: Further analysis shows that dissimilar tasks exhibit divergent attention patterns, which explains why MTL is harmful on certain tasks. We have released our code and the General Language Analysis Datasets (GLAD) benchmark with 8 datasets covering 10 tasks in the BRAT format at [URL_CITE] https://github.com/neulab/cmu-multinlp , and provide a leaderboard to facilitate future work on generalized models for NLP.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/mlpc-ucsd/BERT_Convolutions\nCitation Context: Full pre-training and fine-tuning details are outlined in Appendix A.1. [URL_CITE] Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), gram-maticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set.\nFootnote or Reference text (if exists): 2 Code is available at https://github.com/mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).\nSection Titles (if exists):  ['3 Integrating lightweight convolutions', '3.2 Composite attention and lightweight convolution experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/JRC1995/Multilingual-BERT-Disaster\nCitation Context: Our code, dataset, and other resources are available on Github. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/JRC1995/ Multilingual-BERT-Disaster\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/songhaoyu/KvPI\nCitation Context: The code, data, and trained model are available at [URL_CITE] https://github.com/songhaoyu/KvPI.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Task I: Reranking Retrieved Responses', '5.5 Reproducibility']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UFAL-DSG/tgen\nCitation Context: The generator source code, along with config-uration files for experiments on the BAGEL data set, is available for download on Github. [URL_CITE]\nFootnote or Reference text (if exists): 11 https://github.com/UFAL-DSG/tgen\nSection Titles (if exists):  ['8 Conclusions and Further Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://infotabs.github.io/\nCitation Context: The dataset, along with associated scripts, are avail-able at [URL_CITE] https://infotabs.github.io/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://huggingface.co/datasets?search=kilt\nCitation Context: KILT data and code are available at https://github.com/facebookresearch/KILT. [URL_CITE]\nFootnote or Reference text (if exists): 1 and at https://huggingface.co/datasets? search=kilt\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ghpaetzold/questplusplus\nCitation Context: The source code for the framework, the datasets and extra resources can be downloaded from [URL_CITE] https://github.com/ghpaetzold/questplusplus.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Remarks']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/lemmonation/jm-nat\nCitation Context: As for evaluation, we use BLEU scores (Pap-ineni et al., 2002) as the evaluation metric, and 20.26 / /23.86 /28.86 29.72 21.61 23.94 \u2020 404 \u2020 ms 1.50\u00d725.48 29.32 30.19 25.94 30.42 \u2217 62 \u2217 ms 9.79\u00d729.90 32.53 33.23 27.03 31.71 \u2217 161 \u2217 ms 3.77\u00d730.53 33.08 33.31 27.05 31.51 32.97 33.21 31.27 45 ms 13.5\u00d7 27.69 106 ms 5.73\u00d732.24 33.52 33.72 32.59 report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized case-insensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq (Ott et al., 2019) and is avaliable at [URL_CITE] https://github.com/lemmonation/jm-nat.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['5 Experiments', '5.1.5 Training and Inference']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/andreasvc/discodop\nCitation Context: For the evaluation, we use the corresponding module of discodop. [URL_CITE] We report several metrics (as implemented in discodop):\nFootnote or Reference text (if exists): 4 http://github.com/andreasvc/discodop\nSection Titles (if exists):  ['3 Experiments', '3.2 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://pan.webis.de\nCitation Context: For the lexicalized experiments, we adopt the features from the best performing system at the lat-est PAN evaluation campaign [URL_CITE] (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).\nFootnote or Reference text (if exists): 3 http://pan.webis.de\nSection Titles (if exists):  ['3 Experiments', '3.1 Lexical vs Bleached Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.ark.cs.cmu.edu/MT/\nCitation Context: We also performed the paired bootstrap re-sampling test (Koehn, 2004). [URL_CITE] We sampled 2000 samples for each significance test.\nFootnote or Reference text (if exists): 3 We used the code available at http://www.ark.cs.cmu.edu/MT/.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/rtmdrr/testSignificanceNLP.git\nCitation Context: We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental re-sults, statistical significance testing is often ig-nored or misused. We conclude with a brief dis-cussion of open issues that should be properly ad-dressed so that this important tool can be applied in NLP research in a statistically sound manner [URL_CITE] .\nFootnote or Reference text (if exists): 1 The code for all statistical tests detailed in this pa-per is found on: https://github.com/rtmdrr/testSignificanceNLP.git\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nCitation Context: For evaluation we use labeled attachment score (LAS). Significance testing is performed using a randomization test, with the script from the CoNLL 2017 Shared Task. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/udapi/udapi-python/blob/master/udapi/block/eval/conll17.py\nSection Titles (if exists):  ['3 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/huggingface/pytorch-pretrained-BERT\nCitation Context: bert-base-uncased). [URL_CITE] We optimize us-ing ADAM (Kingma and Ba, 2015) with an initial learning rate of 5e-5 and a warm-up rate of 0.1. We regularize using Dropout (Srivastava et al., 2014) after the BERT encoder with a rate of 0.4.\nFootnote or Reference text (if exists): 2 We use the BERT implementation from https://github.com/huggingface/ pytorch-pretrained-BERT\nSection Titles (if exists):  ['4 Experiment', '4.1 Experimental setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/\nCitation Context: Our model is initialized using a pre-trained BERT model [URL_CITE] , and the other parameters are randomly initialized. During training, we first pre-train an LM over all of the raw text to acquire the entity-enhanced model parameters and then fine-tune the parameters using the NER task.\nFootnote or Reference text (if exists): 2 https://github.com/google-research/ bert, which is pre-trained on Chinese Wikipedia.\nSection Titles (if exists):  ['3 Method', '3.6 Training Procedure']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nCitation Context: For all our experiments we use the RAT-SQL of-ficial implementation and the pre-trained BERT-Large from Google. [URL_CITE] We follow the original set-tings to get the pre-fine-tuned/pre-adapted models. 8.76 18.51 24.00 16.66 21.96 23.16 \u00b1 0.5% For adaptation and fine-tuning, we decrease the learning rate of BERT parameters by 50 times to 6e-8 to avoid overfitting.\nFootnote or Reference text (if exists): 6 We use the BERT-Large, Uncased (Whole Word Mask-ing) model from https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\nSection Titles (if exists):  ['A.3 Implementation Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://bit.ly/2w7nUQK\nCitation Context: We use three BERT encoders to initialize our token node representation: 1) BERT-base: a BERT-base-uncased model finetuned on SQuAD 2.0; 2) BERT-large: a BERT-large-uncased model finetuned on SQuAD 2.0; 3) BERT-syn: Google\u2019s BERT-large-uncased model pre-trained on SQuAD2.0 with N-Gram Masking and Syn-thetic Self-Training. [URL_CITE] Since the Natural Question dataset does not provide sentence-level informa-tion, we additionally use spacy (Honnibal and Mon-tani, 2017) as the sentence segmentor to get the boundaries of sentences.\nFootnote or Reference text (if exists): 2 This model can be downloaded at https://bit.ly/2w7nUQK.\nSection Titles (if exists):  ['4 Experiments', '4.2 Experimental Settings']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/microsoft/MT-DNN\nCitation Context: We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b) [URL_CITE] . Training details are presented in Appendix B.2.\nFootnote or Reference text (if exists): 6 https://github.com/microsoft/MT-DNN\nSection Titles (if exists):  ['4 Experiments', '4.2 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/google-research/bert\nCitation Context: We also calculate the answer recall after document prun-ing, which indicates the performance upper bound. Model settings We initialize our model us-ing two publicly available uncased versions of BERT [URL_CITE] : BERT BASE and BERT LARGE , and refer readers to Devlin et al. (2018) for details on model sizes.\nFootnote or Reference text (if exists): 5 https://github.com/google-research/bert\nSection Titles (if exists):  ['4 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://catalog.ldc.upenn.edu/LDC2006T06\nCitation Context: We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [URL_CITE] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community.\nFootnote or Reference text (if exists): 1 https://catalog.ldc.upenn.edu/LDC2006T06\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://homes.cs.washington.edu/~eunsol/project_page/acl16\nCitation Context: We evaluate the approach on a newly gathered corpus with dense document-level sentiment la-bels in news articles. [URL_CITE] This data includes compre-hensively annotated sentiment between all entity pairs, including those that do not appear together in any single sentence. Experiments demon-strate that the global model significantly improves performance over a pairwise classifier and other strong baselines.\nFootnote or Reference text (if exists): 1 All data will be made publicly available. You can browse it at http://homes.cs.washington.edu/\u02dceunsol/project_page/acl16, and download it from the author\u2019s webpage.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://corpus.korean.go.kr\nCitation Context: We construct an experimental dataset using the cor-pus that we produced in-house and the public Ko-rean dialogue corpus [URL_CITE] . We split these corpora into three, and each is for training, validation, and test.\nFootnote or Reference text (if exists): 1 https://corpus.korean.go.kr Meeting of the Special Interest Group on Discourse and Dialogue, pages 285\u2013294.\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/UKPLab/emnlp2017-cmapsum-corpus\nCitation Context: We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization. [URL_CITE]\nFootnote or Reference text (if exists): 1 Available at https://github.com/UKPLab/ emnlp2017-cmapsum-corpus\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://ai.stanford.edu/\nCitation Context: We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outper-forms competitive baselines and other neural language models. [URL_CITE]\nFootnote or Reference text (if exists): 1 The dataset and word vectors can be downloaded at http://ai.stanford.edu/ \u223c ehhuang/.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: [URL_CITE] https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: In this demonstration (demo site: [URL_CITE] https://ibm.biz/acl21-loa), we present a Log-ical Optimal Actions (LOA) architecture for neuro-symbolic RL applications with LNN (Riegel et al., 2020) for text-based interaction games. While natural language-based interactive agents are the ambitious but attractive target as real-world applications of neuro-symbolic, it is not easy to provide an environment for the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: We propose a novel demonstration (URL: [URL_CITE] https://ibm.biz/acl21-loa) which provides to play the text-based games on the web interface and visu-alize the benefit of the neuro-symbolic algorithm. This application helps the human user understand the trained network and the reason for taken action by the agent.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Media\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ibm/loa\nCitation Context: We also extend more complicated LNN for other difficult games on the demo site. At the same time, we open the source code for the demonstration (URL: [URL_CITE] https://github.com/ibm/loa).\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['4 Conclusion']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/yftah89/Neural-SCLDomain-Adaptation\nCitation Context: \u2022 AE-SCL and AE-SCL-SR: We use the code from the author\u2019s github: [URL_CITE] https://github.com/yftah89/Neural-SCLDomain-Adaptation.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ibm.biz/acl21-loa\nCitation Context: Our LOA also provides open-sourced implementation in Python for the reinforce-ment learning environment to facilitate an ex-periment for studying neuro-symbolic agents. Demo site: [URL_CITE] https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nCitation Context: We use translation benchmarks IWSLT14 En\u2013De, En\u2013Fr, En\u2013It, and IWSLT15 En\u2013Vi in our experi-ments. The datasets of IWSLT14 are pre-processed with the script in Fairseq [URL_CITE] . For IWSLT14 datasets, we use tst2011 as validation set and tst2012 as test set.\nFootnote or Reference text (if exists): 3 https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh\nSection Titles (if exists):  ['6 Experiments', '6.1 Neural Machine Translation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/translation-task.html\nCitation Context: Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks(Cettolo et al., 2014) and WMT 2016 [URL_CITE] cor-pora. The statistics of the datasets can be found in Table 1.\nFootnote or Reference text (if exists): 3 http://www.statmt.org/wmt16/translation-task.html\nSection Titles (if exists):  ['5 Experiments', '5.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt16/\nCitation Context: We have evaluated these systems over three publicly-available datasets from the 2016 ACL Conference on Machine Translation (WMT16) [URL_CITE] and the 2016 International Workshop on Spoken Language Translation (IWSLT16) . Table 1 lists the datasets and their main features.\nFootnote or Reference text (if exists): 4 WMT16: http://www.statmt.org/wmt16/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://statmt.org/wmt16\nCitation Context: We use TED tst2012 as development set and TED tst2013 as the test set. For a large-scale dataset, we select an English-German corpus from the WMT16 Eval-uate Campaign [URL_CITE] , which contains 4.5M sentence pairs. Newstest 2013 is used as the development set and Newstest 2015 is used as the test set.\nFootnote or Reference text (if exists): 5 http://statmt.org/wmt16\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.phontron.com/kftt\nCitation Context: Datasets. We evaluate TrDec on three datasets: 1) the KFTT (ja-en) dataset (Neubig, 2011) [URL_CITE] , which consists of Japanese-English Wikipedia arti-cles; 2) the IWSLT2016 German-English (de-en) dataset (Cettolo et al., 2016), which consists of TED Talks transcriptions; and 3) the LORELEI Oromo-English (or-en) dataset , which largely con-sists of texts from the Bible. Details are in Tab.\nFootnote or Reference text (if exists): Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.\nSection Titles (if exists):  ['5 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://wit3.fbk.eu/\nCitation Context: Datasets We evaluate our method on the IWSLT datasets which contain multiple languages from TED talks. We collect datasets from the IWSLT evaluation campaign [URL_CITE] from years 2011 to 2018, which consist of the translation pairs of 23 languages\u2194English. The detailed description of the training/validation/test set of the 23 transla-tion pairs can be found in Supplementary Mate-rials (Section 1).\nFootnote or Reference text (if exists): 6 https://wit3.fbk.eu/\nSection Titles (if exists):  ['4 Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010 [URL_CITE] ; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Awais Athar. 2010. Dependensee: A dependency parse visualization tool. http://chaoticity.com/dependensee-a-dependency-parse-visualisation-tool. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://nlpviz.bpodgursky.com\nCitation Context: Figure 1 was generated from (Podgursky, 2015) [URL_CITE] , which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom).\nFootnote or Reference text (if exists): Ben Podgursky. 2015. Nlpviz. http://nlpviz.bpodgursky.com. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop). Dis-placy (Montani, 2016) [URL_CITE] provides a general API for the static rendering of various dependency parsing schemes, e.g. Universal Dependencies (Nivre et al., 2016) and Stanford Dependencies (De Marneffe and Manning, 2008).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://spacy.io/\nCitation Context: 4. Parse tree depth: dependency parse tree depth using spaCy (Honnibal and Montani, 2017) [URL_CITE] .\nFootnote or Reference text (if exists): Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed-dings, convolutional neural networks and incremen-tal parsing. https://spacy.io/.\nSection Titles (if exists):  ['-', '8.1 Definitions of Complexity Measures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://mallet.cs.umass.edu/\nCitation Context: We use Berkeley Parser and Stanford Parser to create the constituent and dependency parse trees. Besides, the ME tool ( Maxent ) [URL_CITE] is employed to train individual component classifiers and lp_solver is used to construct our global argument inference model.\nFootnote or Reference text (if exists): 4 http://mallet.cs.umass.edu/\nSection Titles (if exists):  ['5 Experimentation', '5.1 Experimental Settings and Baseline']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://demos.explosion.ai/\nCitation Context: Figure 1 was generated from (Podgursky, 2015), which provides open-source code and a web interface for a static rendering of a con-stituency parse. There are a number of libraries (Stenetorp et al., 2012; Montani, 2016 [URL_CITE] ; Athar, 2010; Yimam et al., 2013) that provide static ren-derings of dependency parses similar to Figure 1 (bottom). Among these, Brat (Stenetorp et al., 2012) provides some interactive elements (like mouseover highlighting of subtrees, and the abil-ity to add dependencies via drag-and-drop).\nFootnote or Reference text (if exists): Ines Montani. 2016. Displacy dependency vi-sualizer. https://demos.explosion.ai/displacy. Accessed: 2017-04-27.\nSection Titles (if exists):  ['2 Related Work']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.reddit.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack for work com-munication and Reddit [URL_CITE] for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 2 https://www.reddit.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 2 https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/pkb\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Anna146/CHARM\nCitation Context: The salient contributions of this paper are: (1) a method for inferring both seen and previously un-seen (zero-shot) attribute values from a user\u2019s con-versational utterances; (2) a comprehensive evalua-tion for the profession and hobby attributes over a large dataset of Reddit discussions; and (3) labeled data and code as resources for later research. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/Anna146/CHARM\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack [URL_CITE] and Whatsapp , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 1 https://slack.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://www.whatsapp.com/\nCitation Context: With the continuing growth of Internet and social media, online group chat channels, e.g., Slack and Whatsapp [URL_CITE] , among many others, have become in-creasingly popular and played a significant social and economic role. Along with the convenience of instant communication brought by these applica-tions, the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the exam-ple shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of mes-sages in the chat channel.\nFootnote or Reference text (if exists): 2 https://www.whatsapp.com/\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://slack.com\nCitation Context: Online conversations are increasingly significant for communication, e.g., Slack [URL_CITE] for work com-munication and Reddit for general discussion. To organize overwhelming information from these conversations, researchers have been working on summarizing online conversations (Bhatia et al., 2014; Carenini et al., 2007; Mehdad et al., 2013, 2014; Oya et al., 2014).\nFootnote or Reference text (if exists): 1 https://slack.com\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TalLinzen/rnn_agreement\nCitation Context: (2016). [URL_CITE] Word types beyond the most frequent 10,000 are converted to their respective POS tags. We summarize the corpus statistics of the dataset, along with the test set distribution of the num-ber of attractors, in Table 1.\nFootnote or Reference text (if exists): 1 The dataset and scripts are obtained from https://github.com/TalLinzen/rnn_agreement.\nSection Titles (if exists):  ['2 Number Agreement with LSTM Language Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.aclweb.org/anthology/N/N09/N09-2047\nCitation Context: Since POS tagging, supertagging, and parsing decisions are made independently for each word in a sentence, our system can parallelize computation once the sentence is encoded in the BiLSTM layers. Our current implementation pro-cesses 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009) [URL_CITE] . 5\nFootnote or Reference text (if exists): Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen Rambow, and Beno\u0131\u0302t Sagot. 2009. MICA: A probabilistic dependency parser based on tree insertion grammars (application note). In NAACL-HLT (short). Association for Computational Linguistics, Boulder, Colorado, pages 185\u2013188. http://www.aclweb.org/anthology/N/N09/N09-2047.\nSection Titles (if exists):  ['3 Results and Discussion', '3.3 Joint Models']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Paper\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter [URL_CITE] (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 4 https://code.google.com/archive/p/ark-tweet-nlp/downloads(twpos-data-v0.3.tgz)\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/berlino/tensor2struct-public\nCitation Context: \u2022 We perform experiments on two text-to-semantic compositional datasets: COGS and SCAN. Our new training objectives lead to significant improvements in accuracy over a baseline parser trained with conventional su-pervised learning. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementations are available at https://github.com/berlino/tensor2struct-public.\nSection Titles (if exists):  ['\u2217 Equal contribution.']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Oneplus/Tweebank\nCitation Context: For POS tagging, we use three datasets Ritter11- T-POS (Ritter et al., 2011), ARK-Twitter (Gim-pel et al., 2011; Owoputi et al., 2013) and T WEEBANK - V 2 [URL_CITE] (Liu et al., 2018). For NER, we employ datasets from the WNUT16 NER shared task (Strauss et al., 2016) and the WNUT17 shared task on novel and emerging entity recognition (Derczynski et al., 2017). For text classification, we employ the 3-class sentiment analysis dataset from the SemEval2017 Task 4A (Rosenthal et al., 2017) and the 2-class irony detection dataset from the SemEval2018 Task 3A (Van Hee et al., 2018).\nFootnote or Reference text (if exists): 5 https://github.com/Oneplus/Tweebank\nSection Titles (if exists):  ['3 Experimental setup', 'Downstream task datasets']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Extend"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.jhu.edu/~ozaidan/AOC\nCitation Context: We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. [URL_CITE] This corpus is highly dialectal\nFootnote or Reference text (if exists): 1 Available from http://www.cs.jhu.edu/~ozaidan/AOC\nSection Titles (if exists):  ['3.2. Experiments and Results']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.infoplease.com/people.html\nCitation Context: We download from infoplease.com [URL_CITE] and biogra-phy.com two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 2 http://www.infoplease.com/people.html\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nCitation Context: We conduct experiments on the ACE2005 corpus. [URL_CITE] It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al.\nFootnote or Reference text (if exists): 6 http://www.itl.nist.gov/iad/mig/tests/ace/2005/\nSection Titles (if exists):  ['4 Experiments']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://goo.gl/language/wiki-atomic-edits\nCitation Context: Our full corpus is available for download at [URL_CITE] http: //goo.gl/language/wiki-atomic-edits . The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['Description of Data Release']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://news.bbc.co.uk/\nCitation Context: We downloaded 3,361 news articles from the BBC News website. [URL_CITE] Each article was accompa-nied with an image and its caption. We thus created a database of image-caption-document tuples.\nFootnote or Reference text (if exists): 2 http://news.bbc.co.uk/\nSection Titles (if exists):  ['3 BBC News Database']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Supplement\nTYPE: Website\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.statmt.org/wmt19/translation-task.html\nCitation Context: To prove the univer-sality of our approach, we further experiment on different data volumes, which are sampled from WMT19 En-De. [URL_CITE] The Small and Medium corpora respectively consist of 1.0M and 4.5M sentence pairs, and Large one is the whole dataset which contains 36M sentence pairs. We preprocess all data via BPE (Sennrich et al., 2016) with 32K merge operations.\nFootnote or Reference text (if exists): 4 http://www.statmt.org/wmt19/translation-task.html\nSection Titles (if exists):  ['3 Experiment', '3.1 Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: DataSource\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.biography.com/search/index.jsp\nCitation Context: We download from infoplease.com and biogra-phy.com [URL_CITE] two corpora of people\u2019s biographies, which include 24,975 and 24,345 bios respectively. We scan each whole corpus and extract people having spouse information.\nFootnote or Reference text (if exists): 3 http://www.biography.com/search/index.jsp\nSection Titles (if exists):  ['4 Experiments and Results', '4.1 Experimental Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Dataset\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Adaxry/ss_on_decoding_steps\nCitation Context: The main contributions of this paper can be sum-marized as follows [URL_CITE] :\nFootnote or Reference text (if exists): 3 Codes are available at https://github.com/Adaxry/ss_on_decoding_steps.\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/shacharosn/CRE\nCitation Context: We release the challenge set to encourage future research on better models. [URL_CITE]\nFootnote or Reference text (if exists): 1 GitHub repository with data and code: https://github.com/shacharosn/CRE\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.biu.ac.il/~nlp/downloads/\nCitation Context: To our knowledge, this is the first such in-depth study. [URL_CITE]\nFootnote or Reference text (if exists): 4 The guidelines and the dataset are available at http://www.cs.biu.ac.il/\u02dcnlp/downloads/\nSection Titles (if exists):  ['3 Motivation and Goals']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/hiaoxui/D2T-Grounding\nCitation Context: Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/ChenWu98/Point-Then-Operate\nCitation Context: Experimental re-sults on two text style transfer datasets show that our method significantly outperforms re-cent methods and effectively addresses the aforementioned challenges. [URL_CITE]\nFootnote or Reference text (if exists): 1 Our code is available at https://github.com/ChenWu98/Point-Then-Operate.\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nCitation Context: To the best of our knowledge, this is the first work in attempting to generate simi-les. By framing the task as a style-transfer problem we make three contributions: [URL_CITE]\nFootnote or Reference text (if exists): 4 Code & Data at https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Mixed\nFUNCTION: Introduce"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/OpenKE\nCitation Context: Our experiments are run on the server with the following configurations: OS of Ubuntu 16.04.6 LTS, CPU of Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz, and GPU of GeForce RTX 2080 Ti. We use OpenKE [URL_CITE] for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters.\nFootnote or Reference text (if exists): 5 https://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://openke.thunlp.org/\nCitation Context: Besides the toolkit, we also provide the pre-trained embeddings of several well-known large-scale KGs, which can be used directly for other relevant works without repeatedly spending much time for embedding KGs. In this paper, we mainly present the architecture design and implementa-tion of OpenKE, as well as the benchmark eval-uation results of some typical KE models im-plemented with OpenKE. Other related resources and details can be found on [URL_CITE] http://openke.thunlp.org/.\nFootnote or Reference text (if exists): nan\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Mixed\nTYPE: Mixed\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/thunlp/OpenKE\nCitation Context: In this section, we mainly present the implemen-tations of acceleration modules and special sam-pling algorithm in OpenKE. OpenKE has been available to the public on GitHub [URL_CITE] and is open-source under the MIT license.\nFootnote or Reference text (if exists): 1 http://github.com/thunlp/OpenKE\nSection Titles (if exists):  ['4 Implementations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/KB2E\nCitation Context: In these tables, the difference between our implementations and the paper reported results are listed in the parentheses. To demonstrate the efficiency of OpenKE, we select TransE as a rep-resentative and implement it with both OpenKE and KB2E [URL_CITE] , and then compare their training time. KB2E is a widely-used toolkit for KE models on GitHub.\nFootnote or Reference text (if exists): 4 https://github.com/thunlp/KB2E\nSection Titles (if exists):  ['5 Evaluations']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/TimDettmers/ConvE\nCitation Context: We use OpenKE for re-implementing TransE, Com-plEx, and RotatE. For the rest models, we use the original codes for ConvE [URL_CITE] , TuckER 7 , Multi-types of KGC models including embedding-based, multi-hop reasoning (reinforcement learning), and rule-based models, these models largely have their own hyperparameters. To avoid exhaustive param-eter search in a large range, we conduct a series of preliminary experiments and find that the sug-gested parameters work well on Wikidata-based data.\nFootnote or Reference text (if exists): 6 https://github.com/TimDettmers/ConvE\nSection Titles (if exists):  ['F Experiment Setup']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Produce"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/thunlp/Fast-TransX\nCitation Context: In fact, we often rely on simple personal computers for model validation. Hence, we enable OpenKE to adapt models for parallel learning on CPU [URL_CITE] be-sides employing GPU learning, which allow users to make full use of all available computing re-sources. The parallel learning method is shown in Algorithm 1.\nFootnote or Reference text (if exists): 2 https://github.com/thunlp/Fast-TransX\nSection Titles (if exists):  ['4 Implementations', '4.2 Parallel Learning']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://zapier.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/Leechikara/Dialogue-Based-Anti-Fraud\nCitation Context: Furthermore, our learned dialogue strategies are interpretable and flexi-ble, which can help promote real-world appli-cations. [URL_CITE]\nFootnote or Reference text (if exists): 1 https://github.com/Leechikara/ Dialogue-Based-Anti-Fraud\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/jind11/TextFooler\nCitation Context: We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler [URL_CITE] and BERTAt-tack are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set.\nFootnote or Reference text (if exists): 11 https://github.com/jind11/TextFooler\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/LinyangLee/BERT-Attack\nCitation Context: We use RoBERTa distill as our main backbone for fast inference purpose. TextFooler and BERTAt-tack [URL_CITE] are built with their open source implemen-tation provided by the authors. In the implementa-tion of TextFooler+LM, we use small sized GPT-2 language model (Radford et al., 2019) to further select those candidate tokens that have top 20% perplexity in the candidate token set.\nFootnote or Reference text (if exists): 12 https://github.com/LinyangLee/ BERT-Attack\nSection Titles (if exists):  ['A Appendix', 'A.1 Additional Experiment Details']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://github.com/chaitanyamalaviya/lang-reps\nCitation Context: Experiments show that the pro-posed method is able to infer not only syn-tactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phy-logenetic neighbors. [URL_CITE]\nFootnote or Reference text (if exists): 1 Code and learned vectors are available at http://github.com/chaitanyamalaviya/lang-reps\nSection Titles (if exists):  ['References']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Compare"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://ifttt.com/\nCitation Context: If the condition is satisfied, the required processing con-sists of calling the execution of actions grounded in the effectors. Giving a conversational assistant the capacity to learn rules verbally opens the pos-sibility of teaching more complex and personal-ized rules, especially compared to visual program-ming tools such as IFTTT and Zapier [URL_CITE] . LIA can ask questions if it cannot parse specifics parts of a user-statement (e.g., if it cannot understand the if-condition, see Figure 2 for an example).\nFootnote or Reference text (if exists): 1 https://ifttt.com/, https://zapier.com/\nSection Titles (if exists):  ['2 Core competencies', '2.1 Learning Procedures']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Compare"
                }
            ]
        }
    ],
    [
        {
            "generated_text": [
                {
                    "role": "System",
                    "content": "Your task is to classify the role that resources play in the research activities (ROLE), the type of artifact (TYPE) referred to the URL and the citation reason (FUNCTION). I will provide you with a URL, citation context and section titles.\n\n\nHere is the classification schema for the artifact role:\n1. Method: corresponding to the artifact type \"Tool\" and \"Code\"\n2. Material: corresponding to the artifact type \"Dataset\", \"Knowledge\" and \"DataSource\"\n3. Supplement: corresponding to the artifact type \"Document\", \"Paper\", \"Media\" and \"Webcite\"\n4. Mixed: corresponding to the artifact type \"Mixed\"\n\nHere is the classification schema for the artifact type:\n1. Tool: toolkit, software, system\n2. Code: codebase, library, API\n3. Dataset: corpus, image, sets\n4. Knowledge: lexicon, knowledge graph\n5. DataSource: source data for the Dataset/Knowledge\n6. Document: specifications, guidelines\n7. Paper: scholarly papers\n8. Media: games, music, videos\n9. Website: services, homepages\n10. Mixed: citations referring to multiple resources\n\nHere is the classification schema for the citation reason:\n1. Use: Used in the citing paper\u2019s research\n2. Produce: First produced or released by the citing paper\u2019s research\n3. Compare: Compared with other resources\n4. Extend: Used in the citing paper\u2019s research but are improved, upgraded, or changed during the research\n5. Introduce: Introduced the resources or the related information\n6. Other: The URL citation does not belong to the above categories"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.isi.edu/~cyl/ROUGE\nCitation Context: For evaluation, we used the new automatic sum-mary evaluation metric, ROUGE [URL_CITE] , which was used for the first time in DUC 2004. ROUGE is a recall-based metric for fixed-length summaries which is based on n-gram co-occurence.\nFootnote or Reference text (if exists): 1 http://www.isi.edu/\u02dccyl/ROUGE\nSection Titles (if exists):  ['4 Experiments on DUC 2004 data', '4.1 DUC 2004 data and ROUGE']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Code\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/EducationalTestingService/skll\nCitation Context: Next, we describe our implementations of the response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Scho\u0308lkopf, 2004), with the complexity parame-ter tuned by cross-validation on the training data. [URL_CITE]\nFootnote or Reference text (if exists): 2 We used the implementation of SVR in scikit-learn (Pe-dregosa et al., 2011) via SKLL (https://github.com/EducationalTestingService/skll) version 0.27.0. Other than the complexity parameter, we used the defaults.\nSection Titles (if exists):  ['3 Models for Short Answer Scoring']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://svmlight.joachims.org/svmstruct.html\nCitation Context: Our model is based on Tsochantaridis et al.\u2019s (2005) framework for learning Support Vector Machines (SVMs) with structured output spaces, us-ing the SVM struct implementation. [URL_CITE] We briefly sum-marise the approach below; for a more detailed de-scription we refer the interested reader to Tsochan-taridis et al. (2005).\nFootnote or Reference text (if exists): 1 http://svmlight.joachims.org/svmstruct.html\nSection Titles (if exists):  ['3 Problem Formulation', '3.3 The Large Margin Model']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://barbar.cs.lth.se:8081\nCitation Context: For this reason, the output of SRL systems has been com-monly used for feature-based FGOA models (Kim and Hovy, 2006; Johansson and Moschitti, 2013; Choi et al., 2006; Yang and Cardie, 2013). Ad-ditionally, a considerable amount of training data is available for training SRL models (Table [URL_CITE] in Sec. 3), which made neural SRL models success-ful (Zhou and Xu, 2015; Yang and Mitchell, 2017).\nFootnote or Reference text (if exists): 2 http://barbar.cs.lth.se:8081\nSection Titles (if exists):  ['1 Introduction']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Material\nTYPE: Knowledge\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  https://github.com/glampouras/JLOLS_NLG\nCitation Context: \u2022 LOLS : [URL_CITE] The system by Lampouras and Vlachos (2016) learns sentence planning and surface reali-sation using Locally Optimal Learning to Search ( LOLS ), an imitation learning framework which learns using BLEU and ROUGE as non-decomposable loss functions.\nFootnote or Reference text (if exists): 4 https://github.com/glampouras/JLOLS_ NLG\nSection Titles (if exists):  ['2 End-to-End NLG Systems']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                },
                {
                    "role": "user",
                    "content": "Please classify the artifact type and the citation reason for the following URL and citation sentence.\nURL:  http://www.cs.cornell.edu/~rs/sfour/\nCitation Context: \u2022 SFOUR: SFOUR is a structured prediction approach that trains an end-to-end system with a large-margin method to optimize a convex relaxation of ROUGE (Sipos et al., 2012). We use the publicly available imple-mentation. [URL_CITE]\nFootnote or Reference text (if exists): 6 http://www.cs.cornell.edu/\u02dcrs/sfour/\nSection Titles (if exists):  ['4 Evaluation', '4.3 End-to-End Evaluation']"
                },
                {
                    "role": "assistant",
                    "content": "ROLE: Method\nTYPE: Tool\nFUNCTION: Use"
                }
            ]
        }
    ]
]